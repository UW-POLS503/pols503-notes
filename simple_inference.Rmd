# Simple Inference

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Means

### Confidence Interval

A (1 - $\alpha$)% confidence interval of the population mean using the sample mean, $\bar{x} = \frac{1}{n} \sum x_i$ as the estimator is, 
$$
CI(\mu) = \bar{x} \pm t^*_{df,1 - \alpha / 2} \frac{s}{\sqrt{n}}
$$
where $s$ is the sample standard deviation, $t^*_$ is the quantile of the Student's $t$ distribution with $n - 1$ degrees of freedom such that $P(X \leq t^*) = 1 - \alpha / 2$.

Assumptions:

- Normal population distribution or large $n$. If the population distribution is normal, the sampling distribution of $\bar{x}$ is also normal, and the confidence interval will have exact coverage. However, due to the CLT this does not require that the population distribution be normal, and the coverage will be exact as $n \to \infty$. The worst cases are skewed population distribuiton. Rules of thumb include $n \geq 30$ as long as the population distribution is not too skewed.
- $x_i$ are i.i.d. If the $x_i$ are not 

Other notes

- As $n \to \infty$, the critical value $t^*_{df,1-\alpha/2}$ approaches the value of 
- For a 95% confidence interval $t^*_{n-1,0.975} \geq z^*_{0.975} \approx 1.96$, so for informal applications you can take $t^*_{n-1,0.975} \approx 2$.

### Test

The test statistic for a hypothesis test of the population mean using the sample mean as an estimator where,
$$
\begin{aligned}[t]
H_0: \mu = \mu_0 \\
H_a: \mu \neq \mu_0
\end{aligned}
$$

$$
t^* = \frac{\bar{x} - \mu_0}{SE_{\bar{x}}
$$
where $SE_{\bar{x}} = s / \sqrt{n}$ is the standard error of the sample mean, $s$ is the sample standard deviation, and $n$ is the number of observations. The test-statistic is distributed Student's-$t$ with $n - 1$ degrees of freedom.

The test statistic $t^*$ is distributed Student's-$t$ with $n - 1$ degrees of freedom.

We can also test one-sided hypothesis, such as whether the mean is greater than some hypothesized value,
$$
\begin{aligned}[t]
H_0:& \mu = \mu_0, \\
H_a:& \mu > \mu_0,  \\
\end{aligned}
$$
or less than some hypothesized value,
$$
\begin{aligned}[t]
H_0:& \mu = \mu_0 , \\
H_a:& \mu < \mu_0 . \\
\end{aligned} 
$$
Note that in these cases the null hypothesis is surpisingly an equality statement. 
This is because in a hypothesis test the sampling distribution of the test statistic assumes that the null hypothesis is true.
We can calculate the sampling distribution of the test statistic when $\mu = \mu_0$, but we couldn't calculate it for $\mu \leq 0$, because that is not a single value.
However, you can think of the implicit null hypotheses as being the $H_0: \mu \leq \mu_0$ and $H_a: \mu \geq \mu_a$. 

Notes:

- If $p_2$ is the $p$-value of a two-sided test, then $p_{1}$, the p-value of an equivalent one-sided test is $p_1 = 2 \times p_2$.
- Even though many hypotheses in applied research have a direction associated with them, they are almost always tested with two-sided tests rather than the more-appropriate one-sided test. This is a simply convention, and as seen before the p-value of a two-sided test is simply a multiple of the one-sided test. The reasons for this convention are rather stupid, as the two-sided tests are seen to be more "conservative" since the p-value is one-half that of the equivalent one-sided test. Why researchers don't just account for that and instead run the "wrong test" is unknown.
- Rule of thumb is $t \leq 2$ means $p \leq 0.05$ for a two sided hypothesis test. And in the common case of $H_0: \mu = 0$ (regression coefficients, difference in means), $t = \frac{\bar{x}}{SE_{\bar{x}}}$.


## Difference of Means

Suppose there are two independent populations with sizes $n_1$ and $n_2$, means of $\mu_1$ and $\mu_2$, and standard deviations of $\sigma_1$ and $\sigma_2$.
The standard error for the sampling distribution is approximately,
$$
SE_{\bar{x}_2 - \bar{x}_1} = \sqrt{\frac{\sigma^2_1}{n_1} + \frac{\sigma^2_2}{n_2}}
$$

The standard error of the difference of means follows from the result that for two independent random variables, $X$ and $Y$, 
$$
\Var(a + bX + cY) = a^2 \Var(X) + b^2 \Var(Y) .
$$
The variance of the difference of two random variables follows,
$$
\Var(X - Y) = \Var(1 \cdot X + (-1) \cdot Y) = \Var(X) + \Var(Y)./
$$
Note that this results **requires** the random variables $X$ and $Y$ to be **independent**. If they are not, then,
$$
\Var(a + bX + cY) = a^2 \Var(X) + b^2 \Var(Y) - ab \Cov(X, Y)
$$
This means that if the samples are not independent then the standard error is incorrect by a factor related to the covariance of the two samples: when they are positively correlated, it is an underestimate, and when they are negatively correlated, it is an overestimate. *Prove it to yourself*

As with the single sample population mean test, the confidence interval for a difference in means is
$$
\text{point estimate} \pm \text{critical value} \times \text{SE}.
$$
Specifically, the confidence interval for a (1 - $\alpha$) $\times$ 100% confidence interval of the population mean (using the sample mean as an estimate) is,
$$
(\bar{x}_2 - \bar{x}_1) \pm t^*_{df,1 - \alpha/2} \times SE_{\bar{x}_2 - \bar{x}_1}
$$
where $t^*_{df,1-\alpha/2}$ is the quantile of the Student's-$t$ distribution where $P(X \leq t^*) = 1 - \alpha/2$.
The value of $df$ is a function of the sample sizes of both $n_1$ and $n_2$.
The exact df of $df$ is not important since software will calculate it for you, but the degrees of freedom is always larger than the degrees of freedom that you would use for a one sample test of the smaller sample, $df \geq \min(n_1, n_2) - 1$.

The hypothesis test for 
$$
\begin{aligned}[t]
H_0: & \mu_2 - \mu_1 = \delta \\
H_0: & \mu_2 - \mu_1 \neq \delta \\
\end{aligned}
$$
uses the test statistic
$$
T = \frac{(\bar{x}_2 - \bar{x}_1 - \delta)}{SE_{\bar{x}_2 - \bar{x}_1}},
$$
which is distributed Student's $t$ with the a degrees of freedom the same as in the confidence interval ($df \geq \min(n_1, n_2) - 1$).

## Paired Difference of Means

For paired differences in means, use one sample confidence intervals and hypothesis tests for with the sample mean of the differences, $\frac{1}{n} \sum_i x_{i,1} - x_{i,2}$, as the estimator of the difference of the population means.

Notes:

- Discuss how paired differences in means are "controlling" for confounders
- Show differences in efficiency of paired differences in means



## Proportions

### One-Sample

#### Confidence interval

The standard error of the population using the sample proportion as an estimator is,
$$
SE_{\hat{p}} = \sqrt{\frac{p (1 - p)}{n}} \approx \sqrt{\frac{\hat{p} (1 - \hat{p})}{n}} .
$$
The confidence interval is,
$$
CI(p, \alpha) = \hat{p} \pm z^*_{1 - \alpha/2} \cdot SE_{\hat{p}} .
$$



#### Hypothesis Testing

For the hypothesis test of the difference in population proportions,
$$
\begin{aligned}[t]
H_0: & p = p_0 , \\
H_a: & p \neq p_0 ,
\end{aligned}
$$
a test statistic is,
$$
\begin{aligned}[t]
z = \frac{\hat{p} - p_0}{SE_{\hat{p}}}
\end{aligned} 
$$
which is approximately distributed standard normal.

Notes:

The sampling distribution of the test statistic aproaches the normal distribution as $n \to infty$ by the CLT. This takes longer as $p$ is closer to the extremes of either 0 or 1.

A simple fix to the confidence intervals which improves their coverage in small samples is to use a proportion that adds two successes and two failures,
$$
\tilde{p} = \frac{x + 2}{n + 4},
$$
for the standard error,
$$
SE_{\tilde{p}} = \sqrt{\frac{\tilde{p} (1 - \tilde{p})}{n}} .
$$

Alternatively, the quantiles of the binomial distribution with proportion $\hat{p}$ and size $n$.



### Two-Sample (Difference of Proportions)

#### Standard Error

The standard error of the difference in sample proportions, $\hat{p}_2 - \hat{p}_2$ is
$$
SE_{\hat{p}_2 - \hat{p}_1} = \sqrt{\frac{p_1 (1 - p_1)}{n_1} + \frac{p_2 (1 - p_2)}{n_2}} \approx \sqrt{\frac{\hat{p}_1 (1 - \hat{p}_1)}{n_1} + \frac{\hat{p}_2 (1 - \hat{p}_2)}{n_2}} .
$$
Note that $SE_{\hat{p}_2 - \hat{p}_2}$ depends on the unknown population proportions, $p_2$ and $p_1$, and we'll have to plug in values based on sample statistics.
Also, with proportions, the standard error varies with the proportions themselves.
This is unlike the sampling distributions of the sample mean and difference of sample means, in which the sampling distribution of the test statistic depended on the unknown population variance(s), but not the mean(s).

#### Confidence Intervals

The (1 - $\alpha$)$\times$ 100% confidence interval of the difference in proportions using the difference in sample proportions as the estimator is,
$$
CI(p_2 - p_1) = (\hat{p}_2 - \hat{p}_1) \pm z^*_{1 - \alpha / 2} \text{SE}_{\hat{p}_2 - \hat{p}_1}
$$
where $z^*_{1 - \alpha / 2}$ is the quantile, $z$, in the standard normal distribution such that $P(X \leq z) = 1 - \alpha / 2$.

Notes:

The sampling distribution follows the CLT, but the appropriateness of the normal approximation depends on the values of $n_1$, $n_2$, $p_1$, and $p_2$.


#### Hypothesis Test

For the hypothesis test of the difference in population proportions,
$$
\begin{aligned}[t]
H_0: & (p_1 - p_2) = 0 \\
H_a: & (p_1 - p_2) \neq 0 
\end{aligned},
$$
a test statistic is,
$$
\begin{aligned}[t]
z &= \frac{(\hat{p}_2 - \hat{p}_1)}{\text{SE}_{\hat{p}}, \\
\hat{p} &= \frac{\hat{p}_1 n_1 + \hat{p}_2 n_2}{n_1 + n_2} \\
\text{SE}_{\hat{p}} &= \sqrt{\frac{\hat{p} (1 - \hat{p})}{n_1} + \frac{\hat{p} (1 - \hat{p})}{n_2}}
\end{aligned} 
$$
which is distributed standard normal.
Note that standard error used test statistic for the difference of proportions is different than that used in the confidence interval.
Recall that that null hypothesis significance testing assumes that the null hypothesis is correct, which in this case is $p_1 = p_2$.



## Tables

### One-way Table

The test statistic for a one-way table with $k$ categories for the hypothesis test that,
$$
\begin{aligned}[t]
H_0:& \text{$O_i = E_i$ for all $i \in k$} \\
H_a:& \text{at least one $O_i \neq E_i$ for $i \in k$} \\
\end{aligned}
$$
is
$$
\chi^2 = \sum_{i = 1}^k \frac{(O_i - E_i)^2}{E_i} ,
$$
where $O_1, \dots, O_k$ are the observed counts, and $E_1, \dots, E_k$ are the expected counts.
The test statistic is approximately distributed $\chi^2$ with $k - 1$ degrees of freedom.

For this test to be a reasonable approximation:

- the cases must be independent
- each cell count must have an expected count greater than 5

### Two-way tables

The test statistic for a one-way table with $R$ rows and $C$ columns,
$$
\begin{aligned}[t]
H_0:& \text{rows and columns are independent} \\
H_a:& \text{rows and columns are not independent},
\end{aligned}
$$
is
$$
\chi^2 = \sum_{i = 1}^k \frac{(O_i - E_i)^2}{E_i} ,
$$
where $O_{i,j}$ is the observed count in row $i$ and column $j$, and $E_{i,j}$ is the expected count in row $i$, column $j$ assuming that rows and columns are independent.
Supposing that rows and columns are independent the expected count in a cell is,
$$
E_{i,j} = \frac{r_i \times c_j}{R C}
$$

where $r_i$ is the sum of counts in row $i$, and $c_j$ is the sum of counts in column $j$, and $RC$ is the sum of counts in the table.
The test statistic $\chi^2$ is distributed Chi-squared with degrees of freedom,
$$
df = (R - 1) \times (C - 1)
$$


For a 2 x 2 contingency table, use a two-sample proportion test

