# Interpreting Coefficients

That the coefficients are the marginal effects of each predictor makes linear regression
particularly easy to interpret. However, this interpretation of predictors
becomes more complicated once a variable is included in multiple terms through
interactions or nonlinear functions, such as polynomials.

Consider the regression,
$$
Y_i = \beta_0 + \beta_1 X + \beta_2 Z + \varepsilon
$$
The regression coefficient $\beta_1$ it the change in the expected value of $Y$ associated with a one-unit change in $X$ holding $Z$ constant,
$$
\begin{aligned}[t]
E(Y | X = x, Z = z) - E(Y | X = x + 1, Z = z) &= (\beta_0 + \beta_1 X + \beta_2 z) - (\beta_0 + \beta_1 (x + 1) + \beta_2 z) \\
&= \beta_1 x - \beta_1 (x + 1) \\
&= \beta_1 (x - x + 1) \\
&= \beta_1
\end{aligned}
$$

More formally, the coefficient $\beta_k$ is the partial derivative of $E(Y | X)$ with respect to $X_k$,
$$
\begin{aligned}[t]
\frac{\partial\,E(Y | X)}{\partial\,X_k} =  \frac{\partial}{\partial\,X_k} \left( \beta_0 + \sum_{k = 1}^K \beta_k X_k) \right) = \beta_k
\end{aligned}
$$


Implications:

1. If $X$ is multiplied by a constant scalar $a$,
    $$
    E(Y | X) = \tilde{\beta}_0 + \tilde{\beta}_1 a X = \beta_0 + (a \beta_1) X .
    $$
2. If $X_k$ has a scalar $a$ added to it,
    $$
    E(Y | X) = \tilde{\beta}_0 + \tilde{\beta}_1 (X + a) = (\beta_0 + \tilde{\beta}_1 a) + \tilde{\beta}_1 X
    $$
    Thus, $\tilde{\beta}_0 = (\beta_0 + \beta_1 a)$ and $\tilde{\beta}_1 = \beta_1$.
    

Consider the regression model
$$
\vec{Y} = \vec{\beta} \mat{X} + \vec{\epsilon}
$$

Rather than staring by asking what do the regression coefficients, $\vec{\beta}$, mean, we should start by asking what we want to estimate (i.e. the estimand) and then figure out how to extract that from the regression model.
Let's start with what it that we want to calculate. 
We want to calculate the "marginal effect" of changing the $j$th predictors while holding other predictors constant.

In particular, one common estimand is the predicted change in the expected value of $Y$ from a change in the $j$th predictor variable while holding the other predictors constant.
The regression model is a model of the expected value of $Y$ as a function of $\mat{X}$,
$$
\hat{\vec{y}} = \E(Y) = \hat{\vec{beta}} \mat{X}
$$
For a continuous variable, $\vec{x}_j$, this is called the "marginal effect" and it is the partial derivative of the regression line with respect to $\vec{x}_j$,
$$
ME_{i,j} =  \frac{\partial \E( Y_i | x_{i,j}, x_{i,-j})}{\partial x_{i,j}}
$$
For a discrete change in $x_j$, this is called the "partial effect" or "first difference", and is simply the difference of predicted values,
$$
ME_{i,j} =  \E(Y_i | x_{i,j}, x_{i,-j}) - \E(Y_i | x_{i,j} + \Delta x_{i,j}, x_{i,-j})
$$


Now consider the linear regression with two predictors for a change in $x_1$,
$$
\begin{aligned}[t]
ME_j &=  E(y | x_1, \tilde{x}_2) - E(y | x_1 + \Delta x_1, \tilde{x}_2)
\end{aligned}
$$
Since the linear regression equation is $E(y | x)$, this simplifies to
$$
\begin{aligned}[t]
ME_j &=  (\beta_0 + \beta_1 x_1 + \tilde{x}_2) - (\beta_0 + \beta_1 (x_1 + \Delta x_1) \tilde{x}_2) \\
&= \beta_1 \Delta x_1
\end{aligned}
$$
or as $\Delta x_1 \to 0$, this simplifies to the coefficient itself.
$$
\begin{aligned}[t]
ME_j &=  \frac{\partial E(y | x_1, x_2)}{\partial x_1} \\
\frac{\partial (\beta_0 + \beta_1 x_1 + \tilde{x}_2)}{\partial x_1}
&= \beta_1
\end{aligned}
$$

All of the previous equations were at the population level.
The sample estimate for the marginal effect is
$$
\widehat{ME}_j = \hat{\beta}_1
$$

So, for a linear regression, the marginal effect of $x_j$, defined as the change in the expected value of $y$ for a small a unit of $j$

The equation presented above is not **causal**, it is simply a function derived from the population or estimated equation. 

If population equation is not the as the linear regression,  $\hat{\beta_j}$ can still be viewed as an estimator of $ME_j$. In OLS, the $ME_j$ is weighted by observations with the most variation in $x_j$, after accounting for the parts of $x_j$ and $y$ predicted by the other predictors. See the discussion @AngristPischke2009a and @AronowSamii2015a. 

For regressions other than OLS, the coefficients are not the $ME_j$.
It is a luxury that the coefficients happen to have a nice interpretation in OLS.
In most other regressions, the coefficients are not directly useful. This is yet another
reason to avoid the mindless presentation of tables and star-worshiping. 
The researcher should focus on inference about the research quantity of interest, whether
or not that happens to be conveniently provided as a parameter of the model that was estimated.

## Interactions and Polynomials

Even for OLS, if $x_j$ is included as part of a function, e.g. a polynomial or an interaction, then its coefficient cannot be interpreted as the marginal effect. Suppose that the regression equation is
$$
\vec{y} = \vec{\beta}_0 + \vec{\beta}_1 x_1 + \vec{\beta}_2 x_1^2 + \vec{\beta}_3 x_2,
$$
then the marginal effect of $x_1$ is,
$$
\begin{aligned}[t]
ME_j &=  \frac{\partial E(y | x_1, x_2)}{\partial x_1} \\
&= \frac{\partial (\beta_0 + \beta_1 x_1 + \beta_1 x_1^2 +  \beta_3 \tilde{x}_2)}{\partial x_1} \\
&= \beta_1 + 2 \beta_2 x_1
\end{aligned}
$$
Note that the marginal effect of $x_1$ is **not**, $\beta_1$. 
That would require a change in $x_1$ while holding $x_1 ^ 2$ constant, which is a logical impossibility.
Instead, the marginal effect of $x_1$ depends on the value of $x_2$ at which it is evaluated, and, thus, observations will have different marginal effects.

Similarly, if there is an interaction between $x_1$ and $x_2$, the coefficient of a predictor
is not its marginal effect.
For example, in
$$
y = \vec{\beta}_0 + \vec{\beta}_1 x_1 + \vec{\beta}_2 x_1 + \vec{\beta}_3 x_1 x_2
$$
the marginal effect of $x_1$ is
$$
\begin{aligned}[t]
ME_j &=  \frac{\partial E(y | x_1, x_2)}{\partial x_1} \\
&= \frac{\partial (\beta_0 + \beta_1 x_1 + \beta_1 x_1^2 +  \beta_2 \tilde{x}_2)}{\partial x_1} \\
&= \beta_1 + \beta_3 x_2
\end{aligned}
$$
Now the marginal effect of $x_1$ is a function of another variable $x_2$.

## Average Marginal Effects

For marginal effects that are functions of the data, there are multiple ways to calculate them. They include,

- AME: Average Marginal Effect. Average the marginal effects at each observed $x$.
- MEM: Marginal Effect at the mean. Calculate the marginal effect with all observations at their means or other central values.
- MER: Marginal Effect at a representative value. Similar to MEM but with another meaningful value.

Of these, the AME is the preferred one; marginal effects should be calculated for all observations, and then averaged [@HanmerKalkan2012a].

<!-- When it is discrete change in $x$, it is called a partial effect (APE) or a first difference. -->
<!-- The difference in the expected value of y, given a change in $x_j$ from $x^*$ to $x^* + \Delta$ is $\beta_j \Delta$, and the standard error can be approximated  by [https://en.wikipedia.org/wiki/Delta_method](https://en.wikipedia.org/wiki/Delta_method), -->
<!-- $$ -->
<!-- \se(\hat{\beta}_j \Delta x) = \sqrt{\Var\hat{\beta_j} (\Delta x)^2 } = \se\hat{\beta_j} \Delta x. -->
<!-- $$ -->
<!-- The Delta method can be used to analytically derive approximations of the standard errors for other nonlinear functions and interaction in regression, but it scales poorly, and it is often easier to use bootstrapping or software than calculate it by hand. See the [margins](https://github.com/leeper/margins) package. -->


When it is discrete change in $x$, it is called a partial effect (APE) or a first difference.
The difference in the expected value of y, given a change in $x_j$ from $x^*$ to $x^* + \Delta$ is $\beta_j \Delta$, and the standard error can be calculated analytically by the [Delta method](https://en.wikipedia.org/wiki/Delta_method),
$$
\se(\hat{\beta}_j \Delta x_j) = \sqrt{\Var\hat{\beta}_j (\Delta x_j)^2} = \se\hat{\beta}_j \Delta x_j.
$$
The Delta method can be used to analytically derive approximations of the standard errors for other nonlinear functions and interaction in regression, but it scales poorly, and it is often easier to use bootstrapping or software than calculate it by hand. See the [margins](https://github.com/leeper/margins) package.


## Standardized Coefficients

A standardized coefficient is the coefficient on $X$, when $X$ is standardized so that $\mean(X) = 0$ and $\Var(X) = 1$.
In that case, $\beta_1$ is the change in $\E(Y)$ associated with a one standard deviation change in $X$.

Additionally, if all predictors are set so that $\mean(X) = 0$, $\beta_0$ is the expected value of $Y$
when all $X$ are at their means.
However, if any variables appear in multiple terms, then the standardized coefficients are not particularly
useful.

Standardized coefficients are generally not used in political science. (King How Not to Lie with Statistics, p. 669)
More often, the effects of variables are compared by the first difference between the value of the
variable at the mean, and a one standard deviation change.
While, this is equivalent to the standardized coefficient

Note, that standardizing variables can help computationally in some cases.
In OLS, there is a closed-form solution, so iterative optimization algorithms are not
needed in to find the best parameters. However, in more complicated models which require iterative optimization,
standardizing variables can often improve the performance of the optimization.
Thus standardizing variables before analysis is common in machine learning.
However, the purpose is for ease of computation, not for ease of interpretation.
