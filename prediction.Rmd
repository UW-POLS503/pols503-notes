---
output: html_document
editor_options:
  chunk_output_type: console
---
# Prediction

## Prerequisites {-}

```{r message=FALSE}
library("tidyverse")
library("broom")
library("jrnoldmisc")
```

You will need to install **jrnoldmisc** with
```{r eval=FALSE}
devtools::install_github("jrnold/jrnoldmisc")
```

## Prediction Questions vs. Causal Questions

Prediction vs. Causal questions can be reduced to: Do you care about $\hat{y}$ or $\hat{beta}$?

Take a standard regression model,
$$
y = X \beta + \epsilon .
$$
We can use regression for prediction or causal inference.
The difference is what we care about.

In a prediction *prediction problem* we are interested in $\hat{y} = X \hat{\beta}$.
The values of $\hat{\beta}$ are not interesting in and of themselves.

In a *causal-inference problem* we are are interested in getting the best estimate of $\beta$, or more generally $\partial y / \partial x$ (the change in the response due to a change in x).

If we had a complete model of the world, then we could use the same model for both these tasks.
However, we don't and never will.
So there are different methods for each of these questions that are tailored to improving our estimates of those.

## Why is prediction important?

Much of the emphasis in social science is on "causal" questions, and "prediction" is often discussed pejoratively.
Apart from the fact that this belief is often due to a deep ignorance of statistics and the philosophy of science and a lack of introspection into their own research, there are a few reasons why understanding prediction questions.

## Many problems are prediction problems

Causal inferential methods are best for estimating the effect of a policy intervention.
Many problems in the political science are discussed as if they are causal, but any plausible research question is predictive since there is no plausible intervention to estimate.
I would place many questions in international relations and comparative politics in this realm.

### Counterfactuals

The fundamental problem of causal inference is a prediction problem.
We do not observe the counterfactuals, so we must predict what would have happened if a different treatment were applied.
The currently developed causal inference methods are adapting methods and insights from machine learning into these causal inference models.

### Controls

The bias-variance trade-off is useful for helping to think about and choose control variables.

### What does overfitting mean

The term overfitting is often informally used.
It has no meaning outside of prediction.

## Prediction vs. Explanation

Consider this regression model,
$$
y = \beta_1 x_1 + \beta_2 x_2 + \epsilon
$$
where $y$ is a $n \times 1$ vector and $\epsilon$ is a $n \times 1$ vector,
$$
\epsilon_i \sim \mathrm{Normal}(0, \sigma^2).
$$

We will estimate two models on this data and compare their predictive performance:

The *true model*,
$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon
$$
and the *underspecified model*,
$$
y = \beta_0 + \beta_1 x_1 + \epsilon
$$

We will evaluate their performance by repeatedly sampling from the true distribution and comparing their out of sample performance.

Write a function to simulate from the population.
We will include the sample size, regression standard deviation, correlation between the covariates, and the coefficients as arguments.

-   `size`: sample size
-   `sigma`: the standard deviation of the population errors
-   `rho`: the correlation between $x_1$ and $x_2$
-   `beta`: the coefficients ($\beta_0$, $\beta_1$, $\beta_2$)

```{r sim_data}
sim_data <- function(size = 100, beta = c(0, 1, 1),
                     rho = 0, sigma = 1) {
  # Create a matrix of size 1
  dat <- jrnoldmisc::rmvtnorm_df(size, loc = rep(0, 2), R = equicorr(2, rho))
  # calc mean
  dat$fx <- model.matrix(~ X1 + X2, data = dat) %*% beta %>%
    as.numeric()
  dat$y <- dat$fx + rnorm(size, 0, sigma ^ 2L)
  dat$y_test <- dat$fx + rnorm(size, 0, sigma ^ 2L)
  dat
}
```

The output of `sim_data` is a data frame with `size` rows and columns

-   `X1, X2`: The values of $x_1$ and $x_2$
-   `fx`: The mean function $f(x) = \beta_0 + \beta_1 x_1 + \beta_2 x_2$
-   `y`: The values of $y$ in the sample that will be used to train the model.
-   `y_test`: Another draw of $y$ from the population which will be used to evaluate the trained model.

```{r}
head(sim_data(100))
```

For each training and test samples we draw we want to

1.  fit the *true model* using `y`
1.  evaluate the prediction accuracy of the *true model* on `y_test`
1.  fit the *underspecified model* using `y`
1.  evaluate the prediction accuracy of the *underspecified model* on `y_test`

The function `sim_predict` does this

```{r}
sim_predict <- function(f, data) {
  # run regression
  mod <- lm(f, data = data)
  # predict the y_test values
  augdat <- augment(mod, data = data) %>%
    # evaluate and return MSE
    mutate(err_out = (.fitted - y_test) ^ 2,
           err_in = (.fitted - y) ^ 2)
  tibble(r_squared = glance(mod)$r.squared,
         mse_in = mean(augdat$err_in),
         mse_out = mean(augdat$err_out))
}
```

So each simulation is:
```{r}
data <- sim_data(100, rho = 0.9, sigma = 3)
mod_under <- sim_predict(y ~ X1, data = data)
mod_under
mod_true <- sim_predict(y ~ X1 + X2, data = data)
mod_true
```

We are estimating the expected error of new data.
Without an analytical solution, we need to simulate this.

The `run_sim` function simulates new test and training samples of `y` and `y_test`, runs both the true and underspecified models on them, and returns the results as a data frame with two rows the columns

-   `r_squared`: In-sample $R^2$
-   `mse_in`: In-sample mean-squared-error.
-   `mse_out`: Out-of-sample mean-squared-error.
-   `model`: Either "true" or "underspecified" to indicate the model.
-   `.iter`: An iteration number, used only for bookkeeping.

```{r}
run_sim <- function() {
  data <- sim_data(100, rho = 0.9, sigma = 3)
  mod_under <- sim_predict(y ~ X1, data = data) %>%
    mutate(model = "underspecified")
  mod_true <- sim_predict(y ~ X1 + X2, data = data) %>%
    mutate(model = "true")
  bind_rows(mod_under, mod_true)
}
```

Run the simulation `n_sims` times and then calculate the mean $R^2$, in-sample MSE, and out-of-sample MSE:
```{r}
n_sims <- 512
rerun(n_sims, run_sim()) %>%
  bind_rows() %>%
  group_by(model) %>%
  summarise_all(funs(mean))
```

Generally, the underspecified model can yield more accurate predictions when [@Shmueli2010a]:

-   data are very noisy (large $\sigma$). In these cases, increasing the
    complexity of the model will increase variance with little decrease in the
    variance since most of the variation in the sample is simply noise.

-   magnitude of omitted variables are small. In this case, those
    omitted variables don't predict the response well, but could increase the 
    overfitting in samples.

-   predictors are highly correlated. In this case, the information contained 
    in the omitted variables is largely contained in the original variables.

-   sample size is small or the range of left out variables is small.

See @Shmueli2010a for more.

**Exercise**  Try different parameter values for the simulation to confirm this.

The take-away. Prediction doesn't necessarily select the "true model", and knowing the "true model" may not help prediction.

Note that this entire exercise operated in an environment in which we knew the true model and thus does not resemble any realistic situation.
Since "all models are wrong" the question is not whether it is useful to use the "true" model.
What this simulation reveals is our models of the world are contingent on the size and quality of the data.
If the data are noisy or few, then we need to use simpler models.
If the covariates are highly correlated, it may not matter which one one we use in our theory.

## Bias-Variance Tradeoff

Consider the general regression setup,
$$
Y = f(\Vec{X}) + \epsilon,
$$
where
$$
\begin{aligned}[t]
\E[\epsilon] &= 0 & \Var[\epsilon] &= \sigma^2 .
\end{aligned}
$$
When given a random pair $(X, Y)$, we would like to "predict" $Y$ with some function of $X$, say, $f(X)$.
However, in general we do not know $f(X)$.
So given some data consisting of realizations of pairs of $X$ and $Y$, $\mathcal{D} = (x_i, y_i)$, the goal of regression is to estimate function $\hat{f}$ that is a good approximation of the true function $f$.

<!-- this discussion is alternating between discussing predicting f(X) and Y -->

What is a good $\hat{f}$ function? 
A good $\hat{f}$ will have low **expected prediction error** (EPE), which is the
error for predicting a new observation.
$$
\begin{aligned}[t]
EPE(Y, \hat{f}(x)) &= \mathbb{E}\left[(y - \hat{f}(x))^2\right] \\
    &= \underbrace{\left(\mathbb{E}(\hat{f}(x)) - f(x)\right)^{2}}_{\text{bias}} +
    \underbrace{\mathbb{E}\left[\hat{f}(x) - \mathbb{E}(\hat{f}(x))\right]^2}_{\text{variance}} +   \underbrace{\mathbb{E}\left[y - f(x)\right]^{2}}_{\text{irreducible   error}} \\
    &= \underbrace{\mathrm{Bias}^2 + \mathbb{V}[\hat{f}(x)]}_{\text{reducible error}} + \sigma^2
\end{aligned}
$$

In general, there is a bias-variance tradeoff.
The following three plots are three stylized examples of bias variance tradeoffs:
when the variance influence the prediction error more than bias, when neither is 
dominant, and when the bias is more important.

```{r, fig.height = 4, fig.width = 12, echo = FALSE}
x <- seq(0.01, 0.99, length.out = 1000)

bind_rows(
  tibble(
    x = x,
    b = 0.05 / x,
    v = 5 * x ^ 2 + 0.5,
    bayes = 4,
    epe = b + v + bayes,
    label = "More Dominant Variance"
  ),
  tibble(
    x = x,
    b = 0.05 / x,
    v = 5 * x ^ 4 + 0.5,
    bayes = 4,
    epe = b + v + bayes,
    label = "Neutral"
  ),
  tibble(
    x = x,
    b = 6 - 6 * x ^ (1 / 4),
    v = 5 * x ^ 6 + 0.5,
    bayes = 4,
    epe = b + v + bayes,
    label = "More Dominant Bias"
  )
) %>%
  select(x, b, v, epe, label) %>%
  gather(variable, value, -label, -x) %>%
  mutate(variable = factor(recode(variable, b = "Bias", v = "Variance", 
                                  epe = "Expected Predicted Error"),
                          levels = c("Bias", "Variance", 
                                     "Expected Predicted Error")),
         label = factor(label,
                        labels = c("More Dominant Bias", "Neutral", 
                                   "More Dominant Variance"))) %>%
  ggplot(aes(x = x, y = value, colour = variable)) +
    geom_line() +
    facet_wrap(~ label, nrow = 1) +
    labs(x = "Model Complexity", y = "Error") +
    theme_minimal() +
    theme(legend.position = "bottom",
          panel.grid = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank())

```

As model complexity increases, bias decreases, while variance increases.
There is some some sweet spot in model complexity that minimizes the expected prediction error.
By understanding the tradeoff between bias and variance, we can find a model complexity to predict unseen observations well.

```{r, fig.height = 6, fig.width = 10, echo = FALSE}
local({
  # derived from https://raw.githubusercontent.com/daviddalpiaz/r4sl/master/08-tradeoff.Rmd # nolint
  f <- function(x) {
    ((x - 50) / 50) ^ 2 + 2
  }
  g <- function(x) {
    1 - ((x - 50) / 50)
  }

  tibble(
    x = seq(0, 100, by = 0.001),
    "Train" = g(x),
    "(Expected) Test" = f(x)
  ) %>%
    gather(type, error, -x) %>%
    ggplot(aes(x = x, y = error, colour = type)) +
    geom_line() +
    labs(colour = "Error", x = "Model complexity", y = "Error") +
    theme(legend.position = "bottom")
})

```

### Example

Consider the function,
$$
y = x^2 + \epsilon
$$
where $\epsilon \sim \mathrm{Normal}(0, 1)$

Here is an example of some data generated 
from this model.
We will write a function to calculate $f(x)$.
```{r}
regfunc <- function(x) {
  x ^ 2
}
```
Write a function that draws a single sample from the model.
```{r}
sim_data <- function(x) {
  sigma <- 1
  # number of rows
  n <- length(x)
  # proportion of observations in the test set
  p_test <- 0.3
  tibble(x = x,
         fx = regfunc(x),
         y = fx + rnorm(n, 0, sd = sigma),
         test = sample(c(TRUE, FALSE), size = n, replace = TRUE))
}
```
Calculate this function 
```{r}
n <- seq(0, 1, length.out = 30)
sim_data(n) %>%
  ggplot(aes(x = x)) +
  geom_point(aes(y = y, colour = test)) +
  geom_line(aes(y = fx))

```

For fit the data we will estimate polynomial models of increasing complexity, from only an intercept to a polynomial of degree 4.

-   $y_i = \beta_0$ 
-   $y_i = \beta_0 + \beta_1 x$
-   $y_i = \beta_0 + \beta_1 x + \beta_2 x^2$
-   $y_i = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3x^3$
-   $y_i = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_3 x^4$

We will write a function to estimate these models.
As input it takes the `degree` of the polynomial, 
the `data` to use to estimate it, and (optionally)
an `.iter` variable that can be used to keep track of which iteration it is from.
```{r}
est_poly <- function(degree, data, .iter = NULL) {
  if (degree == 0) {
    mod <- lm(y ~ 1, data = filter(data, !test))
  } else {
    mod <- lm(y ~ poly(x, degree), data = filter(data, !test))
  }
  out <- augment(mod, newdata = filter(data, test)) %>%
    mutate(degree = degree) %>%
    select(-.se.fit)
  out[[".iter"]] <- .iter
  out
}
```

For example, we will use a fixed $x$ in each model.
We will use an evenly spaced grid between 0 and 1.
```{r}
x <- seq(0, 1, length.out = 100)
data <- sim_data(x)
est_poly(2, data)
```

Draw one sample from the data and run all model
```{r}
run_sim <- function(.iter) {
  # degrees of models to evaluate
  degrees <- 0:5
  # the grid of data to sample
  x <- seq(0, 1, length.out = 64)
  data <- sim_data(x)
  # run all models
  map_df(degrees, est_poly, data = data, .iter = .iter)
}
```

Run the full simulation, drawing `n_sims` samples, running all the different models estimates.
```{r}
n_sims <- 2 ^ 12
all_sims <- map_df(seq_len(n_sims), ~ run_sim(.x))
```

For each model plot the expected regression line at the values of $x$, which we'll define as the average prediction of the model at each point.[^fhat]
$$
\hat{f}(X = x) = \frac{1}{S} \sum_{s = 1}^S \hat{E}(y | X = x)
$$
```{r}
ggplot() +
  geom_line(data = filter(all_sims, .iter < 10),
            mapping = aes(x = x, y = .fitted, group = .iter)) +
  geom_line(data = filter(all_sims, .iter == 1),
            mapping = aes(x = x, y = fx), colour = "red") +
  facet_wrap(~ degree)
```

Now calculate the bias and variance of these models at each $x$.
```{r}
poly_estimators <- all_sims %>%
  group_by(degree, x) %>%
  summarise(estimate = mean(.fitted),
            variance = var(.fitted),
            mse_in = mean((.fitted - fx)[!test]),
            mse_out = mean((.fitted - fx)[test], na.rm = TRUE),
            fx = mean(fx))
```

Plot the values of $\hat{f}(x)$ for all models against the true model.
On average squared model fits the true function well, and higher order polynomials cannot improve it.
```{r}
ggplot(poly_estimators, aes(x = x, y = estimate, colour = factor(degree))) +
  geom_line()
```

```{r}
poly_estimators %>%
  mutate(bias = estimate - fx) %>%
  group_by(degree) %>%
  summarise(bias2 = mean(bias ^ 2), 
            variance = mean(variance, na.rm = TRUE),
            mse_in = mean(mse_in, na.rm = TRUE),
            mse_out = mean(mse_out, na.rm = TRUE)) %>%
  gather(variable, value, -degree) %>%
  ggplot(aes(x = degree, y = value, colour = variable)) +
  geom_line()
```

Since $\hat{f}$ varies sample to sample, there is variance in $\hat{f}$.
However, OLS requires zero bias in sample, and thus means that there is no trade-off.

### Overview

-   low bias, high variance (overfit)

    -   more complex (flexible functions)
    -   estimated function closer to the true function
    -   estimated function varies more, sample to sample
    -   overfit

-   high bias, low variance (underfit)

    -   simple function
    -   simpler estimated function
    -   estimated function varies less, sample to sample
    -   underfit

What to do?

-   low bias, high variance: simplify model
-   high bias, low variance: make model more complex
-   high bias, high variance: more data
-   low bias, low variance: your good

The general rule.

-   more training data reduces both bias and variance
-   regularization and model selection methods can choose an optimal bias/variance trade-off

## Prediction policy problems

@KleinbergLudwigMullainathanEtAl2015a distinguish two types of policy questions.
Consider two questions related to rain.

1.  In 2011, Governor Rick Perry of Texas [designated days for prayer for rain](https://en.wikipedia.org/wiki/Days_of_Prayer_for_Rain_in_the_State_of_Texas)
    in order to end the Texas drought.

1.  It is cloudy out. Do you bring an umbrella (or rain coat) when leaving the house?

How does the pray-for-rain problem differ from the umbrella problem?

-   Prayer problems are causal questions, because the payoff depends on the causal question as to whether a prayer-day can cause rain.
-   Umbrella questions are prediction problems, because an umbrella does not cause rain. However, the utility of bringing an umbrella depends on the probability of rain.

Many policy problems are a mix of prediction and causation.
The policymaker needs to know whether the intervention has a causal effect, and also the predicted value of some other value which will determine how useful the intervention is.
More formally, let $y$ be an outcome variable which depends on the values of $x$ ($x$ may cause $y$).
Let $u(x, y)$ be the policymaker's payoff function. 
The change in utility with response to a new policy ($\partial u(x, y) / \partial x)$ can be decomposed into two terms,
$$
\frac{\partial u(x, y)}{\partial x} =
\frac{\partial u}{\partial x} \times \underbrace{y}_{\text{prediction}} +
\frac{\partial u}{\partial y} \times
\underbrace{\frac{\partial y}{\partial x}}_{\text{causation}} .
$$
Understanding the payoff of a policy requires understanding the two unknown terms

-   $\frac{\partial u}{\partial x}$: how does $x$ affect the utility. This needs to evaluated at the value of $y$, which needs to be predicted. The utility of carrying an umbrella depends on whether it rains or no. This is predictive.
-   $\frac{\partial y}{\partial x}$: how does $y$ change with changes in $x$? This is causal.

<!--
## Freedman's Paradox

Create a matrix with `n` rows and `k` columns (variables).
```{r}
k <- 51
n <- 100
```

Suppose that all entries in this matrix are uncorrelated, e.g.
```{r}
X <- rmvtnorm_df(n, loc = rep(0, k))
```

```{r}
mod1 <- lm(X1 ~ ., data = X)
broom::glance(mod1)
```

-   What is the $R^2$ and $p$-value of the $F$-test of this regression?
-   How many significant variables at the 5% level are there?
-   Keep all the variables significant at the 25% level.
-   Rerun the regression using those variables.

```{r}
thresh <- 0.25
varlist <- filter(tidy(mod1), p.value < thresh,
                  term != "(Intercept)")[["term"]]
f <- as.formula(str_c("X1 ~ ", str_c(varlist, collapse = " + ")))
mod2 <- lm(f, data = X)
```

```{r}
glance(mod2)
```

```{r}
tidy(mod2) %>%
  filter(p.value < 0.05)
```

The takeaway is that model selection can create variables that appear important even when they are not.
Inference (calculating standard errors) after model selection is very difficult to do correctly.
Recall that to be correct, the definition of the sampling distribution (used in confidence intervals and hypothesis testing) would have to include all possible ways in which the data were generated.
The previous analysis omitted that.
If the effect of omitting the model selection stage didn't seem to make much of a difference in the final outcomes, it may not be fine to simplify by ignoring it.
However, this example shows that the effect of omitting this stage is large.
-->

### References

Parts of the bias-variance section are derived from R for Statistical Learning, [Bias-Variance Tradeoff](https://daviddalpiaz.github.io/r4sl/biasvariance-tradeoff.html)

Also see:

-   [Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html)

[^fhat]: These lines are not smooth due to Monte Carlo error (error coming from taking a finite number of samples).
