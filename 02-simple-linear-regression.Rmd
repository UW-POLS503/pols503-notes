---
title: Simple Linear Regression
---

# Simple Linear Regression

## Representing the Relationship between two variables

Many theories in social science is how two variables are related

– Does turnout vary by types of mailers received?
– Is the quality of political institutions related to average incomes?
- Does conflict mediation help reduce civil conflict?

- Y - the dependent variable or outcome or regressand or left-hand-side variable
or response
    – Voter turnout
    – Log GDP per capita
    – Number of battle deaths
    - I added this also

- X - the independent variable or explanatory variable or regressor or righthand-side
variable or treatment or predictor
    – Social pressure mailer versus Civic Duty Mailer
    – Average Expropriation Risk
    – Presence of conflict mediation

A common goal of these analyses is to understand how $Y$ varies as a function of $X$,




## Conditional Expectation Function

There are many ways to summarize the relationship between two variables: visually with scatterplots, covariance,
and correlation.

When we want to describe an outcome or estimate the effect of a covariate on an outcome, one way to model this is through a conditional expectation---what is the expected value (mean) of the outcome for a given value of the covariate(s).

The **conditional expectation function (CEF)**, also called the **regression function** of $Y$ given $X$ is the function that gives the expected value (mean) of $Y$ at various values of $x$.
It is denoted $\E(Y | X = x)$.

Note that the CEF is a function of the population.
In the same way that the expected value of a distribution is a parameter, the conditional expected values of the distribution given values of $x$ is also a  parameter.

Like other parameters, the CEF throws away a lot of data.
It does not describe all the ways that the distribution of $Y$ changes as a function of $x$; it only describes how the *mean* of $Y$ changes as a function of $x$.

### Three uses of regression

1. Description - parsimonious summary of the data
2. Prediction/Estimation/Inference - learn about parameters of the joint distribution
of the data
3. Causal Inference - evaluate counterfactuals

## Population Linear Regression Function

The *simple* (*bivariate*) linear regression, regression is linear regression with one outcome variable and one covariate,
$$
r(x) = \E[Y | X = x] = \beta_0 + \beta_1 X
$$

This describes a data generating process in the population where

- Y : dependent variable (random variable)
- X : independent variable (random variable)
- $\beta_0$: population intercept
- $\beta_1$: popluation slope

We will want to estimate $\beta_0$ and $\beta_1$.

## Sample Linear Regression Function

The sample linear regression function is
$$
\hat{r}(x_i) = \hat{y}_i = \hat\beta_0 + \hat\beta_1 x_i
$$
- \hat{y}_i : fitted or predicted value
- \hat{beta}_i
- x_i : observed values of the predictors
- $\hat\beta_0$: estimated intercept
- $\hat\beta_1$: estimated slope
- This also yields the residuals $\hat\epsilon_i$, which are the difference between the observed value of $y_i$ and the predicted value $\hat{y}_i$,
    $$
    \hat\epsilon_i = y_i - \hat{y}_i
    $$

## Ordinary Least Squares

Ordinary least squares is an estimator of the slope an intercept of the regression line.
It finds the slope and intercept that minimizes the sum of the squared residuals.
The estimates
$$
\begin{aligned}[t]
(\hat{\beta}_0, \hat{\beta}_1) &= \argmin_{b_0, b_1} \sum_{i = 1}^N (y_i - b_0 - b_1 x_i)^2 \\
&= \argmin_{b_0, b_1} \sum_{i = 1}^N (y_i - \hat{y_i})^2 \\
&= \argmin_{b_0, b_1} \sum_{i = 1}^N \hat{\epsilon}_i^2
\end{aligned}
$$
where, $\hat{y}_i$ are the fitted values,
$$
\hat{y}_i = b_0 + b_1 x_i
$$
and $\hat{\epsilon}_i$ are the esimated residuals,
$$
\hat{\epsilon}_i = y_i - \hat{y}_i .
$$

INSERT PLOT

A nice feature of OLS is that that the solution to the minimization problem can be found analytically.

The solutions to OLS are
$$
\begin{aligned}[t]
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat{\beta_1} &= \frac{\sum_{i = 1}^n (x_i - \bar{x})^2 (y_i - \bar{y})}{\sum_{i = 1}^n (x_i - \bar{x})^2} = \frac{\cov(x, y)}{\Var{x}}
\end{aligned}
$$

In other words the regression slop is the covariance between the outcome and predictor variable, scaled by the variance of the predictor variable.
Since $\cor(x, y) = \cov(x, y) / (\sd{x} \sd{y})$,
the regression slope can be expressed as a function of the correlation between $x$ and $y$,
$$
\hat{y} = \cor(x, y) \cdot \frac{\sd{x}}{\sd{y}} .
$$

Questions:

- How does the slope of the regression vary with the sample correlation? covariance?
- What happens when $x$ doesn't vary? In other words $x_1 = x_2 = \dots = x_n$. Answer this using the equation, and by sketching what this data and the regression line would look like?
- What happens when $y$ doesn't vary?

In more complicated problems, the solutions have to be found through an iterative optimization problem.

### Mechanical properties of the OLS statistic

These properties are related to the algorithm used the estimate OLS, and are
not directly related to how well OLS works as an estimator.
These properties follow directly from the first-order conditions used to find the OLS estimates.

1. Residuals are zero on average. And by implications, the residuals sum to zero.

    $$
    \frac{1}{N} \sum_{i = 1}^N \hat{\epsilon}_i = 0
    $$

2. The residuals are uncorrelated with the covariate,

    $$
    \cov(x_i, \hat{\epsilon}_i) = 0
    $$

3. The residuals are uncorrelated with the fitted values

    $$
    \cov(\hat{y}_i, \hat{\epsilon}_i) = 0
    $$

4. The regression line goes through $(\bar{y}, \bar{x})$.

Note that 2 and 3 are properites of the estimated residuals, not the population residuals.
If the population residuals are correlated with the covariate or outcome, then OLS estimator will have problems.


### OLS slope is a weighted sum of the outcomes

The OLS estimator for the slope, $\hat{\beta}_1$, can be written as a weighted sum of the outcomes,
$$
\hat{\beta}_1 = \sum_{i = 1}^n w_i y_i
$$
where
$$
w_i = \frac{(x_i - \bar{x})^2}{\sum_{i = 1}^n (x_i - \bar{x})^2}
$$

This is important for several reasons.
Even though the regression line includes all observations in its calculation, not all observations are equally important in determining the regression line.
Those observations far from the average value of $x$ are given more weight.[^ch2-outlier]

See the paper by XXXX on panel data and regression weighting.

[ch2-outlier]: Though whether those observations, if dropped, would affect the values of $\hat{\beta}_i$ depends on whether the outcome of that observation is similar to its predicted value.

This also implies that $\hat{\beta}$ is a random variable, since it is the sum of random variables, $Y_i$.

For the derivation, see <http://www.mattblackwell.org/files/teaching/s07-simple-regression.pdf>

## Properties of the OLS Estimator

Remember, OLS is a statistic---a mechanical function of the data.
We plug in data and get a result.
It may be more complicated than sample means, variances, medians, or other summary stats seen earlier, but it is analgous.

Since it is a function of the sample data, an the sample is a random variable, the OLS statistic has a sampling distribution.
This sampling distribution will determine how well the OLS statistic works as an estimator of the population linear regression line.

INSERT FAKE DATA EXAMPLE


## Properties of the Estimator

- Unbiased
- Consistent
- Asymptotically normal
- Efficiency. An efficient estimator is the estimator with the lowest *standard error*. OLS is BLUE---the Best Linear Unbiased Estimator. This is proven by the *Gauss-Markov Theorem*. For the set of the class of linear, unbiased, estimators (with a few regularity conditions), OLS is the efficient estimator.

![OLS Is BLUE (Best Linear Unbiased Estimator)](img/tobias-funke-blue.jpeg)

## Assumptions for unbiasedness and consistency of OLS

1. Linearity
2. Random (i.i.d.) sample
3. Variation in $x_i$
4. Zero conditional mean of the errors

These assumptions are about using the OLS statistic as an estimator of the population linear regression line. The only assumption that is necessary to calculate a sample regression line is variation in $x$. If the other assumptions are not met, then the sample OLS statistic can be calculated, but it may be a poor estimator of the population regression line.


## Hypothesis Tests for Regression


## Confidence Intervals for Regression

## Goodness of Fit

How well does the regression line fit the data?
There are two commonly used statistics to judge this.

- prediction error, or the standard error of the regression
- $R^2$ (R-squared) and adjusted $R^2$

$$
SST = \sum_{i = 1}^n (y_i - \bar{y})^2
$$
$$
SSR = \sum_{i = 1}^n (y_i - \hat{y})^2
$$

$R^2$ is a form of model comparison.
It compares the regression against a benchmark null model, which in this case is a model with only the intercept $y_i = \beta_0 = \bar{y}$.

Some propertie of $R$-squared:

- $R^2$ implies no relationship. If $R^2 = 0$, then $\beta_0 = 0$.
- $R^2$ implies a perfect linear fit
- $R^2$ is the correlation squared.




## References

- Much of this is taken from Matthew Blackwell lecture notes for Govt 2002 <http://www.mattblackwell.org/files/teaching/s07-simple-regression.pdf>
- Tobias Funke meme <http://memegenerator.net/instance2/5082792>. Generated by Jeffrey Arnold. Hat-tip to Brenton Kenkel for the refence. <http://bkenkel.com/psci8357/notes/02-reintroduction.pdf>.
