# Regularization

> never trust ols with more than five regressors
> --- [Zvi Grilliches](http://www.nber.org/econometrics_minicourse_2015/nber_slides11.pdf)

> Regularization theory was one of the first signs of the existence of intelligent inference
> --- [Zapnik](http://www.nber.org/econometrics_minicourse_2015/nber_slides11.pdf)

Rather than choose the best fit, there is some penalty to avoid over-fitting.
This is to choose the optimal optimal point on the expected predicted value.

There are two questions

1.  method of regularizaton
1.  amount of regularization

There are several choices of the former, chosen for different reasons.

The latter is almost always chosen by cross-validation.

While OLS is okay for estimating $\beta$ (best linear unbiased property).
However, with $K \geq 3$ regressors, OLS is poor.

The approaches to regualization in regression are

1.  Shrink estimates to zero (Ridge)
1.  Sparsity, limit number of non-zero estimates (Lasso)
1.  Combination of the two (Bridge)

## Ridge Regression

$$
\hat{\beta}_{\text{OLS}} = \arg \min_{\beta} \sum_{i=1}^{n} (y_i - \Vec{x}_{i} \Vec{\beta})^{2}
$$

Regularized regression adds a penalty that is a function of $\beta$.
This encourages $\beta$ to be close to zero.
$$
\hat{\beta}_{\text{regularized}} = \arg \min_{\beta} \sum_{i=1}^{n} (y_i - \Vec{x}_{i} \Vec{\beta})^{2} + \text{penalty}(\beta)
$$

**Ridge** regression penalizes the $\Vec{\beta}$ vector by the 
$$
\hat{\beta}_{\text{ridge}} = \arg \min_{\beta} \sum_{i=1}^{n} (y_i - \Vec{x}_{i} \Vec{\beta})^{2} + \sum_{k = 1}^{p} \beta_k^2
$$

**Lasso** penalizes the coefficients by an the $L1$ norm. 
Suppose we want to find the best subset of $\leq k$ covariates .
$$
\hat{\beta}_{\text{lasso}} = \arg \min_{\beta} \sum_{i=1}^{n} (y_i - \Vec{x}_{i} \Vec{\beta})^{2} + \lambda \sum_{k = 1}^p |\beta_k|
$$

-   *relaxed* or *post* LASSO

    -   Run LASSO to select non-zero coefficients
    -   Run OLS on the selected coefficients
    
-   good properties when the true model is sparse
-   it true distribution of coefficents is a few big ones and many small ones, LASSO will do better.
    If many small/modest sized effects, ridge may do better.
-   LASSO does not work well with highly correlated coefficients.

    -   Ridge: $\hat{\beta}_{1} + \hat{\beta}_{2} \approx (\beta_1 + \beta_2)/ 2$
    -   LASSO: Indifferent between $\hat{\beta}_1 = 0$, $\hat{\beta}_2 = \beta_1 + \beta_2$, $\hat{\beta}_1 = \beta_1 + \beta_2$, and $\hat{\beta}_2 = 0$.
    
-   Oracle property. If the true model is sufficiently sparse, we can ignore the 

**Bridge** regression penalizes the $\Vec{\beta}$ vector by the 
$$
\hat{\beta}_{\text{bridge}} = \arg \min_{\beta} \sum_{i=1}^{n} (y_i - \Vec{x}_{i} \Vec{\beta})^{2} + \lambda_1 \sum_{k = 1}^{p} |\beta_k| + \lambda_2 \sum_{k = 1}^{p} \beta_k^2
$$

How to find the value of $\lambda$? Cross validation. 
The function `cv.glmet()` uses cross-valiation to select the penalty parameter.

## Double Lasso

Consider a regression

$$
$$

1.   a lasso regression predicting the dependent variable, and keeping track of the variables with non-zero estimated coefficients: 

1.   


## References

It is a few years old, but the [2015 NBER Summer course](http://www.nber.org/econometrics_minicourse_2015/nber_slides11.pdf) has a good introduction to machine learning that is targeted at social scientists.

