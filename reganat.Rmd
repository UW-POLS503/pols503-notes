# Regression Anatomy

*Regression anatomy* is a term from @AngristPischke2009 for the coefficient of
variable $x$ linear regression in a multivariate bivariate model is equivalent to
the coefficient of a bivariate model using the residual from a regression of 
that variable regressed on all the other variables. 
Regression anatomy provides an answer to the *how* of control variables.

Consider a regression with two variables,
$$
\vec{y} = \beta_0 + \beta_1 \vec{x}_1  + \beta_1 \vec{x}_2 + u .
$$
Then
$$
\beta_1 = \frac{\cov(\vec{y}, \tilde{\vec{u}})}{\var(\tilde{\vec{u}})} .
$$
where $\tilde{\vec{u}}$ is the vector of residuals from the regression of $\vec{x}_1$ on $\vec{x}_2$,
$$
\begin{aligned}[t]
\vec{x}_1 = \gamma_0 + \gamma_1 \vec{x}_2 + v .
\end{aligned}
$$
This can be extended to more than two two variables by repeating the above steps as many times as necessary.

See Frisch and Waugh (1933), Lovell (1963), Angrist and Pischke (2009)
A complete proof can be found in advanced econometrics textbooks
like Davidson and MacKinnon (1993, p. 19–24) or Ruud (2000, p. 54–60),

Notes:

- This is a mechanical property of OLS.
- Does not depend an underlying Data Generating Process (DGP).
- Useful for understanding using OLS for causal inference, but it does not depend on causal inference.

## Example

For this example we will use the `Duncan` data from the **car** package.
```{r Duncan}
data("Duncan", package = "car")
Duncan <- rownames_to_column(Duncan, var = "occupation")
```

Run the regression of `prestige` on `education` and `income`:
```{r mod1}
mod1 <- lm(prestige ~ education + income, data = Duncan)
mod1
```

Now produce the coefficient for `education` by regression anatomy methods:
First, regress `education` on `income`:
```{r mod2a}
mod2a <- lm(education ~ income, data = Duncan)
mod2a
```
Get the residuals from that regression, and add them to the model.
For convenience, I will use the `add_residuals` function from the **modelr** package.
```{r mod2a_add_resids}
Duncan <- add_residuals(Duncan, mod2a)
```
Now, regress `prestige` on those residuals,
```{r mod2b}
mod2b <- lm(prestige ~ resid, data = Duncan)
mod2b
```
The coefficients of these two regressions are numerically equal; they are slightly
different due to floating point errors.
```{r}
coef(mod1)["education"] - coef(mod2b)["resid"]
```

Consder the relatinship between `education` on `income`:
```{r}
ggplot(Duncan, aes(x = education, y = income)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

```{r}
Duncan %>%
  select(occupation, education, resid, prestige) %>%
  gather(variable, value, -occupation, -prestige) %>%
  ggplot(aes(x = value, color = variable)) +
    geom_density() +
    geom_rug()

```
```{r}
var(Duncan$education)
var(Duncan$resid)
```


```{r}
Duncan %>%
  select(occupation, education, resid, prestige) %>%
  gather(variable, value, -occupation, -prestige) %>%
  ggplot(aes(x = value, y = prestige, color = variable)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE)

```




## Questions

-   How does the variance of a variable compare the variance of a variable after
    regressing it on other control variables? 
    What is the implication of that result for its standard error? 

-   If two variables are co-linear what is the residual after the first stage?
    How does that explain why you can't estimate a linear regression with collinear variables.

-   Suppose that two variables are uncorrelated. What are the implications for this
    in the first stage and second stage of regression anatomy.
    
-   Consider two hightly correlated variables. What are the implications for this
    in the first stage and second stage of regression anatomy. How does that help
    you to understand regression anatomy.
  
-   Confirm that regression anatomy also works for the coefficient of `income` in the
    previous regressions.
  
-   Conduct regression anatomy for the coefficient of `education` in
    ```r
    lm(prestige ~ education + type, data = Duncan)
    ```
    
-   Does the second stage of regression anatomy produce the correct standard errors? 
    
-   Consider the following variations to regression anatomy:

    -   Regress `prestige ~ income`. Add a column `resid_prestige` with the 
        residuals from that regressin. Regress `resid_prestige ~ resid`.
        Does it work? 
    -   Instead regress `resid_prestige ~ education`. Does it work?
