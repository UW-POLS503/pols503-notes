# Properties of the OLS Estimator

## What makes an estimator good?

> A mathematician, a physicist and a statistician went hunting for deer. When
> they chanced upon one buck lounging about, the mathematician fired first,
> missing the buck's nose by a few inches. The physicist then tried his hand, and
> missed the tail by a wee bit. The statistician started jumping up and down
> saying "We got him! We got him!"

Estimators are evaluated not on how close an estimate in a given sample is to the population, but how their sampling distributions compare to the population.
In other words, judge the *methodology* (estimator), not the *result* (estimate).[^ols-properties-references]

Let $\theta$ be the population parameter, and $\hat\theta$ be an estimator of that population parameter.

Bias

:   The bias of an estimator is the difference between the mean of its sampling distribution and the population parameter, $$\Bias(\hat\theta) = \E(\hat\theta) - \theta .$$

Variance

:   The variance of the estimator is the variance of its sampling distribution, $\Var(\theta)$.

Efficiency (Mean squared error)

:   An efficient estimator is one that minimizes a given "loss function", which is a penalty for missing the population average. The most common loss function is squared loss, which gives the *Mean Squared Error (MSE)* of an estimator.
:   $$\MSE(\hat\theta) = \E\left[{(\hat\theta - \theta)}^{2}\right] =  (\E(\hat\theta) - \theta)^2 + \E(\hat\theta - \E(\hat\theta))^2 = \Bias(\hat\theta)^2 + \Var(\hat\theta)$$
:   The mean squared error is a function of both the bias and variance of an estimator.
:   This means that some biased estimators can be more efficient :   than unbiased estimators if their variance offsets their bias.[^mse]

```{r results='asis',echo=FALSE}
clocks <-
  data.frame(Biased = c("Yes", "No", "Yes", "Yes", "No"),
             Variance = c("High", "High", "Low", "Low", "Low"),
             check.names = FALSE,
             row.names =
               c("Stopped clock",
               "Random clock",
               "Clock that is \"a lot \" fast",
               "Clock that is \"a little\" fast",
               "Atomic clock")
             )
```


Consistency is an asymptotic property[^asymptotic], that roughly states that an estimator converges to the truth as the number of observations grows, $\E(\hat\theta - \theta) \to 0$ as $N \to \infty$.
Roughly, this means that if you had enough (infinite) data, the estimator will give you the true value of the parameter.

### Properties of OLS


| Assumption | Formal statement | Consequence of violation |
|:-------------------|:-------------------|:-----------------------|
| No (perfect) collinearity | $\rank(\mat{X}) = K, K < N$ | Coefficients unidentified |
| $\mat{X}$ is exogenous | $\E(\mat{X} \vec{\varepsilon}) = 0$ | Biased, even as $N \to \infty$ |
| Disturbances have mean 0 | $\E(\varepsilon) = 0$ | Biased, even as $N \to \infty$ |
| No serial correlation | $\E(\varepsilon_i \varepsilon_j) = 0$, $i \neq j$ | Unbiased, wrong se |
| Homoskedastic errors | $\E(\vec{\varepsilon}\T \vec{\varepsilon})$ | Unbiased, wrong se |
| Gaussian errors | $\varepsilon \sim \dnorm(0, \sigma^2)$ | Unbiased, se wrong unless $N \to \infty$ |

<!--
1. Nonlinearity
    - Result: biased/inconsistent estimates
    - Diagnose: scatterplots, added variable plots, component-plus-residual plots
    - Correct: transformations, polynomials, different model
2. iid/random sample
    - Result: no bias with appropriate alternative assumptions (structured dependence)
    - Result (ii): violations imply heteroskedasticity
    - Result (iii): outliers from different distributions can cause inefficiency/bias
    - Diagnose/Correct: next week!
3. Perfect collinearity
    - Result: can't run OLS
    - Diagnose/correct: drop one collinear term
4. Zero conditional mean error
    - Result: biased/inconsistent estimates
    - Diagnose: very difficult
    - Correct: instrumental variables (Gov 2002)
5. Heteroskedasticity
    - Result: SEs are biased (usually downward)
    - Diagnose/correct: next week!
6. Non-Normality
    - Result: critical values for $t$ and $F$ tests wrong
    - Diagnose: checking the (studentized) residuals, QQ-plots, etc
    - Correct: transformations, add variables to $\X$, different model
-->   

Note that these assumptions can be sometimes be written in largely equivalent, but slightly different forms.

When is a variable *endogenous*

1. Omitted variables
2. Measurement error
3. Simultaneity

Assumptions of CLR models

1. No perfect collinearity: No exact linear relationships in the predictors. $X$ is full rank.
2. Linearity: Outcome variable is a linear function of a specific set of independent variables and a disturbance: $$\vec{y} = \mat{X} \vec{\beta} + \vec{\varepsilon}$$.
3. Observations on independent samples can be considered fixed in repeated samples or $X$ is uncorrelated with the errors.
4. Expected value of the disturbance term is zero.
5. Homoskedasticity: Disturbances have the same variance and are uncorrelated: $\var(\varepsilon_i) = \sigma^2$, $\cov(\varepsilon_i, \varepsilon_j) = 0$ for all $i \neq j$.
6. Error terms are distributed normal.


- OLS solution exists with unique $\beta$: 1
- OLS is unbiased and consistent: 1-4
- OLS is best-linear unbiased estimator (BLUE) Gauss-Markov. Large scale inference. 1-5.
- OLS small scale inference: 1-6. Best unbiased estimator (not just among linear)

Why OLS?

- Computational cost: There exists a closed form solution to the OLS estimate and standard errors.
- Least squares loss: OLS minimizes least squared residuals, and thus is optimal for this criteria. Note, that this is only for within sample.
- Hightest R^2: Follows from the previous.
- Unbiased:
- Best unbiased:
- Mean squared error: OLS is **not** the minimum MSE model.
- Asymptotic criteria: Asymptotically unbiased and consitent.
- Maximum likelihood: OLS is equivalent to the MLE estimator for $\beta$.


## References

- Wooldrige, Ch 3.
- Fox, Ch 6, 9.


## Sampling Distribution of OLS Coefficients

## t-tests for single parameters

The sampling distribution of the OLS parameters is
$$
\vec{\beta} \sim \dmvnorm(\vec{beta}, \sigma^2 (\mat{X}' \mat{X})^{-1}).
$$
Thus, the variance of the coefficients is
$$
\Var(\hat{\beta}) = \sigma^2 (\mat{X}' \mat{X})^{-1} .
$$
which is a symmetric matrix,
$$
\Var(\hat{\beta}) =
\begin{bmatrix}
\Var(\hat{\beta}_0) & \Cov(\hat{\beta}_0, \hat{\beta}_1) & \Cov(\hat{\beta}_0, \hat{\beta}_1) & \cdots & \Cov(\hat{\beta}_0, \hat{\beta}_K) \\
\Cov(\hat{\beta}_0, \hat{\beta}_1) & \Var(\hat{\beta}_1) & \Cov(\hat{\beta}_1, \hat{\beta}_2) & \cdots & \Cov(\hat{\beta}_1, \hat{\beta}_K) \\
\Cov(\hat{\beta}_0, \hat{\beta}_2) & \Cov(\hat{\beta}_1, \hat{\beta}_2) & \Cov(\hat{\beta}_2) & \cdots & \Cov(\hat{\beta}_2, \hat{\beta}_K) \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\Cov(\hat{\beta}_0, \hat{\beta}_K) & \Cov(\hat{\beta}_1, \hat{\beta}_K) & \Cov(\hat{\beta}_K) & \cdots & \Var( \hat{\beta}_k)
\end{bmatrix}
$$
On the diagonal are the variances of the parameters, and the off-diagonal elements are the covariances of the parameters.

The null hypothesis and alternative hypotheses for two-sided tests are,
$$
\begin{aligned}[t]
H_0: &\beta_k = \beta_0 \\
H_a: &\beta_k \neq \beta_0
\end{aligned}
$$

Then in large samples,
$$
\frac{\hat{\beta}_k - \beta_k}{\se(\widehat{\beta}_k)} \sim \dnorm(0, 1)
$$
In small samples,
$$
\frac{\hat{\beta}_k - \beta_k}{\se(\widehat{\beta}_k)} \sim \dt{N - (K + 1)}
$$


The estimated standard errors of $\hat{\beta}$ come from
$$
\begin{aligned}[t]
\var(\hat{\vec{\beta}}) &= \hat{\sigma}^2 (\mat{X}' \mat{X})^{-1} \\
\hat{\sigma}^2 &= \frac{\vec{\epsilon}'\vec{epsilon}}{(N - (K + 1))}
\end{aligned}
$$

So, under the common null hypothesis test for $\beta_k = 0$,
$$
\frac{\hat{\beta}_k}{\se(\widehat{\beta}_k)} \sim \dt{N - (K + 1)}
$$

And the confidence intervals for a $(1 - \alpha) \times 100$ confidence interval for  $\hat{\beta}_k$ are,
$$
\hat{\beta}_k \pm t^*_{\alpha / 2} \times \se(\hat{\beta}_K)
$$
where $t^*_{\alpha / 2}$ is the quantile of the $\dt_{n - (K + 1)}$ distribution such that $P(T \leq t^*) > 1 - \alpha / 2$.


## F-tests of Multiple Hypotheses
