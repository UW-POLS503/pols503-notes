---
title: Omitted Variable Bias and Measurement Error
---

# Omitted Variable Bias and Measurement Error

## Omitted Variable Bias


----------------- ------------------- ------------------ ------------------
$\Cov(X_1, X_2)$  $\Cov(X_2, Y) > 0$  $\Cov(X_2, Y) = 0$ $\Cov(X_2, Y) < 0$
----------------- ------------------- ------------------ ------------------
$> 0$             +                   0                  -
$0$               0                   0                  0
$< 0$             -                   0                  +
----------------- ------------------- ------------------ ------------------

### What's the problem?


### What to do about it?

Summary:

1. OVB is intrinsic to observational methods relying on selection on observables---not just regression.
2. Control for all plausible "pre-treatment" variables
2. Reason about possible biases due to OVB
3. Sensitivity of coefficients to inclusion of control variables is an indication of the plausibility of OVB. @AltonjiElderTaber2005. formalize this.

In practice, this is a primary problem of many papers and papers; and for good reason, it biases the coefficient of interest. Reviewers and discussants will often ask about whether you have considered controlling for *foo*. Although these may be legitimate concerns, not all commenters understand the purpose of control variables. There two arguments to consider when addressing these arguments.

1. The omitted variable has to plausibly be correlated with *both* the variable of interest *and* the outcome variable, and the burden is on the commenter to provide at a confounding variable and plausible relationships. Simpy stating that there could be an unobservable variable is trivially true, uninteresting, and not a fatal critique. That said, the evidentiary content of your methods would be higher if you used methods less susceptible to potential unobserved confounders.
2. The omitted variable should be a *good* control and not a "post treatment" variable. If the omitted variable should not be one of the causal pathways by which $X$ affects $Y$, it should not be controlled for. If $Z$ affects the values of $X$ and also affects $Y$, then it needs to be controlled for.

There are two common ways of assessing plausibility.

1. **Informal method**. This is what you see in many empirical papers. Estimate the model including different control variables.
The less sensitive the coefficient(s) of the variables of interest are to the inclusion of control variables, the more plausible it is that the variable of interest is not sensitive to unobserved variables [@AngristPischke2014]. @Oster2013 states

    > A common heuristic for evaluating the robustness of a result to omitted variable bias concerns is to look
    > at the sensitivity of the treatment effect to inclusion of observed controls. In three top general interest
    > economics journals in 2012, 75% of non-experimental empirical papers included such sensitivity analysis. The
    > intuitive appeal of this approach lies in the idea that the bias arising from the observed controls is informative
    > about the bias that arises from the unobserved ones.

    Note that what is important is that the *coefficient* is stable to the inclusion of controls, not that the coefficient remains statistically significant (which seems to be what many authors focus on).

2. **Formal method** Several papers, including @AltonjiElderTaber2005, @BellowsMiguel2009, and @Oster2013, formalize the intuition behind the heuristic of coefficient stability to assess the sensitivity of the treatment to OVB.

@AltonjiElderTaber2005 propose a method for assessing the potential impact of the omitted variable bias as the importance of the omitted variable needed to explain away the entire effect. That work addresses the case for a dichotomous treatment variable, and assumed joint normality. @BellowsMiguel2009 extend the method to continuous treatment variables.

The statistic proposed by @BellowsMiguel2009 is simple,
$$
\delta = \frac{\hat{\beta}_F}{\hat{\beta}_R - \hat{\beta}_C},
$$
where $\delta$ is
$$
\Cov{x, \tilde{w}}{x, w' \gamma}
$$
$\delta$ is interpreted as the how strong the covariance between the unobserved part of the controls and the treatment variable must be relative to the covariance between the observed part of the controls and the treatment variable to explain away the entire effect of $x$ on $y$.
A larger ratio suggests it is implausible that omitted variable bias could explain away the entire observed effect.

Suppose we would like to estimate
$$
y = \beta x + \gamma z
$$
If $z$ is left out of the OLS estimation, then the estimates of $\beta$ will have omitted variable bias,
$$
\plim \hat\beta_{OLS,NC} = \beta + \gamma \frac{\Cov(x, z)}{\Var{x}}
$$
Suppose that instead of $z$ we observe a set of controls $w^*$ that are related to the full set of controls,
$$
z = w' \beta + \omega
$$
See Appendix A of @BellowsMiguel2009 for the derivation.


**TODO** Insert path diagram.

OVB is a intrinsic problem in observational research, and there is nothing you can do to ever ensure that you have controlled for all relevant variables (however, all inference is uncertain, even the designs discussed next, so people should learn to deal with uncertainty).
Also, methods such as matching, propensity scores, or inverse weighting still depend on assumptions about selection on observables, even if they may be less sensitive to certain kinds of modeling assumptions.
The alternative is to use designs which do not require directly controlling for observable differences. Examples of these designs include: experiments (obviously), natural experiments, instrumental variables, and regression discontinuity.

## Measurement Error

### What's the problem?

It biases coefficients:

1. Variable with measurement error: biases $\beta$ towards zero (**attenuation bias**)
2. Other variables: Biases $\beta$ similarly to omitted variable bias. In other words, when a variable has measurement error it is an imperfect control. You can think of omitted variables as the limit of the effect of measurement error as it increases.


### What to do about it?

There's no easy fix within the OLS framework.

1. If the measurement error is in the variable of interest, then the variable will be biased towards zero, and your estimate is too large.
2. Find better measures with lower measurement errors. If the variable is the variable of interest, then perhaps combine multiple variables into a single index. If the measurement error is in the control variables, then include several measures. That these measure correlate closely increases their standard errors, but the control variables are not the object of the inferential analysis.
3. More complicated methods: errors in variable models, structural equation models, instrumental variable (IV) models, and Bayesian methods.
