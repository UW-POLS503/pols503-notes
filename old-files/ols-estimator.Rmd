---
title: "Ordinary Least Squares Estimator"
---

# Linear Regression and the Ordinary Least Squares (OLS) Estimator

Since we will largely be concerned with using linear regression for inference, we will start by discussion the population parameter of interest (population linear regression function), then the sample statistic (sample linear regression function) and estimator (ordinary least squares).

We will then consider the properties of the OLS estimator.

## Linear Regression Function

The **population linear regression function** is
$$
r(x) = \E[Y | X = x] = \beta_0 + \sum_{k = 1}^{K} \beta_{k} x_k .
$$
The population linear regression function is defined for random variables, and will be the object to be estimated.

Names for $\Vec{y}$

- dependent variable
- explained variable
- response variable
- predicted variable
- regressand
- outcome variable

Names for $\Mat{X}$,

- indpendent variables
- explanatory varaibles
- treatment and control variables
- predictor variables
- covariates
- regressors

To estimate the unkonwn population linear regression, we will use the **sample linear regression function**,
$$
\hat{r}(x_i) = \hat{y}_i = \hat\beta_0 + \sum_{k = 1}^{K} \hat\beta_{k} x_k .
$$
However, we

$\hat{Y}_i$ are the fitted or predicted value
The **residuals** or **errors** are the prediction errors of the estimates
$$
\hat{\epsilon}_i = y_i - \hat{y}_i
$$


$\Vec{\beta}$ are the parameters; $\beta_0$ is called the *intercept*, and
$\beta_{1}, \dots, \beta_{K}$ are called the *slope parameters*, or *coefficients*.    

We will then consider the properties of the OLS estimator.

The linear regression can be more compactly written in matrix form,
$$
\begin{aligned}[t]
  \begin{bmatrix}
    y_1 \\
    y_2 \\
    \vdots \\
    y_N
  \end{bmatrix} &=
  \begin{bmatrix}
    1 & x_{1,1} & x_{2,1} & \cdots & x_{K,1} \\
    1 & x_{1,2} & x_{2,2} & \cdots & x_{K,2} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & x_{1,N}& x_{2,n} & \cdots & x_{K,N}
  \end{bmatrix}
  \begin{bmatrix}
    \beta_0 \\
    \beta_1 \\
    \beta_2 \\
    \vdots \\
    \beta_K
    \end{bmatrix}
  +
  \begin{bmatrix}
    \varepsilon_1 \\
    \varepsilon_2 \\
    \vdots \\
    \varepsilon_N
  \end{bmatrix}
\end{aligned} .
$$
More compactly, the linear regression model can be written as,
$$
\begin{aligned}[t]
  \underbrace{\Vec{y}}_{N \times 1} &=
  \underbrace{\Mat{X}}_{N \times K} \,\,
  \underbrace{\Vec{\beta}}_{K \times 1} +
  \underbrace{\Vec{\varepsilon}}_{N \times 1} .
\end{aligned}
$$
The matrix $\Mat{X}$ is called the *design* matrix.
Its rows are each observation in the data.
Its columns are the intercept, a column vector of 1's, and the values of each predictor.

## Ordinary Least Squares

Ordinary least squares (OLS) is an estimator of the slope and statistic of the regression line[^ols-gls].
OLS finds values of the intercept and slope coefficients by minimizing the squared errors,
$$
\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_K
=
\argmin_{b_0, b_1, \dots, b_k} \sum_{i = 1}^{N}  \underbrace{{\left(y_i - b_0 - \sum_{k = 1}^{K} b_k x_{i,k} \right)}^2}_{\text{squared error}},
$$
or, in matrix notation,
$$
\begin{aligned}[t]
\hat{\Vec{\beta}} &= \argmin_{\Vec{b}} \sum_{i = 1}^N (y_i - \Vec{b}\T \Vec{x}_i)^2 \\
&= \argmin_{\Vec{b}} \sum_{i = 1}^N u_i^2 \\
&= \argmin_{\Vec{b}} \Vec{u}' \Vec{u}
\end{aligned}
$$
where $\Vec{u} = \Vec{y} - \Mat{X} \Vec{\beta}$.

In most statistical models, including even generalized linear models such as logit, the solution to this minimization problem would be solved with optimization methods that require iteration.
One nice feature of OLS is that there is a closed form solution for $\hat{\beta}$ even in the multiple regression case, so no iterative optimization methods need to be used.

In the bivariate regression case, the OLS estimators for $\beta_0$ and $\beta_1$ are
$$
\begin{aligned}[t]
\hat{\beta}_0 &= \bar{y} - \hat\beta_1 \bar{x} \\
\hat{\beta}_1 7= \frac{\sum_{i = 1}^N (x_i - \bar{x}) (y_i - \bar{y})}{\sum_{i = 1}^N (x_i - \bar{x})^2} \\
&= \frac{\Cov(\Vec{x} \Vec{y})}{\Var{\Vec{x}}}
&= \frac{\text{Sample covariance betweeen $\Vec{x}$ and $\Vec{y}$}}{\text{Sample variance of $\Vec{x}$}} .
\end{aligned}
$$
In the multiple regression case, the OLS estimator for $\hat{\Vec{\beta}}$ is
$$
\hat{\Vec{\beta}} = \left( \Mat{X}' \Mat{X} \right)^{-1} \Mat{X}' \Vec{y} .
$$
The term $\Mat{X}' \Mat{X}$ is similar to the variance of $\Vec{x}$ in the bivariate case.
The term $\Mat{X}' \Vec{y}$ is similar to the covariance between $\Mat{X}$ and $\Vec{y}$ in the bivariate case.

The sample linear regression function estimated by OLS has the following properties:

1. Residuals sum to zero,
    $$
    \sum_{i = 1}^N \hat{\epsilon}_i = 0 .
    $$
    This implies that the mean of residuals is also 0.
2. The regression function passes through the point $(\bar{\Vec{y}}, \bar{\Vec{x}}_1, \dots, \bar{\Vec{x}_K})$.
    In other words, the following is always true,
    $$
    \bar{\Vec{y}} = \hat\beta_0 + \sum_{k = 1}^K \hat\beta_k \bar{\Vec{x}}_k .
    $$
3. The residuals are uncorrelated with the predictor
    $$
    \sum_{i = 1}^N x_i \hat{\epsilon}_i = 0
    $$
4. The residuals are uncorrelated with the fitted values
    $$
    \sum_{i = 1}^N \hat{y}_i \hat{\varepsilon}_i = 0
    $$


## Properties of the OLS Estimator

### What makes an estimator good?

Estimators are evaluated not on how close an estimate in a given sample is to the population, but how their sampling distributions compare to the population.
In other words, judge the *methodology* (estimator), not the *result* (estimate).[^ols-properties-references]

Let $\theta$ be the population parameter, and $\hat\theta$ be an estimator of that population parameter.

Bias

:   The bias of an estimator is the difference between the mean of its sampling distribution and the population parameter, $$\Bias(\hat\theta) = \E(\hat\theta) - \theta .$$

Variance

:   The variance of the estimator is the variance of its sampling distribution, $\Var(\theta)$.

Efficiency (Mean squared error)

:   An efficient estimator is one that minimizes a given "loss function", which is a penalty for missing the population average. The most common loss function is squared loss, which gives the *Mean Squared Error (MSE)* of an estimator.
:   $$\MSE(\hat\theta) = \E\left[{(\hat\theta - \theta)}^{2}\right] =  (\E(\hat\theta) - \theta)^2 + \E(\hat\theta - \E(\hat\theta))^2 = \Bias(\hat\theta)^2 + \Var(\hat\theta)$$
:   The mean squared error is a function of both the bias and variance of an estimator.
:   This means that some biased estimators can be more efficient :   than unbiased estimators if their variance offsets their bias.[^mse]

```{r results='asis',echo=FALSE}
clocks <-
  data.frame(Biased = c("Yes", "No", "Yes", "Yes", "No"),
             Variance = c("High", "High", "Low", "Low", "Low"),
             check.names = FALSE,
             row.names =
               c("Stopped clock",
               "Random clock",
               "Clock that is \"a lot \" fast",
               "Clock that is \"a little\" fast",
               "Atomic clock")
             )
```


Consistency is an asymptotic property[^asymptotic], that roughly states that an estimator converges to the truth as the number of observations grows, $\E(\hat\theta - \theta) \to 0$ as $N \to \infty$.
Roughly, this means that if you had enough (infinite) data, the estimator will give you the true value of the parameter.

### Properties of OLS


| Assumption | Formal statement | Consequence of violation |
|:-------------------|:-------------------|:-----------------------|
| No (perfect) collinearity | $\rank(\Mat{X}) = K, K < N$ | Coefficients unidentified |
| $\Mat{X}$ is exogenous | $\E(\Mat{X} \Vec{\varepsilon}) = 0$ | Biased, even as $N \to \infty$ |
| Disturbances have mean 0 | $\E(\varepsilon) = 0$ | Biased, even as $N \to \infty$ |
| No serial correlation | $\E(\varepsilon_i \varepsilon_j) = 0$, $i \neq j$ | Unbiased, wrong se |
| Homoskedastic errors | $\E(\Vec{\varepsilon}\T \Vec{\varepsilon})$ | Unbiased, wrong se |
| Gaussian errors | $\varepsilon \sim \dnorm(0, \sigma^2)$ | Unbiased, se wrong unless $N \to \infty$ |

<!--
1. Nonlinearity
    - Result: biased/inconsistent estimates
    - Diagnose: scatterplots, added variable plots, component-plus-residual plots
    - Correct: transformations, polynomials, different model
2. iid/random sample
    - Result: no bias with appropriate alternative assumptions (structured dependence)
    - Result (ii): violations imply heteroskedasticity
    - Result (iii): outliers from different distributions can cause inefficiency/bias
    - Diagnose/Correct: next week!
3. Perfect collinearity
    - Result: can't run OLS
    - Diagnose/correct: drop one collinear term
4. Zero conditional mean error
    - Result: biased/inconsistent estimates
    - Diagnose: very difficult
    - Correct: instrumental variables (Gov 2002)
5. Heteroskedasticity
    - Result: SEs are biased (usually downward)
    - Diagnose/correct: next week!
6. Non-Normality
    - Result: critical values for $t$ and $F$ tests wrong
    - Diagnose: checking the (studentized) residuals, QQ-plots, etc
    - Correct: transformations, add variables to $\X$, different model
-->   

Note that these assumptions can be sometimes be written in largely equivalent, but slightly different forms.

When is a variable *endogenous*

1. Omitted variables
2. Measurement error
3. Simultaneity

Assumptions of CLR models

1. No perfect collinearity: No exact linear relationships in the predictors. $X$ is full rank.
2. Linearity: Outcome variable is a linear function of a specific set of independent variables and a disturbance: $$\Vec{y} = \Mat{X} \Vec{\beta} + \Vec{\varepsilon}$$.
3. Observations on independent samples can be considered fixed in repeated samples or $X$ is uncorrelated with the errors.
4. Expected value of the disturbance term is zero.
5. Homoskedasticity: Disturbances have the same variance and are uncorrelated: $\Var(\varepsilon_i) = \sigma^2$, $\Cov(\varepsilon_i, \varepsilon_j) = 0$ for all $i \neq j$.
6. Error terms are distributed normal.


- OLS solution exists with unique $\beta$: 1
- OLS is unbiased and consistent: 1-4
- OLS is best-linear unbiased estimator (BLUE) Gauss-Markov. Large scale inference. 1-5.
- OLS small scale inference: 1-6. Best unbiased estimator (not just among linear)

Why OLS?

- Computational cost: There exists a closed form solution to the OLS estimate and standard errors.
- Least squares loss: OLS minimizes least squared residuals, and thus is optimal for this criteria. Note, that this is only for within sample.
- Hightest R^2: Follows from the previous.
- Unbiased:
- Best unbiased:
- Mean squared error: OLS is **not** the minimum MSE model.
- Asymptotic criteria: Asymptotically unbiased and consitent.
- Maximum likelihood: OLS is equivalent to the MLE estimator for $\beta$.


## References

- @Wooldridge2013a, Ch 3.
- @Fox2016a, Ch 6, 9.

<!-- Footnotes -->

[^ols-properties-refences]: This section draws materials from  Chris Adolph's 503 slides [Linear Regression in Matrix Form / Properties & Assumption of Linear Regression](http://faculty.washington.edu/cadolph/503/topic4.pw.pdf).

[^clocks]: Example from [Chris Adolph](http://faculty.washington.edu/cadolph/503/topic3.pw.pdf)

[^mse]: It follows from the definition of MSE, that biased estimator, $\hat\theta_{B}$, has a lower MSE than an unbiased estimator, $\hat\theta_{U}$, if $\Bias(\theta_B)^2 < \Var(\theta_U) - \Var(\theta_B)$.

[^asymptotic]: As the number of observations goes to infinity.

[^ols-gls]: Ordinary least squares is distinguished from, and a special case of *generalized least squares* (GLS), which adds an additional $N \times N$ matrix to the objective function,
            $$
            \hat{\beta}_{WLS} = \argmin_{\Vec{b}} \sum_{i = 1}^N (y_i - \Vec{x}\T \Vec{b})\T \Omega (y_i - \Vec{x}\T \Vec{b}) .
            $$
            Weighted least squares (WLS) is another special case of GLS.
