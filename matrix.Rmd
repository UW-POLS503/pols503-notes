# Matrix Algebra Review

## Vectors

-   A *vector* is a list of numbers or random variables.

-   A $1 \times k$ *row vector* is arranged
    $$
    b = \begin{bmatrix} b_1 & b_2 & b_3 & \dots & b_k \end{bmatrix}
    $$
-   A $1 \times k$ *column vector* is arranged
    $$
    a = \begin{bmatrix} b_1 \\ b_2 \\ b_3\\ \dots \\ b_k \end{bmatrix}
    $$
-    Convention: assume vectors are *column* vectors

Vector Examples

-   Vector of all covariates for a particular unit $i$.
    $$
    x_i = \begin{bmatrix}
    1 \\ x_{i1} \\ x_{i2} \\ \dots \\ x_{ik}
    \end{bmatrix}
    $$

## Matrices

-   A **matrix** is a rectangular array of numbers
-   A matrix is $n \times k$ ("$n$ by $k$") if it has $n$ rows and $k$ columns
-   A matrix
    $$
    A = \begin{bmatrix}
    a_{11} & a_{12} & \dots & a_{1k} \\
    a_{21} & a_{22} & \dots & a_{2k} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{n1} & a_{n2} & \dots & a_{nk}
    \end{bmatrix}
    $$

### Examples of matrices

The **design matrix** is the matrix of predictors/covariates in a regression:
$$
    X = \begin{bmatrix}
    1 & x_{11} & x_{12} & \dots & a_{1k} \\
    1 & x_{21} & x_{22} & \dots & a_{2k} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & x_{n1} & a_{n2} & \dots & a_{nk}
    \end{bmatrix}
$$
The vector of ones is the constant.

Use R function `model.matrix` to create the design matrix from a formula and a data frame.

## Matrix Operations

### Transpose

The **transpose** of a matrix $A$ flips the rows and columns. It is denoted $A'$ or $A^{T}$.

The transpose of a $3 \times 2$ matrix is a $2 \times 3$ matrix,
$$
A = \begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
a_{31} & a_{32} 
\end{bmatrix}  =
\begin{bmatrix}
a_{11} & a_{21}  & a_{31} \\
a_{12} & a_{22}  & a_{32}
\end{bmatrix}
$$

### Transposing vectors

Transposing turns a $1 \times k$ row vector into a $k \times 1$ column vector and vice-versa.

$$
\begin{aligned}[t]
x_i &= \begin{bmatrix}
1 \\
x_{i1} \\
x_{i2} \\
\vdots \\
x_{ik}
\end{bmatrix} &

x_i' &= 
\begin{bmatrix}
1 &
x_{i1} &
x_{i2} &
\dots &
x_{ik}
\end{bmatrix} 
\end{aligned}
$$

```{r}
A <- matrix(1:6, ncol = 3, nrow = 2)
A
t(A)
a <- 1:6
t(a)
```


How does $X$ relate to the model specification? 
See the `model.matrix`

What is the variance covariance matrix? 


The OLS estimator of coefficients is
$$
\hat{\beta} = \underbrace{(X' X)^{-1}}_{Var(X)} \underbrace{X' y}_{Cov(X, Y)}
$$

### OLS in Matrix Form

$$
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_k x_{ik} + \epsilon_i
$$
We can write it as
$$
\begin{aligned}[t]
Y_i &= 
\begin{bmatrix}
1 & x_{i1} & x_{i2} & \dots & x_{ik} 
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_k
\end{bmatrix} 
+ \epsilon_i \\
&= \underbrace{{x_i'}}_{1 \times k + 1} \underbrace{\beta}_{k + 1 \times 1} + \epsilon_i
\end{aligned}
$$

$$
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_k x_{ik} + \epsilon_i
$$
We can write it as
$$
\begin{aligned}[t]
\begin{bmatrix}
Y_1 \\
Y_2 \\
\vdots \\
Y_n
\end{bmatrix} 
&= 
\begin{bmatrix}
1 & x_{11} & x_{12} & \dots & x_{1k}  \\
1 & x_{21} & x_{22} & \dots & x_{2k}  \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \dots & x_{nk} 
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_k
\end{bmatrix} 
+ \begin{bmatrix} 
\epsilon_1 \\
\epsilon_2 \\
\vdots \\
\epsilon_n 
\end{bmatrix} \\
\underbrace{y}_{(n \times 1)} &= 
\underbrace{{x_i'}}_{(n \times k + 1)} \underbrace{\beta}_{(k + 1 \times 1)} +
\underbrace{\epsilon}_{(n \times 1)}
\end{aligned}
$$

The regression standard error of the regression is
$$
\hat{\sigma}_y^2 = \frac{\sum_i^n \epsilon_i^2}{n - k - 1}
$$
Write this using matrix notation.


Note that 
$$
E(X_i)^2 = \frac{\sum X_i^2}{n}
$$
In matrix notation this is,
$$
\begin{bmatrix}
x_1 & x_2 & \dots & x_n
\end{bmatrix} 
\begin{bmatrix} 
x_1 \\ x_2 \\ \dots \\ x_n
\end{bmatrix} =
x' x
$$
If $\bar{X} = 0$, then 
$$
\frac{X' X}{N} = Var(X)
$$

What is the vcov matrix of $\beta$? 
When would it be diagonal?
What is on the off-diagonal?
What is on the diagonal? 
Extract the standard errors from it.

OLS Standard errors
$$
\hat{\beta}_{OLS} = (X' X)^{-1} X' y
$$


$$
V(\hat{\beta}) =
\begin{bmatrix}
V(\hat{\beta}_0) & Cov(\hat{\beta}_0, \hat{\beta}_1) & \dots & Cov(\hat{\beta}_0, \hat{\beta}_k) \\
Cov(\hat{\beta}_1, \hat{\beta}_0) & V(\hat{\beta}_1) & \dots & Cov(\hat{\beta}_1, \hat{\beta}_k) \\
\vdots & \vdots & \ddots & \vdots \\
Cov(\hat{\beta}_k, \hat{\beta}_0) & Cov(\hat{\beta}_k, \hat{\beta}_1) & \dots & V(\hat{\beta}_k) \\
\end{bmatrix}
$$

Which of these matrices are 

1. Homoskedastic
2. Heteroskedastic
3. Clustered standard errors
4. Serially correlated


Show how $(X' X)^{-1} X' y$ is equivalent to the bivariate estimator.

1. Write out $\beta$ and plug in for the true $Y$ in terms of $X$ and $\epsilon$
2. Take the variance of $\hat{\beta} - \beta$

$$
\hat{\beta} = \beta + (X' X)^{-1} X' \epsilon \\
\Var(\hat{\beta} - \beta) =  var((X' X)^{-1} X' \epsilon) \\
$$
We know that 
- $(X' X)^{-1} X' \epsilon$ has mean zero since $E(X' \epsilon) = 0$.
- $var(z) = E(Z^2) - 0$
- In matrix form $Z Z'$ to get full matrix form


$$
V((X' X)^{-1} X' \epsilon) = (X' X)^{-1} X' \epsilon \epsilon' X (X' X)^{-1} = (X' X)^{-1} X' \Sigma X (X' X)^{-1}
$$
We need a way to estimate $\hat{\Sigma}$. 
But it has $n (n + 1) / 2$ elements ... and we have only $n$ observations, and $n - k - 1$ degrees of freedom left after estimating the coefficients.


If homoskedasticity, $\Sigma = \sigma^2 I$.
$$
V((X' X)^{-1} X' \epsilon) = \sigma^2 (X' X)^{-1} 
$$

- Panel of countries. Correlation within each year that is always the same

# OLS Assumptions

1. Linearity: $y_i = x'_i \beta + u_i$
2. Random/iid sample: $(y_i, x'_i)$ are a iid sample from population
3. No perfect collinearity: $X$ is an $n \times (k + 1)$ matrix with rank $k + 1$
4. Zero conditional mean $E(u_i | x_i) = 0$
5. Homoskedasticity: $V(u_i | x_i) = \sigma_u^2$
6. Normality $u_i | x_i \sim N(0, \sigma_u^2)$


- Unbiasedness: 1--5 in large samples


Gauss-Markov

Suppose  that
$$
y = X \beta + \epsilon
$$
where $y, \epsilon \in R^n$, $\beta \in R^K$ and $X \in R^{n \times K}$.

With the following assumptions on the errors,

- mean zero $E(\epsilon_i | x_i) = 0$
- homoskedastic $Var(\epsilon_i) = \sigma^2 < \infty$
- uncorrelated $Cov(\epsilon_i, \epsilon_j) = 0$ for all $i \neq j$.

Then OLS is BLUE: "best linear unbiased estimator"

-   linear estimator: estimator can be written as a weighted sum of the responses. OLS is a linear estimator since
    $$
    \hat{\beta} = \underbrace{(X' X)^{-1} X'}_{\text{weight}} y
    $$
- unbiased: $E(\hat{\beta} - \beta) = 0$
- best: of all the unbiased linear estimators it has the lowest variance, $V(\beta)$.
