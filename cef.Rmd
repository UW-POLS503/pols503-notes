# What is Regression?

```{r message=FALSE}
library("tidyverse")
library("modelr")
library("stringr")
```


## Joint vs. Conditional models

In most problems, the researcher is concerned with *relationships* between multiple variables. For example, suppose that we want to model the relationship between two variables, $Y$ and $X$. There are two main approaches to modeling this relationship.

1. **joint model:** Jointly model $Y$ and $X$ as $f(Y, X)$. For example, we can model $Y$ and $X$ as coming from a bivariate normal distribution.[^generative]
2. **conditional model:** Model $Y$ as a conditional function of $X$. This means we calculate a function of $Y$ for each value of $X$.[^discriminative] Most often we focus on modeling a conditional statistic of $Y$, and linear regression will focus on modeling the conditional mean of $Y$, $\E(Y | X)$.

regression (conditional) model
:  $p(y | x_1, \dots, x_k) = f(x_1, \dots, x_k)$ and $x_1, \dots, x_k$ are given.
  
joint model
:   $p(y, x_1, \dots, x_k) = f(y, x_1, \dots, x_k)$
  
If we knew the joint model we could calculate the conditional model, 
$$
(y | x_1, \dots, x_k) = \frac{p(y, x_1, \dots, x_k)}{p(x_1, \dots, x_k)} .
$$ 
However, especially when there are specific outcome variables of interest, the *conditional model*, i.e. regression, is easier because the analyst can focus on modeling how  $Y$ varies with respect to $X$, without necessarily having to model the process by which $X$ is generated.
However, that very convenience of not modeling the process which generates $X$ will be the problem when regression is used for causal inference on observational data.

[^generative]: In machine learning, these are called [generative models](https://en.wikipedia.org/wiki/Generative_model).

[^discriminative]: In machine learning, these are called [discriminative models](https://en.wikipedia.org/wiki/Discriminative_model).

At its most general, "*regression analysis*, broadly construed, traces the distribution of a response variable (denoted by $Y$)---or some characteristic of this distribution (such as its mean)---as a function of one of more explanatory variables [@Fox2016a, p. 15]." is a procedure that is used to summarize *conditional* relationships. That is, the average value of an outcome variable conditional on different values of one or more explanatory variables.

While this section will generally cover linear regression, it is not the only form of **regression**. So let's start with a definition of regression.

Most generally, a regression represents a function of a variable, $Y$ as a function of another variable or variables, $X$, and and error.
$$
g(Y_i) = f(X_i) + \text{error}_i
$$

The **conditional expectation function** (CEF) or **regression function** of $Y$ given $X$ is denoted,
$$
\mu(x) = \E\left[Y | X = x\right]
$$

But if regression represents $Y$ as a function of $X$, what's the alternative? 
Instead of modeling $Y$ as a function of $X$, we could jointly model both $Y$ and $X$. 
A regression model of $Y$ and $X$ would be multivariate function, $f(Y, X)$.
In machine learning these approaches are sometimes called **descriminative** (regression) and **generative** (joint models) models.


# Bivariate Regression Model

Given two vectors, $\vec{y} = (y_1, y_2, ..., y_n)$, and $\vec{x} = (x_1, x_2, ..., x_n)$, the **regression line** is,
$$
\E(y | y) = \hat{y_i} = \hat\beta_0 + \hat{\beta} x_i
$$
where $\hat{y_i}$ is called the **fitted value**,
and the residual is
$$
\hat{\epsilon}_i = y_i - \hat{y}_i
$$

The OLS estimator for $\hat{\vec{\beta}}$ finds the values of $\hat{\beta}_0$ and $\hat{\beta}_1$ that minimize the sum of squared residuals of the regression line,
$$
\hat{\beta}_0, \hat{\beta}_1 = \argmin_{b_0, b_1}\sum_{i = 1}^n {( y_i - b_0 - b_1 x_i )}^2 = \argmin_{b_0, b_1}\sum_{i = 1}^n \hat{\epsilon}_i^2
$$

Terminology

- **esitmated coefficients:** $\hat\beta_0$, $\hat\beta_1$
- **estimated intercept:** $\hat\beta_0$
- **estimated slope:** $\hat\beta_1$
- **predicted values, fitted values:** $\hat\y_i$
- **residuals, prediction errors:** $\hat\epsilon_i = y_i - \hat{y}_i$


The solution to that minimization problem is the following closed-form solution[^closedform],
$$
\begin{aligned}
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat{\beta}_1 &= \frac{\sum_{i = 1}^N (x_i - \bar{x})^2 (y_i - \bar{y})^2}{\sum_{i = 1}^n (x_i - \bar{x})^2} 
\end{aligned}
$$

[^closedform]: A closed form solution is one that can be expressed in terms of simple functions.
  Many other statistical estimators do not have closed-form solutions and must be solved numerically.

Properties of OLS

1. The regression line goes through the means of $X$ and $Y$, $(\hat{y}, \hat{x})$.
2. The sum (and mean) of the errors are zero,
    $$
    0 = \sum_{i =1}^n \hat\epsilon_i = \sum_{i =1}^n (y_i - \hat\beta_0 - \hat\beta_1 x_i)
    $$
    This is a mechanical property and a direct consequence of finding the minimum sum of squared errors.
3.  The slope coefficient is the ratio of the covariance of $X$ and $Y$ to the variance of $X$,
    $$
    \begin{aligned}[t]
    \hat{\beta}_1 &= \frac{\sum_{i = 1}^N (x_i - \bar{x})^2 (y_i - \bar{y})^2}{\sum_{i = 1}^n (x_i - \bar{x})^2} \\
     &= \frac{\Cov(x_i, y_i)}{\Var(x_i)}
    \end{aligned}
    $$
4. The slope coefficient is the scaled correlation between $X$ and $Y$,
    $$
    \begin{aligned}[t]
    \hat{\beta}_1 &= \frac{\Cov(x_i, y_i)}{\Var(x_i)} \\
    &= \frac{\sd_x \sd_y \Cor(x, y)}{\Var(x_i)} \\
    &= \frac{\sd_y}{\sd_x} \Cor(x, y)
    \end{aligned}
    $$

### OLS is the weighted sum of outcomes

In OLS, the coefficients are weighted averages of the dependent variables,
$$
\hat{\beta}_1 = \sum_{i = 1}^n \frac{(x_i - \bar{x})}{{(x_i - \bar{x})}^2} = \sum_{i = 1}^{n} w_i y_i
$$
where the weights are,
$$
w_i = \frac{x_i - \bar{x}}{\sum_{i = 1}^n (x_i - \bar{x})^2}
$$

Alternatively, we can rewrite the estimation error (difference between the parameter, $\beta$, and the estimate, $\hat{\beta}$) as a weighted sum of the errors,
$$
\hat\beta_1 - \beta_1  = \sum_{i = 1}^n w_i \epsilon_i
$$

**note** Linear regression is **linear** not because $y = b_0 + b_2$, but because the predictions can be represented as weighted sums of outcome, $\hat{y} = w_i y_i$. If we were to estimate $\hat{\beta}_0$ or $\hat{\beta}_1$ with a different objective function, then it would not longer be linear.

# Covariance and Correlation

Plot of covariance and correlation

The population covariance for random variables $X$ and $Y$ is,
$$
\Cov(X, Y) = \E[(X - \mu_x)(Y - \mu_y)]
$$

The sample covariance for vectors $\vec{x}$ and $\vec{y}$ is,
$$
\Cov(x_i, y_i) = \frac{1}{n} \sum_{i = 1}^n (x_i - \bar{x})(y_i - \bar{y})
$$

Some properties of covariance are:

- $\Cov(X, X) = \Var(X, X)$. Why?
- $\Cov(X, Y)$ has a domain of $(-\infty, \infty)$. What would a covariance of $-\infty$ look like? of $\infty$? of 0?

Like variance is defined on the scale of the squared variable ($X^2$), the covariance is defined on the scale of the product of the variables ($X Y$), which makes it difficult to interpret.

However unlike taking the square root of the variables, in correlation is standardized to a domain of $[-1, 1]$.
$$
\Cor(X, Y) = \frac{\E[(X - \mu_x)(Y - \mu_y)]}{\sigma_x \sigma_y}
$$
$$
\Cor(x_i, y_i) = \frac{\sum_{i = 1}^n (x_i - \bar{x})(y_i - \bar{y})}{s_x s_y}
$$

# Goodness of Fit

What measures do we have for how well a line fits the data?

## Root Mean Squared Error and Standard Error

The first is the mean squared error,
$$
MSE = \frac{1}{n} \sum_{i = 1}^n \hat\epsilon_i
$$
or root mean squared error,
$$
RMSE = \sqrt{\frac{1}{n} \sum_{i = 1}^n \hat\epsilon_i}
$$

The standard error, $\hat\sigma$ is similar, but is an estimator of the standard deviation of the population errors, $\epsilon_i \sim N(0, \sigma)$
$$
\hat\sigma = MSE = \sqrt{\frac{1}{n - k - 1} \sum_{i = 1}^n \hat\epsilon_i}
$$
where $k + 1$ is the number of coefficients (including the intercept) in the regression. 
In the simple (bivariate) regression model $k = 1$, since there is one variable in addition to the intercept.

The only difference between the $RMSE$ and $\sigma\hat$ is the denominator; $\sigma\hat$ adjusts for the degrees of freedom. As the sample size gets large relative to the number of variables,  $n - k \to \infty$, the standard error of the regression approaches the MSE, since $1 / (n - k - 1) \to 1 / n$.


## R squared



The coefficient of determination, $R^2$, 



- If $R^2 = 1$, then $SSE = 0$, and all points of $y_i$  fall on a straight line.
    However, if all values of $y_i$ are equal ($SSE = SST = 0$), then the $R^2$ is undefined.
- If $R^2 = 0$, then there is no relationship. $SSE = SST$, meaning that including $x_i$ does not
    reduce the residuals any more than using the mean of $\vec{y}$.
- In the bivariate regression case,
    $$
    R^2 = \cor(x, y)^2 ,
    $$
    hence its name (since $r$ is the letter usually used to indicate correlation).
- In the more general case, $R^2$ is the squared correlation between the outcome and the fitted values of the regression, 
    $$
    R^2 = \cor(\vec{y}, \hat{\vec{y}}).
    $$

The common interpretation of $R^2$ is "the fraction of variation in $y_i$ that is explained by the regression ($x_i$)."
In this context, "explained" should **not** be interpreted a "caused."

Consider the 
$$
Y = a X + \epsilon
$$
The variance of $Y$ is $a^2 \Var(X) + \Var(\epsilon)$ (supposing $\cor(X, \epsilon) = 0$ ).
The "variance explained" by the regression is simply $a^2 \var(x)$, and 
$$
R^2 = \frac{a^2 \var(X)}{a^2 \var(X) + \var(\epsilon)}
$$
As the variation in $X$ gets large, $\var(X) \to \infty$, then the regression "explains" everything, $R^2 \to 1$, and as the variance in $X$ gets small, $\var(X)  \to 0$, then the regression explains nothing, $R^2 \to 0$.


## Maximum Likelihood

The OLS estimator of linear regression finds $\beta_0$ and $\beta_1$ by minimizing the squared error. 
One nice property of this estimator is that it agrees with the maximum likelihood estimator of the  coefficients.[^mle-sigma]
[Maximum likelihood estimation (MLE)](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) is a general statistical estimator.
It finds parameters by doing what its name says, maximizing a likelihood function.
A likelihood function, $f(\vec{y} | \vec{\theta})$, is the probability of observing the data, $\vec{x}$, *given* parameter values, $\vec{y}$.
The MLE value of the parameters, $\hat{\vec{\theta}}$, is the value of the parameters that maximizes the probability of observing the data.
While no distributional assumptions were needed to calculate the OLS estimator (though some are needed for inference in small samples), the MLE requires specifying distributions for the data.
In linear regression, we assume the following model,
$$
\begin{aligned}[t]
Y_i &= \beta_0 + \beta_1 X_i + \epsilon_i
\end{aligned}
$$
where $\epsilon_i \sim N(0, \sigma^2)$. That must assume the errors are distributed normally in order to calculate the estimates of $\vec{\beta}$ is different than OLS.
Then the MLE function is,
$$
\begin{aligned}[t]
\hat\beta_0^{(MLE)}, \hat\beta_1^{(MLE)}, \hat\sigma^{(MLE)} &= \argmax_{\beta_0, \beta_1, \sigma} \sum_{i = 1}^n \frac{1}{\sqrt{2 \sigma^2 \pi}} \exp \left( - \frac{1}{2 \sigma^2} (y_i - \beta_0 - \beta_1 x_i)^2 \right) \\
&= \argmax_{\beta_0, \beta_1, \sigma} \sum_{i = 1}^n \frac{1}{\sqrt{2 \sigma^2 \pi}} \exp \left( - \frac{1}{2 \sigma^2} \cdot \epsilon^2 \right)
\end{aligned}
$$
For numerical reasons [^ll], in practice, the log-likelihood maximized instead of the likelihood,
$$
\begin{aligned}[t]
\hat\beta_0^{(MLE)}, \hat\beta_1^{(MLE)}, \hat\sigma^{(MLE)} &= \argmax_{\beta_0, \beta_1, \sigma} -\frac{n}{2} \log \sigma^2 - \sum_{i = 1}^n \left( - \frac{1}{2 \sigma^2} (y_i - \beta_0 - \beta_1 x_i)^2 \right) , \\
&= \argmax_{\beta_0, \beta_1, \sigma} -\frac{n}{2} \log \sigma^2 - \sum_{i = 1}^n \left( - \frac{1}{2 s^2} \cdot \epsilon^2 \right) .
\end{aligned}
$$

Even though the estimators are different, both MLE and OLS will produce the same linear regression estimates for $\beta_0$ and $\beta_1$,
$$
\hat\beta_0^{(MLE)} =  \hat\beta_0^{(OLS)} \text{ and } \hat\beta_1^{(MLE)} =  \hat\beta_1^{(OLS)} .
$$

Some intuition as to why the OLS and MLE estimates agree can be gained from noticing that the likelihood function of the normal distribution includes the negative sum of squared errors, so maximizing the likelihood, minimizes the squared errors.

That is all that will be said about MLE for now, since it is not necessary for most of the material on linear models.
But MLE is perhaps the most commonly used to estimator and will reappear many times, notably with generalized linear models, e.g. logit, probit, binomial, Poisson models.

[^ll]: Probabilities can get quite small, so multiplying them together can result in numbers too small to represent as different than zero. Adding the logarithms of probabilities can represent much smaller floating point numbers.

[^mle-sigma]: However, the MLE estimator of the regression standard error is not the same as the OLS estimator, $\hat\sigma_{MLE} \neq \hat\sigma_{OLS}$.

# Anscombe quartet

```{r}
anscombe_tidy <-
  anscombe %>%
  mutate(obs = row_number()) %>%
  gather(variable_dataset, value, -obs) %>%
  separate(variable_dataset, c("variable", "dataset"), sep = c(1)) %>%
  spread(variable, value) %>%
  arrange(dataset, obs)

```

What are summary statistics of the four Anscombe datasets?
```{r}
ggplot(anscombe_tidy, aes(x = x, y = y)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~ dataset, ncol = 2)
```

What are the mean, standard deviation, correlation coefficient, and regression
coefficients of each line? 
```{r}
anscombe_summ <-
  anscombe_tidy %>%
  group_by(dataset) %>%
  summarise(
    mean_x = mean(x),
    mean_y = mean(y),
    sd_x = sd(x),
    sd_y = sd(y),
    cov = cov(x, y),
    cor = cor(x, y),
    coefs = list(coef(lm(y ~ x, data = .)))
  ) %>%
  mutate(
    intercept = map_dbl(coefs, "(Intercept)"),
    slope = map_dbl(coefs, "x")
  ) %>%
  select(-coefs)

```

WUT? They are the same? But they look so different. Of course that was the point ...

Since this all revolves around covariance, lets calculate the values of $x_i - \bar{x}$,
$y_i - \bar{y}$, and $(x_i - \bar{x}) (y_i - \bar{y})$ for each obs for each variable.
```{r}
anscombe_tidy <-
  anscombe_tidy %>%
  group_by(dataset) %>%
  mutate(mean_x = mean(x),
         diff_mean_x = x - mean_x,
         mean_y = mean(y),
         diff_mean_y = y - mean_y,
         diff_mean_xy = diff_mean_x * diff_mean_y,
         quadrant = 
           if_else(
             diff_mean_x > 0, 
             if_else(diff_mean_y > 0, 1, 2),
             if_else(diff_mean_y > 0, 4, 3),
           ))
```


```{r}
ggplot(anscombe_tidy, aes(x = x, y = y,
                          size = abs(diff_mean_xy),
                          colour = factor(sign(diff_mean_xy)))) +
  geom_point() +
  geom_hline(data = anscombe_summ,
             aes(yintercept = mean_y)) +
  geom_vline(data = anscombe_summ,
             aes(xintercept = mean_x)) +  
  facet_wrap(~ dataset, ncol = 2)
```

```{r}
ggplot(anscombe_tidy, aes(x = x, y = y, colour = factor(sign(diff_mean_xy)))) +
  geom_point() +
  geom_segment(mapping = aes(xend = mean_x, yend = mean_y)) +
  geom_hline(data = anscombe_summ,
             aes(yintercept = mean_y)) +
  geom_vline(data = anscombe_summ,
             aes(xintercept = mean_x)) + 
  facet_wrap(~ dataset, ncol = 2)
```

```{r}
ggplot(anscombe_tidy, aes(x = x, y = y, colour = factor(sign(diff_mean_xy)))) +
  geom_point() +
  geom_segment(mapping = aes(xend = x, yend = mean_y)) +
  geom_segment(mapping = aes(xend = mean_x, yend = y)) +  
  geom_hline(data = anscombe_summ,
             aes(yintercept = mean_y)) +
  geom_vline(data = anscombe_summ,
             aes(xintercept = mean_x)) +  
  facet_wrap(~ dataset, ncol = 2)

```

<a title="By DenisBoigelot, original uploader was Imagecreator (Own work, original uploader was Imagecreator) [CC0], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File%3ACorrelation_examples2.svg"><img width="256" alt="Correlation examples2" src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/256px-Correlation_examples2.svg.png"/></a>

```{r echo=FALSE}
# TODO: translate the original R code to produce a dataset and 
# reproduce plot in ggplot2
library("mvtnorm")
mv_normal <- function(cor = 0.8, n = 1000) {
    out <- as_tibble(rmvnorm(n, c(0, 0), matrix(c(1, cor, cor, 1), ncol = 2)))
    colnames(out) <- c("x", "y")
    tibble(title = str_c("Bivariate Normal (rho = ", cor, ")"),
           cor = round(cor(out$x, out$y), 1),
           data = list(out))
}

rotate <- function(t, X) {
  out <- as_tibble((X %*% matrix(c(cos(t), sin(t), -sin(t), cos(t)), ncol = 2)))
  colnames(out) <- c("x", "y")
  tibble(title = str_c("Rotation (", t, ")"),
         cor = round(cor(out$x, out$y), 1),
         data = list(out))
}

line_rotation <- function(t, n = 1000) {
  X <- rmvnorm(n, c(0, 0), matrix(c(1, 1, 1, 1), ncol = 2))
  out <- as_tibble((X %*% matrix(c(cos(t), sin(t), -sin(t), cos(t)), ncol = 2)))
  colnames(out) <- c("x", "y")
  tibble(title = str_c("Rotation (", t, ")"),
         cor = if (t == pi / 4) NA else round(cor(out$x, out$y), 1),
         data = list(out))  
}

wave <- function(n = 1000) {
  x <- runif(n, -1, 1)
  y <- 4 * (x ^ 2 - 1 / 2) ^ 2 + runif(n, -1, 1) / 3
  tibble(title = "y = 4 * (x ^ 2 - 1) / 2 ^ 2 + U(-1/3, 1/3)",
         cor = round(cor(x, y), 1),
         data = list(tibble(x = x, y = y)))
}

crescent <- function(n = 1000) {
  x <- runif(n, -1, 1)
  y <- 2 * x ^ 2 + runif(n, -1, 1)
  tibble(title = "y = 2 * x ^ 2 + U(-1, 1)",
         cor = round(cor(x, y), 1),
         data = list(tibble(x = x, y = y)))
}

double_crescent <- function(n = 1000) {
  x <- runif(n, -1, 1)
  y <- (x^2 + runif(n, 0, 1/2)) * sample(seq(-1, 1, 2), n, replace = TRUE)
  tibble(
    title = "y = c(-1, 1) * (x^2 + U(0, 1/2))",
    cor = round(cor(x, y), 1),
    data = list(tibble(x = x, y = y))
  )
}

circle <- function(n = 1000) {
  x <- runif(n, -1, 1)
  y = cos(x * pi) + rnorm(n, 0, 1 / 8)
  x = sin(x * pi) + rnorm(n, 0, 1 / 8)
  tibble(
    title = "x = cos(x * pi), y = cos(x * pi)",
    cor = round(cor(x, y), 1),
    data = list(tibble(x = x, y = y))
  )
}

four_mvnormals <- function(x, y = x, n = 1000) {
   out <- rbind(rmvnorm(n / 4, c( x,  y)),
                rmvnorm(n / 4, c(-x,  y)),
                rmvnorm(n / 4, c(-x, -y)),
                rmvnorm(n / 4, c( x, -y))) %>%
     as_tibble()
   colnames(out) <- c("x", "y")
   tibble(title = "Four multivariate normals",
          cor = round(cor(out$x, out$y), 1),
          data = list(out))
}


corr_examples <- bind_rows(
  purrr::map_df(c(1, 0.8, 0.4, 0.0, -0.4, -0.8, -1.0), mv_normal),
  purrr::map_df(c(0, pi / 12, pi / 6, pi / 4, pi / 2 - pi / 6, pi / 2 - pi / 12, pi / 2),
              line_rotation),
  wave(),
  rotate(-pi / 8, cbind(runif(1000, -1, 1), runif(1000, -1, 1))),
  rotate(-pi / 4, cbind(runif(1000, -1, 1), runif(1000, -1, 1))),
  crescent(),
  double_crescent(),
  circle(),
  four_mvnormals(3)
) %>%
  mutate(.id = row_number())

corr_examples %>%
  unnest(data) %>%
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_text(data = corr_examples,
            mapping = aes(label = sprintf("%.1f", cor), x = Inf, y = Inf),
            vjust = "inward", hjust = "inward") +
  facet_wrap(~ .id, ncol = 7)

```





## Conditional expectation function

The conditional expectation function 

### Conditional expectation function with discrete covariates

Before turning to considering continuous variable, it is useful to consider the 
conditional expectation function for a discrete $Y$ and $X$.

Consider the `r rdoc("datasets", "Titanic")` dataset included in the recommended R package `r rpkg("datasets")`.
It is a cross-tabulation of the survival of the 2,201 passengers in the sinking of the [Titanic](https://en.wikipedia.org/wiki/RMS_Titanic) in 1912, as well as characteristics of those
passengers: passenger class, gender, and age.
```{r}
Titanic <- as_tibble(datasets::Titanic) %>%
  mutate(Survived = (Survived == "Yes"))
```
The proportion of passengers who survived was
```{r}
summarise(Titanic, prop_survived = sum(n * Survived) / sum(n))
```

Since `Survived` is a 

A conditional expectation function is a function that calculates the mean of `Y` for different values of `X`. For example, the conditional expectation function for 

Calculate the CEF for `Survived` conditional on `Age`,
```{r}
Titanic %>% group_by(Age) %>% summarise(prop_survived = sum(n * Survived) / sum(n))
```
conditional on `Sex`,
```{r}
Titanic %>% group_by(Sex) %>% summarise(prop_survived = sum(n * Survived) / sum(n))
```
conditional on `Class`,
```{r}
Titanic %>%
  group_by(Class) %>%
  summarise(prop_survived = sum(n * Survived) / sum(n))
```
finally, conditional on all combinations of the other variables (`Age`, `Sex`, `Class`),
```{r}
titanic_cef_3 <-
  Titanic %>% 
  group_by(Class, Age, Sex) %>%
  summarise(prop_survived = sum(n * Survived) / sum(n))
titanic_cef_3
```

The CEF can be used to predict outcome variables given $X$ variables. 
What is the predicted probability of survival for each of these characters from the movie *Titanic*?

- Rose (Kate Winslet): 1st class, adult female (survived)
- Jack (Leonardo DiCaprio): 3rd class, adult male (did not survive)
- Cal (Billy Zane) : 1st class, adult male (survived)

```{r}
titanic_chars <- 
  tribble(
    ~ name, ~ Class, ~ Age, ~ Sex, ~ Survived,
    "Rose", "1st", "Adult", "Female", TRUE,
    "Jack", "3rd", "Adult", "Male", FALSE,
    "Cal", "1st", "Adult", "Male", TRUE
  )

left_join(titanic_chars, titanic_cef_3,
          by = c("Class", "Age", "Sex"))

```

Rose was predicted to survive 97% of 1st class adult females survived, and she did.
Jack was not predicted to survive (only 16% of 3rd class adult males survived, and he did not.[^jack] 
Cal was not predicted to survive (33% of 1st class adult males survived), but he did, though through less than honorable means in the movie.

[^jack]: However, this CEF does not condition on holding onto a piece of flotsam with enough room for two.

- Note that we haven't made any assumptions about distributions of the variables.
- In this case, the outcome variable in the CEF was a binary variable, and we calculated a proportion. However, the proportion is the expected value (mean) of a binary variable, so the calculation of the CEF wouldn't change. 
- If we continued to condition on more discrete variables, the number of observed cell sizes would get smaller and smaller (and possibly zero), with larger standard errors.

But what happens if the conditioning variables are continuous? 




## Regression to the Mean

@Galton1886a examined the joint distribution of the heights of parents and their children. He was estimating the average height of children conditional upon the height of their parents. He found that this relationship was approximately linear with a slope of 2/3. 

This means that on average taller parents had taller children, but the children of taller parents were on average shorter than they were, and the children of shorter parents were on average taller than they were. In other words, the children's height was more average than parent's height. 

This phenomenon was called regression to the mean, and the term regression is now used to describe conditional relationships (Hansen 2010).

His key insight was that if the marginal distributions of two variables are the same, then the linear slope will be less than one. 

He also found that when the variables are standardized, the slope of the regression of $y$ on $x$ and $x$ on $y$ are the same. They are both the correlation between $x$ and $y$, and they both show regression to the mean.

```{r}
library("HistData")
```


```{r}
Galton <- as_tibble(Galton)
Galton
```

1. Calculate the regression of children's heights on parents. Interpret the regression.
```{r}
child_reg <- lm(child ~ parent, data=Galton)
child_reg
```



## Reverse Regression

3. Calculate the regression of parent's heights on children's heights. Interpret the regression.
```{r}
parent_reg <- lm(parent ~ child, data=Galton)
parent_reg
```

5. Check the mean and variance of parent's and children's height
```{r}
mean(Galton$parent)
mean(Galton$child)

var(Galton$parent)
var(Galton$child)
```

6. Perform both regressions using standardized variables.
```{r}
parent.std <- (Galton$parent-mean(Galton$parent))/sd(Galton$parent)
child.std <- (Galton$child-mean(Galton$child))/sd(Galton$child)

summary(child.std.reg <- lm(child.std ~ parent.std))
summary(parent.std.reg <- lm(parent.std ~ child.std))

```

Regression calculates the conditional expectation function, $f(Y, X) = \E(Y | X) + \epsilon$, but we could instead jointly model $Y$ and $X$.

This is a topic for multivariate statistics (principal components, factor analysis, clustering).
In this case, an alternative would be to model the heights of fathers and sons as a bivariate normal distribution.
```{r}
ggplot(Galton, aes(y = child, x = parent)) +
  geom_jitter() +
  geom_density2d()
```
```{r}
# covariance matrix
Galton_mean <- c(mean(Galton$parent), mean(Galton$child))
# variance covariance matrix
Galton_cov <- cov(Galton)
Galton_cov
var(Galton$parent)
var(Galton$child)
cov(Galton$parent, Galton$child)
```
Calculate density for a multivariate normal distribution
```{r}
library("mvtnorm")
Galton_mvnorm <- function(parent, child) {
  # mu and Sigma will use the values calculated earlier
  dmvnorm(cbind(parent, child), mean = Galton_mean,
          sigma = Galton_cov)
}
```

```{r}
Galton_mvnorm(Galton$parent[1], Galton$child[1])
```

```{r}
Galton_dist <- Galton %>%
  modelr::data_grid(parent = seq_range(parent, 50), child = seq_range(child, 50)) %>%
  mutate(dens = map2_dbl(parent, child, Galton_mvnorm))
```
Why don't I calculate the mean and density using the data grid? 

```{r}
library("viridis")
ggplot(Galton_dist, aes(x = parent, y = child)) +
  geom_raster(mapping = aes(fill = dens)) +
  #geom_contour(mapping = aes(z = dens), colour = "white", alpha = 0.3) +
  #geom_jitter(data = Galton, colour = "white", alpha = 0.2) +
  scale_fill_viridis() +
  theme_minimal() +
  theme(panel.grid = element_blank()) +
  labs(y = "Parent height (in)", x = "Child height (in)")
```

Using the [plotly](https://plot.ly/r/getting-started/) library
we can make an interactive 3D plot:

```{r}
x <- unique(Galton_dist$parent)
y <- unique(Galton_dist$child)
z <- Galton_dist %>%
     arrange(child, parent) %>%
     spread(parent, dens) %>%
     select(-child) %>%
     as.matrix()
plotly::plot_ly(z = z, type = "surface")
                
```

But with regression we are calculating only one margin.

```{r}
Galton_means <- Galton %>%
  group_by(parent) %>%
  summarise(child = mean(child))
ggplot(Galton, aes(x = factor(parent), y = child)) +
  geom_jitter(width = 0) +
  geom_point(data = Galton_means, colour = "red")
```

Note that in this example, it doesn't really matter since a bivariate normal distribution happens to describe the data very well.
This is not true in general, and we are simplifying our analysis by calculating the CEF rather than jointly modeling both.
