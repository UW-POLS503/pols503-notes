<<<<<<< HEAD
# (PART) Linear Models

# What is Regression? 

While this section will generally cover linear regression, it is not the only form of **regression**. So let's start with a definition of regression.

Most generally, a regression represents a function of a variable, $Y$ as a function of another variable or variables, $X$, and and error.
$$
g(Y_i) = f(X_i) + \text{error}_i
$$

The **conditional expectation function** (CEF) or **regression function** of $Y$ given $X$ is denoted,
$$
\mu(x) = \E\left[Y | X = x\right]
$$

But if regression represents $Y$ as a function of $X$, what's the alternative? 
Instead of modeling $Y$ as a function of $X$, we could jointly model both $Y$ and $X$. 
A regression model of $Y$ and $X$ would be multivariate function, $f(Y, X)$.
In machine learning these approaches are sometimes called **descriminative** (regression) and **generative** (joint models) models.

<!--
Historical note: this was a source of contention in the development of regression.
Yule first developed multivariate regression, however, Galton preferred the use of "surfaces"
-->


# Bivariate Regression Model

Given two vectors, $\vec{y} = (y_1, y_2, ..., y_n)$, and $\vec{x} = (x_1, x_2, ..., x_n)$, the **regression line** is,
$$
\E(y | y) = \hat{y_i} = \hat\beta_0 + \hat{\beta} x_i
$$
where $\hat{y_i}$ is called the **fitted value**,
and the residual is
$$
\hat{\epsilon}_i = y_i - \hat{y}_i
$$

The OLS estimate for $\hat{\vec{\beta}}$ is the values of $\hat{\beta}_0$ and $\hat{\beta}_1$ that minimize the sum of squared residuals of the regression line,
$$
\hat{\beta}_0, \hat{\beta}_1 = \argmin_{b_0, b_1}\sum_{i = 1}^n {( y_i - b_0 - b_1 x_i )}^2 = \argmin_{b_0, b_1}\sum_{i = 1}^n \hat{\epsilon}_i^2
$$

For OLS the solution to this minimization problem has a closed form solution[^closedform],
$$
\begin{aligned}
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat{\beta}_1 &= \frac{\sum_{i = 1}^N (x_i - \bar{x})^2 (y_i - \bar{y})^2}{\sum_{i = 1}^n (x_i - \bar{x})^2} 
\end{aligned}
$$

Properties of OLS

1. It goes through the mean, $(\hat{y}, \hat{x})$
2. The mean and sum of the errors is zero

### OLS is the weighted sum of outcomes

In OLS, the coefficients are weighted averages of the dependent variables,
$$
\hat{\beta}_1 = \sum_{i = 1}^n \frac{(x_i - \bar{x})}{{(x_i - \bar{x})}^2}
$$


**Technical note** Linear regression is **linear** not because $y = b_0 + b_2$, but because the predictions can be represented as weighted sums of outcome, $\hat{y} = w_i y_i$.
If we were to estimate $\hat{\beta}_0$ or $\hat{\beta}_1$ with a different objective function, then it would not longer be linear.


# Covariance and Correlation

Plot of covariance and correlation

The population covariance for random variables $X$ and $Y$ is,
$$
\Cov(X, Y) = \E[(X - \mu_x)(Y - \mu_y)]
$$

The sample covariance for vectors $\vec{x}$ and $\vec{y}$ is,
$$
\Cov(x_i, y_i) = \frac{1}{n} \sum_{i = 1}^n (x_i - \bar{x})(y_i - \bar{y})
$$

Some properties of covariance are:

- $\Cov(X, X) = \Var(X, X)$. Why?
- $\Cov(X, Y)$ has a domain of $(-\infty, \infty)$. What would a covariance of $-\infty$ look like? of $\infty$? of 0?

Like variance is defined on the scale of the squared variable ($X^2$), the covariance is defined on the scale of the product of the variables ($X Y$), which makes it difficult to interpret.

However unlike taking the square root of the variables, in correlation is standardized to a domain of $[-1, 1]$.
$$
\Cor(X, Y) = \frac{\E[(X - \mu_x)(Y - \mu_y)]}{\sigma_x \sigma_y}
$$
$$
\Cor(x_i, y_i) = \frac{\sum_{i = 1}^n (x_i - \bar{x})(y_i - \bar{y})}{s_x s_y}
$$


Graphical interpretation of correlation and covariance

Examples of association but not correlation

Anscombe quartet

# Omitted Variable Bias




# Outliers

![This is an example of how not to run a regresion](img/laffer.png)

Weighting function of a regression



## References

- Bailey (Ch 3) for an easy introduction to this.
=======
---
title: "Regression.Rmd"
author: "Jeffrey Arnold"
date: "3/31/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup

```{r}
library("datums")
library("tidyverse")
filter <- dplyr::filter
```

# Pauperism Data

```{r}
pauperism_year %>%
  filter(year == 1871) %>%  
  select(outratio, pauper2) %>%
  drop_na() %>%
  ggplot(aes(x = outratio, pauper2)) +
  geom_point() + 
  geom_smooth(method = "lm")
```

```{r}
pauperism_year %>%
  filter(year == 1871) %>%  
  select(outratio, pauper2) %>%
  drop_na() %>%
  cor()
```

Regressions for each year
```{r}
pauperism_year %>%
  group_by(year) %>%
  summarise(mod = list(lm(outratio ~ pauper2, data = .)))
```


# Anscombe quartet

```{r}
anscombe_tidy <-
  anscombe %>%
  mutate(obs = row_number()) %>%
  gather(variable_dataset, value, -obs) %>%
  separate(variable_dataset, c("variable", "dataset"), sep = c(1)) %>%
  spread(variable, value) %>%
  arrange(dataset, obs)

```

What are summary statistics of the four anscombe datasets?
```{r}
ggplot(anscombe_tidy, aes(x = x, y = y)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~ dataset, ncol = 2)
```

What are the mean, standard deviation, correlation coefficient, and regression
coefficients of each line? 
```{r}
anscombe_summ <-
  anscombe_tidy %>%
  group_by(dataset) %>%
  summarise(
    mean_x = mean(x),
    mean_y = mean(y),
    sd_x = sd(x),
    sd_y = sd(y),
    cov = cov(x, y),
    cor = cor(x, y),
    coefs = list(coef(lm(y ~ x, data = .)))
  ) %>%
  mutate(
    intercept = map_dbl(coefs, "(Intercept)"),
    slope = map_dbl(coefs, "x")
  ) %>%
  select(-coefs)

```

WUT? They are the same? But they look so different. Of course that was the point ...

Since this all revolves around covariance, lets calculate the values of $x_i - \bar{x}$,
$y_i - \bar{y}$, and $(x_i - \bar{x}) (y_i - \bar{y})$ for each obs for each variable.
```{r}
anscombe_tidy <-
  anscombe_tidy %>%
  group_by(dataset) %>%
  mutate(mean_x = mean(x),
         diff_mean_x = x - mean_x,
         mean_y = mean(y),
         diff_mean_y = y - mean_y,
         diff_mean_xy = diff_mean_x * diff_mean_y,
         quadrant = 
           if_else(
             diff_mean_x > 0, 
             if_else(diff_mean_y > 0, 1, 2),
             if_else(diff_mean_y > 0, 4, 3),
           ))
```

```{r}
ggplot(anscombe_tidy, aes(x = x, y = y,
                          size = abs(diff_mean_xy),
                          colour = factor(sign(diff_mean_xy)))) +
  geom_point() +
  geom_hline(data = anscombe_summ,
             aes(yintercept = mean_y)) +
  geom_vline(data = anscombe_summ,
             aes(xintercept = mean_x)) +  
  facet_wrap(~ dataset, ncol = 2)
```

```{r}
ggplot(anscombe_tidy, aes(x = x, y = y, colour = factor(sign(diff_mean_xy)))) +
  geom_point() +
  geom_segment(mapping = aes(xend = mean_x, yend = mean_y)) +
  geom_hline(data = anscombe_summ,
             aes(yintercept = mean_y)) +
  geom_vline(data = anscombe_summ,
             aes(xintercept = mean_x)) + 
  facet_wrap(~ dataset, ncol = 2)
```

```{r}
ggplot(anscombe_tidy, aes(x = x, y = y, colour = factor(sign(diff_mean_xy)))) +
  geom_point() +
  geom_segment(mapping = aes(xend = x, yend = mean_y)) +
  geom_segment(mapping = aes(xend = mean_x, yend = y)) +  
  geom_hline(data = anscombe_summ,
             aes(yintercept = mean_y)) +
  geom_vline(data = anscombe_summ,
             aes(xintercept = mean_x)) +  
  facet_wrap(~ dataset, ncol = 2)

```

# What is regression? 

Different conditional means ... 

Use pauperism data


# Linear Regression 

Objective function and weights


```{r}
pauperism_year %>%
  filter(year == 1871) %>%
  select(outratio, pauper2) %>%
  drop_na() %>%
  lm(pauper2 ~ outratio, data = .)
```

```{r}

```

>>>>>>> 57ba5d707f17d64714f64a2273c623d0e4d6ed58
