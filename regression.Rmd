# (PART) Linear Models

# What is Regression?

While this section will generally cover linear regression, it is not the only form of **regression**. So let's start with a definition of regression.

Most generally, a regression represents a function of a variable, $Y$ as a function of another variable or variables, $X$, and and error.
$$
g(Y_i) = f(X_i) + \text{error}_i
$$

The **conditional expectation function** (CEF) or **regression function** of $Y$ given $X$ is denoted,
$$
\mu(x) = \E\left[Y | X = x\right]
$$

But if regression represents $Y$ as a function of $X$, what's the alternative? 
Instead of modeling $Y$ as a function of $X$, we could jointly model both $Y$ and $X$. 
A regression model of $Y$ and $X$ would be multivariate function, $f(Y, X)$.
In machine learning these approaches are sometimes called **descriminative** (regression) and **generative** (joint models) models.


# Bivariate Regression Model

Given two vectors, $\vec{y} = (y_1, y_2, ..., y_n)$, and $\vec{x} = (x_1, x_2, ..., x_n)$, the **regression line** is,
$$
\E(y | y) = \hat{y_i} = \hat\beta_0 + \hat{\beta} x_i
$$
where $\hat{y_i}$ is called the **fitted value**,
and the residual is
$$
\hat{\epsilon}_i = y_i - \hat{y}_i
$$

The OLS estimate for $\hat{\vec{\beta}}$ is the values of $\hat{\beta}_0$ and $\hat{\beta}_1$ that minimize the sum of squared residuals of the regression line,
$$
\hat{\beta}_0, \hat{\beta}_1 = \argmin_{b_0, b_1}\sum_{i = 1}^n {( y_i - b_0 - b_1 x_i )}^2 = \argmin_{b_0, b_1}\sum_{i = 1}^n \hat{\epsilon}_i^2
$$

For OLS the solution to this minimization problem has a closed form solution[^closedform],
$$
\begin{aligned}
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat{\beta}_1 &= \frac{\sum_{i = 1}^N (x_i - \bar{x})^2 (y_i - \bar{y})^2}{\sum_{i = 1}^n (x_i - \bar{x})^2} 
\end{aligned}
$$

Properties of OLS

1. It goes through the mean, $(\hat{y}, \hat{x})$
2. The mean and sum of the errors is zero

### OLS is the weighted sum of outcomes

In OLS, the coefficients are weighted averages of the dependent variables,
$$
\hat{\beta}_1 = \sum_{i = 1}^n \frac{(x_i - \bar{x})}{{(x_i - \bar{x})}^2}
$$


**Technical note** Linear regression is **linear** not because $y = b_0 + b_2$, but because the predictions can be represented as weighted sums of outcome, $\hat{y} = w_i y_i$.
If we were to estimate $\hat{\beta}_0$ or $\hat{\beta}_1$ with a different objective function, then it would not longer be linear.


# Covariance and Correlation

Plot of covariance and correlation

The population covariance for random variables $X$ and $Y$ is,
$$
\Cov(X, Y) = \E[(X - \mu_x)(Y - \mu_y)]
$$

The sample covariance for vectors $\vec{x}$ and $\vec{y}$ is,
$$
\Cov(x_i, y_i) = \frac{1}{n} \sum_{i = 1}^n (x_i - \bar{x})(y_i - \bar{y})
$$

Some properties of covariance are:

- $\Cov(X, X) = \Var(X, X)$. Why?
- $\Cov(X, Y)$ has a domain of $(-\infty, \infty)$. What would a covariance of $-\infty$ look like? of $\infty$? of 0?

Like variance is defined on the scale of the squared variable ($X^2$), the covariance is defined on the scale of the product of the variables ($X Y$), which makes it difficult to interpret.

However unlike taking the square root of the variables, in correlation is standardized to a domain of $[-1, 1]$.
$$
\Cor(X, Y) = \frac{\E[(X - \mu_x)(Y - \mu_y)]}{\sigma_x \sigma_y}
$$
$$
\Cor(x_i, y_i) = \frac{\sum_{i = 1}^n (x_i - \bar{x})(y_i - \bar{y})}{s_x s_y}
$$


# Anscombe quartet

```{r}
anscombe_tidy <-
  anscombe %>%
  mutate(obs = row_number()) %>%
  gather(variable_dataset, value, -obs) %>%
  separate(variable_dataset, c("variable", "dataset"), sep = c(1)) %>%
  spread(variable, value) %>%
  arrange(dataset, obs)

```

What are summary statistics of the four anscombe datasets?
```{r}
ggplot(anscombe_tidy, aes(x = x, y = y)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~ dataset, ncol = 2)
```

What are the mean, standard deviation, correlation coefficient, and regression
coefficients of each line? 
```{r}
anscombe_summ <-
  anscombe_tidy %>%
  group_by(dataset) %>%
  summarise(
    mean_x = mean(x),
    mean_y = mean(y),
    sd_x = sd(x),
    sd_y = sd(y),
    cov = cov(x, y),
    cor = cor(x, y),
    coefs = list(coef(lm(y ~ x, data = .)))
  ) %>%
  mutate(
    intercept = map_dbl(coefs, "(Intercept)"),
    slope = map_dbl(coefs, "x")
  ) %>%
  select(-coefs)

```

WUT? They are the same? But they look so different. Of course that was the point ...

Since this all revolves around covariance, lets calculate the values of $x_i - \bar{x}$,
$y_i - \bar{y}$, and $(x_i - \bar{x}) (y_i - \bar{y})$ for each obs for each variable.
```{r}
anscombe_tidy <-
  anscombe_tidy %>%
  group_by(dataset) %>%
  mutate(mean_x = mean(x),
         diff_mean_x = x - mean_x,
         mean_y = mean(y),
         diff_mean_y = y - mean_y,
         diff_mean_xy = diff_mean_x * diff_mean_y,
         quadrant = 
           if_else(
             diff_mean_x > 0, 
             if_else(diff_mean_y > 0, 1, 2),
             if_else(diff_mean_y > 0, 4, 3),
           ))
```


```{r}
ggplot(anscombe_tidy, aes(x = x, y = y,
                          size = abs(diff_mean_xy),
                          colour = factor(sign(diff_mean_xy)))) +
  geom_point() +
  geom_hline(data = anscombe_summ,
             aes(yintercept = mean_y)) +
  geom_vline(data = anscombe_summ,
             aes(xintercept = mean_x)) +  
  facet_wrap(~ dataset, ncol = 2)
```

```{r}
ggplot(anscombe_tidy, aes(x = x, y = y, colour = factor(sign(diff_mean_xy)))) +
  geom_point() +
  geom_segment(mapping = aes(xend = mean_x, yend = mean_y)) +
  geom_hline(data = anscombe_summ,
             aes(yintercept = mean_y)) +
  geom_vline(data = anscombe_summ,
             aes(xintercept = mean_x)) + 
  facet_wrap(~ dataset, ncol = 2)
```

```{r}
ggplot(anscombe_tidy, aes(x = x, y = y, colour = factor(sign(diff_mean_xy)))) +
  geom_point() +
  geom_segment(mapping = aes(xend = x, yend = mean_y)) +
  geom_segment(mapping = aes(xend = mean_x, yend = y)) +  
  geom_hline(data = anscombe_summ,
             aes(yintercept = mean_y)) +
  geom_vline(data = anscombe_summ,
             aes(xintercept = mean_x)) +  
  facet_wrap(~ dataset, ncol = 2)

```

<a title="By DenisBoigelot, original uploader was Imagecreator (Own work, original uploader was Imagecreator) [CC0], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File%3ACorrelation_examples2.svg"><img width="256" alt="Correlation examples2" src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/256px-Correlation_examples2.svg.png"/></a>

```{r include = FALSE, eval=FALSE}
# TODO: translate the original R code to produce a dataset and 
# reproduce plot in ggplot2
# mv_normal <- function(n = 1000, cor = 0.8) {
#     out <- as_tibble(rmvnorm(n, c(0, 0), matrix(c(1, i, i, 1), ncol = 2)))
#     colnames(out) <- c("x", "y")
#     tibble(title = "Multivariate normal",
#            cor = round(cor(out$x, out$y), 1),
#            data = list(out))
# }
# 
# rotation <- function(n, t, X) {
#   X <- rmvnorm(n, c(0, 0), matrix(c(1, 1, 1, 1), ncol = 2))
#   out <- X %*% matrix(c(cos(t), sin(t), -sin(t), cos(t)), ncol = 2)
#   tibble(title = "Rotation",
#          cor = round(cor(out$x, out$y, 1)),
#          data = list(out))
# }
# 
# wave <- function(n) {
#   x <- runif(n, -1, 1)
#   y <- 4 * (x ^ 2 - 1 / 2) ^ 2 + runif(n, -1, 1) / 3
#   tibble(title = "y = 4 * (x ^ 2 - 1) / 2 ^ 2 + U(-1/3, 1/3)",
#          cor = round(cor(x, y), 1),
#          data = list(tibble(x = x, y = y)))
# }
# 
#    y = runif(n, -1, 1)
#    xy = rotation(-pi/8, cbind(x,y))
#    lim = sqrt(2+sqrt(2)) / sqrt(2)
#    MyPlot(xy, xlim = c(-lim, lim), ylim = c(-lim, lim))
# 
#    xy = rotation(-pi/8, xy)
#    MyPlot(xy, xlim = c(-sqrt(2), sqrt(2)), ylim = c(-sqrt(2), sqrt(2)))
#    
#    y = 2*x^2 + runif(n, -1, 1)
#    MyPlot(cbind(x,y), xlim = c(-1, 1), ylim = c(-1, 3))
# 
#    y = (x^2 + runif(n, 0, 1/2)) * sample(seq(-1, 1, 2), n, replace = TRUE)
#    MyPlot(cbind(x,y), xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5))
# 
#    y = cos(x*pi) + rnorm(n, 0, 1/8)
#    x = sin(x*pi) + rnorm(n, 0, 1/8)
#    MyPlot(cbind(x,y), xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5))
# 
# four_mvnormals <- function(n, x, y = x) {
#    out <- rbind(rmvnorm(n / 4, c( x,  y)),
#                 rmvnorm(n / 4, c(-x,  y)),
#                 rmvnorm(n / 4, c(-x, -y)),
#                 rmvnorm(n / 4, c( x, -y)))
#    tibble(title = "Four multivariate normals",
#           cor = round(cor(out$x, out$y), 1),
#           out = list())
# }
# 
```

