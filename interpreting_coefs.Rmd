
# Interpreting Regression Coefficients

$$
Y_i = \beta_0 + \beta_1 X_{i} + \beta_2 Z_{i} + \varepsilon_i
$$

The partial effects , or first differences, of a regression is the change
in the expected value of $Y$ with respect to a discrete change of $X$:
$$
\begin{aligned}[t]
\frac{\Delta E(Y)}{\Delta X} &= E(Y | X = x + d) - E(Y | X = x) \\
&= \beta_0 + \beta_1 (x + d) + \beta_2 Z_{i} - (\beta_0 + \beta_1 x + \beta_2 Z_{i}) \\
&= \beta_1 d
\end{aligned}
$$
When $d = 1$ (a one unit change in $X$) the first difference is $\beta_1$, the coefficient of $X$.

The marginal effect is the derivative of the regression function with respect to
a predictor. The marginal effect of $X$ is
$$
\begin{aligned}[t]
\frac{\partial\,E(Y)}{\partial\,X} &= \frac{1}{\partial\,X} (\beta_0 + \beta_1 X + \beta_2 Z) \\
&= \beta_1
\end{aligned}
$$

That the coefficients are the marginal effects of each predictor makes linear regression
particularly easy to interpret. However, this interpretation of predictors
becomes more complicated once a variable is included in multiple terms through
interactions or nonlinear functions, such as polynomials.

## Standardized Coefficients

A standardized coefficient is the coefficient on $X$, when $X$ is standardized so that $\mean(X) = 0$ and $\Var(X) = 1$.
In that case, $\beta_1$ is the change in $\E(Y)$ associated with a one standard deviation change in $X$.

Additionally, if all predictors are set so that $\mean(X) = 0$, $\beta_0$ is the expected value of $Y$
when all $X$ are at their means.
However, if any variables appear in multiple terms, then the standardized coefficients are not particularly
useful.

Standardized coefficients are generally not used in political science. (King How Not to Lie with Statistics, p. 669)
More often, the effects of variables are compared by the first difference between the value of the
variable at the mean, and a one standard deviation change.
While, this is equivalent to the standardized coefficient

Note, that standardizing variables can help computationally in some cases.
In OLS, there is a closed-form solution, so iterative optimization algorithms are not
needed in to find the best parameters. However, in more complicated models which require iterative optimization,
standardizing variables can often improve the performance of the optimization.
Thus standardizing variables before analysis is common in machine learning.
However, the purpose is for ease of computation, not for ease of interpretation.

## Marginal Effects and First Difference

The marginal effect of a regressor is the change in the outcome variable associated with a small change in the predictor,

The **first difference** of a regression of a variable with respect to the dependent variable is for two values of a variable $x_j$ and $x_j + h$ as,
$$
\frac{\Delta \vec{y}}{\Delta x} = y(x_j) - y(x)
$$

The **marginal effect** is the regression coefficient,
$$
\frac{\partial \vec{y}}{\partial \vec{x}} = \beta_1 .
$$


For a predictor with only a linear term,
$$
\vec{y} = \beta_0 + \beta_1 \vec{x}_1 ,
$$

However, if a predictor appears in multiple terms, the marginal effect will be a more complicated function.
For example, if $x$ appears as a squared term and an interaction,
$$
\vec{y} = \beta_0 + \beta_1 \vec{x} + \beta_2 \vec{x}^2 + \beta_3 \vec{z} + \beta_4 \vec{x} \vec{z} + \beta_5 \vec{x} \vec{z}^2
$$
then its marginal effect is a function of both its current value, $\vec{x}$, and the value of $\vec{z}$,
$$
\frac{\partial \vec{y}}{\partial \vec{x}} = \beta_1 + 2 \beta_2 \vec{x} + \beta_4 \vec{z} + 2 \beta_5 \vec{x} \vec{z}
$$


## References

The R package [marfx](https://github.com/jrnold/marfx) is still under development, but
includes the ability to estimate average marginal and finite difference effects.

Some useful R packages

- **ggplot2** and **broom**: Once point estimates and the confidence interval are calculated, it is easy to plot them.
- [coefplot](https://cran.r-project.org/web/packages/coefplot/index.html): Plots point estimates and confidence intervals for fitted models.
- [dotwhisker](https://cran.r-project.org/web/packages/dotwhisker/index.html): Another point-estimate and confidence interval plot pacakge for fitted models.

Packages for marginal effects or similar

- [margins](https://github.com/leeper/margins) port of the margins command
- Built-in `predict()` function calculates predictions and confidence intervals
- [mfx](https://cran.r-project.org/web/packages/mfx/mfx.pdf) marginal effects for beta, logit, negative binomial, poisson and probit models.
- **erer**: `maBina`: marginal effects for binary choice models; `ocME`: marginal effects for ordered logit or probit.
- **car**: `mmps`: Marginal  model plots.
