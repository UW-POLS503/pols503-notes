<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>_main.utf8</title>
  <meta name="description" content="">
  <meta name="generator" content="bookdown 0.7.7 and GitBook 2.6.7">

  <meta property="og:title" content="_main.utf8" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="_main.utf8" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="cross-validation.html"><a href="#cross-validation"><i class="fa fa-check"></i><b>1</b> Cross-Validation</a><ul>
<li class="chapter" data-level="" data-path=""><a href="#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="1.1" data-path="cross-validation.html"><a href="#example-predicting-bordeaux-wine"><i class="fa fa-check"></i><b>1.1</b> Example: Predicting Bordeaux Wine</a></li>
<li class="chapter" data-level="1.2" data-path="cross-validation.html"><a href="#cross-validation-1"><i class="fa fa-check"></i><b>1.2</b> Cross Validation</a></li>
<li class="chapter" data-level="1.3" data-path="cross-validation.html"><a href="#out-of-sample-error"><i class="fa fa-check"></i><b>1.3</b> Out-of-Sample Error</a><ul>
<li class="chapter" data-level="1.3.1" data-path="cross-validation.html"><a href="#held-out-data"><i class="fa fa-check"></i><b>1.3.1</b> Held-out data</a></li>
<li class="chapter" data-level="1.3.2" data-path="cross-validation.html"><a href="#k-fold-cross-validation"><i class="fa fa-check"></i><b>1.3.2</b> k-fold Cross-validation</a></li>
<li class="chapter" data-level="1.3.3" data-path="cross-validation.html"><a href="#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>1.3.3</b> Leave-one-Out Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="cross-validation.html"><a href="#approximations"><i class="fa fa-check"></i><b>1.4</b> Approximations</a></li>
<li class="chapter" data-level="1.5" data-path="cross-validation.html"><a href="#references"><i class="fa fa-check"></i><b>1.5</b> References</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:title:end-->
<!--bookdown:title:start-->
<div id="cross-validation" class="section level1">
<h1><span class="header-section-number">1</span> Cross-Validation</h1>
<div id="prerequisites" class="section level2 unnumbered">
<h2>Prerequisites</h2>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;tidyverse&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;broom&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;modelr&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;purrr&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;stringr&quot;</span>)</code></pre>
</div>
<div id="example-predicting-bordeaux-wine" class="section level2">
<h2><span class="header-section-number">1.1</span> Example: Predicting Bordeaux Wine</h2>
<p>The <code>bordeaux</code> dataset contains the prices of vintages of Bordeaux wines sold at auction in
New York, as well as the the weather and rainfall for the years of the wine.
AshenfelterAshemoreLalonde1995a uses this data was used to show that the quality of a wine vintage, as measured by its price, can largely be predicted by a small number of variables.
At the time, these prediction were not taken kindly by wine connoisseurs.[^wine]</p>
<pre class="sourceCode r"><code class="sourceCode r">bordeaux &lt;-<span class="st"> </span>datums<span class="op">::</span>bordeaux <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">lprice =</span> <span class="kw">log</span>(price <span class="op">/</span><span class="st"> </span><span class="dv">100</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(<span class="op">!</span><span class="kw">is.na</span>(price))</code></pre>
<p>The data contains 27 prices of Bordeaux wines sold at auction in New York in 1993 for vintages from 1952 to 1980; the years 1952 and 1954 were excluded because they were rarely sold.
Prices are measured as an index where 100 is the price of the 1961.</p>
<p>The dataset also includes three predictors of the price of each vintage:</p>
<ul>
<li><code>time_sv</code>: Time since the vintage, where 0 is 1983.</li>
<li><code>wrain</code>: Winter (October–March) rain</li>
<li><code>hrain</code>: Harvest (August–September) rain</li>
<li><code>degrees</code>: Average temperature in degrees centigrade from April to September in the vintage year.</li>
</ul>
<p>The first variable to consider is the age of the vintage and the price:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="kw">filter</span>(bordeaux, <span class="op">!</span><span class="kw">is.na</span>(price), <span class="op">!</span><span class="kw">is.na</span>(vint)),
       <span class="kw">aes</span>(<span class="dt">y =</span> <span class="kw">log</span>(price), <span class="dt">x =</span> vint)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>)</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Ashenfelter, Ashmore, and Lalonde (1995) run two models.
All models were estimated using OLS with log-price as the outcome variable. The predictors in the models were:</p>
<ol style="list-style-type: decimal">
<li>vintage age</li>
<li>vintage age, winter rain, harvest rain</li>
</ol>
<p>We’ll start by considering these models.
Since we are running several models, we’ll define the model formulas in a list</p>
<pre class="sourceCode r"><code class="sourceCode r">mods_f &lt;-<span class="st"> </span><span class="kw">list</span>(lprice <span class="op">~</span><span class="st"> </span>time_sv,
               lprice <span class="op">~</span><span class="st"> </span>time_sv <span class="op">+</span><span class="st"> </span>wrain <span class="op">+</span><span class="st"> </span>hrain <span class="op">+</span><span class="st"> </span>degrees)</code></pre>
<p>Run each model and store the results in a data frame as list column of <code>lm</code> objects:</p>
<pre class="sourceCode r"><code class="sourceCode r">mods_res &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">model =</span> <span class="kw">seq_along</span>(mods_f),
  <span class="dt">formula =</span> <span class="kw">map_chr</span>(mods_f, deparse),
  <span class="dt">mod =</span> <span class="kw">map</span>(mods_f, <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(.x, <span class="dt">data =</span> bordeaux))
)
mods_res</code></pre>
<pre><code>## # A tibble: 2 x 3
##   model formula                                    mod     
##   &lt;int&gt; &lt;chr&gt;                                      &lt;list&gt;  
## 1     1 lprice ~ time_sv                           &lt;S3: lm&gt;
## 2     2 lprice ~ time_sv + wrain + hrain + degrees &lt;S3: lm&gt;</code></pre>
<p>Now that we have these models, extract the coefficients into a data frame with the <strong>broom</strong> function <code>tidy</code>:</p>
<pre class="sourceCode r"><code class="sourceCode r">mods_coefs &lt;-<span class="st"> </span>mods_res <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># Add column with the results of tidy for each model</span>
<span class="st">  </span><span class="co"># conf.int = TRUE adds confidence intervals to the data</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">tidy =</span> <span class="kw">map</span>(mod, tidy, <span class="dt">conf.int =</span> <span class="ot">TRUE</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># use unnest() to expand the data frame to one row for each row in the tidy</span>
<span class="st">  </span><span class="co"># elements</span>
<span class="st">  </span><span class="kw">unnest</span>(tidy, <span class="dt">.drop =</span> <span class="ot">TRUE</span>)
<span class="kw">glimpse</span>(mods_coefs)</code></pre>
<pre><code>## Observations: 7
## Variables: 9
## $ model     &lt;int&gt; 1, 1, 2, 2, 2, 2, 2
## $ formula   &lt;chr&gt; &quot;lprice ~ time_sv&quot;, &quot;lprice ~ time_sv&quot;, &quot;lprice ~ ti...
## $ term      &lt;chr&gt; &quot;(Intercept)&quot;, &quot;time_sv&quot;, &quot;(Intercept)&quot;, &quot;time_sv&quot;, ...
## $ estimate  &lt;dbl&gt; -2.025199170, 0.035429560, -12.145333577, 0.02384741...
## $ std.error &lt;dbl&gt; 0.2472286519, 0.0136624941, 1.6881026571, 0.00716671...
## $ statistic &lt;dbl&gt; -8.191604, 2.593199, -7.194665, 3.327521, 2.420525, ...
## $ p.value   &lt;dbl&gt; 1.522111e-08, 1.566635e-02, 3.278791e-07, 3.055739e-...
## $ conf.low  &lt;dbl&gt; -2.534376e+00, 7.291126e-03, -1.564624e+01, 8.984546...
## $ conf.high &lt;dbl&gt; -1.516022230, 0.063567993, -8.644422940, 0.038710280...</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">walk</span>(mods_res<span class="op">$</span>mod, <span class="op">~</span><span class="st"> </span><span class="kw">print</span>(<span class="kw">summary</span>(.x)))</code></pre>
<pre><code>## 
## Call:
## lm(formula = .x, data = bordeaux)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.8545 -0.4788 -0.0718  0.4562  1.2457 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -2.02520    0.24723  -8.192 1.52e-08 ***
## time_sv      0.03543    0.01366   2.593   0.0157 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5745 on 25 degrees of freedom
## Multiple R-squared:  0.212,  Adjusted R-squared:  0.1804 
## F-statistic: 6.725 on 1 and 25 DF,  p-value: 0.01567
## 
## 
## Call:
## lm(formula = .x, data = bordeaux)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.46027 -0.23864  0.01347  0.18600  0.53446 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1.215e+01  1.688e+00  -7.195 3.28e-07 ***
## time_sv      2.385e-02  7.167e-03   3.328  0.00306 ** 
## wrain        1.167e-03  4.820e-04   2.421  0.02420 *  
## hrain       -3.861e-03  8.075e-04  -4.781 8.97e-05 ***
## degrees      6.164e-01  9.518e-02   6.476 1.63e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.2865 on 22 degrees of freedom
## Multiple R-squared:  0.8275, Adjusted R-squared:  0.7962 
## F-statistic: 26.39 on 4 and 22 DF,  p-value: 4.058e-08</code></pre>
<p>Likewise, extract model statistics such as, <span class="math inline">\(R^2\)</span>, adjusted <span class="math inline">\(R^2\)</span>, and <span class="math inline">\(\hat{\sigma}\)</span>:</p>
<pre class="sourceCode r"><code class="sourceCode r">mods_glance &lt;-
<span class="st">  </span><span class="kw">mutate</span>(mods_res, <span class="dt">.x =</span> <span class="kw">map</span>(mod, glance)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">unnest</span>(.x, <span class="dt">.drop =</span> <span class="ot">TRUE</span>)
mods_glance <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(formula, r.squared, adj.r.squared, sigma)</code></pre>
<pre><code>## # A tibble: 2 x 4
##   formula                                    r.squared adj.r.squared sigma
##   &lt;chr&gt;                                          &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;
## 1 lprice ~ time_sv                               0.212         0.180 0.574
## 2 lprice ~ time_sv + wrain + hrain + degrees     0.828         0.796 0.287</code></pre>
</div>
<div id="cross-validation-1" class="section level2">
<h2><span class="header-section-number">1.2</span> Cross Validation</h2>
<pre class="sourceCode r"><code class="sourceCode r">k &lt;-<span class="st"> </span><span class="dv">20</span>
f &lt;-<span class="st"> </span><span class="kw">map</span>(<span class="kw">seq_len</span>(k),
         <span class="op">~</span><span class="st"> </span><span class="kw">as.formula</span>(<span class="kw">str_c</span>(<span class="st">&quot;lprice ~ poly(time_sv, &quot;</span>, .x, <span class="st">&quot;)&quot;</span>)))
<span class="kw">names</span>(f) &lt;-<span class="st"> </span><span class="kw">seq_len</span>(k)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">mods_overfit &lt;-<span class="st"> </span><span class="kw">map</span>(f, <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(.x, <span class="dt">data =</span> bordeaux))
fits &lt;-<span class="st"> </span><span class="kw">map_df</span>(mods_overfit, glance, <span class="dt">.id =</span> <span class="st">&quot;.id&quot;</span>)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">fits <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(.id, r.squared, adj.r.squared, df.residual) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(stat, value, <span class="op">-</span>.id) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">.id =</span> <span class="kw">as.integer</span>(.id)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> .id, <span class="dt">y =</span> value)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>stat, <span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>)</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>The larger the polynomial, the more wiggly the line.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;modelr&quot;</span>)
<span class="kw">invoke</span>(gather_predictions, <span class="dt">.x =</span> <span class="kw">c</span>(<span class="kw">list</span>(<span class="dt">data =</span> bordeaux), mods_overfit)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> vint)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> lprice)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> pred, <span class="dt">group =</span> model, <span class="dt">colour =</span> <span class="kw">as.numeric</span>(model)))</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-12-1.png" width="672" />
Intuitively it seems that as we increase the flexibility of the model by increasing the number of variables the model is overfitting the data, but what does it actually mean to overfit?
If we use <span class="math inline">\(R^2\)</span> as the “measure of fit”, more variables always leads to better fit.
Adjusted <span class="math inline">\(R^2\)</span> does not increase, because the decrease in errors is offset by the increase in the degrees of freedom.
However, there is little justification for the specific formula of <span class="math inline">\(R^2\)</span>.</p>
<p>The problem with over-fitting is that the model starts to fit peculiarities of the sample (errors) rather than the true model.
Since we never know the true model, all we can check is if the model predicts new data.</p>
</div>
<div id="out-of-sample-error" class="section level2">
<h2><span class="header-section-number">1.3</span> Out-of-Sample Error</h2>
<p>To compare predictive models, we want to compare how well they predicts (duh).
This means estimating how well it will work on new data.
The problem with this is that new data is just that, …, new.</p>
<p>The trick is to reuse the sample data to get an estimate of how well the model will work on new data.
This is done by fitting the model on a subset of the data, and predicting another subset of the data which was not used to fit the model; often this is done repeatedly.
There are a variety of ways to do this, depending on the nature of the data and the predictive task.
However, they all implicitly assume that the sample of data that was used to fit the model is representative of future data.</p>
<pre class="sourceCode r"><code class="sourceCode r">many_mods &lt;-<span class="st"> </span><span class="kw">list</span>(
  lprice <span class="op">~</span><span class="st"> </span>time_sv,
  lprice <span class="op">~</span><span class="st"> </span>wrain,
  lprice <span class="op">~</span><span class="st"> </span>hrain,
  lprice <span class="op">~</span><span class="st"> </span>degrees,
  lprice <span class="op">~</span><span class="st"> </span>wrain <span class="op">+</span><span class="st"> </span>hrain <span class="op">+</span><span class="st"> </span>degrees,
  lprice <span class="op">~</span><span class="st"> </span>time_sv <span class="op">+</span><span class="st"> </span>wrain <span class="op">+</span><span class="st"> </span>hrain <span class="op">+</span><span class="st"> </span>degrees,
  lprice <span class="op">~</span><span class="st"> </span>time_sv <span class="op">+</span><span class="st"> </span>wrain <span class="op">*</span><span class="st"> </span>hrain <span class="op">*</span><span class="st"> </span>degrees,
  lprice <span class="op">~</span><span class="st"> </span>time_sv <span class="op">*</span><span class="st"> </span>(wrain <span class="op">+</span><span class="st"> </span>hrain <span class="op">+</span><span class="st"> </span>degrees),
  lprice <span class="op">~</span><span class="st"> </span>time_sv <span class="op">*</span><span class="st"> </span>wrain <span class="op">*</span><span class="st"> </span>hrain <span class="op">*</span><span class="st"> </span>degrees
)</code></pre>
<div id="held-out-data" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Held-out data</h3>
<p>A common rule of thumb is to use 70% of the data for training,
and 30% of the data for testing.</p>
<p>In this case, let’s partition the data to use the first 70% of the observations as training data, and the remaining 30% of the data as testing.</p>
<pre class="sourceCode r"><code class="sourceCode r">n_test &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="fl">0.3</span> <span class="op">*</span><span class="st"> </span><span class="kw">nrow</span>(bordeaux))
n_train &lt;-<span class="st"> </span><span class="kw">nrow</span>(bordeaux) <span class="op">-</span><span class="st"> </span>n_test

mod_train &lt;-<span class="st"> </span><span class="kw">lm</span>(lprice <span class="op">~</span><span class="st"> </span>time_sv <span class="op">+</span><span class="st"> </span>wrain <span class="op">+</span><span class="st"> </span>hrain <span class="op">+</span><span class="st"> </span>degrees,
                <span class="dt">data =</span> <span class="kw">head</span>(bordeaux, n_train))
mod_train</code></pre>
<pre><code>## 
## Call:
## lm(formula = lprice ~ time_sv + wrain + hrain + degrees, data = head(bordeaux, 
##     n_train))
## 
## Coefficients:
## (Intercept)      time_sv        wrain        hrain      degrees  
##  -1.080e+01    1.999e-02    9.712e-04   -4.461e-03    5.533e-01</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># in-sample RMSE</span>
<span class="kw">sqrt</span>(<span class="kw">mean</span>(mod_train<span class="op">$</span>residuals <span class="op">^</span><span class="st"> </span><span class="dv">2</span>))</code></pre>
<pre><code>## [1] 0.2280059</code></pre>
<p>The out-of-sample RMSE is higher than the in-sample RMSE.</p>
<pre class="sourceCode r"><code class="sourceCode r">outsample &lt;-<span class="st"> </span><span class="kw">augment</span>(mod_train, <span class="dt">newdata =</span> <span class="kw">tail</span>(bordeaux, n_test))
<span class="kw">sqrt</span>(<span class="kw">mean</span>( (outsample<span class="op">$</span>lprice <span class="op">-</span><span class="st"> </span>outsample<span class="op">$</span>.fitted) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>))</code></pre>
<pre><code>## [1] 0.351573</code></pre>
<p>This is common, but not necessarily the case.
But note that this value is highly dependent on the subset of data used for testing.
In some sense, we may choose as model that “overfits” the testing data.</p>
</div>
<div id="k-fold-cross-validation" class="section level3">
<h3><span class="header-section-number">1.3.2</span> k-fold Cross-validation</h3>
<p>A more robust approach is to repeat this training/testing split multiple times.</p>
<p>The most common approach is to to partition the data into k-folds,
and use each fold once as the testing subset, where the model is fit on the other <span class="math inline">\(k - 1\)</span> folds.</p>
<p>A common rule of thumb is to use 5 to 10 folds.</p>
<pre class="sourceCode r"><code class="sourceCode r">cv &lt;-<span class="st"> </span>modelr<span class="op">::</span><span class="kw">crossv_kfold</span>(bordeaux, <span class="dt">k =</span> <span class="dv">10</span>)

cv_rmse &lt;-<span class="st"> </span><span class="cf">function</span>(f, cv) {
  fits &lt;-<span class="st"> </span><span class="kw">map</span>(cv<span class="op">$</span>train, <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(f, <span class="dt">data =</span> .x, <span class="dt">model =</span> <span class="ot">FALSE</span>))
}</code></pre>
<p>TODO: plot of observations included in testing and training folds</p>
<p>TODO: plot cross validation results</p>
<p>TODO: Do model comparison</p>
</div>
<div id="leave-one-out-cross-validation" class="section level3">
<h3><span class="header-section-number">1.3.3</span> Leave-one-Out Cross-Validation</h3>
<p>Leave-one-out cross validation estimates is a <span class="math inline">\(k\)</span>-fold cross-validation with folds equal to the number of observations.
The model is fit <span class="math inline">\(n\)</span> times, leaving training the model on <span class="math inline">\(n - 1\)</span> observations and predicted the remaining observation.</p>
<pre class="sourceCode r"><code class="sourceCode r">cv &lt;-<span class="st"> </span>modelr<span class="op">::</span><span class="kw">crossv_kfold</span>(bordeaux, <span class="dt">k =</span> <span class="kw">nrow</span>(bordeaux))</code></pre>
</div>
</div>
<div id="approximations" class="section level2">
<h2><span class="header-section-number">1.4</span> Approximations</h2>
<p>For some models, notably linear regression, analytical approximations to the expected out of sample error can be made.
Each of these approximations will make some slightly different assumptions to plug in some unknown values.</p>
<p>In linear regression, the LOO-CV MSE can be calculated analytically, and without simulation. It is (ISLR, p. 180):
<span class="math display">\[
\text{LOO-CV} = \frac{1}{n} \sum_{i = 1}^n {\left(\frac{y_i - \hat{y}_i}{1 - h_i} \right)}^2 = \frac{1}{n} \sum_{i = 1}^n {\left(\frac{\hat{\epsilon}_i}{1 - h_i} \right)}^2 = \frac{1}{n} \times \text{PRESS}
\]</span>
where PRESS is the predictive residual sum of squares, and <span class="math inline">\(h_i\)</span> is the <em>hat-value</em> of observation <span class="math inline">\(i\)</span> <span class="citation">[@Fox2016a, p. 270, 289]</span>,
<span class="math display">\[
h_i = \Mat{X}(\Mat{X}&#39; \Mat{X})^{-1} \Mat{X}&#39;
\]</span></p>
<pre class="sourceCode r"><code class="sourceCode r">loocv &lt;-<span class="st"> </span><span class="cf">function</span>(x) {
  <span class="kw">mean</span>( (<span class="kw">residuals</span>(x) <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">hatvalues</span>(x))) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)
}</code></pre>
<p>An alternative approximation of the expected out-of-sample error is the generalized cross-validation criterion (GCV) is <span class="citation">[@Fox2016a]</span>,
<span class="math display">\[
\text{GCV} = \frac{n \times RSS}{df_{res}^2} = \frac{n \sum \hat{\epsilon}^2}{(n - k - 1)^2}
\]</span></p>
<pre class="sourceCode r"><code class="sourceCode r">gcv &lt;-<span class="st"> </span><span class="cf">function</span>(x) {
  err &lt;-<span class="st"> </span><span class="kw">residuals</span>(x)
  rss &lt;-<span class="st"> </span><span class="kw">sum</span>(err <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)
  <span class="kw">length</span>(err) <span class="op">*</span><span class="st"> </span>rss <span class="op">/</span><span class="st"> </span>(x[[<span class="st">&quot;df.residual&quot;</span>]] <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)
}</code></pre>
<p>Other measures that are also equivalent to some form of an estimate of the out-of-sample error are the <a href="https://en.wikipedia.org/wiki/Akaike_information_criterion">AIC</a> and <a href="https://en.wikipedia.org/wiki/Bayesian_information_criterion">BIC</a>.</p>
</div>
<div id="references" class="section level2">
<h2><span class="header-section-number">1.5</span> References</h2>
<ul>
<li><a href="https://tomhopper.me/2014/05/16/can-we-do-better-than-r-squared/" class="uri">https://tomhopper.me/2014/05/16/can-we-do-better-than-r-squared/</a></li>
<li><a href="http://andrewgelman.com/2007/08/29/rsquared_useful/" class="uri">http://andrewgelman.com/2007/08/29/rsquared_useful/</a></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
