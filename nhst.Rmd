# NHST

**NOTES** Neyman-Pearson vs. Fisher's interpretation of p-values.

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4877414/

# Confidence Intervals

> The formal view of the P value
as a probability conditional on the null is mathematically correct but typically irrelevant to
research goals (hence, the popularity of alternative—if wrong—interpretations) Gelman 2013. http://www.stat.columbia.edu/~gelman/research/published/pvalues3.pdf


Gelman - P-values and Statistical Practice: http://andrewgelman.com/2015/09/04/p-values-and-statistical-practice-2/ and paper http://www.stat.columbia.edu/~gelman/research/published/pvalues3.pdf


> The ASA’s statement on p-values says, “Valid scientific conclusions based on p-values and related statistics cannot be drawn without at least knowing how many and which analyses were conducted.” I agree, but knowledge of how many analyses were conducted etc. is not enough. The whole point of the “garden of forking paths” (Gelman and Loken, 2014) is that to compute a valid p-value you need to know what analyses would have been done had the data been different. Even if the researchers only did a single analysis of the data at hand, they well could’ve done other analyses had the data been different. Remember that “analysis” here also includes rules for data coding, data exclusion, etc.

> When I was sent an earlier version of the ASA’s statement, I suggested changing the sentence to, “Valid p-values cannot be drawn without knowing, not just what was done with the existing data, but what the choices in data coding, exclusion, and analysis would have been, had the data been different. This ‘what would have been done under other possible datasets’ is central to the definition of p-value.” The concern is not just multiple comparisons, it is multiple potential comparisons.

> Even experienced users of statistics often have the naive belief that if they did not engage in “cherry-picking . . . data dredging, significance chasing, significance questing, selective inference and p-hacking” (to use the words of the ASA’s statement), and if they clearly state how many and which analyses were conducted, then they’re ok. In practice, though, as Simmons, Nelson, and Simonsohn (2011) have noted, researcher degrees of freedom (including data-exclusion rules; decisions of whether to average groups, compare them, or analyze them separately; choices of regression predictors and iteractions; and so on) can and are performed after seeing the data.

> A scientific hypothesis in a field such as psychology, economics, or medicine can correspond to any number of statistical hypotheses, and if the ASA is going to issue a statement warning about p-values, I think it necessary to emphasize that researcher degrees of freedom—the garden of forking paths—can and does occur even without people realizing what they are doing. A researcher will see the data and make a series of reasonable, theory-respecting choices, ending up with an apparently successful—that is, “statistically significant”—finding, without realizing that the nominal p-value obtained is meaningless.

> Ultimately the problem is not with p-values but with null-hypothesis significance testing, that parody of falsificationism in which straw-man null hypothesis A is rejected and this is taken as evidence in favor of preferred alternative B (see Gelman, 2014). Whenever this sort of reasoning is being done, the problems discussed above will arise. Confidence intervals, credible intervals, Bayes factors, cross-validation: you name the method, it can and will be twisted, even if inadvertently, to create the appearance of strong evidence where none exists.

# Current Guidelines and Preferences

In general, confidence intervals are preferred to p-values. 
And substantive values and effect sizes are preferred to directly interpreting coefficients.


> The use of asterisks or other symbols to represent varying levels of statistical significance is strongly discouraged. Authors who report that results are “statistically significant” should use the 0.05 level or lower; statistical tests using the 0.10 level are not acceptable for work submitted to the *AJPS*. [AJPS](https://ajps.org/guidelines-for-manuscripts/)

> In general, the uncertainty of numerical estimates is best conveyed by confidence intervals or standard errors (or complete likelihood functions or posterior distributions). Regardless of whether the manuscript uses conventional null hypothesis testing, tables should not routinely report t-statistics or p-values for tests of the null hypothesis that each coefficient is zero. The use of “stars” to report different levels of statistical significance is generally unnecessary; when discussing statistical significance the level should be assumed to be .05, unless otherwise stated in the text.

> When model coefficients are not easily interpretable by the reader, other more understandable quantities should be produced for the reader along with their estimation uncertainty. The manuscript should clearly state how these quantities of interest were produced or estimated, and the manuscript should focus the discussion on the derived and understandable quantities rather than the less interpretable original estimates. [Political Analysis](https://academic.oup.com/pan/pages/Instructions_To_Authors)
