<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Data Analysis Notes</title>
  <meta name="description" content="These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Data Analysis Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington." />
  <meta name="github-repo" content="jrnold/intro-methods-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Data Analysis Notes" />
  
  <meta name="twitter:description" content="These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington." />
  

<meta name="author" content="Jeffrey B. Arnold">


<meta name="date" content="2018-05-07">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="regression.html">
<link rel="next" href="regression-discontinuity.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intro Method Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="part"><span><b>I Exploratory Data Analysis</b></span></li>
<li class="part"><span><b>II Programming</b></span></li>
<li class="part"><span><b>III Linear Regression</b></span></li>
<li class="chapter" data-level="2" data-path="regression-anatomy.html"><a href="regression-anatomy.html"><i class="fa fa-check"></i><b>2</b> Regression Anatomy</a><ul>
<li class="chapter" data-level="2.1" data-path="regression-anatomy.html"><a href="regression-anatomy.html#example"><i class="fa fa-check"></i><b>2.1</b> Example</a></li>
<li class="chapter" data-level="2.2" data-path="regression-anatomy.html"><a href="regression-anatomy.html#variations"><i class="fa fa-check"></i><b>2.2</b> Variations</a></li>
<li class="chapter" data-level="2.3" data-path="regression-anatomy.html"><a href="regression-anatomy.html#questions"><i class="fa fa-check"></i><b>2.3</b> Questions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html"><i class="fa fa-check"></i><b>3</b> OLS in Matrix Form</a><ul>
<li class="chapter" data-level="" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#setup"><i class="fa fa-check"></i>Setup</a></li>
<li class="chapter" data-level="3.1" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#purpose"><i class="fa fa-check"></i><b>3.1</b> Purpose</a></li>
<li class="chapter" data-level="3.2" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#matrix-algebra-review"><i class="fa fa-check"></i><b>3.2</b> Matrix Algebra Review</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#vectors"><i class="fa fa-check"></i><b>3.2.1</b> Vectors</a></li>
<li class="chapter" data-level="3.2.2" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#matrices"><i class="fa fa-check"></i><b>3.2.2</b> Matrices</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#matrix-operations"><i class="fa fa-check"></i><b>3.3</b> Matrix Operations</a><ul>
<li class="chapter" data-level="3.3.1" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#transpose"><i class="fa fa-check"></i><b>3.3.1</b> Transpose</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#matrices-as-vectors"><i class="fa fa-check"></i><b>3.4</b> Matrices as vectors</a></li>
<li class="chapter" data-level="3.5" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#special-matrices"><i class="fa fa-check"></i><b>3.5</b> Special matrices</a></li>
<li class="chapter" data-level="3.6" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#multiple-linear-regression-in-matrix-form"><i class="fa fa-check"></i><b>3.6</b> Multiple linear regression in matrix form</a></li>
<li class="chapter" data-level="3.7" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#residuals"><i class="fa fa-check"></i><b>3.7</b> Residuals</a></li>
<li class="chapter" data-level="3.8" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#scalar-inverses"><i class="fa fa-check"></i><b>3.8</b> Scalar inverses</a></li>
<li class="chapter" data-level="3.9" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#matrix-inverses"><i class="fa fa-check"></i><b>3.9</b> Matrix Inverses</a></li>
<li class="chapter" data-level="3.10" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#ols-estimator"><i class="fa fa-check"></i><b>3.10</b> OLS Estimator</a></li>
<li class="chapter" data-level="3.11" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#implications-of-ols"><i class="fa fa-check"></i><b>3.11</b> Implications of OLS</a><ul>
<li class="chapter" data-level="3.11.1" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#ols-in-matrix-form-1"><i class="fa fa-check"></i><b>3.11.1</b> OLS in Matrix Form</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#covariancevariance-interpretation-of-ols"><i class="fa fa-check"></i><b>3.12</b> Covariance/variance interpretation of OLS</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="collinearity-and-multicollinearity.html"><a href="collinearity-and-multicollinearity.html"><i class="fa fa-check"></i><b>4</b> Collinearity and Multicollinearity</a><ul>
<li class="chapter" data-level="4.1" data-path="collinearity-and-multicollinearity.html"><a href="collinearity-and-multicollinearity.html#perfect-collinearity"><i class="fa fa-check"></i><b>4.1</b> (Perfect) collinearity</a></li>
<li class="chapter" data-level="4.2" data-path="collinearity-and-multicollinearity.html"><a href="collinearity-and-multicollinearity.html#what-to-do-about-it"><i class="fa fa-check"></i><b>4.2</b> What to do about it?</a></li>
<li class="chapter" data-level="4.3" data-path="collinearity-and-multicollinearity.html"><a href="collinearity-and-multicollinearity.html#multicollinearity"><i class="fa fa-check"></i><b>4.3</b> Multicollinearity</a></li>
<li class="chapter" data-level="4.4" data-path="collinearity-and-multicollinearity.html"><a href="collinearity-and-multicollinearity.html#what-do-do-about-it"><i class="fa fa-check"></i><b>4.4</b> What do do about it?</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bootstrapping.html"><a href="bootstrapping.html"><i class="fa fa-check"></i><b>5</b> Bootstrapping</a><ul>
<li class="chapter" data-level="5.1" data-path="bootstrapping.html"><a href="bootstrapping.html#non-parametric-bootstrap"><i class="fa fa-check"></i><b>5.1</b> Non-parametric bootstrap</a></li>
<li class="chapter" data-level="5.2" data-path="bootstrapping.html"><a href="bootstrapping.html#standard-errors"><i class="fa fa-check"></i><b>5.2</b> Standard Errors</a></li>
<li class="chapter" data-level="5.3" data-path="bootstrapping.html"><a href="bootstrapping.html#confidence-intervals"><i class="fa fa-check"></i><b>5.3</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.4" data-path="bootstrapping.html"><a href="bootstrapping.html#alternative-methods"><i class="fa fa-check"></i><b>5.4</b> Alternative methods</a><ul>
<li class="chapter" data-level="5.4.1" data-path="bootstrapping.html"><a href="bootstrapping.html#parametric-bootstrap"><i class="fa fa-check"></i><b>5.4.1</b> Parametric Bootstrap</a></li>
<li class="chapter" data-level="5.4.2" data-path="bootstrapping.html"><a href="bootstrapping.html#clustered-bootstrap"><i class="fa fa-check"></i><b>5.4.2</b> Clustered bootstrap</a></li>
<li class="chapter" data-level="5.4.3" data-path="bootstrapping.html"><a href="bootstrapping.html#time-series-bootstrap"><i class="fa fa-check"></i><b>5.4.3</b> Time series bootstrap</a></li>
<li class="chapter" data-level="5.4.4" data-path="bootstrapping.html"><a href="bootstrapping.html#how-to-sample"><i class="fa fa-check"></i><b>5.4.4</b> How to sample?</a></li>
<li class="chapter" data-level="5.4.5" data-path="bootstrapping.html"><a href="bootstrapping.html#caveats"><i class="fa fa-check"></i><b>5.4.5</b> Caveats</a></li>
<li class="chapter" data-level="5.4.6" data-path="bootstrapping.html"><a href="bootstrapping.html#why-use-bootstrapping"><i class="fa fa-check"></i><b>5.4.6</b> Why use bootstrapping?</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="bootstrapping.html"><a href="bootstrapping.html#bagging"><i class="fa fa-check"></i><b>5.5</b> Bagging</a></li>
<li class="chapter" data-level="5.6" data-path="bootstrapping.html"><a href="bootstrapping.html#hypothesis-testing"><i class="fa fa-check"></i><b>5.6</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="5.7" data-path="bootstrapping.html"><a href="bootstrapping.html#how-many-samples"><i class="fa fa-check"></i><b>5.7</b> How many samples?</a></li>
<li class="chapter" data-level="5.8" data-path="bootstrapping.html"><a href="bootstrapping.html#references"><i class="fa fa-check"></i><b>5.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="prediction.html"><a href="prediction.html"><i class="fa fa-check"></i><b>6</b> Prediction</a><ul>
<li class="chapter" data-level="" data-path="prediction.html"><a href="prediction.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="6.1" data-path="prediction.html"><a href="prediction.html#prediction-questions-vs.causal-questions"><i class="fa fa-check"></i><b>6.1</b> Prediction Questions vs. Causal Questions</a></li>
<li class="chapter" data-level="6.2" data-path="prediction.html"><a href="prediction.html#why-is-prediction-important"><i class="fa fa-check"></i><b>6.2</b> Why is prediction important?</a></li>
<li class="chapter" data-level="6.3" data-path="prediction.html"><a href="prediction.html#many-problems-are-prediction-problems"><i class="fa fa-check"></i><b>6.3</b> Many problems are prediction problems</a><ul>
<li class="chapter" data-level="6.3.1" data-path="prediction.html"><a href="prediction.html#counterfactuals"><i class="fa fa-check"></i><b>6.3.1</b> Counterfactuals</a></li>
<li class="chapter" data-level="6.3.2" data-path="prediction.html"><a href="prediction.html#controls"><i class="fa fa-check"></i><b>6.3.2</b> Controls</a></li>
<li class="chapter" data-level="6.3.3" data-path="prediction.html"><a href="prediction.html#what-does-overfitting-mean"><i class="fa fa-check"></i><b>6.3.3</b> What does overfitting mean</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="prediction.html"><a href="prediction.html#prediction-vs.explanation"><i class="fa fa-check"></i><b>6.4</b> Prediction vs. Explanation</a></li>
<li class="chapter" data-level="6.5" data-path="prediction.html"><a href="prediction.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>6.5</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="6.5.1" data-path="prediction.html"><a href="prediction.html#example-1"><i class="fa fa-check"></i><b>6.5.1</b> Example</a></li>
<li class="chapter" data-level="6.5.2" data-path="prediction.html"><a href="prediction.html#overview"><i class="fa fa-check"></i><b>6.5.2</b> Overview</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="prediction.html"><a href="prediction.html#prediction-policy-problems"><i class="fa fa-check"></i><b>6.6</b> Prediction policy problems</a><ul>
<li class="chapter" data-level="6.6.1" data-path="prediction.html"><a href="prediction.html#references-1"><i class="fa fa-check"></i><b>6.6.1</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>7</b> Cross-Validation</a><ul>
<li class="chapter" data-level="" data-path="cross-validation.html"><a href="cross-validation.html#prerequisites-1"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="7.1" data-path="cross-validation.html"><a href="cross-validation.html#example-predicting-bordeaux-wine"><i class="fa fa-check"></i><b>7.1</b> Example: Predicting Bordeaux Wine</a></li>
<li class="chapter" data-level="7.2" data-path="cross-validation.html"><a href="cross-validation.html#cross-validation-1"><i class="fa fa-check"></i><b>7.2</b> Cross Validation</a></li>
<li class="chapter" data-level="7.3" data-path="cross-validation.html"><a href="cross-validation.html#out-of-sample-error"><i class="fa fa-check"></i><b>7.3</b> Out-of-Sample Error</a><ul>
<li class="chapter" data-level="7.3.1" data-path="cross-validation.html"><a href="cross-validation.html#held-out-data"><i class="fa fa-check"></i><b>7.3.1</b> Held-out data</a></li>
<li class="chapter" data-level="7.3.2" data-path="cross-validation.html"><a href="cross-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>7.3.2</b> <span class="math inline">\(k\)</span>-fold Cross-validation</a></li>
<li class="chapter" data-level="7.3.3" data-path="cross-validation.html"><a href="cross-validation.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>7.3.3</b> Leave-one-Out Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="cross-validation.html"><a href="cross-validation.html#approximations"><i class="fa fa-check"></i><b>7.4</b> Approximations</a></li>
<li class="chapter" data-level="7.5" data-path="cross-validation.html"><a href="cross-validation.html#references-2"><i class="fa fa-check"></i><b>7.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>8</b> Regularization</a><ul>
<li class="chapter" data-level="8.1" data-path="regularization.html"><a href="regularization.html#ridge-regression"><i class="fa fa-check"></i><b>8.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="8.2" data-path="regularization.html"><a href="regularization.html#regularization-for-causal-inference"><i class="fa fa-check"></i><b>8.2</b> Regularization for Causal Inference</a></li>
<li class="chapter" data-level="8.3" data-path="regularization.html"><a href="regularization.html#references-3"><i class="fa fa-check"></i><b>8.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>9</b> Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="regression.html"><a href="regression.html#observational-studies"><i class="fa fa-check"></i><b>9.1</b> Observational Studies</a></li>
<li class="chapter" data-level="9.2" data-path="regression.html"><a href="regression.html#regression-and-causality"><i class="fa fa-check"></i><b>9.2</b> Regression and Causality</a></li>
<li class="chapter" data-level="9.3" data-path="regression.html"><a href="regression.html#treatment-effects"><i class="fa fa-check"></i><b>9.3</b> Treatment Effects</a></li>
<li class="chapter" data-level="9.4" data-path="regression.html"><a href="regression.html#ols"><i class="fa fa-check"></i><b>9.4</b> OLS</a></li>
<li class="chapter" data-level="9.5" data-path="regression.html"><a href="regression.html#heterogeneous-effects-and-regression"><i class="fa fa-check"></i><b>9.5</b> Heterogeneous Effects and Regression</a></li>
<li class="chapter" data-level="9.6" data-path="regression.html"><a href="regression.html#balance-and-imbalance"><i class="fa fa-check"></i><b>9.6</b> Balance and Imbalance</a></li>
<li class="chapter" data-level="9.7" data-path="regression.html"><a href="regression.html#key-things"><i class="fa fa-check"></i><b>9.7</b> Key Things</a></li>
<li class="chapter" data-level="9.8" data-path="regression.html"><a href="regression.html#limited-dependent-variables"><i class="fa fa-check"></i><b>9.8</b> Limited Dependent Variables</a></li>
<li class="chapter" data-level="9.9" data-path="regression.html"><a href="regression.html#assessing-selection-on-observables"><i class="fa fa-check"></i><b>9.9</b> Assessing Selection on Observables</a></li>
<li class="chapter" data-level="9.10" data-path="regression.html"><a href="regression.html#omitted-variable-tests"><i class="fa fa-check"></i><b>9.10</b> Omitted Variable Tests</a></li>
<li class="chapter" data-level="9.11" data-path="regression.html"><a href="regression.html#my-advice"><i class="fa fa-check"></i><b>9.11</b> My Advice</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="panel-data-fixed-effects-and-difference-in-difference.html"><a href="panel-data-fixed-effects-and-difference-in-difference.html"><i class="fa fa-check"></i><b>10</b> Panel Data: Fixed Effects and Difference-in-Difference</a><ul>
<li class="chapter" data-level="10.1" data-path="panel-data-fixed-effects-and-difference-in-difference.html"><a href="panel-data-fixed-effects-and-difference-in-difference.html#panel-longitudinal-data"><i class="fa fa-check"></i><b>10.1</b> Panel (Longitudinal) Data</a></li>
<li class="chapter" data-level="10.2" data-path="panel-data-fixed-effects-and-difference-in-difference.html"><a href="panel-data-fixed-effects-and-difference-in-difference.html#terminology"><i class="fa fa-check"></i><b>10.2</b> Terminology</a></li>
<li class="chapter" data-level="10.3" data-path="panel-data-fixed-effects-and-difference-in-difference.html"><a href="panel-data-fixed-effects-and-difference-in-difference.html#fixed-effects"><i class="fa fa-check"></i><b>10.3</b> Fixed Effects</a><ul>
<li class="chapter" data-level="10.3.1" data-path="panel-data-fixed-effects-and-difference-in-difference.html"><a href="panel-data-fixed-effects-and-difference-in-difference.html#estimators"><i class="fa fa-check"></i><b>10.3.1</b> Estimators</a></li>
<li class="chapter" data-level="10.3.2" data-path="panel-data-fixed-effects-and-difference-in-difference.html"><a href="panel-data-fixed-effects-and-difference-in-difference.html#issues-with-fixed-effects"><i class="fa fa-check"></i><b>10.3.2</b> Issues with Fixed Effects</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="panel-data-fixed-effects-and-difference-in-difference.html"><a href="panel-data-fixed-effects-and-difference-in-difference.html#difference-in-difference"><i class="fa fa-check"></i><b>10.4</b> Difference-in-Difference</a><ul>
<li class="chapter" data-level="10.4.1" data-path="panel-data-fixed-effects-and-difference-in-difference.html"><a href="panel-data-fixed-effects-and-difference-in-difference.html#basic-differences-in-differences-model"><i class="fa fa-check"></i><b>10.4.1</b> Basic differences-in-differences model</a></li>
<li class="chapter" data-level="10.4.2" data-path="panel-data-fixed-effects-and-difference-in-difference.html"><a href="panel-data-fixed-effects-and-difference-in-difference.html#potential-outcomes-approach-to-did"><i class="fa fa-check"></i><b>10.4.2</b> Potential Outcomes Approach to DiD</a></li>
<li class="chapter" data-level="10.4.3" data-path="panel-data-fixed-effects-and-difference-in-difference.html"><a href="panel-data-fixed-effects-and-difference-in-difference.html#threats-to-identification"><i class="fa fa-check"></i><b>10.4.3</b> Threats to identification</a></li>
<li class="chapter" data-level="10.4.4" data-path="panel-data-fixed-effects-and-difference-in-difference.html"><a href="panel-data-fixed-effects-and-difference-in-difference.html#robustness-checks"><i class="fa fa-check"></i><b>10.4.4</b> Robustness Checks</a></li>
<li class="chapter" data-level="10.4.5" data-path="panel-data-fixed-effects-and-difference-in-difference.html"><a href="panel-data-fixed-effects-and-difference-in-difference.html#extensions"><i class="fa fa-check"></i><b>10.4.5</b> Extensions</a></li>
<li class="chapter" data-level="10.4.6" data-path="panel-data-fixed-effects-and-difference-in-difference.html"><a href="panel-data-fixed-effects-and-difference-in-difference.html#standard-error-issues"><i class="fa fa-check"></i><b>10.4.6</b> Standard Error Issues</a></li>
<li class="chapter" data-level="10.4.7" data-path="panel-data-fixed-effects-and-difference-in-difference.html"><a href="panel-data-fixed-effects-and-difference-in-difference.html#other-did-approaches"><i class="fa fa-check"></i><b>10.4.7</b> Other DiD Approaches</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="panel-data-fixed-effects-and-difference-in-difference.html"><a href="panel-data-fixed-effects-and-difference-in-difference.html#lagged-dependent-variables"><i class="fa fa-check"></i><b>10.5</b> Lagged Dependent Variables</a></li>
<li class="chapter" data-level="10.6" data-path="panel-data-fixed-effects-and-difference-in-difference.html"><a href="panel-data-fixed-effects-and-difference-in-difference.html#random-effects"><i class="fa fa-check"></i><b>10.6</b> Random Effects</a><ul>
<li class="chapter" data-level="10.6.1" data-path="panel-data-fixed-effects-and-difference-in-difference.html"><a href="panel-data-fixed-effects-and-difference-in-difference.html#how-to-estimate-random-effects"><i class="fa fa-check"></i><b>10.6.1</b> How to estimate random effects?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="regression-discontinuity.html"><a href="regression-discontinuity.html"><i class="fa fa-check"></i><b>11</b> Regression Discontinuity</a><ul>
<li class="chapter" data-level="11.1" data-path="regression-discontinuity.html"><a href="regression-discontinuity.html#examples-1"><i class="fa fa-check"></i><b>11.1</b> Examples</a></li>
<li class="chapter" data-level="11.2" data-path="regression-discontinuity.html"><a href="regression-discontinuity.html#example-close-elections"><i class="fa fa-check"></i><b>11.2</b> Example: Close Elections</a></li>
<li class="chapter" data-level="11.3" data-path="regression-discontinuity.html"><a href="regression-discontinuity.html#software"><i class="fa fa-check"></i><b>11.3</b> Software</a></li>
<li class="chapter" data-level="11.4" data-path="regression-discontinuity.html"><a href="regression-discontinuity.html#references-4"><i class="fa fa-check"></i><b>11.4</b> References</a></li>
</ul></li>
<li class="part"><span><b>IV Presentation</b></span></li>
<li class="chapter" data-level="12" data-path="formatting-tables.html"><a href="formatting-tables.html"><i class="fa fa-check"></i><b>12</b> Formatting Tables</a><ul>
<li class="chapter" data-level="12.1" data-path="formatting-tables.html"><a href="formatting-tables.html#overview-of-packages"><i class="fa fa-check"></i><b>12.1</b> Overview of Packages</a></li>
<li class="chapter" data-level="12.2" data-path="formatting-tables.html"><a href="formatting-tables.html#summary-statistic-table-example"><i class="fa fa-check"></i><b>12.2</b> Summary Statistic Table Example</a></li>
<li class="chapter" data-level="12.3" data-path="formatting-tables.html"><a href="formatting-tables.html#regression-table-example"><i class="fa fa-check"></i><b>12.3</b> Regression Table Example</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="reproducible-research.html"><a href="reproducible-research.html"><i class="fa fa-check"></i><b>13</b> Reproducible Research</a></li>
<li class="chapter" data-level="14" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html"><i class="fa fa-check"></i><b>14</b> Typesetting and Word Processing Programs</a><ul>
<li class="chapter" data-level="14.1" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html#latex"><i class="fa fa-check"></i><b>14.1</b> LaTeX</a><ul>
<li class="chapter" data-level="14.1.1" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html#learning-latex"><i class="fa fa-check"></i><b>14.1.1</b> Learning LaTeX</a></li>
<li class="chapter" data-level="14.1.2" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html#using-latex"><i class="fa fa-check"></i><b>14.1.2</b> Using LaTeX</a></li>
<li class="chapter" data-level="14.1.3" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html#latex-with-r"><i class="fa fa-check"></i><b>14.1.3</b> LaTeX with R</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html#word"><i class="fa fa-check"></i><b>14.2</b> Word</a><ul>
<li class="chapter" data-level="14.2.1" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html#general-advice"><i class="fa fa-check"></i><b>14.2.1</b> General Advice</a></li>
<li class="chapter" data-level="14.2.2" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html#using-r-with-word"><i class="fa fa-check"></i><b>14.2.2</b> Using R with Word</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="writing-resources.html"><a href="writing-resources.html"><i class="fa fa-check"></i><b>15</b> Writing Resources</a><ul>
<li class="chapter" data-level="15.1" data-path="writing-resources.html"><a href="writing-resources.html#writing-and-organizing-papers"><i class="fa fa-check"></i><b>15.1</b> Writing and Organizing Papers</a></li>
<li class="chapter" data-level="15.2" data-path="writing-resources.html"><a href="writing-resources.html#finding-research-ideas"><i class="fa fa-check"></i><b>15.2</b> Finding Research Ideas</a></li>
<li class="chapter" data-level="15.3" data-path="writing-resources.html"><a href="writing-resources.html#replications"><i class="fa fa-check"></i><b>15.3</b> Replications</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="" data-path="references-5.html"><a href="references-5.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
\[
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Bias}{Bias}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\RMSE}{RMSE}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\Mat}[1]{\boldsymbol{#1}}
\newcommand{\Vec}[1]{\boldsymbol{#1}}
\newcommand{\T}{'}

\newcommand{\distr}[1]{\mathcal{#1}}
\newcommand{\dnorm}{\distr{N}}
\newcommand{\dmvnorm}[1]{\distr{N}_{#1}}
\newcommand{\dt}[1]{\distr{T}_{#1}}

\newcommand{\cia}{\perp\!\!\!\perp}
\DeclareMathOperator*{\plim}{plim}
\]
<div id="panel-data-fixed-effects-and-difference-in-difference" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> Panel Data: Fixed Effects and Difference-in-Difference</h1>
<p>Another source of variation is repeated measures of the same unit over time.
This can allow for identification with different identifying assumptions.
There are two identification approaches we will focus on</p>
<ol style="list-style-type: decimal">
<li>Fixed Effects</li>
<li>Different-in-Difference</li>
</ol>
<p>See <span class="citation">Angrist and Pischke (<a href="#ref-AngristPischke2014a">2014</a> Ch. 5)</span> and <span class="citation">Angrist and Pischke (<a href="#ref-AngristPischke2009a">2009</a> Ch 5)</span> on fixed effects
and difference-in-difference approaches.</p>
<div id="panel-longitudinal-data" class="section level2">
<h2><span class="header-section-number">10.1</span> Panel (Longitudinal) Data</h2>
<p>In these methods there are repeated measurements of the same unit over time.
This requires different methods and also has implications for causal inference.
While simply having panel data does not identify an effect, it allows the researcher to claim identification using different assumptions than simply selection on observables (as in the cross-sectional case).</p>
</div>
<div id="terminology" class="section level2">
<h2><span class="header-section-number">10.2</span> Terminology</h2>
<p>There are several closely related concepts and terminology to cover.</p>
<dl>
<dt>Panel (longitudinal) data</dt>
<dd><p>small <span class="math inline">\(T\)</span>, large <span class="math inline">\(N\)</span>. Examples: longitudinal surveys with a few rounds.</p>
</dd>
<dt>Time series cross-section data</dt>
<dd><p>large <span class="math inline">\(T\)</span>, medium <span class="math inline">\(N\)</span>. Examples: most country-year panels in CPE/IPE with several decades of data.</p>
</dd>
</dl>
<p>For the purposes of causal inference, identification relies on the same assumptions.
However, different estimators work differently under different data types.
Some estimators work well as <span class="math inline">\(N \to \infty\)</span>, some as <span class="math inline">\(T \to \infty\)</span>, and usually these are not the same.
Additionally, longer time series may require and/or have enough data for the researcher to estimate serial correlation in the errors.</p>
<p>There are some additional related concepts that should also be mentioned at this time, hopefully to spare the reader future confusion (and not to add to it):</p>
<dl>
<dt>Hierarchical Models</dt>
<dd><p>units nested within groups. E.g. children in schools, districts within states</p>
</dd>
<dt>Time-series Models</dt>
<dd><p>large <span class="math inline">\(T\)</span>, usually <span class="math inline">\(N = 1\)</span>, or the different units modeled separately.</p>
</dd>
</dl>
<p>Terminology can be confusing and varies across fields and literatures.
In particular, fixed effects and random effects are used differently and often estimated differently in statistics and econometrics.
This is easily seen by comparing the <strong>lme4</strong> and <strong>plm</strong> packages in R which both estimate fixed and random effects models.
Hierarchical models will often used fixed and random effects even though there is no <em>time</em> component, and thus they are not longitudinal models.
The reason that I bring up this terminology is that if you search for fixed and random effects you can quickly be confused when it seems that people are talking about seemingly different concepts; they more of less may be.</p>
<p>By panel data we will mean repeated measures for a unit, <span class="math inline">\(i \in 1, \dots, N\)</span>, over time, <span class="math inline">\(t \in 1, \dots, T\)</span>.</p>
<ul>
<li>same individuals in multiple surveys over time</li>
<li>countries or districts over years</li>
<li>individuals over time</li>
</ul>
<p>There are many different terms for repeated measurement data, including longitudinal, panel, and time-series cross-sectional data.
Generally,</p>
<p>The issues of causality are mostly the same, for these two types of data.
However, the estimation methods are different. Estimation methods often rely on asymptotic assumptions about observations going to infinity.
In repeated measurements there are two dimensions: number of units, and number of periods.
Different estimators will work better for small <span class="math inline">\(T\)</span> vs. large <span class="math inline">\(T\)</span>, and small <span class="math inline">\(N\)</span> vs. large <span class="math inline">\(N\)</span>.</p>
</div>
<div id="fixed-effects" class="section level2">
<h2><span class="header-section-number">10.3</span> Fixed Effects</h2>
<p>Suppose that there are <span class="math inline">\(i \in 1, \dots n\)</span> unit, and <span class="math inline">\(t \in 1, \dots, T\)</span> time periods.
The key assumption is that the treatment is independent of time, observed covariates,
*and the identity of the observation.
<span class="math display">\[
\E[Y_{it}(0) | U_i, X_{it}, t, D_{it}] = \E[Y_{it}(0) | U_i, X_{it}, t]
\]</span>
This is effectively a control for an unobserved factors for a unit.</p>
<p>We need to make a few assumptions.
Assume that time and linear effects are constant,
<span class="math display">\[
\E[Y_{it}(0) | U_i, X_{it}, t] = \alpha + U_i&#39; \gamma + X&#39;_{it} \beta .
\]</span></p>
<p>Assume that the causal effect is constant, and has a linear functional form:
<span class="math display">\[
\E[Y_{it}(1) | U_i, X_{it}, t] =  \E[Y_{it}(0) | U_i, X_{it}, t] + \tau
\]</span></p>
<p>Together this implies
<span class="math display">\[
\E[Y_{it} | U_i, X_{it}, t, D_{it}] = \alpha + \tau D_{it} + \delta_{t} + U_i&#39; \gamma + X&#39; \beta .
\]</span>
This implies,
<span class="math display">\[
Y_{it} = \alpha_i + \delta_t + \tau D_{it} + X&#39;_{it} \beta + \epsilon_{it}
\]</span>
where
<span class="math display">\[
\epsilon_{it} = Y_{it}(0) - \E[Y_{it}(0) | U_i, X_{it}, t] .
\]</span>
This implies that the fixed effects regression will be a CEF if <span class="math inline">\(\epsilon_{it}\)</span> has an expected value of 0.</p>
<p>Fixed effects allows us to identify causal effects within units, and it is constant within the unit.
You can think of this as a special kind of control.</p>
<p>This requires some more stringent functional forms assumptions than regression, but it also can handle a specific form of unobserved confounders.</p>
<div id="estimators" class="section level3">
<h3><span class="header-section-number">10.3.1</span> Estimators</h3>
<p>Given this model, there are several different estimators that are used.</p>
<div id="within-estimator" class="section level4">
<h4><span class="header-section-number">10.3.1.1</span> Within Estimator</h4>
<p>First calculate the unit-level averages,
<span class="math display">\[
\bar{Y}_i = \alpha_i + \bar{\delta} + \tau \bar{D}_{i} + \bar{X}_{it}&#39; \beta + \bar{\epsilon}_{i} .
\]</span>
The within estimator subtracts these unit-level means from the response, treatment, and all the controls:
<span class="math display">\[
Y_{it} - \bar{Y}_i = (\delta_t - \bar{\delta}) + (X_{it} - \bar{X}_i)&#39; \beta + \tau (D_{it} - \bar{D}_i) + (\epsilon_{it} - \bar{\epsilon}_{i})
\]</span>
Also note that since <span class="math inline">\(\bar{Y}_i\)</span> are unit averages,
and the unobserved effect is constant over time, subtracting off the mean also subtracts that unobserved effect.</p>
<p>Implications are:</p>
<ul>
<li>Cannot use fixed effects to estimate causal effects of treatments that are constant within a unit.</li>
<li>We do not need to include any time-constant controls.</li>
<li>Only removes time-constant unobserved effects in a unit. See difference-in-difference for a method to remove some types of time-varying unobserved values.</li>
</ul>
</div>
<div id="least-squares-dummy-variable-lsdv" class="section level4">
<h4><span class="header-section-number">10.3.1.2</span> Least Squares Dummy Variable (LSDV)</h4>
<p>Dummy variable regression is an alternative way to estimate fixed effects models.
Called the least squared dummy variable (LSDV) estimator.
Include a matrix of indicator variables (<span class="math inline">\(W_i\)</span>) for each observation.
<span class="math display">\[
Y_{it} = \tau D_{it} + w_i&#39; \gamma + x&#39;_{it} \beta + \epsilon_{it}
\]</span></p>
<ul>
<li><p>Within vs. LSDV are equivalent algebraically.</p></li>
<li><p>LSDV is more computationally demanding. With <span class="math inline">\(p\)</span> covariates and <span class="math inline">\(G\)</span> groups,
within estimator’s design matrix has only <span class="math inline">\(p\)</span> columns, whereas the LSDV design matrix has <span class="math inline">\(p + G\)</span> columns.</p></li>
<li><p>If the within estimator is manually estimated by demeaning variables and then using OLS, the standard errors will be incorrect.
They need to account for the degrees of freedom due to calculating the group means.</p></li>
<li><p>In LSDV, the fixed effects themselves are not consistent if <span class="math inline">\(T\)</span> fixed and <span class="math inline">\(N \to \infty\)</span>.
However, the other coefficients are consistent, and those are the ones we care about. <span class="citation">(Angrist and Pischke <a href="#ref-AngristPischke2009a">2009</a>, 224)</span></p></li>
</ul>
</div>
<div id="first-differences-estimation" class="section level4">
<h4><span class="header-section-number">10.3.1.3</span> First-differences estimation</h4>
<p>Given that <span class="math inline">\(U_i\)</span> is constant over time, first difference model is an alternative to mean-differences.
The model is,
<span class="math display">\[
\begin{aligned}[t]
Y_{it} - Y_{i,t-1} &amp;=  (x&#39;_{it} - x&#39;_{i,t-1}) \beta + \tau (D_{it} - D_{i,t-1}) + (\epsilon_{it} - \epsilon_{i,t-1}) \\
\Delta Y_{it} &amp;= \Delta x&#39;_{it} \beta + \tau \Delta D_{it} + \Delta \epsilon_{it}
\end{aligned}
\]</span></p>
<ul>
<li>If <span class="math inline">\(U_i\)</span> are time-fixed, then first-differences are an alternative to mean-differences</li>
<li>If the difference in errors, <span class="math inline">\(\Delta \epsilon_{it}\)</span> are homoskedastic, OLS standard errors work fine.</li>
<li>But implies that original errors must have had serial correlation: <span class="math inline">\(\epsilon_{it} = \epsilon_{i,t-1} + \Delta \epsilon_{it}\)</span>.</li>
<li>If serial correlation, then more efficient than FE.</li>
<li>Robust/sandwich SEs can be used.</li>
</ul>
<p>See <span class="citation">Angrist and Pischke (<a href="#ref-AngristPischke2009a">2009</a> Ch 5.3)</span> for a discussion of the difference between lagged dependent variables and fixed effects.</p>
<ul>
<li><p>LDV and FE estimators bound the causal effect of
interest <span class="citation">(Angrist and Pischke <a href="#ref-AngristPischke2009a">2009</a>, 246)</span>.</p></li>
<li><p>If lagged dependent variable and fixed effect are both included then there is
bias. Though this bias is not too bad and declines with the amount of data (CITE?).
Instrumental variable approaches can be used, which are unbiased but very high
variance, and thus OLS is often as good (same CITE)</p></li>
</ul>
</div>
</div>
<div id="issues-with-fixed-effects" class="section level3">
<h3><span class="header-section-number">10.3.2</span> Issues with Fixed Effects</h3>
<ul>
<li><p>The only use within unit variation. This does not address changes over time.</p></li>
<li><p>Susceptible to measurement error. There may be little variation within each unit.
This will make it harder to estimate effects, which is okay, because the lack of within-unit variation tells us that this is a poor identification strategy.
IV could be used <span class="citation">(Angrist and Pischke <a href="#ref-AngristPischke2014a">2014</a>, 226)</span></p></li>
<li><p>Fixed effects only identifies <strong>contemporaneous effects</strong>.
See <span class="citation">Blackwell (<a href="#ref-Blackwell2013a">2013</a>)</span> for an approach to dynamic panel data .</p></li>
<li><p>Since a fixed effect approach can usually be turned into a
difference-in-difference approach by including period level dummies,
there is often little reason not to do a DiD.</p></li>
</ul>
</div>
</div>
<div id="difference-in-difference" class="section level2">
<h2><span class="header-section-number">10.4</span> Difference-in-Difference</h2>
<p>The difference-in-difference estimator is similar to the fixed effect model, but relies on different assumptions.</p>
<div id="basic-differences-in-differences-model" class="section level3">
<h3><span class="header-section-number">10.4.1</span> Basic differences-in-differences model</h3>
<table>
<thead>
<tr class="header">
<th></th>
<th>Treatment</th>
<th>Control</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Pre</td>
<td><span class="math inline">\(E(Y_{i1}(0) | D_i = 1)\)</span></td>
<td><span class="math inline">\(E(Y_{i1}(0) | D_i = 0)\)</span></td>
</tr>
<tr class="even">
<td>Post</td>
<td><span class="math inline">\(E(Y_{i2}(1) | D_i = 1)\)</span></td>
<td><span class="math inline">\(E(Y_{i2}(0) | D_i = 0)\)</span></td>
</tr>
</tbody>
</table>
<p>The difference-in-difference (DiD) estimator is
<span class="math display">\[
DiD = \underbrace{(\E(Y_{i2}(1) | D_i = 1) -  \E(Y_{i1}(0) | D_i = 1))}_{\text{Treatment Difference}} -
\underbrace{(\E(Y_{i2}(0) | D_i = 0) - E(Y_{i1}(0) | D_i =  0))}_{\text{Control treatment}} .
\]</span></p>
<p>For the general case of multiple time periods <span class="math inline">\(t = 1, \dots, T\)</span>, for observations <span class="math inline">\(i = 1, \dots, N\)</span> in <span class="math inline">\(g = 1, \dots, G\)</span> groups, the DiD regression is,
<span class="math display">\[
y_{itg} = \alpha +
\underbrace{\sum_{t = 1}^T \beta_{t} I(t = \tau)}_{\text{time dummies}} +
\underbrace{\sum_{k = 1}^G \gamma_{g} I(k = g)}_{\text{group dummies}} + \underbrace{\delta D_{tg}}_{\text{treatment}} + \epsilon_{it}
\]</span></p>
<p>Identification comes from inter-temporal variation <em>between</em> groups.</p>
</div>
<div id="potential-outcomes-approach-to-did" class="section level3">
<h3><span class="header-section-number">10.4.2</span> Potential Outcomes Approach to DiD</h3>
<div id="constant-effects-linear-did-model" class="section level4">
<h4><span class="header-section-number">10.4.2.1</span> Constant Effects Linear DiD Model</h4>
<p>Causal effects are constant across individuals and time,
<span class="math display">\[
\E[Y_{it}(1) - Y_{it}(0)] = \tau .
\]</span>
The effects of time <span class="math inline">\(\delta_t\)</span> and individuals <span class="math inline">\(\alpha_g\)</span> are linearly separable,
<span class="math display">\[
\E[Y_{it}(0)] = \delta_t + \alpha_{g} .
\]</span>
Then the model is,
<span class="math display">\[
Y_{igt} = \delta_{t}
\]</span></p>
</div>
</div>
<div id="threats-to-identification" class="section level3">
<h3><span class="header-section-number">10.4.3</span> Threats to identification</h3>
<ul>
<li>Treatment independent of idiosyncratic shocks, so variation in outcome is the same for treated and control groups.</li>
<li>Example: Ashenfelter’s Dip is an empirical phenomena in which people who enroll in job training programs see their earnings decline .</li>
<li>It may be possible to condition on covariates (control) in order to make treatment and shocks independent.</li>
</ul>
</div>
<div id="robustness-checks" class="section level3">
<h3><span class="header-section-number">10.4.4</span> Robustness Checks</h3>
<ul>
<li><p>Lags and Leads</p>
<ul>
<li>If <span class="math inline">\(D_{igt}\)</span> causes <span class="math inline">\(Y_{igt}\)</span>, then current and lagged values should have an effect on <span class="math inline">\(Y_{igt}\)</span>, but future values of <span class="math inline">\(D_{igt}\)</span> should not.</li>
</ul></li>
<li><p>Placebo permutation test. Randomly assign the intervention(s) to create the sampling distribution of the null hypothesis.</p></li>
<li><p>Use different comparison groups. Different groups should
have the same affect.</p></li>
<li><p>Use an outcome variable that you know is not affected by the intervention.
If DiD estimates not zero, then there is some other difference between groups.</p></li>
<li><p>Time Trends</p>
<ul>
<li><p>If more than two time periods, add unit specific linear trends to regression DiD model.
<span class="math display">\[
Y_{igt} = \delta_{t} + \tau G_{i} + \alpha_{0g} + \alpha_{1g} \times t + \epsilon_{igt} ,
\]</span>
where <span class="math inline">\(\alpha_{0g}\)</span> are group fixed effects, <span class="math inline">\(\delta_t\)</span> is the overall (not necessarily linear) time trend,
and <span class="math inline">\(\alpha_{1g}\)</span> is the group linear time trend.</p></li>
<li><p>Helps detect if varying trends when estimated from pre-treatment data.</p></li>
</ul></li>
</ul>
</div>
<div id="extensions" class="section level3">
<h3><span class="header-section-number">10.4.5</span> Extensions</h3>
<p>The general DiD model relies on linear-separability and constant treatment effects.</p>
<p>The <strong>parallel trends</strong> assumption is the important assumption:
<span class="math display">\[
\E[ Y_{i1}(0) - Y_{i0} | X_i, G_i = 1] = \E[ Y_{i1}(0) - Y_{i0} | X_i, G_i = 0].
\]</span>
It says that the potential trend under control is the same for the control and treated groups, conditional on covariates.</p>
<p>With the parallel trends assumption unconditional ATT is,
<span class="math display">\[
\E[Y_{i1}(1) - Y_{i1}(0) | G_i = 1] = \E_{X}[\E[Y_{i1}(1) - Y_{i1}(0) | G_i = 1]] =
, 𝐺𝑖 = 1]].
\]</span>
What we need is an estimator of each CEF.
This doesn’t need to be linear or parametric.</p>
<p>However, cannot estimate ATE because <span class="math inline">\(\E(Y_{i1}(1) | X_i, G_i = 0)\)</span> could be anything.</p>
<p>With covariates we can estimate conditional DiD in several ways.</p>
<ul>
<li>Regression DiD</li>
<li>Match on <span class="math inline">\(X_i\)</span> and then use regular DiD</li>
<li>Weighting approaches Abadie (2005)</li>
</ul>
<p>Regression DiD includes <span class="math inline">\(X_i\)</span> in a linear, additive manner,
<span class="math display">\[
Y_{it} = \mu + x&#39;_i \beta_t + \delta I(t = 1) + \tau(I(t = 1) \times G_i) + \epsilon_{it}
\]</span>
If there are repeated observations, take difference between <span class="math inline">\(t = 0\)</span> and <span class="math inline">\(t = 1\)</span>,
<span class="math display">\[
Y_{i1} - Y_{i0} = \delta + x&#39;_i \beta + \tau G_i +
(\epsilon_{i1} - \epsilon_{i0})
\]</span>
Have <span class="math inline">\(\beta = \beta_1 - \beta_0\)</span>.
Because everyone is untreated in first period, <span class="math inline">\(D_{i1} - D_{i0} = D_{i1}\)</span>.</p>
<p>For panel data, regress changes on treatment.</p>
<p>Depends on constant effects and linearity in <span class="math inline">\(X_i\)</span>.
Matching could reduce model dependence.</p>
</div>
<div id="standard-error-issues" class="section level3">
<h3><span class="header-section-number">10.4.6</span> Standard Error Issues</h3>
<div id="serial-correlation" class="section level4">
<h4><span class="header-section-number">10.4.6.1</span> Serial Correlation</h4>
<p>A major issue with errors in difference-in-difference models is serial correlation <span class="citation">(Bertrand, Duflo, and Mullainathan <a href="#ref-BertrandDufloMullainathan2004a">2004</a>)</span>.
Consider the DiD model,
<span class="math display">\[
Y_{igt} = \mu_g + \delta_t + \tau (I_{it} \times G_i) + \nu_{gt} + \epsilon_{igt} .
\]</span>
The problem is that <span class="math inline">\(\nu_{gt}\)</span> can be serially correlated
<span class="math display">\[
Cor(\nu_{gt}, \nu_{gs}) \neq 0 \text{ for } s \neq t .
\]</span>
An example called <span class="math inline">\(AR(1)\)</span> serial correlation is when each <span class="math inline">\(\nu_t\)</span> is a function of its lag,
<span class="math display">\[
\nu_t = \rho \nu_{t - 1} + \eta_t \text{ where } \rho \in (0, 1).
\]</span>
Since errors are usually positively correlated, the outcomes are correlated over time and effectively there are fewer independent observations in the sample; it’s almost as if the same observation was simply copy and pasted over time with a little error added.
This will mean that the standard errors will likely be too optimistic (too narrow).
See <span class="citation">Bertrand, Duflo, and Mullainathan (<a href="#ref-BertrandDufloMullainathan2004a">2004</a>)</span> for a longer discussion of this.
here are a couple of solutions:</p>
<ul>
<li>Clustered standard errors at the <strong>group</strong> level</li>
<li>Clustered bootstrap (re-sample groups, not individual observations)</li>
<li>Aggregated to <span class="math inline">\(g\)</span> units with two time periods each: pre- and post-intervention. Since correlation makes the panel data closer to simply a two-period DiD, this takes that all the way.</li>
</ul>
<p>All these solutions depend on larger numbers of groups.
Do not use the off-the-shelf clustered standard errors unless the number of groups is large.
See <span class="citation">Esarey and Menger (<a href="#ref-EsareyMenger2018a">2018</a>)</span> for an extensive discussion of this.
Also see the associated <strong>clusterSE</strong> package and <strong>clubSandwich</strong> for implementations of
cluster robust standard errors that work with smaller numbers of clusters.</p>
</div>
</div>
<div id="other-did-approaches" class="section level3">
<h3><span class="header-section-number">10.4.7</span> Other DiD Approaches</h3>
<p>Changes-in-changes <span class="citation">(Athey and Imbens <a href="#ref-AtheyImbens2006a">2006</a>)</span> generalizes DiD to allow for different changes in the distribution of <span class="math inline">\(Y_{it}\)</span>, not just the mean.
This allows for estimating ATT or any changes in distribution (quantiles, variance, etc.).
Unfortunately requires more data than estimating the mean.</p>
<p>Synthetic controls is used when there is one treated group, but many controls. (See Abadie and Gardeazabel and the paper on the Seattle minimum wage).</p>
<p>The basic idea is to compare the time series of the outcome in the treated group to a control.</p>
<ul>
<li>But what if there are many control group?</li>
<li>What if they aren’t comparable to the treated?</li>
</ul>
<p>Synthetic control uses a weighted average of different controls.</p>
</div>
</div>
<div id="lagged-dependent-variables" class="section level2">
<h2><span class="header-section-number">10.5</span> Lagged Dependent Variables</h2>
<p>This is a different thing …</p>
<p>See <span class="citation">Dafoe (<a href="#ref-Dafoe2018a">2018</a>)</span> for advice on when to use LDV.</p>
<p>A different model is to assume a lagged dependent variable,
<span class="math display">\[
Y_{i,t} = \rho Y&#39;_{i,t-1} + X&#39;_{i,t} \beta + \varepsilon_{i,t}
\]</span>
This captures some of the unit-specific aspects that the fixed effects capture.
However, the LDV model is making a different assumption than fixed effects.
The FE model assumes that each unit has a separate effect that is constant over time, while the LDV model assumes that anything specific about a unit is captured through the value of the dependent variable in the previous period.</p>
<p>Beck and Katz recommendation of LDV with PCSE.</p>
<p>The LDV and Fixed Effects models make different assumptions, and they are not nested.
So why not combine them into a single model?
<span class="math display">\[
Y_{i,t} = \rho Y&#39;_{i,t-1} + X&#39;_{i,t} \beta + \alpha_i + \varepsilon_{i,t} .
\]</span>
There is a problem with this approach. OLS is biased. The fixed effect estimator includes demeaned values of the outcome variable and covariates. S
the FE model with a LDV will use <span class="math inline">\(Y_{i,t - 1} - \bar{Y}_{i,t-1}\)</span>. This average includes <span class="math inline">\(Y_{i,t}\)</span> and <span class="math inline">\(Y_{i,t} = ... + \varepsilon_{i,t}\)</span>. T
us by construction, <span class="math inline">\(Y_{i,t} - \bar{Y}_{i,t-1}\)</span> is correlated with the errors.</p>
<p>So what can we do about this? There are two options.</p>
<ol style="list-style-type: decimal">
<li><p>Ignore it. The bias is proportional to <span class="math inline">\(1/T\)</span>. In panels with 20 or more periods, the bias may be small.
Moreover, the bias is generally largest in the coefficient of the lagged dependent variable itself,
which may not be of primary interest. Accept the bias.</p></li>
<li><p>Use both LDV and FE models. The LDV and FE methods can bound the effects of the coefficient of interest. See <span class="citation">Angrist and Pischke (<a href="#ref-AngristPischke2009a">2009</a>)</span>.</p></li>
<li><p>Use IV methods to instrument the lagged dependent variable. See Arrellano-Bond methods.</p></li>
</ol>
<p>This is a case where the difference between panel and TSCS is important.
In many TSCS settings with larger <span class="math inline">\(T\)</span> it is probably fine to estimate fixed effects with LDV.
However, if you have panel data model with few <span class="math inline">\(T\)</span>, then you should use either method 2 or 3.</p>
</div>
<div id="random-effects" class="section level2">
<h2><span class="header-section-number">10.6</span> Random Effects</h2>
<p>Consider the panel data model,
<span class="math display">\[
Y_{i,t} = \alpha + X&#39;_{i,t} \beta + u_i + \varepsilon_{i,t}
\]</span>
In fixed effect, the errors are assumed to be uncorrelated with both the unit effects and the covariates,
<span class="math display">\[
\E(\varepsilon_{i,t} | X_{i}, u_i) = 0 .
\]</span>
With random effects we make an additional assumption, the unit effects are uncorrelated with the covariates,
<span class="math display">\[
\E(u_i | X_i) = \E(u_i) = 0 .
\]</span></p>
<p>What this means that under the assumptions of random effects, omitting <span class="math inline">\(u_i\)</span> would not bias <span class="math inline">\(\beta\)</span> since they are assumed to be uncorrelated with <span class="math inline">\(X\)</span>. Thus, there’s no omitted variable bias.</p>
<p>So why use random effects? To fix standard errors.
<span class="math display">\[
Y_{i,t} = X&#39;_{i,t} \beta + \nu_i
\]</span>
where <span class="math inline">\(\nu_i = u_i + \varepsilon_{i,t}\)</span>.
However, this means that
<span class="math display">\[
\Cov(Y_{i,1}, Y_{i,2} | X_{i,t}) = \sigma^2_u .
\]</span>
This violates the OLS assumption of non-autocorrelation.
Using random effects gets consistent standard errors.</p>
<div id="how-to-estimate-random-effects" class="section level3">
<h3><span class="header-section-number">10.6.1</span> How to estimate random effects?</h3>
<p>There are a variety of methods, but the econometric method is to use <strong>quasi-demeaning</strong> or <strong>partial pooling</strong>,
<span class="math display">\[
(Y_{i,t} - \theta \bar{Y}_i) = (X_{i,t} - \bar{X}_i)&#39; \beta + (\nu_{i,t} - \theta \Var{\nu}_i)
\]</span>
where <span class="math inline">\(\theta \in [0, 1]\)</span> where <span class="math inline">\(\theta = 0\)</span> is OLS, and <span class="math inline">\(\theta = 1\)</span> is fixed effects.
Some math (TM) shows,
<span class="math display">\[
\theta = 1 - \left( \sigma_u^2 / (\sigma^2_u + T \sigma^2_epsilon) \right)^{1/2} .
\]</span>
The <strong>random effects estimator</strong> runs pooled OLS on this model, but replaces <span class="math inline">\(\theta\)</span> with the estimate <span class="math inline">\(\hat{\theta}\)</span>.</p>
<p>See the R package <strong>plm</strong>.</p>
<p>The R package <strong>lme4</strong> and Bayesian methods, e.g. Gelman and Hill, take a different approach to estimating random effects.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-AngristPischke2014a">
<p>Angrist, Joshua D., and Jörn-Steffen Pischke. 2009. <em>Mostly Harmless Econometrics: An Empiricist’s Companion</em>. Pr.</p> 2014. <em>Mastering ‘Metrics</em>. Princeton UP.</p>
</div>
<div id="ref-AngristPischke2009a">
<p>Angrist, Joshua D., and Jörn-Steffen Pischke. 2009. <em>Mostly Harmless Econometrics: An Empiricist’s Companion</em>. Pr.</p>
</div>
<div id="ref-Blackwell2013a">
<p>Blackwell, Matthew. 2013. “A Framework for Dynamic Causal Inference in Political Science.” <em>American Journal of Political Science</em> 57 (2): 504–20. <a href="https://doi.org/10.1111/j.1540-5907.2012.00626.x" class="uri">https://doi.org/10.1111/j.1540-5907.2012.00626.x</a>.</p>
</div>
<div id="ref-BertrandDufloMullainathan2004a">
<p>Bertrand, M., E. Duflo, and S. Mullainathan. 2004. “How Much Should We Trust Differences-in-Differences Estimates?” <em>The Quarterly Journal of Economics</em> 119 (1). Oxford University Press (OUP): 249–75. <a href="https://doi.org/10.1162/003355304772839588" class="uri">https://doi.org/10.1162/003355304772839588</a>.</p>
</div>
<div id="ref-EsareyMenger2018a">
<p>Esarey, Justin, and Andrew Menger. 2018. “Practical and Effective Approaches to Dealing with Clustered Data.” <em>Political Science Research and Methods</em>, January. Cambridge University Press (CUP), 1–19. <a href="https://doi.org/10.1017/psrm.2017.42" class="uri">https://doi.org/10.1017/psrm.2017.42</a>.</p>
</div>
<div id="ref-AtheyImbens2006a">
<p>Athey, Susan, and Guido W. Imbens. 2006. “Identification and Inference in Nonlinear Difference-in-Differences Models.” <em>Econometrica</em> 74 (2). [Wiley, Econometric Society]: 431–97. <a href="http://www.jstor.org/stable/3598807" class="uri">http://www.jstor.org/stable/3598807</a>.</p>
</div>
<div id="ref-Dafoe2018a">
<p>Dafoe, Allan. 2018. “Nonparametric Identification of Causal Effects Under Temporal Dependence.” <em>Sociological Methods &amp; Research</em> 47 (2): 136–68. <a href="https://doi.org/10.1177/0049124115613784" class="uri">https://doi.org/10.1177/0049124115613784</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression-discontinuity.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/jrnold/intro-method-notes/edit/master/panel.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
