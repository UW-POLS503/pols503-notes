<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Data Analysis Notes</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>">
  <meta name="generator" content="bookdown 0.3.6 and GitBook 2.6.7">

  <meta property="og:title" content="Data Analysis Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>" />
  <meta name="github-repo" content="jrnold/intro-methods-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Data Analysis Notes" />
  
  <meta name="twitter:description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>" />
  

<meta name="author" content="Jeffrey B. Arnold">


<meta name="date" content="2017-04-19">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="robust-regression.html">
<link rel="next" href="miscellaneous-regression-stuff.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<link href="libs/plotlyjs-1.16.3/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.16.3/plotly-latest.min.js"></script>
<script src="libs/plotly-binding-4.5.6/plotly.js"></script>



<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intro Method Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="part"><span><b>I Exploratory Data Analysis</b></span></li>
<li class="part"><span><b>II Probability</b></span></li>
<li class="part"><span><b>III Inference</b></span></li>
<li class="part"><span><b>IV Linear Regresssion</b></span></li>
<li class="chapter" data-level="2" data-path="bivariate-ols.html"><a href="bivariate-ols.html"><i class="fa fa-check"></i><b>2</b> Bivariate OLS</a><ul>
<li class="chapter" data-level="2.0.1" data-path="bivariate-ols.html"><a href="bivariate-ols.html#ols-is-the-weighted-sum-of-outcomes"><i class="fa fa-check"></i><b>2.0.1</b> OLS is the weighted sum of outcomes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html"><i class="fa fa-check"></i><b>3</b> Goodness of Fit</a><ul>
<li class="chapter" data-level="3.1" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html#root-mean-squared-error-and-standard-error"><i class="fa fa-check"></i><b>3.1</b> Root Mean Squared Error and Standard Error</a></li>
<li class="chapter" data-level="3.2" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html#r-squared"><i class="fa fa-check"></i><b>3.2</b> R squared</a></li>
<li class="chapter" data-level="3.3" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html#maximum-likelihood"><i class="fa fa-check"></i><b>3.3</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="3.4" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html#regression-line"><i class="fa fa-check"></i><b>3.4</b> Regression Line</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>4</b> Multiple Regression</a></li>
<li class="chapter" data-level="5" data-path="what-is-regression.html"><a href="what-is-regression.html"><i class="fa fa-check"></i><b>5</b> What is Regression?</a><ul>
<li class="chapter" data-level="5.1" data-path="what-is-regression.html"><a href="what-is-regression.html#what-is-regression-and-what-is-it-used-for"><i class="fa fa-check"></i><b>5.1</b> What is Regression and What is it Used For ?</a></li>
<li class="chapter" data-level="5.2" data-path="what-is-regression.html"><a href="what-is-regression.html#joint-vs.conditional-models"><i class="fa fa-check"></i><b>5.2</b> Joint vs.Â Conditional models</a></li>
<li class="chapter" data-level="5.3" data-path="what-is-regression.html"><a href="what-is-regression.html#conditional-expectation-function"><i class="fa fa-check"></i><b>5.3</b> Conditional expectation function</a><ul>
<li class="chapter" data-level="5.3.1" data-path="what-is-regression.html"><a href="what-is-regression.html#discrete-covariates"><i class="fa fa-check"></i><b>5.3.1</b> Discrete Covariates</a></li>
<li class="chapter" data-level="5.3.2" data-path="what-is-regression.html"><a href="what-is-regression.html#continuous-covariates"><i class="fa fa-check"></i><b>5.3.2</b> Continuous Covariates</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="interpreting-coefficients.html"><a href="interpreting-coefficients.html"><i class="fa fa-check"></i><b>6</b> Interpreting Coefficients</a><ul>
<li class="chapter" data-level="6.1" data-path="interpreting-coefficients.html"><a href="interpreting-coefficients.html#interactions-and-polynomials"><i class="fa fa-check"></i><b>6.1</b> Interactions and Polynomials</a></li>
<li class="chapter" data-level="6.2" data-path="interpreting-coefficients.html"><a href="interpreting-coefficients.html#average-marginal-effects"><i class="fa fa-check"></i><b>6.2</b> Average Marginal Effects</a></li>
<li class="chapter" data-level="6.3" data-path="interpreting-coefficients.html"><a href="interpreting-coefficients.html#standardized-coefficients"><i class="fa fa-check"></i><b>6.3</b> Standardized Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="regression-inference.html"><a href="regression-inference.html"><i class="fa fa-check"></i><b>7</b> Regression Inference</a><ul>
<li class="chapter" data-level="7.1" data-path="regression-inference.html"><a href="regression-inference.html#prerequisites"><i class="fa fa-check"></i><b>7.1</b> Prerequisites</a></li>
<li class="chapter" data-level="7.2" data-path="regression-inference.html"><a href="regression-inference.html#sampling-distribution-and-standard-errors-of-coefficients"><i class="fa fa-check"></i><b>7.2</b> Sampling Distribution and Standard Errors of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="single-coefficient.html"><a href="single-coefficient.html"><i class="fa fa-check"></i><b>8</b> Single Coefficient</a><ul>
<li class="chapter" data-level="8.1" data-path="single-coefficient.html"><a href="single-coefficient.html#multiple-coefficients"><i class="fa fa-check"></i><b>8.1</b> Multiple Coefficients</a></li>
<li class="chapter" data-level="8.2" data-path="single-coefficient.html"><a href="single-coefficient.html#general-linear-and-non-linear-tests-of-coefficients"><i class="fa fa-check"></i><b>8.2</b> General Linear and Non-linear tests of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="multiple-testing.html"><a href="multiple-testing.html"><i class="fa fa-check"></i><b>9</b> Multiple Testing</a><ul>
<li class="chapter" data-level="9.1" data-path="multiple-testing.html"><a href="multiple-testing.html#multiple-testing-1"><i class="fa fa-check"></i><b>9.1</b> Multiple Testing</a></li>
<li class="chapter" data-level="9.2" data-path="multiple-testing.html"><a href="multiple-testing.html#data-snooping"><i class="fa fa-check"></i><b>9.2</b> Data snooping</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html"><i class="fa fa-check"></i><b>10</b> Omitted Variable Bias</a><ul>
<li class="chapter" data-level="10.1" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#prerequisites-1"><i class="fa fa-check"></i><b>10.1</b> Prerequisites</a></li>
<li class="chapter" data-level="10.2" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#simpsons-paradox"><i class="fa fa-check"></i><b>10.2</b> Simpsonâs Paradox</a></li>
<li class="chapter" data-level="10.3" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#omitted-variable-bias-1"><i class="fa fa-check"></i><b>10.3</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="10.4" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#measurement-error"><i class="fa fa-check"></i><b>10.4</b> Measurement Error</a><ul>
<li class="chapter" data-level="10.4.1" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#whats-the-problem"><i class="fa fa-check"></i><b>10.4.1</b> Whatâs the problem?</a></li>
<li class="chapter" data-level="10.4.2" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#what-to-do-about-it"><i class="fa fa-check"></i><b>10.4.2</b> What to do about it?</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#more-information"><i class="fa fa-check"></i><b>10.5</b> More Information</a><ul>
<li class="chapter" data-level="10.5.1" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#simpsons-paradox-1"><i class="fa fa-check"></i><b>10.5.1</b> Simpsonâs Paradox</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="outliers.html"><a href="outliers.html"><i class="fa fa-check"></i><b>11</b> Outliers</a></li>
<li class="chapter" data-level="12" data-path="problems-with-errors.html"><a href="problems-with-errors.html"><i class="fa fa-check"></i><b>12</b> Problems with Errors</a><ul>
<li class="chapter" data-level="12.1" data-path="problems-with-errors.html"><a href="problems-with-errors.html#prerequisites-2"><i class="fa fa-check"></i><b>12.1</b> Prerequisites</a></li>
<li class="chapter" data-level="12.2" data-path="problems-with-errors.html"><a href="problems-with-errors.html#heteroskedasticity"><i class="fa fa-check"></i><b>12.2</b> Heteroskedasticity</a><ul>
<li class="chapter" data-level="12.2.1" data-path="problems-with-errors.html"><a href="problems-with-errors.html#example-duncans-occupation-data"><i class="fa fa-check"></i><b>12.2.1</b> Example: Duncanâs Occupation Data</a></li>
<li class="chapter" data-level="12.2.2" data-path="problems-with-errors.html"><a href="problems-with-errors.html#notes"><i class="fa fa-check"></i><b>12.2.2</b> Notes</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="problems-with-errors.html"><a href="problems-with-errors.html#correlated-errors"><i class="fa fa-check"></i><b>12.3</b> Correlated Errors</a></li>
<li class="chapter" data-level="12.4" data-path="problems-with-errors.html"><a href="problems-with-errors.html#non-normal-errors"><i class="fa fa-check"></i><b>12.4</b> Non-normal Errors</a></li>
<li class="chapter" data-level="12.5" data-path="problems-with-errors.html"><a href="problems-with-errors.html#bootstrapping"><i class="fa fa-check"></i><b>12.5</b> Bootstrapping</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="weighted-regression.html"><a href="weighted-regression.html"><i class="fa fa-check"></i><b>13</b> Weighted Regression</a><ul>
<li class="chapter" data-level="13.1" data-path="weighted-regression.html"><a href="weighted-regression.html#weighted-least-squares-wls"><i class="fa fa-check"></i><b>13.1</b> Weighted Least Squares (WLS)</a></li>
<li class="chapter" data-level="13.2" data-path="weighted-regression.html"><a href="weighted-regression.html#when-should-you-use-wls"><i class="fa fa-check"></i><b>13.2</b> When should you use WLS?</a></li>
<li class="chapter" data-level="13.3" data-path="weighted-regression.html"><a href="weighted-regression.html#correcting-for-known-heteroskedasticity"><i class="fa fa-check"></i><b>13.3</b> Correcting for Known Heteroskedasticity</a></li>
<li class="chapter" data-level="13.4" data-path="weighted-regression.html"><a href="weighted-regression.html#sampling-weights"><i class="fa fa-check"></i><b>13.4</b> Sampling Weights</a></li>
<li class="chapter" data-level="13.5" data-path="weighted-regression.html"><a href="weighted-regression.html#references"><i class="fa fa-check"></i><b>13.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="discrete-outcome-variables.html"><a href="discrete-outcome-variables.html"><i class="fa fa-check"></i><b>14</b> Discrete Outcome Variables</a><ul>
<li class="chapter" data-level="14.1" data-path="discrete-outcome-variables.html"><a href="discrete-outcome-variables.html#linear-probability-model"><i class="fa fa-check"></i><b>14.1</b> Linear Probability Model</a></li>
<li class="chapter" data-level="14.2" data-path="discrete-outcome-variables.html"><a href="discrete-outcome-variables.html#logit-model"><i class="fa fa-check"></i><b>14.2</b> Logit Model</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="robust-regression.html"><a href="robust-regression.html"><i class="fa fa-check"></i><b>15</b> Robust Regression</a><ul>
<li class="chapter" data-level="15.1" data-path="robust-regression.html"><a href="robust-regression.html#prerequites"><i class="fa fa-check"></i><b>15.1</b> Prerequites</a></li>
<li class="chapter" data-level="15.2" data-path="robust-regression.html"><a href="robust-regression.html#examples"><i class="fa fa-check"></i><b>15.2</b> Examples</a></li>
<li class="chapter" data-level="15.3" data-path="robust-regression.html"><a href="robust-regression.html#notes-1"><i class="fa fa-check"></i><b>15.3</b> Notes</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html"><i class="fa fa-check"></i><b>16</b> Prediction and Model Comparison</a><ul>
<li class="chapter" data-level="16.1" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#prerequisites-3"><i class="fa fa-check"></i><b>16.1</b> Prerequisites</a></li>
<li class="chapter" data-level="16.2" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#measures-of-prediction"><i class="fa fa-check"></i><b>16.2</b> Measures of Prediction</a></li>
<li class="chapter" data-level="16.3" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#model-comparison"><i class="fa fa-check"></i><b>16.3</b> Model Comparison</a></li>
<li class="chapter" data-level="16.4" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#example-predicting-the-price-of-wine"><i class="fa fa-check"></i><b>16.4</b> Example: Predicting the Price of Wine</a></li>
<li class="chapter" data-level="16.5" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#cross-validation"><i class="fa fa-check"></i><b>16.5</b> Cross-Validation</a></li>
<li class="chapter" data-level="16.6" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#out-of-sample-error"><i class="fa fa-check"></i><b>16.6</b> Out of Sample Error</a><ul>
<li class="chapter" data-level="16.6.1" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#held-out-data"><i class="fa fa-check"></i><b>16.6.1</b> Held-out data</a></li>
<li class="chapter" data-level="16.6.2" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>16.6.2</b> Leave-One-Out Cross-Validation</a></li>
<li class="chapter" data-level="16.6.3" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>16.6.3</b> k-fold Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#analytic-covariance-methods"><i class="fa fa-check"></i><b>16.7</b> Analytic Covariance Methods</a></li>
<li class="chapter" data-level="16.8" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#further-resources"><i class="fa fa-check"></i><b>16.8</b> Further Resources</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="miscellaneous-regression-stuff.html"><a href="miscellaneous-regression-stuff.html"><i class="fa fa-check"></i><b>17</b> Miscellaneous Regression Stuff</a><ul>
<li class="chapter" data-level="17.1" data-path="miscellaneous-regression-stuff.html"><a href="miscellaneous-regression-stuff.html#anscombe-quartet"><i class="fa fa-check"></i><b>17.1</b> Anscombe quartet</a></li>
<li class="chapter" data-level="17.2" data-path="miscellaneous-regression-stuff.html"><a href="miscellaneous-regression-stuff.html#correlation-plots"><i class="fa fa-check"></i><b>17.2</b> Correlation Plots</a></li>
</ul></li>
<li class="part"><span><b>V Programming</b></span></li>
<li class="chapter" data-level="18" data-path="rs-forumula-syntax.html"><a href="rs-forumula-syntax.html"><i class="fa fa-check"></i><b>18</b> Râs Forumula Syntax</a><ul>
<li class="chapter" data-level="18.1" data-path="rs-forumula-syntax.html"><a href="rs-forumula-syntax.html#setup"><i class="fa fa-check"></i><b>18.1</b> Setup</a></li>
<li class="chapter" data-level="18.2" data-path="rs-forumula-syntax.html"><a href="rs-forumula-syntax.html#introduction-to-formula-objects"><i class="fa fa-check"></i><b>18.2</b> Introduction to Formula Objects</a></li>
<li class="chapter" data-level="18.3" data-path="rs-forumula-syntax.html"><a href="rs-forumula-syntax.html#programming-with-formulas"><i class="fa fa-check"></i><b>18.3</b> Programming with Formulas</a></li>
</ul></li>
<li class="part"><span><b>VI Examples</b></span></li>
<li class="chapter" data-level="19" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html"><i class="fa fa-check"></i><b>19</b> Duncan Occupational Prestige</a><ul>
<li class="chapter" data-level="19.1" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#setup-1"><i class="fa fa-check"></i><b>19.1</b> Setup</a></li>
<li class="chapter" data-level="19.2" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#coefficients-standard-errors"><i class="fa fa-check"></i><b>19.2</b> Coefficients, Standard errors</a></li>
<li class="chapter" data-level="19.3" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#residuals-fitted-values"><i class="fa fa-check"></i><b>19.3</b> Residuals, Fitted Values,</a></li>
<li class="chapter" data-level="19.4" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#broom"><i class="fa fa-check"></i><b>19.4</b> Broom</a></li>
<li class="chapter" data-level="19.5" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#plotting-fitted-regression-results"><i class="fa fa-check"></i><b>19.5</b> Plotting Fitted Regression Results</a></li>
</ul></li>
<li class="part"><span><b>VII Presentation</b></span></li>
<li class="chapter" data-level="20" data-path="formatting-tables.html"><a href="formatting-tables.html"><i class="fa fa-check"></i><b>20</b> Formatting Tables</a><ul>
<li class="chapter" data-level="20.1" data-path="formatting-tables.html"><a href="formatting-tables.html#overview-of-packages"><i class="fa fa-check"></i><b>20.1</b> Overview of Packages</a></li>
<li class="chapter" data-level="20.2" data-path="formatting-tables.html"><a href="formatting-tables.html#summary-statistic-table-example"><i class="fa fa-check"></i><b>20.2</b> Summary Statistic Table Example</a></li>
<li class="chapter" data-level="20.3" data-path="formatting-tables.html"><a href="formatting-tables.html#regression-table-example"><i class="fa fa-check"></i><b>20.3</b> Regression Table Example</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="reproducible-research.html"><a href="reproducible-research.html"><i class="fa fa-check"></i><b>21</b> Reproducible Research</a></li>
<li class="chapter" data-level="22" data-path="writing-resources.html"><a href="writing-resources.html"><i class="fa fa-check"></i><b>22</b> Writing Resources</a><ul>
<li class="chapter" data-level="22.1" data-path="writing-resources.html"><a href="writing-resources.html#writing-and-organizing-papers"><i class="fa fa-check"></i><b>22.1</b> Writing and Organizing Papers</a></li>
<li class="chapter" data-level="22.2" data-path="writing-resources.html"><a href="writing-resources.html#finding-research-ideas"><i class="fa fa-check"></i><b>22.2</b> Finding Research Ideas</a></li>
<li class="chapter" data-level="22.3" data-path="writing-resources.html"><a href="writing-resources.html#replications"><i class="fa fa-check"></i><b>22.3</b> Replications</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="multivariate-normal-distribution.html"><a href="multivariate-normal-distribution.html"><i class="fa fa-check"></i><b>A</b> Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
\[
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Bias}{Bias}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\RMSE}{RMSE}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\T}{'}

\newcommand{\distr}[1]{\mathcal{#1}}
\newcommand{\dnorm}{\distr{N}}
\newcommand{\dmvnorm}[1]{\distr{N}_{#1}}
\newcommand{\dt}[1]{\distr{T}_{#1}}

\newcommand{\cia}{\perp\!\!\!\perp}
\DeclareMathOperator*{\plim}{plim}
\]
<div id="prediction-and-model-comparison" class="section level1">
<h1><span class="header-section-number">Chapter 16</span> Prediction and Model Comparison</h1>
<div id="prerequisites-3" class="section level2">
<h2><span class="header-section-number">16.1</span> Prerequisites</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;tidyverse&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;broom&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;modelr&quot;</span>)</code></pre></div>
<pre><code>## 
## Attaching package: &#39;modelr&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:broom&#39;:
## 
##     bootstrap</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;stringr&quot;</span>)</code></pre></div>
</div>
<div id="measures-of-prediction" class="section level2">
<h2><span class="header-section-number">16.2</span> Measures of Prediction</h2>
<p>Ideally the prediction measure should be derived from the problem at hand; There is no uniformly correct measure of accuracy, so absent other costs the the analysis should include the costs of outcomes to the [analyst(<a href="https://en.wikipedia.org/wiki/Decision_theory" class="uri">https://en.wikipedia.org/wiki/Decision_theory</a>).</p>
<p>Categorical variables</p>
<ul>
<li>Accuracy: (True Positive) + (True Negative) / (all observations)</li>
<li>Precision: (True Positives) / (Classifier Positives)</li>
<li>Sensitivity (recall): (True Positive) / (All positives)</li>
<li>Specificity: (True negative) / (Classifier negatives)</li>
<li><a href="https://en.wikipedia.org/wiki/F1_score">F1 score</a> which balances precision (TP / (TP + FP)) and recall (TP / (TP + FN)) as (precision * recall) / (precision + recall)</li>
</ul>
<p>Continuous Variables</p>
<ul>
<li>Root Mean Squared Error (RMSE): <span class="math inline">\(\frac{1}{m} \sum_i (\hat{y}_i - y_i)^2\)</span>. This is weights large errors heavily since it uses âquadratic errorsâ.</li>
<li>Mean Absolute Devision (MAD): <span class="math inline">\(\frac{1}{m} \sum_i \|\hat{y}_i - y_i \|\)</span>. This is robust to large errors, but sensitive to the scale of the forecasts.</li>
<li>Mean Absolute Percentage Error (MAPE): <span class="math inline">\(\frac{1}{m} \sum_i \| (\hat{y}_i - y_i) / y_i \|\)</span>.</li>
</ul>
</div>
<div id="model-comparison" class="section level2">
<h2><span class="header-section-number">16.3</span> Model Comparison</h2>
<p>For comparing models in terms of prediction we want to compare them on their expected error on future data, not their in-sample error. It is easy to minimize in-sample error, use the every-observation-is-special modelâhave a predictor for each observation. However, that model will have no ability to predict future observations.</p>
<p>The fundamental problem to estimating the expected error of the model is that we donât have the future data to evaluate it. Even if we acquire new data that did not exist at the time of fitting the model, all that can be done is to <em>retrospectively</em> evaluate the model performance, perhaps giving a better estimate of the expected error of the model in the future. Yet, any errors of the model with respect to any future data will still be unknown. For example the errors of model based forecasts of the popular vote share, electoral votes, or winner of the U.S. presidential election of 2016 can be transparently evaluated since they can be made prior to the data, and short of access to a time machine, the models cannot overfit or peak on future data. After the election the models can be evaluated. However, at that point it is their expected error in the next election in 2020 which is of interest, and that is still unknown.</p>
<p>There are two main approaches to estimating the prediction error</p>
<ul>
<li>cross validation: Split the data into test and training subsets. The model is fit on the training data, and predictions are made on the test data. This is often done repeatedly.</li>
<li>covariance estimates: These are analytic estimates of the expected error, usually restricted to either linear models and/or only asymptotically valid. But since they do not require resampling, they are fast.</li>
</ul>
<p>So when do these approaches work? When do measures based on in-sample data extrapolate to non-sample errors? Like pretty much every statistical method, they work when the sample used to fit the model resembles the data on which which inference is being made.</p>
</div>
<div id="example-predicting-the-price-of-wine" class="section level2">
<h2><span class="header-section-number">16.4</span> Example: Predicting the Price of Wine</h2>
<p>The <code>bordeaux</code> dataset contains the prices of vintages of Bordeaux wines sold at auction in New York, as well as the the weather and rainfall for the years of the wine. This data was used by economist, Orley Ashenfelter, to show that the quality of a wine vintage can be as measured by its price, can largely be predicted by its age, and the weather (temperature and rainfall) of its vintage year. At the time, these prediction were not taken kindly by the wine conoisseurs.[^wine]</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># devtools::install_github(&quot;jrnold/datums&quot;)</span>
bordeaux &lt;-<span class="st"> </span>datums<span class="op">::</span>bordeaux <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">lprice =</span> <span class="kw">log</span>(price <span class="op">/</span><span class="st"> </span><span class="dv">100</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">filter</span>(<span class="op">!</span><span class="kw">is.na</span>(price))</code></pre></div>
<p>The data contains 27 prices of Bordeaux wines sold at auction in New York in 1993 for vintages from 1952 to 1980; the years 1952 and 1954 were excluded because they were rarely sold. Prices are measured as an index where 100 is the price of the 1961. Since prices are 7</p>
<p>The dataset also includes three predictors of the price of each vintage:</p>
<ul>
<li><code>time_sv</code>: Time since the vintage, where 0 is 1983.</li>
<li><code>wrain</code>: Winter (OctoberâMarch) rain</li>
<li><code>hrain</code>: Harvest (AugustâSeptember) rain</li>
<li><code>degrees</code>: Average temperature in degrees centigrade from April to September in the vintage year.</li>
</ul>
<p>The first variable to consider is the age of the vintage and the price:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="kw">filter</span>(bordeaux, <span class="op">!</span><span class="kw">is.na</span>(price), <span class="op">!</span><span class="kw">is.na</span>(vint)),
       <span class="kw">aes</span>(<span class="dt">y =</span> <span class="kw">log</span>(price), <span class="dt">x =</span> vint)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>)</code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-4-1.svg" width="672" /></p>
<p>Ashenfleter, Ashmore, and Lalonde (1995) run two models. All models were estimated using OLS with log-price as the outcome variable. The predictors in the models were:</p>
<ol style="list-style-type: decimal">
<li>vintage age</li>
<li>vintage age, winter rain, harvest rain</li>
</ol>
<p>Weâll start by considering these models. Since we are running several models, weâll define the model formulae in a list</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mods_f &lt;-<span class="st"> </span><span class="kw">list</span>(lprice <span class="op">~</span><span class="st"> </span>time_sv, 
               lprice <span class="op">~</span><span class="st"> </span>time_sv <span class="op">+</span><span class="st"> </span>wrain <span class="op">+</span><span class="st"> </span>hrain <span class="op">+</span><span class="st"> </span>degrees)</code></pre></div>
<p>and, run each model and store the results in a data frame as list column of <code>lm</code> objects:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mods_res &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">model =</span> <span class="kw">seq_along</span>(mods_f),
  <span class="dt">formula =</span> <span class="kw">map_chr</span>(mods_f, deparse),
  <span class="dt">mod =</span> <span class="kw">map</span>(mods_f, <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(.x, <span class="dt">data =</span> bordeaux))
)
mods_res</code></pre></div>
<pre><code>## # A tibble: 2 Ã 3
##   model                                    formula      mod
##   &lt;int&gt;                                      &lt;chr&gt;   &lt;list&gt;
## 1     1                           lprice ~ time_sv &lt;S3: lm&gt;
## 2     2 lprice ~ time_sv + wrain + hrain + degrees &lt;S3: lm&gt;</code></pre>
<p>Now that we have these models, extract the coefficients into a data frame with the <strong>broom</strong> function <code>tidy</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mods_coefs &lt;-<span class="st"> </span>mods_res <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># Add column with the results of tidy for each model  </span>
<span class="st">  </span><span class="co"># conf.int = TRUE adds confidence intervals to the data</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">tidy =</span> <span class="kw">map</span>(mod, tidy, <span class="dt">conf.int =</span> <span class="ot">TRUE</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># use unnest() to expand the data frame to one row for each row in the tidy</span>
<span class="st">  </span><span class="co"># elements</span>
<span class="st">  </span><span class="kw">unnest</span>(tidy, <span class="dt">.drop =</span> <span class="ot">TRUE</span>)
<span class="kw">glimpse</span>(mods_coefs)</code></pre></div>
<pre><code>## Observations: 7
## Variables: 9
## $ model     &lt;int&gt; 1, 1, 2, 2, 2, 2, 2
## $ formula   &lt;chr&gt; &quot;lprice ~ time_sv&quot;, &quot;lprice ~ time_sv&quot;, &quot;lprice ~ ti...
## $ term      &lt;chr&gt; &quot;(Intercept)&quot;, &quot;time_sv&quot;, &quot;(Intercept)&quot;, &quot;time_sv&quot;, ...
## $ estimate  &lt;dbl&gt; -2.025199170, 0.035429560, -12.145333577, 0.02384741...
## $ std.error &lt;dbl&gt; 0.2472286519, 0.0136624941, 1.6881026571, 0.00716671...
## $ statistic &lt;dbl&gt; -8.191604, 2.593199, -7.194665, 3.327521, 2.420525, ...
## $ p.value   &lt;dbl&gt; 1.522111e-08, 1.566635e-02, 3.278791e-07, 3.055739e-...
## $ conf.low  &lt;dbl&gt; -2.534376e+00, 7.291126e-03, -1.564624e+01, 8.984546...
## $ conf.high &lt;dbl&gt; -1.516022230, 0.063567993, -8.644422940, 0.038710280...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">walk</span>(mods_res<span class="op">$</span>mod, <span class="op">~</span><span class="st"> </span><span class="kw">print</span>(<span class="kw">summary</span>(.x)))</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = .x, data = bordeaux)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.8545 -0.4788 -0.0718  0.4562  1.2457 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -2.02520    0.24723  -8.192 1.52e-08 ***
## time_sv      0.03543    0.01366   2.593   0.0157 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5745 on 25 degrees of freedom
## Multiple R-squared:  0.212,  Adjusted R-squared:  0.1804 
## F-statistic: 6.725 on 1 and 25 DF,  p-value: 0.01567
## 
## 
## Call:
## lm(formula = .x, data = bordeaux)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.46027 -0.23864  0.01347  0.18600  0.53446 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1.215e+01  1.688e+00  -7.195 3.28e-07 ***
## time_sv      2.385e-02  7.167e-03   3.328  0.00306 ** 
## wrain        1.167e-03  4.820e-04   2.421  0.02420 *  
## hrain       -3.861e-03  8.075e-04  -4.781 8.97e-05 ***
## degrees      6.164e-01  9.518e-02   6.476 1.63e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.2865 on 22 degrees of freedom
## Multiple R-squared:  0.8275, Adjusted R-squared:  0.7962 
## F-statistic: 26.39 on 4 and 22 DF,  p-value: 4.058e-08</code></pre>
<p>Likewise, extract model statistics such as, <span class="math inline">\(R^2\)</span>, adjusted <span class="math inline">\(R^2\)</span>, and <span class="math inline">\(\hat{\sigma}\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mods_glance &lt;-
<span class="st">  </span><span class="kw">mutate</span>(mods_res, <span class="dt">.x =</span> <span class="kw">map</span>(mod, glance)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">unnest</span>(.x, <span class="dt">.drop =</span> <span class="ot">TRUE</span>)
mods_glance <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(formula, r.squared, adj.r.squared, sigma)</code></pre></div>
<pre><code>## # A tibble: 2 Ã 4
##                                      formula r.squared adj.r.squared
##                                        &lt;chr&gt;     &lt;dbl&gt;         &lt;dbl&gt;
## 1                           lprice ~ time_sv 0.2119700     0.1804488
## 2 lprice ~ time_sv + wrain + hrain + degrees 0.8275278     0.7961692
## # ... with 1 more variables: sigma &lt;dbl&gt;</code></pre>
</div>
<div id="cross-validation" class="section level2">
<h2><span class="header-section-number">16.5</span> Cross-Validation</h2>
<p>o compare predictive models, we want to compare how well it predicts (duh), which means estimating how well it will work on new data. The problem with this is that new data is just that, â¦, new.</p>
<p>The trick is to resuse the sample data to get an estimate of how well the model will work on new data. This is done by fitting the model on a subset of the data, and predicting another subset of the data which was not used to fit the model; often this is done repeatedly. There are a variety of ways to do this, depending on the nature of the data and the predictive task. However, they all implictly assume that the sample of data that was used to fit the model is representative of future data.</p>
<blockquote>
<p>â¦ model validation is a good, simple, broadly aplicable procedure that is rarely used in social research (Fox, p.Â 630)</p>
<p>The simple idea of splitting a sample into two and then developing the hypothesis on the basis of one part and testing it on the remainder may perhaps be said to be one of the most seriously neglected ideas in statistics, if we measure the degree of neglect by the ratio of the number of cases in where a method could give help to the number where it was actually used. (Barnard 1974, p.Â 133, quoted in Fox, p.Â 630)</p>
</blockquote>
</div>
<div id="out-of-sample-error" class="section level2">
<h2><span class="header-section-number">16.6</span> Out of Sample Error</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">k &lt;-<span class="st"> </span><span class="dv">20</span>
f &lt;-<span class="st"> </span><span class="kw">map</span>(<span class="kw">seq_len</span>(k),
         <span class="op">~</span><span class="st"> </span><span class="kw">as.formula</span>(<span class="kw">str_c</span>(<span class="st">&quot;lprice ~ poly(time_sv, &quot;</span>, .x, <span class="st">&quot;)&quot;</span>)))
<span class="kw">names</span>(f) &lt;-<span class="st"> </span><span class="kw">seq_len</span>(k)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mods_overfit &lt;-<span class="st"> </span><span class="kw">map</span>(f, <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(.x, <span class="dt">data =</span> bordeaux))
fits &lt;-<span class="st"> </span><span class="kw">map_df</span>(mods_overfit, glance, <span class="dt">.id =</span> <span class="st">&quot;.id&quot;</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fits <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(.id, r.squared, adj.r.squared, df.residual) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(stat, value, <span class="op">-</span>.id) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">.id =</span> <span class="kw">as.integer</span>(.id)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> .id, <span class="dt">y =</span> value)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>stat, <span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>)</code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-12-1.svg" width="672" /></p>
<p>The larger the polynomial, the more wiggly the line.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;modelr&quot;</span>)
<span class="kw">invoke</span>(gather_predictions, <span class="dt">.x =</span> <span class="kw">c</span>(<span class="kw">list</span>(<span class="dt">data =</span> bordeaux), mods_overfit)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> vint)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> lprice)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> pred, <span class="dt">group =</span> model, <span class="dt">colour =</span> <span class="kw">as.numeric</span>(model)))</code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-13-1.svg" width="672" /> Intuitively it seems that as we increase the flexibility of the model by increasing the number of variables the model is overfitting the data, but what does it actually mean to overfit? If we use <span class="math inline">\(R^2\)</span> as the âmeasure of fitâ, more variables always leads to better fit. Adjusted <span class="math inline">\(R^2\)</span> does not increase, because the decrease in errors is offset by the increase in the degrees of freedom. However, there is little justification for the specific formula of <span class="math inline">\(R^2\)</span>.</p>
<p>The problem with over-fitting is that the model starts to fit pecularities of the sample (errors) rather than the underlying model. Weâll never know the underlying model, but what we can see is if the model predicts new data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">wine_mods_f &lt;-<span class="st"> </span><span class="kw">list</span>(
  lprice <span class="op">~</span><span class="st"> </span>time_sv,
  lprice <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(time_sv, <span class="dv">2</span>),
  lprice <span class="op">~</span><span class="st"> </span>wrain,
  lprice <span class="op">~</span><span class="st"> </span>hrain,
  lprice <span class="op">~</span><span class="st"> </span>degrees,
  lprice <span class="op">~</span><span class="st"> </span>wrain <span class="op">+</span><span class="st"> </span>hrain <span class="op">+</span><span class="st"> </span>degrees,  
  lprice <span class="op">~</span><span class="st"> </span>time_sv <span class="op">+</span><span class="st"> </span>wrain <span class="op">+</span><span class="st"> </span>hrain <span class="op">+</span><span class="st"> </span>degrees,
  lprice <span class="op">~</span><span class="st"> </span>time_sv <span class="op">+</span><span class="st"> </span>wrain <span class="op">*</span><span class="st"> </span>hrain <span class="op">*</span><span class="st"> </span>degrees,
  lprice <span class="op">~</span><span class="st"> </span>time_sv <span class="op">*</span><span class="st"> </span>(wrain <span class="op">+</span><span class="st"> </span>hrain <span class="op">+</span><span class="st"> </span>degrees),
  lprice <span class="op">~</span><span class="st"> </span>time_sv <span class="op">*</span><span class="st"> </span>wrain <span class="op">*</span><span class="st"> </span>hrain <span class="op">*</span><span class="st"> </span>degrees,
  lprice <span class="op">~</span><span class="st"> </span>time_sv <span class="op">*</span><span class="st"> </span>wrain <span class="op">*</span><span class="st"> </span>hrain <span class="op">*</span><span class="st"> </span>degrees <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(time_sv <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)
)</code></pre></div>
<div id="held-out-data" class="section level3">
<h3><span class="header-section-number">16.6.1</span> Held-out data</h3>
<p>A common rule of thumb is to use 70% of the data for training, and 30% of the data for testing.</p>
<p>In this case, letâs partition the data to use the first 70% of the observations as training data, and the remaining 30% of the data as testing.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n_test &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="fl">0.3</span> <span class="op">*</span><span class="st"> </span><span class="kw">nrow</span>(bordeaux))
n_train &lt;-<span class="st"> </span><span class="kw">nrow</span>(bordeaux) <span class="op">-</span><span class="st"> </span>n_test

mod_train &lt;-<span class="st"> </span><span class="kw">lm</span>(lprice <span class="op">~</span><span class="st"> </span>time_sv <span class="op">+</span><span class="st"> </span>wrain <span class="op">+</span><span class="st"> </span>hrain <span class="op">+</span><span class="st"> </span>degrees, 
                <span class="dt">data =</span> <span class="kw">head</span>(bordeaux, n_train))
mod_train</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = lprice ~ time_sv + wrain + hrain + degrees, data = head(bordeaux, 
##     n_train))
## 
## Coefficients:
## (Intercept)      time_sv        wrain        hrain      degrees  
##  -1.080e+01    1.999e-02    9.712e-04   -4.461e-03    5.533e-01</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># in-sample RMSE</span>
<span class="kw">sqrt</span>(<span class="kw">mean</span>(mod_train<span class="op">$</span>residuals <span class="op">^</span><span class="st"> </span><span class="dv">2</span>))</code></pre></div>
<pre><code>## [1] 0.2280059</code></pre>
<p>The out-of-sample RMSE is higher than the in-sample RMSE.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">outsample &lt;-<span class="st"> </span><span class="kw">augment</span>(mod_train, <span class="dt">newdata =</span> <span class="kw">tail</span>(bordeaux, n_test))
<span class="kw">sqrt</span>(<span class="kw">mean</span>((outsample<span class="op">$</span>lprice <span class="op">-</span><span class="st"> </span>outsample<span class="op">$</span>.fitted) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>))</code></pre></div>
<pre><code>## [1] 0.351573</code></pre>
<p>This is common, but not necessarily the case. But note that this value is highly dependent on the subset of data used for testing. In some sense, we may choose as model that âoverfitsâ the testing data.</p>
</div>
<div id="leave-one-out-cross-validation" class="section level3">
<h3><span class="header-section-number">16.6.2</span> Leave-One-Out Cross-Validation</h3>
<p>For each <span class="math inline">\(i \in 1, \dots, n\)</span></p>
<ol style="list-style-type: decimal">
<li>Estimate the model using all observations but <span class="math inline">\(i\)</span></li>
<li>Predict <span class="math inline">\(\hat{y}_i\)</span> using that model</li>
<li>Calculate some measure(s) of model fit</li>
</ol>
<p>Letâs create a function to fit the model on a dataset dropping a single observation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">i &lt;-<span class="st"> </span><span class="dv">1</span>
f &lt;-<span class="st"> </span>lprice <span class="op">~</span><span class="st"> </span>time_sv <span class="op">+</span><span class="st"> </span>wrain <span class="op">+</span><span class="st"> </span>hrain <span class="op">+</span><span class="st"> </span>degrees
mod &lt;-<span class="st"> </span><span class="kw">lm</span>(f, <span class="dt">data =</span> bordeaux)
mod</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = f, data = bordeaux)
## 
## Coefficients:
## (Intercept)      time_sv        wrain        hrain      degrees  
##  -12.145334     0.023847     0.001167    -0.003861     0.616392</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_loo1 &lt;-<span class="st"> </span><span class="kw">lm</span>(f, <span class="dt">data =</span> bordeaux[<span class="op">-</span>i, ])
mod_loo1</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = f, data = bordeaux[-i, ])
## 
## Coefficients:
## (Intercept)      time_sv        wrain        hrain      degrees  
##  -12.336504     0.025916     0.001188    -0.003832     0.625560</code></pre>
<p>Unsurprisingly the fits of models fit with and without the first observation are similar, since they were fit using <span class="math inline">\(n - 1\)</span> observations.</p>
<p>Now use the model fit <em>without</em> the first observation to</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">yhat_loo1 &lt;-<span class="st"> </span><span class="kw">predict</span>(mod_loo1, <span class="dt">newdata =</span> bordeaux[<span class="dv">1</span>, ])
yhat_loo1</code></pre></div>
<pre><code>##          1 
## -0.7262297</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bordeaux<span class="op">$</span>lprice[<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span>yhat_loo1</code></pre></div>
<pre><code>##          1 
## -0.2724503</code></pre>
<p>Now we want to repeat this for all observations,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_loo &lt;-<span class="st"> </span><span class="cf">function</span>(i, formula, data) {
  <span class="co"># fit without i</span>
  m &lt;-<span class="st"> </span><span class="kw">lm</span>(formula, <span class="dt">data =</span> data[<span class="op">-</span>i, ])
  <span class="co"># predict i</span>
  yhat &lt;-<span class="st"> </span><span class="kw">predict</span>(m, <span class="dt">newdata =</span> data[i, ])
  <span class="kw">tibble</span>(<span class="dt">i =</span> i, <span class="dt">pred =</span> yhat, <span class="dt">resid =</span> yhat <span class="op">-</span><span class="st"> </span>data[[<span class="st">&quot;lprice&quot;</span>]][[i]])
}
cv_loo &lt;-<span class="st"> </span><span class="kw">map_df</span>(<span class="kw">seq_len</span>(<span class="kw">nrow</span>(bordeaux)), fit_loo, <span class="dt">formula =</span> f, <span class="dt">data =</span> bordeaux)

<span class="kw">sqrt</span>(<span class="kw">mean</span>(cv_loo<span class="op">$</span>resid <span class="op">^</span><span class="st"> </span><span class="dv">2</span>))</code></pre></div>
<pre><code>## [1] 0.3230043</code></pre>
</div>
<div id="k-fold-cross-validation" class="section level3">
<h3><span class="header-section-number">16.6.3</span> k-fold Cross-validation</h3>
<p>The most common approach is to to partition the data into k-folds, and use each fold once as the testing subset, where the model is fit on the other <span class="math inline">\(k - 1\)</span> folds.</p>
<p>[Cross validation](<a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)" class="uri">https://en.wikipedia.org/wiki/Cross-validation_(statistics)</a> is a non-parametric method that splits the data into <strong>training</strong> and <strong>test</strong> subsets. The data is fit on the <strong>training</strong> set and then used the predict the <strong>test</strong> set.</p>
<p>The most common form of cross validation is 5- or 10-fold cross validation? Why this number of folds? See ISLR 5.1.4 âBias-Variance Trade-Off for k-Fold Cross Validationâ</p>
<ul>
<li>Larger number of folds requires more computation: a <span class="math inline">\(k\)</span>-fold cross validation requires running the model <span class="math inline">\(k\)</span> times</li>
<li>A large number of folds has low bias because the result of <span class="math inline">\((n - 1)\)</span> observations is approximately the same as the result of <span class="math inline">\(n\)</span> observations</li>
<li>But larger folds results in higher variance. LOOCV is averaging <span class="math inline">\(n\)</span> models, but all those models will be similar, because they share almost all the same observations. But with fewer folds the models are fit with fewer overlapping observations and thus will have less correlated results.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cv_fold5 &lt;-<span class="st"> </span>modelr<span class="op">::</span><span class="kw">crossv_kfold</span>(bordeaux, <span class="dt">k =</span> <span class="dv">5</span>)
<span class="kw">glimpse</span>(cv_fold5)</code></pre></div>
<pre><code>## Observations: 5
## Variables: 3
## $ train &lt;list&gt; [&lt;1952.00000, 1953.00000, 1955.00000, 1957.00000, 1958....
## $ test  &lt;list&gt; [&lt;1952.00000, 1953.00000, 1955.00000, 1957.00000, 1958....
## $ .id   &lt;chr&gt; &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cv_fold5<span class="op">$</span>train[[<span class="dv">1</span>]]</code></pre></div>
<pre><code>## &lt;resample [21 x 7]&gt; 1, 2, 6, 7, 8, 9, 10, 11, 12, 13, ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cv_fold5<span class="op">$</span>test[[<span class="dv">1</span>]]</code></pre></div>
<pre><code>## &lt;resample [6 x 7]&gt; 3, 4, 5, 15, 22, 26</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">as.data.frame</span>(cv_fold5<span class="op">$</span>train[[<span class="dv">1</span>]])</code></pre></div>
<pre><code>## # A tibble: 21 Ã 7
##     vint     price wrain degrees hrain time_sv   lprice
##    &lt;int&gt;     &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;int&gt;    &lt;dbl&gt;
## 1   1952  36.83654   600 17.1167   160      31 -0.99868
## 2   1953  63.48288   690 16.7333    80      30 -0.45440
## 3   1959  65.83622   485 17.4833   187      24 -0.41800
## 4   1960  13.87738   763 16.4167   290      23 -1.97491
## 5   1961 100.00000   830 17.3333    38      22  0.00000
## 6   1962  33.09725   697 16.3000    52      21 -1.10572
## 7   1963  16.84730   608 15.7167   155      20 -1.78098
## 8   1964  30.59450   402 17.2667    96      19 -1.18435
## 9   1965  10.62522   602 15.3667   267      18 -2.24194
## 10  1966  47.26359   819 16.5333    86      17 -0.74943
## # ... with 11 more rows</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">as.data.frame</span>(cv_fold5<span class="op">$</span>test[[<span class="dv">1</span>]])</code></pre></div>
<pre><code>## # A tibble: 6 Ã 7
##    vint    price wrain degrees hrain time_sv   lprice
##   &lt;int&gt;    &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;int&gt;    &lt;dbl&gt;
## 1  1955 44.57665   502 17.1500   130      28 -0.80796
## 2  1957 22.10735   420 16.1333   110      26 -1.50926
## 3  1958 17.96850   582 16.4167   187      25 -1.71655
## 4  1968 10.53803   610 16.2000   292      15 -2.25018
## 5  1975 30.06886   572 16.9500   171       8 -1.20168
## 6  1979 21.44669   717 16.1667   122       4 -1.53960</code></pre>
<p>In k-fold cross-validation, each observation appears in the test-set in one fold, and is in the the training set in the remaining <span class="math inline">\(k - 1\)</span> folds. The following plot shows this,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cv_fold5_obs &lt;-
<span class="st">  </span>cv_fold5 <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">rowwise</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">do</span>(<span class="kw">tibble</span>(<span class="dt">vint =</span> <span class="kw">c</span>(<span class="kw">as.data.frame</span>(.<span class="op">$</span>train)<span class="op">$</span>vint,
                     <span class="kw">as.data.frame</span>(.<span class="op">$</span>test)<span class="op">$</span>vint),
            <span class="dt">fold =</span> .<span class="op">$</span>.id,
            <span class="dt">set  =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;train&quot;</span>, <span class="kw">dim</span>(.<span class="op">$</span>train)[<span class="dv">1</span>]),
                     <span class="kw">rep</span>(<span class="st">&quot;test&quot;</span>, <span class="kw">dim</span>(.<span class="op">$</span>test)[<span class="dv">1</span>]))))

<span class="kw">ggplot</span>(cv_fold5_obs, <span class="kw">aes</span>(<span class="dt">y =</span> <span class="kw">factor</span>(vint), <span class="dt">x =</span> fold, <span class="dt">fill =</span> set)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_raster</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_fill_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;train&quot;</span> =<span class="st"> &quot;black&quot;</span>, <span class="st">&quot;test&quot;</span> =<span class="st"> &quot;gray&quot;</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.ticks =</span> <span class="kw">element_blank</span>(), <span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;vint&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;CV fold&quot;</span>, <span class="dt">fill =</span> <span class="st">&quot;&quot;</span>)</code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-23-1.svg" width="672" /></p>
<p>Example with one fold</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_train &lt;-<span class="st"> </span><span class="kw">lm</span>(f, <span class="dt">data =</span> <span class="kw">as.data.frame</span>(cv_fold5<span class="op">$</span>train[[<span class="dv">1</span>]]))
fit_train</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = f, data = as.data.frame(cv_fold5$train[[1]]))
## 
## Coefficients:
## (Intercept)      time_sv        wrain        hrain      degrees  
##  -1.131e+01    3.114e-02    9.724e-04   -3.965e-03    5.662e-01</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_test &lt;-<span class="st"> </span><span class="kw">augment</span>(fit_train, <span class="dt">newdata =</span> <span class="kw">as.data.frame</span>(cv_fold5<span class="op">$</span>test[[<span class="dv">1</span>]])) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(vint, lprice, .fitted) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">.resid =</span> .fitted <span class="op">-</span><span class="st"> </span>lprice)
fit_test</code></pre></div>
<pre><code>##   vint   lprice    .fitted      .resid
## 1 1955 -0.80796 -0.7542891  0.05367086
## 2 1957 -1.50926 -1.3926673  0.11659270
## 3 1958 -1.71655 -1.4111139  0.30543610
## 4 1968 -2.25018 -2.2342730  0.01590699
## 5 1975 -1.20168 -1.5847434 -0.38306339
## 6 1979 -1.53960 -1.8175094 -0.27790943</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># could also use modelr::rmse()</span>
mod_rmse &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>(<span class="kw">sum</span>(fit_test<span class="op">$</span>.resid <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)))
mod_rmse</code></pre></div>
<pre><code>## [1] 0.5779186</code></pre>
<p>Letâs apply that to each row using <code>map</code>. That way we keep the results together in the same data frame.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_train &lt;-<span class="st"> </span><span class="kw">lm</span>(f, <span class="dt">data =</span> <span class="kw">as.data.frame</span>(cv_fold5<span class="op">$</span>train[[<span class="dv">1</span>]]))
fit_train</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = f, data = as.data.frame(cv_fold5$train[[1]]))
## 
## Coefficients:
## (Intercept)      time_sv        wrain        hrain      degrees  
##  -1.131e+01    3.114e-02    9.724e-04   -3.965e-03    5.662e-01</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_test &lt;-<span class="st"> </span><span class="kw">augment</span>(fit_train, <span class="dt">newdata =</span> <span class="kw">as.data.frame</span>(cv_fold5<span class="op">$</span>test[[<span class="dv">1</span>]])) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(vint, lprice, .fitted) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">.resid =</span> .fitted <span class="op">-</span><span class="st"> </span>lprice)
fit_test</code></pre></div>
<pre><code>##   vint   lprice    .fitted      .resid
## 1 1955 -0.80796 -0.7542891  0.05367086
## 2 1957 -1.50926 -1.3926673  0.11659270
## 3 1958 -1.71655 -1.4111139  0.30543610
## 4 1968 -2.25018 -2.2342730  0.01590699
## 5 1975 -1.20168 -1.5847434 -0.38306339
## 6 1979 -1.53960 -1.8175094 -0.27790943</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># could also use modelr::rmse()</span>
mod_rmse &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>(<span class="kw">sum</span>(fit_test<span class="op">$</span>.resid <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)))
mod_rmse</code></pre></div>
<pre><code>## [1] 0.5779186</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_cv_fold5 &lt;-
<span class="st">  </span>cv_fold5 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="co"># fit each row using train as the data</span>
    <span class="dt">fit_train =</span> <span class="kw">map</span>(train, 
                    <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(f, <span class="dt">data =</span> <span class="kw">as.data.frame</span>(.x))),
    <span class="co"># predicted values</span>
    <span class="dt">predict_test =</span> <span class="kw">map</span>(train, 
                       <span class="op">~</span><span class="st"> </span><span class="kw">predict</span>(fit_train, <span class="dt">newdata =</span> <span class="kw">as.data.frame</span>(.x))),
    <span class="co"># calculate out-of-sample RMSE</span>
    <span class="dt">rmse =</span> <span class="kw">map2_dbl</span>(train, predict_test,
                <span class="op">~</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>((<span class="kw">as.data.frame</span>(.x)<span class="op">$</span>lprice <span class="op">-</span><span class="st"> </span>.y) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>))))</code></pre></div>
<p>Letâs apply this to all the models. In the previous steps we kept a lot of extra information in order to understand how cross-validation worked. But really, all we care about is the average RMSE.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_rmse_fold &lt;-<span class="st"> </span><span class="cf">function</span>(f, train, test) {
  fit &lt;-<span class="st"> </span><span class="kw">lm</span>(f, <span class="dt">data =</span> <span class="kw">as.data.frame</span>(train))
  <span class="co"># sqrt(mean(residuals(fit, newdata = as.data.frame(test)) ^ 2))</span>
  test_data &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(test)
  err &lt;-<span class="st"> </span>test_data<span class="op">$</span>lprice <span class="op">-</span><span class="st"> </span><span class="kw">predict</span>(fit, <span class="dt">newdata =</span> test_data)
  <span class="kw">sqrt</span>(<span class="kw">mean</span>(err <span class="op">^</span><span class="st"> </span><span class="dv">2</span>))
}
<span class="kw">mod_rmse_fold</span>(f, cv_fold5<span class="op">$</span>train[[<span class="dv">1</span>]], cv_fold5<span class="op">$</span>test[[<span class="dv">1</span>]])</code></pre></div>
<pre><code>## [1] 0.2359343</code></pre>
<p>Now calculate this for a single model formula, averaging over all folds:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_rmse &lt;-<span class="st"> </span><span class="cf">function</span>(f, data) {
  <span class="kw">map2_dbl</span>(data<span class="op">$</span>train, data<span class="op">$</span>test, 
           <span class="cf">function</span>(train, test) {
             <span class="kw">mod_rmse_fold</span>(f, train, test)
           }) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mean</span>()
}</code></pre></div>
<p>Now we can apply this to all the models:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_comparison &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">tibble</span>(<span class="dt">formula =</span> wine_mods_f,
         <span class="dt">.name =</span> <span class="kw">map_chr</span>(formula, deparse),
         <span class="dt">.id =</span> <span class="kw">seq_along</span>(wine_mods_f),
         <span class="dt">rmse =</span> <span class="kw">map_dbl</span>(wine_mods_f, mod_rmse, <span class="dt">data =</span> cv_fold5))
mod_comparison <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(.name, rmse)  </code></pre></div>
<pre><code>## # A tibble: 11 Ã 2
##                                                        .name      rmse
##                                                        &lt;chr&gt;     &lt;dbl&gt;
## 1                                           lprice ~ time_sv 0.6086064
## 2                                  lprice ~ poly(time_sv, 2) 0.6100899
## 3                                             lprice ~ wrain 0.6918896
## 4                                             lprice ~ hrain 0.5925636
## 5                                           lprice ~ degrees 0.5025091
## 6                           lprice ~ wrain + hrain + degrees 0.3733184
## 7                 lprice ~ time_sv + wrain + hrain + degrees 0.3250124
## 8                 lprice ~ time_sv + wrain * hrain * degrees 0.4522841
## 9               lprice ~ time_sv * (wrain + hrain + degrees) 0.3926909
## 10                lprice ~ time_sv * wrain * hrain * degrees 1.0599623
## 11 lprice ~ time_sv * wrain * hrain * degrees + I(time_sv^2) 1.4167142</code></pre>
</div>
</div>
<div id="analytic-covariance-methods" class="section level2">
<h2><span class="header-section-number">16.7</span> Analytic Covariance Methods</h2>
<p>For some models, notably linear regression, analytical approximations to the expected out of sample error can be made. Each of these approximations will make some slightly different assumptions to plug in some unknown values.</p>
<p>Adjusted <span class="math inline">\(R^2\)</span> is most often seen in statistical software and in papers (though often never interpreted). It intuitively penalizes a regression for a higher number of predictors; however apart from that intuitive appeal, and unlike the other measures presented here, there is no deeper justification for it (Fox, p.Â 609): <span class="math display">\[
\mathrm{adj}\,R^2 = 1 - \frac{\hat{\sigma}^2}{\Var{(Y)}} = 1 - \frac{n - 1}{n - k - 1} \times \frac{\sum (y_i - \hat{y}_i^2)}{\sum (y_i - \bar{y})^2}
\]</span></p>
<p>In linear regression, the LOO-CV MSE can be calculated analytically, and without simulation. It is (ISLR, p.Â 180): <span class="math display">\[
\text{LOO-CV} = \frac{1}{n} \sum_{i = 1}^n {\left(\frac{y_i - \hat{y}_i}{1 - h_i} \right)}^2 = \frac{1}{n} \sum_{i = 1}^n {\left(\frac{\hat{\epsilon}_i}{1 - h_i} \right)}^2 = \frac{1}{n} \times \text{PRESS}
\]</span> where PRESS is the predictive residual sum of squares, and <span class="math inline">\(h_i\)</span> is the <em>hat-value</em> of observation <span class="math inline">\(i\)</span> (Fox, p.Â 270, 289) <span class="math display">\[
h_i = \mat{X}(\mat{X}&#39; \mat{X})^{-1} \mat{X}&#39;
\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">loocv &lt;-<span class="st"> </span><span class="cf">function</span>(x) {
  <span class="kw">mean</span>((<span class="kw">residuals</span>(x) <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">hatvalues</span>(x))) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)
}</code></pre></div>
<p>An alternative approximation of the expected out-of-sample error is the generalized cross-validation criterion (GCV) is (Fox, 673) <span class="math display">\[
GCV = \frac{n}{df_{res}^2} \times RSS = \frac{n}{(n - k - 1)^2} \times \sum \hat{\epsilon}^2_i
\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">gcv &lt;-<span class="st"> </span><span class="cf">function</span>(x) {
  err2 &lt;-<span class="st"> </span><span class="kw">residuals</span>(x) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>
  n &lt;-<span class="st"> </span><span class="kw">length</span>(err2)
  (n <span class="op">/</span><span class="st"> </span>x[[<span class="st">&quot;df.residual&quot;</span>]] <span class="op">^</span><span class="st"> </span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>(err2)
}</code></pre></div>
<p>Since we generated the LOO data manually using a loop, create a LOO cross validation data frame using <code>crossv_kfold</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cv_loo &lt;-<span class="st"> </span><span class="kw">crossv_kfold</span>(bordeaux, <span class="kw">nrow</span>(bordeaux))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_comparison &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">tibble</span>(<span class="dt">formula =</span> wine_mods_f,
         <span class="dt">.name =</span> <span class="kw">map_chr</span>(formula, deparse),
         <span class="dt">.id =</span> <span class="kw">seq_along</span>(wine_mods_f),
         <span class="dt">rmse_5fold =</span> <span class="kw">map_dbl</span>(wine_mods_f, mod_rmse, <span class="dt">data =</span> cv_fold5),
         <span class="dt">rmse_loo =</span> <span class="kw">map_dbl</span>(wine_mods_f, mod_rmse, <span class="dt">data =</span> cv_loo),
         <span class="dt">mod =</span> <span class="kw">map</span>(formula, lm, <span class="dt">data =</span> bordeaux),
         <span class="dt">gcv =</span> <span class="kw">sqrt</span>(<span class="kw">map_dbl</span>(mod, gcv)),
         <span class="dt">loocv =</span> <span class="kw">sqrt</span>(<span class="kw">map_dbl</span>(mod, loocv))
         )
         
mod_comparison <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(.name, rmse_loo, rmse_5fold, gcv, loocv)</code></pre></div>
<pre><code>## # A tibble: 11 Ã 5
##                                                        .name  rmse_loo
##                                                        &lt;chr&gt;     &lt;dbl&gt;
## 1                                           lprice ~ time_sv 0.5194518
## 2                                  lprice ~ poly(time_sv, 2) 0.5312953
## 3                                             lprice ~ wrain 0.5703790
## 4                                             lprice ~ hrain 0.4635317
## 5                                           lprice ~ degrees 0.4031840
## 6                           lprice ~ wrain + hrain + degrees 0.3092453
## 7                 lprice ~ time_sv + wrain + hrain + degrees 0.2767282
## 8                 lprice ~ time_sv + wrain * hrain * degrees 0.3199831
## 9               lprice ~ time_sv * (wrain + hrain + degrees) 0.3128323
## 10                lprice ~ time_sv * wrain * hrain * degrees 0.7493031
## 11 lprice ~ time_sv * wrain * hrain * degrees + I(time_sv^2) 0.7287324
## # ... with 3 more variables: rmse_5fold &lt;dbl&gt;, gcv &lt;dbl&gt;, loocv &lt;dbl&gt;</code></pre>
<p>Other measures that are also equivalent to some form of an estimate of the out-of-sample error are the AIC and BIC.</p>
</div>
<div id="further-resources" class="section level2">
<h2><span class="header-section-number">16.8</span> Further Resources</h2>
<ul>
<li><span class="citation">Fox (<a href="#ref-Fox2016a">2016</a>)</span> Chapter 22: âModel Selection, Averaging, and Validationâ, p.Â 669.</li>
<li><span class="citation">James et al. (<a href="#ref-JamesWittenHastieEtAl2013a">2013</a>)</span>, Ch. 5. âResampling Methodsâ</li>
<li><span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-HastieTibshiraniFriedman2009a">2009</a>)</span>, Ch. 7. âModel Assessment and Selectionâ</li>
<li><a href="http://robjhyndman.com/hyndsight">Rob Hyndmanâs</a> blog posts on <a href="http://robjhyndman.com/hyndsight/crossvalidation/">cross validation</a>, <a href="http://robjhyndman.com/hyndsight/tscv/">time series cross validation</a>, and <a href="http://robjhyndman.com/hyndsight/loocv-linear-models/">leave-one-out CV in linear models</a>.</li>
</ul>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Fox2016a">
<p>Fox, John. 2016. <em>Applied Regression Analysis &amp; Generalized Linear Models</em>. 3rd ed. Sage.</p>
</div>
<div id="ref-JamesWittenHastieEtAl2013a">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning with Applications in R</em>. 6th ed. Springer.</p>
</div>
<div id="ref-HastieTibshiraniFriedman2009a">
<p>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning</em>. Springer-Verlag New York Inc. <a href="http://www.ebook.de/de/product/8023140/trevor_hastie_robert_tibshirani_jerome_friedman_the_elements_of_statistical_learning.html" class="uri">http://www.ebook.de/de/product/8023140/trevor_hastie_robert_tibshirani_jerome_friedman_the_elements_of_statistical_learning.html</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="robust-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="miscellaneous-regression-stuff.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/jrnold/intro-method-notes/edit/master/prediction.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
