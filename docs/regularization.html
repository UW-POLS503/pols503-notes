<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Data Analysis Notes</title>
  <meta name="description" content="These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.">
  <meta name="generator" content="bookdown 0.7.7 and GitBook 2.6.7">

  <meta property="og:title" content="Data Analysis Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington." />
  <meta name="github-repo" content="jrnold/intro-methods-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Data Analysis Notes" />
  
  <meta name="twitter:description" content="These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington." />
  

<meta name="author" content="Jeffrey B. Arnold">


<meta name="date" content="2018-04-23">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="cross-validation.html">
<link rel="next" href="formatting-tables.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intro Method Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="part"><span><b>I Exploratory Data Analysis</b></span></li>
<li class="part"><span><b>II Programming</b></span></li>
<li class="part"><span><b>III Linear Regression</b></span></li>
<li class="chapter" data-level="2" data-path="regression-anatomy.html"><a href="regression-anatomy.html"><i class="fa fa-check"></i><b>2</b> Regression Anatomy</a><ul>
<li class="chapter" data-level="2.1" data-path="regression-anatomy.html"><a href="regression-anatomy.html#example"><i class="fa fa-check"></i><b>2.1</b> Example</a></li>
<li class="chapter" data-level="2.2" data-path="regression-anatomy.html"><a href="regression-anatomy.html#variations"><i class="fa fa-check"></i><b>2.2</b> Variations</a></li>
<li class="chapter" data-level="2.3" data-path="regression-anatomy.html"><a href="regression-anatomy.html#questions"><i class="fa fa-check"></i><b>2.3</b> Questions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html"><i class="fa fa-check"></i><b>3</b> OLS in Matrix Form</a><ul>
<li class="chapter" data-level="" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#setup"><i class="fa fa-check"></i>Setup</a></li>
<li class="chapter" data-level="3.1" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#purpose"><i class="fa fa-check"></i><b>3.1</b> Purpose</a></li>
<li class="chapter" data-level="3.2" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#matrix-algebra-review"><i class="fa fa-check"></i><b>3.2</b> Matrix Algebra Review</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#vectors"><i class="fa fa-check"></i><b>3.2.1</b> Vectors</a></li>
<li class="chapter" data-level="3.2.2" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#matrices"><i class="fa fa-check"></i><b>3.2.2</b> Matrices</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#matrix-operations"><i class="fa fa-check"></i><b>3.3</b> Matrix Operations</a><ul>
<li class="chapter" data-level="3.3.1" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#transpose"><i class="fa fa-check"></i><b>3.3.1</b> Transpose</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#matrices-as-vectors"><i class="fa fa-check"></i><b>3.4</b> Matrices as vectors</a></li>
<li class="chapter" data-level="3.5" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#special-matrices"><i class="fa fa-check"></i><b>3.5</b> Special matrices</a></li>
<li class="chapter" data-level="3.6" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#multiple-linear-regression-in-matrix-form"><i class="fa fa-check"></i><b>3.6</b> Multiple linear regression in matrix form</a></li>
<li class="chapter" data-level="3.7" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#residuals"><i class="fa fa-check"></i><b>3.7</b> Residuals</a></li>
<li class="chapter" data-level="3.8" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#scalar-inverses"><i class="fa fa-check"></i><b>3.8</b> Scalar inverses</a></li>
<li class="chapter" data-level="3.9" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#matrix-inverses"><i class="fa fa-check"></i><b>3.9</b> Matrix Inverses</a></li>
<li class="chapter" data-level="3.10" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#ols-estimator"><i class="fa fa-check"></i><b>3.10</b> OLS Estimator</a></li>
<li class="chapter" data-level="3.11" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#implications-of-ols"><i class="fa fa-check"></i><b>3.11</b> Implications of OLS</a><ul>
<li class="chapter" data-level="3.11.1" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#ols-in-matrix-form-1"><i class="fa fa-check"></i><b>3.11.1</b> OLS in Matrix Form</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#covariancevariance-interpretation-of-ols"><i class="fa fa-check"></i><b>3.12</b> Covariance/variance interpretation of OLS</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="collinearity-and-multicollinearity.html"><a href="collinearity-and-multicollinearity.html"><i class="fa fa-check"></i><b>4</b> Collinearity and Multicollinearity</a><ul>
<li class="chapter" data-level="4.1" data-path="collinearity-and-multicollinearity.html"><a href="collinearity-and-multicollinearity.html#perfect-collinearity"><i class="fa fa-check"></i><b>4.1</b> (Perfect) collinearity</a></li>
<li class="chapter" data-level="4.2" data-path="collinearity-and-multicollinearity.html"><a href="collinearity-and-multicollinearity.html#what-to-do-about-it"><i class="fa fa-check"></i><b>4.2</b> What to do about it?</a></li>
<li class="chapter" data-level="4.3" data-path="collinearity-and-multicollinearity.html"><a href="collinearity-and-multicollinearity.html#multicollinearity"><i class="fa fa-check"></i><b>4.3</b> Multicollinearity</a></li>
<li class="chapter" data-level="4.4" data-path="collinearity-and-multicollinearity.html"><a href="collinearity-and-multicollinearity.html#what-do-do-about-it"><i class="fa fa-check"></i><b>4.4</b> What do do about it?</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bootstrapping.html"><a href="bootstrapping.html"><i class="fa fa-check"></i><b>5</b> Bootstrapping</a><ul>
<li class="chapter" data-level="5.1" data-path="bootstrapping.html"><a href="bootstrapping.html#non-parametric-bootstrap"><i class="fa fa-check"></i><b>5.1</b> Non-parametric bootstrap</a></li>
<li class="chapter" data-level="5.2" data-path="bootstrapping.html"><a href="bootstrapping.html#standard-errors"><i class="fa fa-check"></i><b>5.2</b> Standard Errors</a></li>
<li class="chapter" data-level="5.3" data-path="bootstrapping.html"><a href="bootstrapping.html#confidence-intervals"><i class="fa fa-check"></i><b>5.3</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.4" data-path="bootstrapping.html"><a href="bootstrapping.html#alternative-methods"><i class="fa fa-check"></i><b>5.4</b> Alternative methods</a><ul>
<li class="chapter" data-level="5.4.1" data-path="bootstrapping.html"><a href="bootstrapping.html#parametric-bootstrap"><i class="fa fa-check"></i><b>5.4.1</b> Parametric Bootstrap</a></li>
<li class="chapter" data-level="5.4.2" data-path="bootstrapping.html"><a href="bootstrapping.html#clustered-bootstrap"><i class="fa fa-check"></i><b>5.4.2</b> Clustered bootstrap</a></li>
<li class="chapter" data-level="5.4.3" data-path="bootstrapping.html"><a href="bootstrapping.html#time-series-bootstrap"><i class="fa fa-check"></i><b>5.4.3</b> Time series bootstrap</a></li>
<li class="chapter" data-level="5.4.4" data-path="bootstrapping.html"><a href="bootstrapping.html#how-to-sample"><i class="fa fa-check"></i><b>5.4.4</b> How to sample?</a></li>
<li class="chapter" data-level="5.4.5" data-path="bootstrapping.html"><a href="bootstrapping.html#caveats"><i class="fa fa-check"></i><b>5.4.5</b> Caveats</a></li>
<li class="chapter" data-level="5.4.6" data-path="bootstrapping.html"><a href="bootstrapping.html#why-use-bootstrapping"><i class="fa fa-check"></i><b>5.4.6</b> Why use bootstrapping?</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="bootstrapping.html"><a href="bootstrapping.html#bagging"><i class="fa fa-check"></i><b>5.5</b> Bagging</a></li>
<li class="chapter" data-level="5.6" data-path="bootstrapping.html"><a href="bootstrapping.html#hypothesis-testing"><i class="fa fa-check"></i><b>5.6</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="5.7" data-path="bootstrapping.html"><a href="bootstrapping.html#how-many-samples"><i class="fa fa-check"></i><b>5.7</b> How many samples?</a></li>
<li class="chapter" data-level="5.8" data-path="bootstrapping.html"><a href="bootstrapping.html#references"><i class="fa fa-check"></i><b>5.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="prediction.html"><a href="prediction.html"><i class="fa fa-check"></i><b>6</b> Prediction</a><ul>
<li class="chapter" data-level="" data-path="prediction.html"><a href="prediction.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="6.1" data-path="prediction.html"><a href="prediction.html#prediction-questions-vs.causal-questions"><i class="fa fa-check"></i><b>6.1</b> Prediction Questions vs. Causal Questions</a></li>
<li class="chapter" data-level="6.2" data-path="prediction.html"><a href="prediction.html#why-is-prediction-important"><i class="fa fa-check"></i><b>6.2</b> Why is prediction important?</a></li>
<li class="chapter" data-level="6.3" data-path="prediction.html"><a href="prediction.html#many-problems-are-prediction-problems"><i class="fa fa-check"></i><b>6.3</b> Many problems are prediction problems</a><ul>
<li class="chapter" data-level="6.3.1" data-path="prediction.html"><a href="prediction.html#counterfactuals"><i class="fa fa-check"></i><b>6.3.1</b> Counterfactuals</a></li>
<li class="chapter" data-level="6.3.2" data-path="prediction.html"><a href="prediction.html#controls"><i class="fa fa-check"></i><b>6.3.2</b> Controls</a></li>
<li class="chapter" data-level="6.3.3" data-path="prediction.html"><a href="prediction.html#what-does-overfitting-mean"><i class="fa fa-check"></i><b>6.3.3</b> What does overfitting mean</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="prediction.html"><a href="prediction.html#prediction-vs.explanation"><i class="fa fa-check"></i><b>6.4</b> Prediction vs. Explanation</a></li>
<li class="chapter" data-level="6.5" data-path="prediction.html"><a href="prediction.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>6.5</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="6.5.1" data-path="prediction.html"><a href="prediction.html#example-1"><i class="fa fa-check"></i><b>6.5.1</b> Example</a></li>
<li class="chapter" data-level="6.5.2" data-path="prediction.html"><a href="prediction.html#overview"><i class="fa fa-check"></i><b>6.5.2</b> Overview</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="prediction.html"><a href="prediction.html#prediction-policy-problems"><i class="fa fa-check"></i><b>6.6</b> Prediction policy problems</a><ul>
<li class="chapter" data-level="6.6.1" data-path="prediction.html"><a href="prediction.html#references-1"><i class="fa fa-check"></i><b>6.6.1</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>7</b> Cross-Validation</a><ul>
<li class="chapter" data-level="" data-path="cross-validation.html"><a href="cross-validation.html#prerequisites-1"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="7.1" data-path="cross-validation.html"><a href="cross-validation.html#example-predicting-bordeaux-wine"><i class="fa fa-check"></i><b>7.1</b> Example: Predicting Bordeaux Wine</a></li>
<li class="chapter" data-level="7.2" data-path="cross-validation.html"><a href="cross-validation.html#cross-validation-1"><i class="fa fa-check"></i><b>7.2</b> Cross Validation</a></li>
<li class="chapter" data-level="7.3" data-path="cross-validation.html"><a href="cross-validation.html#out-of-sample-error"><i class="fa fa-check"></i><b>7.3</b> Out-of-Sample Error</a><ul>
<li class="chapter" data-level="7.3.1" data-path="cross-validation.html"><a href="cross-validation.html#held-out-data"><i class="fa fa-check"></i><b>7.3.1</b> Held-out data</a></li>
<li class="chapter" data-level="7.3.2" data-path="cross-validation.html"><a href="cross-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>7.3.2</b> <span class="math inline">\(k\)</span>-fold Cross-validation</a></li>
<li class="chapter" data-level="7.3.3" data-path="cross-validation.html"><a href="cross-validation.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>7.3.3</b> Leave-one-Out Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="cross-validation.html"><a href="cross-validation.html#approximations"><i class="fa fa-check"></i><b>7.4</b> Approximations</a></li>
<li class="chapter" data-level="7.5" data-path="cross-validation.html"><a href="cross-validation.html#references-2"><i class="fa fa-check"></i><b>7.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>8</b> Regularization</a><ul>
<li class="chapter" data-level="8.1" data-path="regularization.html"><a href="regularization.html#ridge-regression"><i class="fa fa-check"></i><b>8.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="8.2" data-path="regularization.html"><a href="regularization.html#regularization-for-causal-inference"><i class="fa fa-check"></i><b>8.2</b> Regularization for Causal Inference</a></li>
<li class="chapter" data-level="8.3" data-path="regularization.html"><a href="regularization.html#references-3"><i class="fa fa-check"></i><b>8.3</b> References</a></li>
</ul></li>
<li class="part"><span><b>IV Presentation</b></span></li>
<li class="chapter" data-level="9" data-path="formatting-tables.html"><a href="formatting-tables.html"><i class="fa fa-check"></i><b>9</b> Formatting Tables</a><ul>
<li class="chapter" data-level="9.1" data-path="formatting-tables.html"><a href="formatting-tables.html#overview-of-packages"><i class="fa fa-check"></i><b>9.1</b> Overview of Packages</a></li>
<li class="chapter" data-level="9.2" data-path="formatting-tables.html"><a href="formatting-tables.html#summary-statistic-table-example"><i class="fa fa-check"></i><b>9.2</b> Summary Statistic Table Example</a></li>
<li class="chapter" data-level="9.3" data-path="formatting-tables.html"><a href="formatting-tables.html#regression-table-example"><i class="fa fa-check"></i><b>9.3</b> Regression Table Example</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="reproducible-research.html"><a href="reproducible-research.html"><i class="fa fa-check"></i><b>10</b> Reproducible Research</a></li>
<li class="chapter" data-level="11" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html"><i class="fa fa-check"></i><b>11</b> Typesetting and Word Processing Programs</a><ul>
<li class="chapter" data-level="11.1" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html#latex"><i class="fa fa-check"></i><b>11.1</b> LaTeX</a><ul>
<li class="chapter" data-level="11.1.1" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html#learning-latex"><i class="fa fa-check"></i><b>11.1.1</b> Learning LaTeX</a></li>
<li class="chapter" data-level="11.1.2" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html#using-latex"><i class="fa fa-check"></i><b>11.1.2</b> Using LaTeX</a></li>
<li class="chapter" data-level="11.1.3" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html#latex-with-r"><i class="fa fa-check"></i><b>11.1.3</b> LaTeX with R</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html#word"><i class="fa fa-check"></i><b>11.2</b> Word</a><ul>
<li class="chapter" data-level="11.2.1" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html#general-advice"><i class="fa fa-check"></i><b>11.2.1</b> General Advice</a></li>
<li class="chapter" data-level="11.2.2" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html#using-r-with-word"><i class="fa fa-check"></i><b>11.2.2</b> Using R with Word</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="writing-resources.html"><a href="writing-resources.html"><i class="fa fa-check"></i><b>12</b> Writing Resources</a><ul>
<li class="chapter" data-level="12.1" data-path="writing-resources.html"><a href="writing-resources.html#writing-and-organizing-papers"><i class="fa fa-check"></i><b>12.1</b> Writing and Organizing Papers</a></li>
<li class="chapter" data-level="12.2" data-path="writing-resources.html"><a href="writing-resources.html#finding-research-ideas"><i class="fa fa-check"></i><b>12.2</b> Finding Research Ideas</a></li>
<li class="chapter" data-level="12.3" data-path="writing-resources.html"><a href="writing-resources.html#replications"><i class="fa fa-check"></i><b>12.3</b> Replications</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="" data-path="references-4.html"><a href="references-4.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
\[
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Bias}{Bias}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\RMSE}{RMSE}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\Mat}[1]{\boldsymbol{#1}}
\newcommand{\Vec}[1]{\boldsymbol{#1}}
\newcommand{\T}{'}

\newcommand{\distr}[1]{\mathcal{#1}}
\newcommand{\dnorm}{\distr{N}}
\newcommand{\dmvnorm}[1]{\distr{N}_{#1}}
\newcommand{\dt}[1]{\distr{T}_{#1}}

\newcommand{\cia}{\perp\!\!\!\perp}
\DeclareMathOperator*{\plim}{plim}
\]
<div id="regularization" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Regularization</h1>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;glmnet&quot;</span>)</code></pre>
<pre><code>## Loading required package: Matrix</code></pre>
<pre><code>## 
## Attaching package: &#39;Matrix&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:tidyr&#39;:
## 
##     expand</code></pre>
<pre><code>## Loading required package: foreach</code></pre>
<pre><code>## 
## Attaching package: &#39;foreach&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:purrr&#39;:
## 
##     accumulate, when</code></pre>
<pre><code>## Loaded glmnet 2.0-16</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;tidyverse&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;broom&quot;</span>)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">UScrime &lt;-<span class="st"> </span>MASS<span class="op">::</span>UScrime <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate_at</span>(<span class="kw">vars</span>(y, M, Ed, Po1, Po2, LF, M.F, Pop,
                 NW, U1, U2, GDP, Ineq, Prob, Time),
            <span class="kw">funs</span>(log))

varlist &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;M&quot;</span>, <span class="st">&quot;Ed&quot;</span>, <span class="st">&quot;Po1&quot;</span>, <span class="st">&quot;Po2&quot;</span>, <span class="st">&quot;LF&quot;</span>, <span class="st">&quot;M.F&quot;</span>, <span class="st">&quot;Pop&quot;</span>, <span class="st">&quot;NW&quot;</span>,
             <span class="st">&quot;U1&quot;</span>, <span class="st">&quot;U2&quot;</span>, <span class="st">&quot;GDP&quot;</span>, <span class="st">&quot;Ineq&quot;</span>, <span class="st">&quot;Prob&quot;</span>, <span class="st">&quot;Time&quot;</span>)</code></pre>
<p>By default, <code>glmnet</code> will return and entire range of coefficients.</p>
<pre class="sourceCode r"><code class="sourceCode r">mod_lasso &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="kw">as.matrix</span>(UScrime[, varlist]), UScrime[[<span class="st">&quot;y&quot;</span>]])
mod_ridge &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="kw">as.matrix</span>(UScrime[, varlist]), UScrime[[<span class="st">&quot;y&quot;</span>]], <span class="dt">alpha =</span> <span class="dv">0</span>)     </code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">bind_rows</span>(
  <span class="kw">mutate</span>(<span class="kw">tidy</span>(mod_lasso), <span class="dt">model =</span> <span class="st">&quot;Lasso&quot;</span>),
  <span class="kw">mutate</span>(<span class="kw">tidy</span>(mod_ridge), <span class="dt">model =</span> <span class="st">&quot;Ridge&quot;</span>)
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(term <span class="op">!=</span><span class="st"> &quot;(Intercept)&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> dev.ratio, <span class="dt">y =</span> estimate, <span class="dt">colour =</span> term)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>model, <span class="dt">ncol =</span> <span class="dv">1</span>)</code></pre>
<p><img src="regularization_files/figure-html/unnamed-chunk-5-1.svg" width="672" /></p>
<p>Alternatively, the lasso and ridge regression models are the solutions to the problems
<span class="math display">\[
\hat{\beta}_{\text{lasso}} = \arg \min_\beta \left\{  \sum_{i =1}^n \left(y_i - \beta_0 - \sum_{j = 1}^p \beta_j x_{ij} \right)^{2} \right\} \text{s.t.} \sum_{j = 1}^p \beta_j^2 \leq c, 
\]</span>
and
<span class="math display">\[
\begin{aligned}[t]
\hat{\beta}_{\text{lasso}} &amp;= \arg \min_\beta \left\{  \sum_{i =1}^n \left(y_i - \beta_0 - \sum_{j = 1}^p \beta_j x_{ij} \right)^{2} \right\} \\
\text{s.t.}&amp; \sum_{j = 1}^p |\beta_j| \leq c
\end{aligned}
\]</span></p>
<p>In other words, these methods try to find the <span class="math inline">\(\Vec{\beta}\)</span> with the smallest sum of squared errore that has a <span class="math inline">\(\Vec{\beta}\)</span> with a norm less than <span class="math inline">\(c\)</span>.
The value of <span class="math inline">\(c\)</span> corresponds to some value of <span class="math inline">\(\lambda\)</span> in the previous methods.</p>
<p>Think of <span class="math inline">\(c\)</span> as a fixed <em>budget</em>. The lasso and ridge regressions try to find the variables that explain <span class="math inline">\(y\)</span> the best without going over the budget <span class="citation">(James et al. <a href="#ref-JamesWittenHastieEtAl2013a">2013</a>, 221)</span>:</p>
<p>Consider the case with only coefficients: <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>.
In the lasso, we want to find the values of <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>
<span class="math display">\[
|\beta_1| + |\beta_2| \leq c
\]</span></p>
<p><img src="img/islr-fig-6.7.png" /><!-- --></p>
<blockquote>
<p>never trust OLS with more than five regressors
— <a href="http://www.nber.org/econometrics_minicourse_2015/nber_slides11.pdf">Zvi Grilliches</a></p>
<p>Regularization theory was one of the first signs of the existence of intelligent inference
— <a href="http://www.nber.org/econometrics_minicourse_2015/nber_slides11.pdf">Zapnik</a></p>
</blockquote>
<p>Rather than choose the best fit, there is some penalty to avoid over-fitting.
This is to choose the optimal optimal point on the expected predicted value.</p>
<p>There are two questions</p>
<ol style="list-style-type: decimal">
<li>method of regularization</li>
<li>amount of regularization</li>
</ol>
<p>There are several choices of the former, chosen for different reasons.</p>
<p>The latter is almost always chosen by cross-validation.</p>
<p>While OLS is okay for estimating <span class="math inline">\(\beta\)</span> (best linear unbiased property).
However, with <span class="math inline">\(K \geq 3\)</span> regressors, OLS is poor.</p>
<p>The approaches to regularization in regression are</p>
<ol style="list-style-type: decimal">
<li>Shrink estimates to zero (Ridge)</li>
<li>Sparsity, limit number of non-zero estimates (Lasso)</li>
<li>Combination of the two (Bridge)</li>
</ol>
<div id="ridge-regression" class="section level2">
<h2><span class="header-section-number">8.1</span> Ridge Regression</h2>
<p><span class="math display">\[
\hat{\beta}_{\text{OLS}} = \arg \min_{\beta} \sum_{i=1}^{n} (y_i - \Vec{x}_{i} \Vec{\beta})^{2}
\]</span></p>
<p>Regularized regression adds a penalty that is a function of <span class="math inline">\(\beta\)</span>.
This encourages <span class="math inline">\(\beta\)</span> to be close to zero.
<span class="math display">\[
\hat{\beta}_{\text{regularized}} = \arg \min_{\beta} \sum_{i=1}^{n} (y_i - \Vec{x}_{i} \Vec{\beta})^{2} + \lambda f(\beta)
\]</span></p>
<p>Where <span class="math inline">\(\lambda\)</span> is a penalty parameter, and <span class="math inline">\(f(\beta)\)</span> is a function that increases in the total magnitudes of the coefficients.</p>
<ul>
<li><span class="math inline">\(\lambda \to \infty\)</span>: all coefficients are zero</li>
<li><span class="math inline">\(\lambda \to 0\)</span>: same as OLS</li>
</ul>
<p>How do we choose the value of <span class="math inline">\(\lambda\)</span>?</p>
<ul>
<li>Currently: cross-validation</li>
<li>Historically: there were some default plug-in estimators, especially for ridge regression.</li>
</ul>
<p><strong>Ridge</strong> regression penalizes the <span class="math inline">\(\Vec{\beta}\)</span> vector by the
<span class="math display">\[
\hat{\beta}_{\text{ridge}} = \arg \min_{\beta} \sum_{i=1}^{n} (y_i - \Vec{x}_{i} \Vec{\beta})^{2} + \sum_{k = 1}^{p} \beta_k^2
\]</span></p>
<p><strong>Lasso</strong> penalizes the coefficients by an the <span class="math inline">\(L1\)</span> norm.
Suppose we want to find the best subset of <span class="math inline">\(\leq k\)</span> covariates .
<span class="math display">\[
\hat{\beta}_{\text{lasso}} = \arg \min_{\beta} \sum_{i=1}^{n} (y_i - \Vec{x}_{i} \Vec{\beta})^{2} + \lambda \sum_{k = 1}^p |\beta_k|
\]</span></p>
<ul>
<li><p>If true distribution of coefficients is a few big ones and many small ones,
LASSO will do better. If many small/modest sized effects, ridge may do better.</p></li>
<li><p>LASSO does not work well with highly correlated coefficients.</p>
<ul>
<li>Ridge: <span class="math inline">\(\hat{\beta}_{1} + \hat{\beta}_{2} \approx (\beta_1 + \beta_2)/ 2\)</span>.</li>
<li>LASSO: Indifferent between <span class="math inline">\(\hat{\beta}_1 = 0\)</span>, <span class="math inline">\(\hat{\beta}_2 = \beta_1 + \beta_2\)</span>, <span class="math inline">\(\hat{\beta}_1 = \beta_1 + \beta_2\)</span>, and <span class="math inline">\(\hat{\beta}_2 = 0\)</span>.</li>
</ul></li>
<li><p>Approximate best-subset selection. Suppose that we would really like to select
the best subset of <span class="math inline">\(q &lt; k\)</span> coefficients and set the rest to zero (this is the variable selection problem).
That is a hard problem since there are <span class="math inline">\(\binom{k}{q}\)</span>.
Lasso can be viewed as an approximation of the problem.</p></li>
<li><p>Oracle property. If the true model is sufficiently sparse, we can ignore the
selection stage and use OLS standard errors of the non-zero variables
for inference.</p></li>
</ul>
<p><strong>Bridge</strong> regression penalizes the <span class="math inline">\(\Vec{\beta}\)</span> vector by the
<span class="math display">\[
\hat{\beta}_{\text{bridge}} = \arg \min_{\beta} \sum_{i=1}^{n} (y_i - \Vec{x}_{i} \Vec{\beta})^{2} + \lambda_1 \sum_{k = 1}^{p} |\beta_k| + \lambda_2 \sum_{k = 1}^{p} \beta_k^2
\]</span></p>
<p>Bridge regression has some of the properties of both ridge and Lasso.
It will select correlated regressors, yet also shrink coefficients to zero for
a sparse solution.</p>
<p>The R package <strong><a href="https://cran.r-project.org/package=glmnet">glmnet</a></strong> is the most commonly used package to estimate
Lasso, ridge, and bridge regression for linear and generalized linear models.
However, these methods are common enough that all machine learning frameworks
will have some implementation of them. See other packages for variations on the
lasso that take into account other dependencies in the data.</p>
<p>How to find the value of <span class="math inline">\(\lambda\)</span>? Cross validation.
The function <code>cv.glmet()</code> uses cross-validation to select the penalty parameter.</p>
</div>
<div id="regularization-for-causal-inference" class="section level2">
<h2><span class="header-section-number">8.2</span> Regularization for Causal Inference</h2>
<p>Belloni, Chernozhukov, and Hansen (2014) propose a simple method for using Lasso
for causal effects.</p>
<p>What’s the problem with regularized regression for causal inference?
Suppose we estimate a model with the aim to recover <span class="math inline">\(\beta_1\)</span>.
<span class="math display">\[
\Vec{y} = \alpha + \beta x + \gamma_1 z_1 + \cdots + \gamma_k z_{k-1} + \epsilon
\]</span>
If we estimate it with a regularized model, like lasso, then <span class="math inline">\(\beta_1\)</span> will be shrunk in addition to the controls.
If we instead do not shrink <span class="math inline">\(\beta_1\)</span> but we shrink the controls enough.
It will be closer to <strong>not</strong> controlling for the other variables since any part of
of the treatment prediction of the outcome explained by the controls will be shrunk since those coefficients are penalized, but the treatment coefficient is not.</p>
<ol style="list-style-type: decimal">
<li><p>Run Lasso with the outcome <span class="math inline">\(y\)</span> on all controls, <span class="math inline">\(z_1, \dots, \z_k\)</span>.
Keep all non-zero coefficients.</p></li>
<li><p>Run Lasso with the treatment <span class="math inline">\(z\)</span> on all controls, <span class="math inline">\(z_1, \dots, z_k\)</span>.
Keep all non-zero coefficients.</p></li>
<li><p>Run OLS with the outcome <span class="math inline">\(y\)</span> on the treatment, <span class="math inline">\(x\)</span>, and all variables with
a non-zero coefficient in either step 1 or 2.</p></li>
</ol>
<p>If the <strong>true model is sparse</strong> (and asymptotics), then by the Oracle property,
we can treat the standard errors of the OLS coefficients in the last step as
if the selection stage did not occur.</p>
<p>See <a href="https://arxiv.org/pdf/1603.01700.pdf" class="uri">https://arxiv.org/pdf/1603.01700.pdf</a> and the <strong><a href="https://cran.r-project.org/package=hdm">hdm</a></strong> which implements this method, and extensions to work with high dimensional data in R.</p>
</div>
<div id="references-3" class="section level2">
<h2><span class="header-section-number">8.3</span> References</h2>
<p>It is a few years old, but the <a href="http://www.nber.org/econometrics_minicourse_2015/nber_slides11.pdf">2015 NBER Summer course</a> has a good introduction to machine learning that is targeted at social scientists.</p>

</div>
</div>



</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-JamesWittenHastieEtAl2013a">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning with Applications in R</em>. 6th ed. Springer.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="cross-validation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="formatting-tables.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/jrnold/intro-method-notes/edit/master/regularization.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
