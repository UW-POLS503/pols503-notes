<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Data Analysis Notes</title>
  <meta name="description" content="These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.">
  <meta name="generator" content="bookdown 0.7.7 and GitBook 2.6.7">

  <meta property="og:title" content="Data Analysis Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington." />
  <meta name="github-repo" content="jrnold/intro-methods-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Data Analysis Notes" />
  
  <meta name="twitter:description" content="These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington." />
  

<meta name="author" content="Jeffrey B. Arnold">


<meta name="date" content="2018-04-20">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="regression-anatomy.html">
<link rel="next" href="collinearity-and-multicollinearity.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intro Method Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="part"><span><b>I Exploratory Data Analysis</b></span></li>
<li class="part"><span><b>II Programming</b></span></li>
<li class="part"><span><b>III Linear Regression</b></span></li>
<li class="chapter" data-level="2" data-path="regression-anatomy.html"><a href="regression-anatomy.html"><i class="fa fa-check"></i><b>2</b> Regression Anatomy</a><ul>
<li class="chapter" data-level="2.1" data-path="regression-anatomy.html"><a href="regression-anatomy.html#example"><i class="fa fa-check"></i><b>2.1</b> Example</a></li>
<li class="chapter" data-level="2.2" data-path="regression-anatomy.html"><a href="regression-anatomy.html#variations"><i class="fa fa-check"></i><b>2.2</b> Variations</a></li>
<li class="chapter" data-level="2.3" data-path="regression-anatomy.html"><a href="regression-anatomy.html#questions"><i class="fa fa-check"></i><b>2.3</b> Questions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html"><i class="fa fa-check"></i><b>3</b> OLS in Matrix Form</a><ul>
<li class="chapter" data-level="" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#setup"><i class="fa fa-check"></i>Setup</a></li>
<li class="chapter" data-level="3.1" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#purpose"><i class="fa fa-check"></i><b>3.1</b> Purpose</a></li>
<li class="chapter" data-level="3.2" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#matrix-algebra-review"><i class="fa fa-check"></i><b>3.2</b> Matrix Algebra Review</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#vectors"><i class="fa fa-check"></i><b>3.2.1</b> Vectors</a></li>
<li class="chapter" data-level="3.2.2" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#matrices"><i class="fa fa-check"></i><b>3.2.2</b> Matrices</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#matrix-operations"><i class="fa fa-check"></i><b>3.3</b> Matrix Operations</a><ul>
<li class="chapter" data-level="3.3.1" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#transpose"><i class="fa fa-check"></i><b>3.3.1</b> Transpose</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#matrices-as-vectors"><i class="fa fa-check"></i><b>3.4</b> Matrices as vectors</a></li>
<li class="chapter" data-level="3.5" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#special-matrices"><i class="fa fa-check"></i><b>3.5</b> Special matrices</a></li>
<li class="chapter" data-level="3.6" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#multiple-linear-regression-in-matrix-form"><i class="fa fa-check"></i><b>3.6</b> Multiple linear regression in matrix form</a></li>
<li class="chapter" data-level="3.7" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#residuals"><i class="fa fa-check"></i><b>3.7</b> Residuals</a></li>
<li class="chapter" data-level="3.8" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#scalar-inverses"><i class="fa fa-check"></i><b>3.8</b> Scalar inverses</a></li>
<li class="chapter" data-level="3.9" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#matrix-inverses"><i class="fa fa-check"></i><b>3.9</b> Matrix Inverses</a></li>
<li class="chapter" data-level="3.10" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#ols-estimator"><i class="fa fa-check"></i><b>3.10</b> OLS Estimator</a></li>
<li class="chapter" data-level="3.11" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#implications-of-ols"><i class="fa fa-check"></i><b>3.11</b> Implications of OLS</a><ul>
<li class="chapter" data-level="3.11.1" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#ols-in-matrix-form-1"><i class="fa fa-check"></i><b>3.11.1</b> OLS in Matrix Form</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="ols-in-matrix-form.html"><a href="ols-in-matrix-form.html#covariancevariance-interpretation-of-ols"><i class="fa fa-check"></i><b>3.12</b> Covariance/variance interpretation of OLS</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="collinearity-and-multicollinearity.html"><a href="collinearity-and-multicollinearity.html"><i class="fa fa-check"></i><b>4</b> collinearity and Multicollinearity</a><ul>
<li class="chapter" data-level="4.1" data-path="collinearity-and-multicollinearity.html"><a href="collinearity-and-multicollinearity.html#perfect-collinearity"><i class="fa fa-check"></i><b>4.1</b> (Perfect) collinearity</a></li>
<li class="chapter" data-level="4.2" data-path="collinearity-and-multicollinearity.html"><a href="collinearity-and-multicollinearity.html#what-to-do-about-it"><i class="fa fa-check"></i><b>4.2</b> What to do about it?</a></li>
<li class="chapter" data-level="4.3" data-path="collinearity-and-multicollinearity.html"><a href="collinearity-and-multicollinearity.html#multicollinearity"><i class="fa fa-check"></i><b>4.3</b> Multicollinearity</a></li>
<li class="chapter" data-level="4.4" data-path="collinearity-and-multicollinearity.html"><a href="collinearity-and-multicollinearity.html#what-do-do-about-it"><i class="fa fa-check"></i><b>4.4</b> What do do about it?</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bootstrapping.html"><a href="bootstrapping.html"><i class="fa fa-check"></i><b>5</b> Bootstrapping</a><ul>
<li class="chapter" data-level="5.1" data-path="bootstrapping.html"><a href="bootstrapping.html#non-parametric-bootstrap"><i class="fa fa-check"></i><b>5.1</b> Non-parametric bootstrap</a></li>
<li class="chapter" data-level="5.2" data-path="bootstrapping.html"><a href="bootstrapping.html#standard-errors"><i class="fa fa-check"></i><b>5.2</b> Standard Errors</a></li>
<li class="chapter" data-level="5.3" data-path="bootstrapping.html"><a href="bootstrapping.html#confidence-intervals"><i class="fa fa-check"></i><b>5.3</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.4" data-path="bootstrapping.html"><a href="bootstrapping.html#alternative-methods"><i class="fa fa-check"></i><b>5.4</b> Alternative methods</a><ul>
<li class="chapter" data-level="5.4.1" data-path="bootstrapping.html"><a href="bootstrapping.html#parametric-bootstrap"><i class="fa fa-check"></i><b>5.4.1</b> Parametric Bootstrap</a></li>
<li class="chapter" data-level="5.4.2" data-path="bootstrapping.html"><a href="bootstrapping.html#clustered-bootstrap"><i class="fa fa-check"></i><b>5.4.2</b> Clustered bootstrap</a></li>
<li class="chapter" data-level="5.4.3" data-path="bootstrapping.html"><a href="bootstrapping.html#time-series-bootstrap"><i class="fa fa-check"></i><b>5.4.3</b> Time series bootstrap</a></li>
<li class="chapter" data-level="5.4.4" data-path="bootstrapping.html"><a href="bootstrapping.html#how-to-sample"><i class="fa fa-check"></i><b>5.4.4</b> How to sample?</a></li>
<li class="chapter" data-level="5.4.5" data-path="bootstrapping.html"><a href="bootstrapping.html#caveats"><i class="fa fa-check"></i><b>5.4.5</b> Caveats</a></li>
<li class="chapter" data-level="5.4.6" data-path="bootstrapping.html"><a href="bootstrapping.html#why-use-bootstrapping"><i class="fa fa-check"></i><b>5.4.6</b> Why use bootstrapping?</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="bootstrapping.html"><a href="bootstrapping.html#bagging"><i class="fa fa-check"></i><b>5.5</b> Bagging</a></li>
<li class="chapter" data-level="5.6" data-path="bootstrapping.html"><a href="bootstrapping.html#hypothesis-testing"><i class="fa fa-check"></i><b>5.6</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="5.7" data-path="bootstrapping.html"><a href="bootstrapping.html#how-many-samples"><i class="fa fa-check"></i><b>5.7</b> How many samples?</a></li>
<li class="chapter" data-level="5.8" data-path="bootstrapping.html"><a href="bootstrapping.html#references"><i class="fa fa-check"></i><b>5.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="prediction.html"><a href="prediction.html"><i class="fa fa-check"></i><b>6</b> Prediction</a><ul>
<li class="chapter" data-level="" data-path="prediction.html"><a href="prediction.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="6.1" data-path="prediction.html"><a href="prediction.html#prediction-questions-vs.causal-questions"><i class="fa fa-check"></i><b>6.1</b> Prediction Questions vs. Causal Questions</a></li>
<li class="chapter" data-level="6.2" data-path="prediction.html"><a href="prediction.html#why-is-prediction-important"><i class="fa fa-check"></i><b>6.2</b> Why is prediction important?</a></li>
<li class="chapter" data-level="6.3" data-path="prediction.html"><a href="prediction.html#many-problems-are-prediction-problems"><i class="fa fa-check"></i><b>6.3</b> Many problems are prediction problems</a><ul>
<li class="chapter" data-level="6.3.1" data-path="prediction.html"><a href="prediction.html#counterfactuals"><i class="fa fa-check"></i><b>6.3.1</b> Counterfactuals</a></li>
<li class="chapter" data-level="6.3.2" data-path="prediction.html"><a href="prediction.html#controls"><i class="fa fa-check"></i><b>6.3.2</b> Controls</a></li>
<li class="chapter" data-level="6.3.3" data-path="prediction.html"><a href="prediction.html#what-does-overfitting-mean"><i class="fa fa-check"></i><b>6.3.3</b> What does overfitting mean</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="prediction.html"><a href="prediction.html#prediction-vs.explanation"><i class="fa fa-check"></i><b>6.4</b> Prediction vs. Explanation</a></li>
<li class="chapter" data-level="6.5" data-path="prediction.html"><a href="prediction.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>6.5</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="6.5.1" data-path="prediction.html"><a href="prediction.html#example-1"><i class="fa fa-check"></i><b>6.5.1</b> Example</a></li>
<li class="chapter" data-level="6.5.2" data-path="prediction.html"><a href="prediction.html#overview"><i class="fa fa-check"></i><b>6.5.2</b> Overview</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="prediction.html"><a href="prediction.html#prediction-policy-problems"><i class="fa fa-check"></i><b>6.6</b> Prediction policy problems</a><ul>
<li class="chapter" data-level="6.6.1" data-path="prediction.html"><a href="prediction.html#references-1"><i class="fa fa-check"></i><b>6.6.1</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>7</b> Cross-Validation</a><ul>
<li class="chapter" data-level="" data-path="cross-validation.html"><a href="cross-validation.html#prerequisites-1"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="7.1" data-path="cross-validation.html"><a href="cross-validation.html#example-predicting-bordeaux-wine"><i class="fa fa-check"></i><b>7.1</b> Example: Predicting Bordeaux Wine</a></li>
<li class="chapter" data-level="7.2" data-path="cross-validation.html"><a href="cross-validation.html#cross-validation-1"><i class="fa fa-check"></i><b>7.2</b> Cross Validation</a></li>
<li class="chapter" data-level="7.3" data-path="cross-validation.html"><a href="cross-validation.html#out-of-sample-error"><i class="fa fa-check"></i><b>7.3</b> Out-of-Sample Error</a><ul>
<li class="chapter" data-level="7.3.1" data-path="cross-validation.html"><a href="cross-validation.html#held-out-data"><i class="fa fa-check"></i><b>7.3.1</b> Held-out data</a></li>
<li class="chapter" data-level="7.3.2" data-path="cross-validation.html"><a href="cross-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>7.3.2</b> k-fold Cross-validation</a></li>
<li class="chapter" data-level="7.3.3" data-path="cross-validation.html"><a href="cross-validation.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>7.3.3</b> Leave-one-Out Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="cross-validation.html"><a href="cross-validation.html#approximations"><i class="fa fa-check"></i><b>7.4</b> Approximations</a></li>
<li class="chapter" data-level="7.5" data-path="cross-validation.html"><a href="cross-validation.html#references-2"><i class="fa fa-check"></i><b>7.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>8</b> Regularization</a><ul>
<li class="chapter" data-level="8.1" data-path="regularization.html"><a href="regularization.html#ridge-regression"><i class="fa fa-check"></i><b>8.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="8.2" data-path="regularization.html"><a href="regularization.html#regularization-for-causal-inference"><i class="fa fa-check"></i><b>8.2</b> Regularization for Causal Inference</a></li>
<li class="chapter" data-level="8.3" data-path="regularization.html"><a href="regularization.html#references-3"><i class="fa fa-check"></i><b>8.3</b> References</a></li>
</ul></li>
<li class="part"><span><b>IV Presentation</b></span></li>
<li class="chapter" data-level="9" data-path="formatting-tables.html"><a href="formatting-tables.html"><i class="fa fa-check"></i><b>9</b> Formatting Tables</a><ul>
<li class="chapter" data-level="9.1" data-path="formatting-tables.html"><a href="formatting-tables.html#overview-of-packages"><i class="fa fa-check"></i><b>9.1</b> Overview of Packages</a></li>
<li class="chapter" data-level="9.2" data-path="formatting-tables.html"><a href="formatting-tables.html#summary-statistic-table-example"><i class="fa fa-check"></i><b>9.2</b> Summary Statistic Table Example</a></li>
<li class="chapter" data-level="9.3" data-path="formatting-tables.html"><a href="formatting-tables.html#regression-table-example"><i class="fa fa-check"></i><b>9.3</b> Regression Table Example</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="reproducible-research.html"><a href="reproducible-research.html"><i class="fa fa-check"></i><b>10</b> Reproducible Research</a></li>
<li class="chapter" data-level="11" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html"><i class="fa fa-check"></i><b>11</b> Typesetting and Word Processing Programs</a><ul>
<li class="chapter" data-level="11.1" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html#latex"><i class="fa fa-check"></i><b>11.1</b> LaTeX</a><ul>
<li class="chapter" data-level="11.1.1" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html#learning-latex"><i class="fa fa-check"></i><b>11.1.1</b> Learning LaTeX</a></li>
<li class="chapter" data-level="11.1.2" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html#using-latex"><i class="fa fa-check"></i><b>11.1.2</b> Using LaTeX</a></li>
<li class="chapter" data-level="11.1.3" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html#latex-with-r"><i class="fa fa-check"></i><b>11.1.3</b> LaTeX with R</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html#word"><i class="fa fa-check"></i><b>11.2</b> Word</a><ul>
<li class="chapter" data-level="11.2.1" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html#general-advice"><i class="fa fa-check"></i><b>11.2.1</b> General Advice</a></li>
<li class="chapter" data-level="11.2.2" data-path="typesetting-and-word-processing-programs.html"><a href="typesetting-and-word-processing-programs.html#using-r-with-word"><i class="fa fa-check"></i><b>11.2.2</b> Using R with Word</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="writing-resources.html"><a href="writing-resources.html"><i class="fa fa-check"></i><b>12</b> Writing Resources</a><ul>
<li class="chapter" data-level="12.1" data-path="writing-resources.html"><a href="writing-resources.html#writing-and-organizing-papers"><i class="fa fa-check"></i><b>12.1</b> Writing and Organizing Papers</a></li>
<li class="chapter" data-level="12.2" data-path="writing-resources.html"><a href="writing-resources.html#finding-research-ideas"><i class="fa fa-check"></i><b>12.2</b> Finding Research Ideas</a></li>
<li class="chapter" data-level="12.3" data-path="writing-resources.html"><a href="writing-resources.html#replications"><i class="fa fa-check"></i><b>12.3</b> Replications</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="" data-path="references-4.html"><a href="references-4.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
\[
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Bias}{Bias}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\RMSE}{RMSE}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\Mat}[1]{\boldsymbol{#1}}
\newcommand{\Vec}[1]{\boldsymbol{#1}}
\newcommand{\T}{'}

\newcommand{\distr}[1]{\mathcal{#1}}
\newcommand{\dnorm}{\distr{N}}
\newcommand{\dmvnorm}[1]{\distr{N}_{#1}}
\newcommand{\dt}[1]{\distr{T}_{#1}}

\newcommand{\cia}{\perp\!\!\!\perp}
\DeclareMathOperator*{\plim}{plim}
\]
<div id="ols-in-matrix-form" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> OLS in Matrix Form</h1>
<div id="setup" class="section level2 unnumbered">
<h2>Setup</h2>
<p>This will use the <code>Duncan</code> data in a few examples.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;tidyverse&quot;</span>)
<span class="kw">data</span>(<span class="st">&quot;Duncan&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;carData&quot;</span>)</code></pre>
</div>
<div id="purpose" class="section level2">
<h2><span class="header-section-number">3.1</span> Purpose</h2>
<p>We can write regression model as,
<span class="math display">\[
y_i = \beta_0 + x_{i1} \beta_1 + x_{i2} \beta_2 + \cdots + x_{ik} \beta_k + u_k .
\]</span>
It will be cleaner to write the linear regression as
<span class="math display">\[
y_i = \Vec{x}_{i} \Vec{\beta} + u_i
\]</span>
where <span class="math inline">\(\Vec{x}_i\)</span> is a <span class="math inline">\(1 \times (K + 1)\)</span> row vector and <span class="math inline">\(\Vec{\beta}\)</span> is a <span class="math inline">\((K + 1) \times 1\)</span> column vector for a single observation <span class="math inline">\(i\)</span>.</p>
<p>Or we can write it as,
<span class="math display">\[
\Vec{y} = \Mat{X} \Vec{\beta} + \Vec{u}
\]</span>
where <span class="math inline">\(\Vec{y}\)</span> is a <span class="math inline">\(N \times 1\)</span> row vector, <span class="math inline">\(\Mat{X}\)</span> is a <span class="math inline">\(N \times (K + 1)\)</span> matrix, and <span class="math inline">\(\Vec{\beta}\)</span> is a <span class="math inline">\((K + 1) \times 1\)</span> column vector for all <span class="math inline">\(N\)</span> observations.</p>
</div>
<div id="matrix-algebra-review" class="section level2">
<h2><span class="header-section-number">3.2</span> Matrix Algebra Review</h2>
<div id="vectors" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Vectors</h3>
<ul>
<li><p>A <em>vector</em> is a list of numbers or random variables.</p></li>
<li><p>A <span class="math inline">\(1 \times k\)</span> <em>row vector</em> is arranged
<span class="math display">\[
\Vec{b} = \begin{bmatrix} b_1 &amp; b_2 &amp; b_3 &amp; \dots &amp; b_k \end{bmatrix}
\]</span></p></li>
<li><p>A <span class="math inline">\(1 \times k\)</span> <em>column vector</em> is arranged
<span class="math display">\[
\Vec{a} = \begin{bmatrix} b_1 \\ b_2 \\ b_3\\ \dots \\ b_k \end{bmatrix}
\]</span></p></li>
<li><p>Convention: assume vectors are <em>column</em> vectors</p></li>
<li><p>Convention: use lower-case <strong>bold</strong> Latin letters, e.g. <span class="math inline">\(\Vec{x}\)</span>.</p></li>
</ul>
<p>Vector Examples</p>
<ul>
<li><p>Vector of all covariates for a particular unit <span class="math inline">\(i\)</span> as a row vector,
<span class="math display">\[
\Vec{x}_{i} = \begin{bmatrix}
x_{i1} &amp; x_{i2} &amp; \dots &amp; x_{ik}
\end{bmatrix}
\]</span></p>
<p>E.g. in the Duncan data,
<span class="math display">\[
\Vec{x}_{i} = \begin{bmatrix}
\mathtt{education}_{i} &amp; \mathtt{income}_{i} &amp; \mathtt{type}_i
\end{bmatrix}
\]</span></p></li>
<li><p>Vector of the values of covariate <span class="math inline">\(k\)</span> for all observations,
<span class="math display">\[
x_{.,k} = \begin{bmatrix}
1 \\ x_{i1} \\ x_{i2} \\ \dots \\ x_{ik}
\end{bmatrix}
\]</span></p>
<p>E.g. For the <code>education</code> variable in the column vector.
<span class="math display">\[
\Vec{x}_{i} = \begin{bmatrix}
\mathtt{education}_{1} \\ \mathtt{education}_{2} \\ \dots &amp; \vdots &amp; \mathtt{education}_N
\end{bmatrix}
\]</span></p></li>
</ul>
</div>
<div id="matrices" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Matrices</h3>
<ul>
<li><p>A <strong>matrix</strong> is a rectangular array of numbers</p></li>
<li><p>A matrix is <span class="math inline">\(n \times k\)</span> (“<span class="math inline">\(n\)</span> by <span class="math inline">\(k\)</span>”) if it has <span class="math inline">\(n\)</span> rows and <span class="math inline">\(k\)</span> columns</p></li>
<li><p>A matrix
<span class="math display">\[
A = \begin{bmatrix}
a_{11} &amp; a_{12} &amp; \dots &amp; a_{1k} \\
a_{21} &amp; a_{22} &amp; \dots &amp; a_{2k} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{n1} &amp; a_{n2} &amp; \dots &amp; a_{nk}
\end{bmatrix}
\]</span></p></li>
</ul>
<div id="examples" class="section level4">
<h4><span class="header-section-number">3.2.2.1</span> Examples</h4>
<p>The <strong>design matrix</strong> is the matrix of predictors/covariates in a regression:
<span class="math display">\[
    X = \begin{bmatrix}
    1 &amp; x_{11} &amp; x_{12} &amp; \dots &amp; a_{1k} \\
    1 &amp; x_{21} &amp; x_{22} &amp; \dots &amp; a_{2k} \\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    1 &amp; x_{n1} &amp; a_{n2} &amp; \dots &amp; a_{nk}
    \end{bmatrix}
\]</span>
The vector of ones is the constant.</p>
<p>In the Duncan data, for the regression
<code>prestige ~ income + education + type</code>, the design matrix is,
<span class="math display">\[
    X = \begin{bmatrix}
    1 &amp; \mathtt{income}_{1} &amp; \mathtt{education}_{1} &amp; \mathtt{wc}_{1} &amp; \mathtt{prof}_{1} \\
    1 &amp; \mathtt{income}_{2} &amp; \mathtt{education}_{2} &amp; \mathtt{wc}_{2} &amp; \mathtt{prof}_{2} \\
    \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
    1 &amp; \mathtt{income}_{N} &amp; \mathtt{education}_{N} &amp; \mathtt{wc}_{N} &amp; \mathtt{prof}_{N} \\
    \end{bmatrix}
\]</span></p>
<p>Use R function <code>model.matrix</code> to create the design matrix from a formula and a data frame.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">model.matrix</span>(prestige <span class="op">~</span><span class="st"> </span>income <span class="op">+</span><span class="st"> </span>education <span class="op">+</span><span class="st"> </span>type, <span class="dt">data =</span> Duncan)</code></pre>
<pre><code>##                    (Intercept) income education typeprof typewc
## accountant                   1     62        86        1      0
## pilot                        1     72        76        1      0
## architect                    1     75        92        1      0
## author                       1     55        90        1      0
## chemist                      1     64        86        1      0
## minister                     1     21        84        1      0
## professor                    1     64        93        1      0
## dentist                      1     80       100        1      0
## reporter                     1     67        87        0      1
## engineer                     1     72        86        1      0
## undertaker                   1     42        74        1      0
## lawyer                       1     76        98        1      0
## physician                    1     76        97        1      0
## welfare.worker               1     41        84        1      0
## teacher                      1     48        91        1      0
## conductor                    1     76        34        0      1
## contractor                   1     53        45        1      0
## factory.owner                1     60        56        1      0
## store.manager                1     42        44        1      0
## banker                       1     78        82        1      0
## bookkeeper                   1     29        72        0      1
## mail.carrier                 1     48        55        0      1
## insurance.agent              1     55        71        0      1
## store.clerk                  1     29        50        0      1
## carpenter                    1     21        23        0      0
## electrician                  1     47        39        0      0
## RR.engineer                  1     81        28        0      0
## machinist                    1     36        32        0      0
## auto.repairman               1     22        22        0      0
## plumber                      1     44        25        0      0
## gas.stn.attendant            1     15        29        0      0
## coal.miner                   1      7         7        0      0
## streetcar.motorman           1     42        26        0      0
## taxi.driver                  1      9        19        0      0
## truck.driver                 1     21        15        0      0
## machine.operator             1     21        20        0      0
## barber                       1     16        26        0      0
## bartender                    1     16        28        0      0
## shoe.shiner                  1      9        17        0      0
## cook                         1     14        22        0      0
## soda.clerk                   1     12        30        0      0
## watchman                     1     17        25        0      0
## janitor                      1      7        20        0      0
## policeman                    1     34        47        0      0
## waiter                       1      8        32        0      0
## attr(,&quot;assign&quot;)
## [1] 0 1 2 3 3
## attr(,&quot;contrasts&quot;)
## attr(,&quot;contrasts&quot;)$type
## [1] &quot;contr.treatment&quot;</code></pre>
</div>
</div>
</div>
<div id="matrix-operations" class="section level2">
<h2><span class="header-section-number">3.3</span> Matrix Operations</h2>
<div id="transpose" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Transpose</h3>
<p>The <strong>transpose</strong> of a matrix <span class="math inline">\(A\)</span> flips the rows and columns. It is denoted <span class="math inline">\(A&#39;\)</span> or <span class="math inline">\(A^{T}\)</span>.</p>
<p>The transpose of a <span class="math inline">\(3 \times 2\)</span> matrix is a <span class="math inline">\(2 \times 3\)</span> matrix,
<span class="math display">\[
A = \begin{bmatrix}
a_{11} &amp; a_{12} \\
a_{21} &amp; a_{22} \\
a_{31} &amp; a_{32}
\end{bmatrix}  =
\begin{bmatrix}
a_{11} &amp; a_{21}  &amp; a_{31} \\
a_{12} &amp; a_{22}  &amp; a_{32}
\end{bmatrix}
\]</span></p>
<p>Transposing turns a <span class="math inline">\(1 \times k\)</span> row vector into a <span class="math inline">\(k \times 1\)</span> column vector and vice-versa.</p>
<p><span class="math display">\[
\begin{aligned}[t]
x_i &amp;=
\begin{bmatrix}
1 \\
x_{i1} \\
x_{i2} \\
\vdots \\
x_{ik}
\end{bmatrix} \\
x_i&#39; &amp;=
\begin{bmatrix}
1 &amp;
x_{i1} &amp;
x_{i2} &amp;
\dots &amp;
x_{ik}
\end{bmatrix}
\end{aligned}
\]</span></p>
<pre class="sourceCode r"><code class="sourceCode r">A &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>, <span class="dt">ncol =</span> <span class="dv">3</span>, <span class="dt">nrow =</span> <span class="dv">2</span>)
A</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]    1    3    5
## [2,]    2    4    6</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t</span>(A)</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    1    2
## [2,]    3    4
## [3,]    5    6</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">a &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">6</span>
<span class="kw">t</span>(a)</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]    1    2    3    4    5    6</code></pre>
</div>
</div>
<div id="matrices-as-vectors" class="section level2">
<h2><span class="header-section-number">3.4</span> Matrices as vectors</h2>
<p>A matrix is a collection of row (or column) vectors.</p>
<p>Write the matrix as a collection of row vectors
<span class="math display">\[
A = \begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23}
\end{bmatrix} =
\begin{bmatrix}
\Vec{a}_1&#39; \\
\Vec{a}_2&#39;
\end{bmatrix}
\]</span>
<span class="math display">\[
B = \begin{bmatrix}
b_{11} &amp; b_{12} \\
b_{21} &amp; b_{22} \\
b_{31} &amp; b_{32}
\end{bmatrix} =
\begin{bmatrix}
\Vec{b}_1 \\
\Vec{b}_2
\Vec{b}_3
\end{bmatrix}
\]</span></p>
<p>How does <span class="math inline">\(X\)</span> relate to the model specification?
See the <code>model.matrix</code></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">model.matrix</span>(prestige <span class="op">~</span><span class="st"> </span>education <span class="op">*</span><span class="st"> </span>income <span class="op">+</span><span class="st"> </span>type, <span class="dt">data =</span> Duncan) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">head</span>()</code></pre>
<pre><code>##            (Intercept) education income typeprof typewc education:income
## accountant           1        86     62        1      0             5332
## pilot                1        76     72        1      0             5472
## architect            1        92     75        1      0             6900
## author               1        90     55        1      0             4950
## chemist              1        86     64        1      0             5504
## minister             1        84     21        1      0             1764</code></pre>
<p>The OLS estimator of coefficients is
<span class="math display">\[
\hat{\beta} = \underbrace{(X&#39; X)^{-1}}_{Var(X)} \underbrace{X&#39; y}_{Cov(X, Y)}
\]</span></p>
</div>
<div id="special-matrices" class="section level2">
<h2><span class="header-section-number">3.5</span> Special matrices</h2>
<p>A <strong>square matrix</strong> has equal numbers of rows and columns</p>
<p>The <strong>identity matrix</strong>, <span class="math inline">\(\Mat{I}_K\)</span> is a <span class="math inline">\(K \times K\)</span> square matrix with 1s on the diagonal, and 0s everywhere else.
<span class="math display">\[
\Mat{I}_3 = \begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix}
\]</span></p>
<p>The identity matrix multiplied by any matrix returns the matrix,
<span class="math display">\[
\Mat{A} \Mat{I}_{K} = \Mat{A} = \Mat{I}_{M} \Mat{A}
\]</span>
where <span class="math inline">\(\Mat{A}\)</span> is an <span class="math inline">\(M \times K\)</span> matrix.</p>
<p>In R, to get the diagonal of a matrix use <code>diag()</code>,</p>
<pre class="sourceCode r"><code class="sourceCode r">b &lt;-<span class="st"> </span><span class="kw">diag</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>, <span class="dt">nrow =</span> 2L, <span class="dt">ncol =</span> 2L)
b &lt;-
<span class="kw">diag</span>(b)</code></pre>
<p>The function <code>diag()</code> also creates identity matrices,</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">diag</span>(3L)</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]    1    0    0
## [2,]    0    1    0
## [3,]    0    0    1</code></pre>
<p>The <strong>zero matrix</strong> is a matrix of all zeros,
<span class="math display">\[
\Mat{0}_K =
\begin{bmatrix}
0 &amp; 0 &amp; \dots 0 \\
0 &amp; 0 &amp; \dots 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; vdots \\
0 &amp; 0 &amp; \dots &amp; 0
\end{bmatrix}
\]</span>
The <em>zero vector</em> is a matrix of all zeros,
<span class="math display">\[
\Mat{0}_K =
\begin{bmatrix}
0 \\ 0 \\ \vdots \\ 0
\end{bmatrix}
\]</span>
The <strong>ones vector</strong> is a vector of all ones,
<span class="math display">\[
\Mat{0}_K =
\begin{bmatrix}
1 \\ 1 \\ \vdots \\ 1
\end{bmatrix}
\]</span></p>
</div>
<div id="multiple-linear-regression-in-matrix-form" class="section level2">
<h2><span class="header-section-number">3.6</span> Multiple linear regression in matrix form</h2>
<p>Let <span class="math inline">\(\widehat{\Vec{\beta}}\)</span> be the matrix of estimated regression coefficients, and <span class="math inline">\(\hat{\Vec{y}}\)</span> be the vector of fitted values:
<span class="math display">\[
\begin{aligned}[t]
\widehat{\Vec{\beta}} &amp;=
\begin{bmatrix}
\widehat{\beta}_0 \\
\widehat{\beta}_1 \\
\vdots \\
\widehat{\beta}_K
\end{bmatrix} &amp;
\hat{\Vec{y}} &amp;= \Mat{X} \widehat{\Vec{\beta}}
\end{aligned}
\]</span>
This could be expanded to,
<span class="math display">\[
\begin{aligned}[t]
\hat{\Vec{y}} &amp;=
\begin{bmatrix}
\hat{y}_1 \\
\hat{y}_2 \\
\vdots \\
\hat{y}_N
\end{bmatrix}
&amp;= \Mat{X} \widehat{\Vec{\beta}}
&amp;= \begin{aligned}
1\widehat{\beta}_0 + x_{11} \widehat{\beta}_1 + x_{12} \widehat{\beta}_2 + \dots + x_{1K} \widehat{\beta}_K \\
1\widehat{\beta}_0 + x_{21} \widehat{\beta}_1 + x_{22} \widehat{\beta}_2 + \dots + x_{2K} \widehat{\beta}_K \\
\vdots \\
1\widehat{\beta}_0 + x_{N1} \widehat{\beta}_1 + x_{N2} \widehat{\beta}_2 + \dots + x_{NK} \widehat{\beta}_K \\
\end{aligned}
\end{aligned}
\]</span></p>
</div>
<div id="residuals" class="section level2">
<h2><span class="header-section-number">3.7</span> Residuals</h2>
<p>The residuals of a regression are,
<span class="math display">\[
\Vec{u} = \Vec{y} - \Mat{X} \widehat{\beta}
\]</span></p>
<p>In two dimensions the Euclidian distance is,
<span class="math display">\[
d(a, b) = \sqrt{a^2 + b^2}
\]</span>
Think the hypotenuse of a triangle.</p>
<p>The <strong>norm</strong> or <strong>length</strong> of a vector generalizes the Euclidian distance to multiple dimensions<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p>For a <span class="math inline">\(K \times 1\)</span> vector <span class="math inline">\(\Vec{a}\)</span>,
<span class="math display">\[
| \Vec{a} | = \sqrt{a_1^2 + a_2^2 + \dots + a_K^2}
\]</span></p>
<p>The <strong>norm</strong> can be written as the <strong>inner product</strong>,
<span class="math display">\[
{| \Vec{a} |}^2  = \Vec{a}\T \Vec{a}
\]</span></p>
<p>Note that when the mean of a vector is 0, the norm is equal to <span class="math inline">\(N\)</span> times the sample variance (using the <span class="math inline">\(N\)</span> denominator)
<span class="math display">\[
\begin{aligned}
\Var{\Vec{u}} &amp;= \frac{1}{N} \sum_{i = 1}^N (u_i - \var{u})^2 \\
&amp;= \frac{1}{N} \sum_{i = 1}^N u_i^2 \\
&amp;= \frac{1}{N} \Vec{u}\T \Vec{u} \\
&amp;= \frac{1}{N} {| \Vec{u} |}^2 \\
\end{aligned}
\]</span></p>
</div>
<div id="scalar-inverses" class="section level2">
<h2><span class="header-section-number">3.8</span> Scalar inverses</h2>
<p>What is division? You may think of it as the inverse of multiplication (which it is), but it means that for number <span class="math inline">\(a\)</span> there exists another number (the inverse of <span class="math inline">\(a\)</span>) denoted <span class="math inline">\(a^{-1}\)</span> or <span class="math inline">\(1 / a\)</span> such that <span class="math inline">\(a \times a^{-1} = 1\)</span>.</p>
<p>This inverse does not always exist. There is no inverse for 0: <span class="math inline">\(0 \times ? = 1\)</span> has no solution.</p>
<p>If the inverse exists, we can solve algebraic expressions like <span class="math inline">\(ax = b\)</span> for <span class="math inline">\(x\)</span>,
<span class="math display">\[
\begin{aligned}
ax &amp;= b \\
\frac{1}{a} ax &amp;= \frac{1}{a} b &amp; \text{multiply both sides by the inverse of \[a\]} \\
x = \frac{b}{a}
\end{aligned}
\]</span></p>
<p>We’ll see in matrix algebra, the intuition is similar.</p>
<ul>
<li>The inverse is a matrix such that when it multiplies a number it results in 1 (or the equivalent)</li>
<li>The inverse doesn’t always exist</li>
<li>The inverse can be used to solve</li>
</ul>
</div>
<div id="matrix-inverses" class="section level2">
<h2><span class="header-section-number">3.9</span> Matrix Inverses</h2>
<p>If it exists (it does not always), the <strong>inverse</strong> of square matrix <span class="math inline">\(\Mat{A}\)</span>, denoted <span class="math inline">\(\Mat{A}^{-1}\)</span>, is the matrix such that
<span class="math display">\[
\Mat{A}^{-1} \Mat{A} = \Mat{A} \Mat{A}^{-1} = \Mat{I}
\]</span></p>
<p>The inverse can be used to solve systems of equations (like OLS)
<span class="math display">\[
\begin{aligned}[t]
\Mat{A} \Vec{x} &amp;= \Vec{b} \\
\Mat{A}^{-1} \Mat{A} \Vec{x} &amp;= \Mat{A}^{-1} \Vec{b} \\
I \Vec{x} &amp;= \Mat{A}^{-1} \Vec{b} \\
\Vec{x} &amp;= \Mat{A}^{-1} \Vec{b}
\end{aligned}
\]</span></p>
<p>If the inverse exists, then <span class="math inline">\(\Mat{A}\)</span> is called <strong>invertible</strong> or <strong>nonsingular</strong>.</p>
</div>
<div id="ols-estimator" class="section level2">
<h2><span class="header-section-number">3.10</span> OLS Estimator</h2>
<p>OLS minimizes the sum of squared residuals
<span class="math display">\[
\arg \min
\]</span></p>
</div>
<div id="implications-of-ols" class="section level2">
<h2><span class="header-section-number">3.11</span> Implications of OLS</h2>
<p>Independent variables are orthogonal to the residuals
<span class="math display">\[
\Mat{X}\T \hat{\Vec{u}} = \Mat{X}\T(\Vec{y} - \Mat{X} \widehat{\Vec{\beta}}) = 0
\]</span>
Fitted values are orthogonal to the residuals
<span class="math display">\[
\Vec{y}\T \hat{\Vec{u}} =(\Mat{X} \widehat{\Vec{\beta}})\T \hat{\Vec{u}} = \widehat{\Vec{\beta}}\T \Mat{X}\T \hat{\Vec{u}} = 0
\]</span></p>
<div id="ols-in-matrix-form-1" class="section level3">
<h3><span class="header-section-number">3.11.1</span> OLS in Matrix Form</h3>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_k x_{ik} + \epsilon_i
\]</span>
We can write this as
<span class="math display">\[
\begin{aligned}[t]
Y_i &amp;=
\begin{bmatrix}
1 &amp; x_{i1} &amp; x_{i2} &amp; \dots &amp; x_{ik}
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_k
\end{bmatrix} + \epsilon_i \\
&amp;= \underbrace{{x_i&#39;}}_{1 \times k + 1} \underbrace{\beta}_{k + 1 \times 1} + \epsilon_i
\end{aligned}
\]</span></p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_k x_{ik} + \epsilon_i
\]</span>
We can write it as
<span class="math display">\[
\begin{aligned}[t]
\begin{bmatrix}
Y_1 \\
Y_2 \\
\vdots \\
Y_n
\end{bmatrix}
&amp;=
\begin{bmatrix}
1 &amp; x_{11} &amp; x_{12} &amp; \dots &amp; x_{1k}  \\
1 &amp; x_{21} &amp; x_{22} &amp; \dots &amp; x_{2k}  \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_{n1} &amp; x_{n2} &amp; \dots &amp; x_{nk}
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_k
\end{bmatrix} +
\begin{bmatrix}
\epsilon_1 \\
\epsilon_2 \\
\vdots \\
\epsilon_n
\end{bmatrix} \\
\underbrace{y}_{(n \times 1)} &amp;=
\underbrace{{x_i&#39;}}_{(n \times k + 1)} \underbrace{\beta}_{(k + 1 \times 1)} +
\underbrace{\epsilon}_{(n \times 1)}
\end{aligned}
\]</span></p>
<p>The regression standard error of the regression is
<span class="math display">\[
\hat{\sigma}_y^2 = \frac{\sum_i^n \epsilon_i^2}{n - k - 1}
\]</span>
Write this using matrix notation.</p>
<p>Note that
<span class="math display">\[
E(X_i)^2 = \frac{\sum X_i^2}{n}
\]</span>
In matrix notation this is,
<span class="math display">\[
\begin{bmatrix}
x_1 &amp; x_2 &amp; \dots &amp; x_n
\end{bmatrix}
\begin{bmatrix}
x_1 \\ x_2 \\ \dots \\ x_n
\end{bmatrix} =
x&#39; x
\]</span>
If <span class="math inline">\(\bar{X} = 0\)</span>, then
<span class="math display">\[
\frac{X&#39; X}{N} = Var(X)
\]</span></p>
<ul>
<li>What is the vcov matrix of <span class="math inline">\(\beta\)</span>?</li>
<li>When would it be diagonal?</li>
<li>What is on the off-diagonal?</li>
<li>What is on the diagonal?</li>
<li>Extract the standard errors from it.</li>
</ul>
<p>OLS Standard errors
<span class="math display">\[
\hat{\beta}_{OLS} = (X&#39; X)^{-1} X&#39; y
\]</span></p>
<p><span class="math display">\[
V(\hat{\beta}) =
\begin{bmatrix}
V(\hat{\beta}_0) &amp; Cov(\hat{\beta}_0, \hat{\beta}_1) &amp; \dots &amp; Cov(\hat{\beta}_0, \hat{\beta}_k) \\
Cov(\hat{\beta}_1, \hat{\beta}_0) &amp; V(\hat{\beta}_1) &amp; \dots &amp; Cov(\hat{\beta}_1, \hat{\beta}_k) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
Cov(\hat{\beta}_k, \hat{\beta}_0) &amp; Cov(\hat{\beta}_k, \hat{\beta}_1) &amp; \dots &amp; V(\hat{\beta}_k) \\
\end{bmatrix}
\]</span></p>
<p>Which of these matrices are</p>
<ol style="list-style-type: decimal">
<li>Homoskedastic</li>
<li>Heteroskedastic</li>
<li>Clustered standard errors</li>
<li>Serially correlated</li>
</ol>
<p>Show how <span class="math inline">\((X&#39; X)^{-1} X&#39; y\)</span> is equivalent to the bivariate estimator.</p>
<ol style="list-style-type: decimal">
<li>Write out <span class="math inline">\(\beta\)</span> and plug in for the true <span class="math inline">\(Y\)</span> in terms of <span class="math inline">\(X\)</span> and <span class="math inline">\(\epsilon\)</span></li>
<li>Take the variance of <span class="math inline">\(\hat{\beta} - \beta\)</span></li>
</ol>
<p><span class="math display">\[
\hat{\beta} = \beta + (X&#39; X)^{-1} X&#39; \epsilon \\
\Var(\hat{\beta} - \beta) =  var((X&#39; X)^{-1} X&#39; \epsilon) \\
\]</span>
We know that</p>
<ul>
<li><span class="math inline">\((X&#39; X)^{-1} X&#39; \epsilon\)</span> has mean zero since <span class="math inline">\(E(X&#39; \epsilon) = 0\)</span>.</li>
<li><span class="math inline">\(var(z) = E(Z^2) - 0\)</span></li>
<li>In matrix form <span class="math inline">\(Z Z&#39;\)</span> to get full matrix form</li>
</ul>
<p><span class="math display">\[
V((X&#39; X)^{-1} X&#39; \epsilon) = (X&#39; X)^{-1} X&#39; \epsilon \epsilon&#39; X (X&#39; X)^{-1} = (X&#39; X)^{-1} X&#39; \Sigma X (X&#39; X)^{-1}
\]</span>
We need a way to estimate <span class="math inline">\(\hat{\Sigma}\)</span>.
But it has <span class="math inline">\(n (n + 1) / 2\)</span> elements … and we have only <span class="math inline">\(n\)</span> observations, and <span class="math inline">\(n - k - 1\)</span> degrees of freedom left after estimating the coefficients.</p>
<p>If homoskedasticity, <span class="math inline">\(\Sigma = \sigma^2 I\)</span>.
<span class="math display">\[
V((X&#39; X)^{-1} X&#39; \epsilon) = \sigma^2 (X&#39; X)^{-1}
\]</span></p>
<p>Panel of countries. Correlation within each year that is always the same</p>
<!--
## OLS Assumptions

1. Linearity: $y_i = x'_i \beta + u_i$
2. Random/iid sample: $(y_i, x'_i)$ are a iid sample from population
3. No perfect collinearity: $X$ is an $n \times (k + 1)$ matrix with rank $k + 1$
4. Zero conditional mean $E(u_i | x_i) = 0$
5. Homoskedasticity: $V(u_i | x_i) = \sigma_u^2$
6. Normality $u_i | x_i \sim N(0, \sigma_u^2)$

- Unbiasedness: 1--5 in large samples

Gauss-Markov

Suppose  that
$$
y = X \beta + \epsilon
$$
where $y, \epsilon \in R^n$, $\beta \in R^K$ and $X \in R^{n \times K}$.

With the following assumptions on the errors,

- mean zero $E(\epsilon_i | x_i) = 0$
- homoskedastic $Var(\epsilon_i) = \sigma^2 < \infty$
- uncorrelated $Cov(\epsilon_i, \epsilon_j) = 0$ for all $i \neq j$.

Then OLS is BLUE: "best linear unbiased estimator"

-   linear estimator: estimator can be written as a weighted sum of the responses. OLS is a linear estimator since
    $$
    \hat{\beta} = \underbrace{(X' X)^{-1} X'}_{\text{weight}} y
    $$
- unbiased: $E(\hat{\beta} - \beta) = 0$
- best: of all the unbiased linear estimators it has the lowest variance, $V(\beta)$.
-->
</div>
</div>
<div id="covariancevariance-interpretation-of-ols" class="section level2">
<h2><span class="header-section-number">3.12</span> Covariance/variance interpretation of OLS</h2>
<p><span class="math display">\[
\Mat{X}\T \Vec{y} = \sum_{i = 1}^N
\begin{bmatrix}
y_i \\
y_i x_{i1} \\
y_i x_{i2}\\
\vdots \\
y_i x_{iK}
\end{bmatrix}
\approx
\begin{bmatrix}
n\bar{y} \\
\widehat{\Cov}[y_i, x_{i1}] \\
\widehat{\Cov}[y_i, x_{i2}] \\
\vdots \\
\widehat{\Cov}[y_i, x_{iK}]
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\Mat{X}\T \Mat{X} &amp;= \sum_{i = 1}^N
\begin{bmatrix}
1 &amp; x_{i1} &amp; x_{i2}&amp; \cdots &amp; x_{ik} \\
x_{i1} &amp; x_{i1}^2 &amp; x_{i2} x_{i1} &amp; \cdots &amp; x_{i1} x_{iK} \\
x_{i2} &amp; x_{i1} x_{i2} &amp; x_{i2}^2 &amp; \cdots &amp; x_{i2} x_{iK} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{iK} &amp; x_{i1} x_{iK} &amp; x_{i2} x_{iK} &amp; \cdots &amp; x_{ik} x_{iK}
\end{bmatrix}  \\
&amp;\approx
\begin{bmatrix}
n  &amp; n \bar{x}_1 &amp; n \bar{x}_2 &amp; \cdots &amp; n \bar{x}_K \\
n \bar{x}_3 &amp; \widehat{\Var}[x_{i1}] &amp; \widehat{\Cov}[x_{i1}, x_{i2}] &amp; \cdots &amp; n \widehat{Cov}[x_{i1}, x_{iK}] \\
n \bar{x}_3 &amp; \widehat{\Cov}[x_{i1}, x_{i2}] &amp; \widehat{\Var}[x_{i2}] &amp; \cdots &amp; n \widehat{\Cov}[x_{i2}, x_{iK}] \\
\vdots \\
n \bar{x}_K &amp; \widehat{\Cov}[x_{iK}, x_{i1}] &amp; \widehat{\Cov}[x_{iK}, x_{i2}] &amp; \cdots &amp; n \widehat{\Var}[x_{iK}]
\end{bmatrix}
\end{aligned}
\]</span></p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>This is technically the 2-norm, as there are other norms.<a href="ols-in-matrix-form.html#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression-anatomy.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="collinearity-and-multicollinearity.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/jrnold/intro-method-notes/edit/master/matrix.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
