[
["index.html", "Data Analysis Notes Chapter 1 Introduction", " Data Analysis Notes Jeffrey B. Arnold 2017-04-03 Chapter 1 Introduction Notes used when teaching “POLS/CS&amp;SS 501: Advanced Political Research Design and Analysis” and “POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology” at the University of Washington. \\[ \\] "],
["part-linear-models.html", "Chapter 2 (PART) Linear Models", " Chapter 2 (PART) Linear Models "],
["what-is-regression.html", "Chapter 3 What is Regression?", " Chapter 3 What is Regression? While this section will generally cover linear regression, it is not the only form of regression. So let’s start with a definition of regression. Most generally, a regression represents a function of a variable, \\(Y\\) as a function of another variable or variables, \\(X\\), and and error. \\[ g(Y_i) = f(X_i) + \\text{error}_i \\] The conditional expectation function (CEF) or regression function of \\(Y\\) given \\(X\\) is denoted, \\[ \\mu(x) = \\E\\left[Y | X = x\\right] \\] But if regression represents \\(Y\\) as a function of \\(X\\), what’s the alternative? Instead of modeling \\(Y\\) as a function of \\(X\\), we could jointly model both \\(Y\\) and \\(X\\). A regression model of \\(Y\\) and \\(X\\) would be multivariate function, \\(f(Y, X)\\). In machine learning these approaches are sometimes called descriminative (regression) and generative (joint models) models. "],
["bivariate-regression-model.html", "Chapter 4 Bivariate Regression Model", " Chapter 4 Bivariate Regression Model Given two vectors, \\(\\vec{y} = (y_1, y_2, ..., y_n)\\), and \\(\\vec{x} = (x_1, x_2, ..., x_n)\\), the regression line is, \\[ \\E(y | y) = \\hat{y_i} = \\hat\\beta_0 + \\hat{\\beta} x_i \\] where \\(\\hat{y_i}\\) is called the fitted value, and the residual is \\[ \\hat{\\epsilon}_i = y_i - \\hat{y}_i \\] The OLS estimate for \\(\\hat{\\vec{\\beta}}\\) is the values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) that minimize the sum of squared residuals of the regression line, \\[ \\hat{\\beta}_0, \\hat{\\beta}_1 = \\argmin_{b_0, b_1}\\sum_{i = 1}^n {( y_i - b_0 - b_1 x_i )}^2 = \\argmin_{b_0, b_1}\\sum_{i = 1}^n \\hat{\\epsilon}_i^2 \\] For OLS the solution to this minimization problem has a closed form solution[^closedform], \\[ \\begin{aligned} \\hat{\\beta}_0 &amp;= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\ \\hat{\\beta}_1 &amp;= \\frac{\\sum_{i = 1}^N (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum_{i = 1}^n (x_i - \\bar{x})^2} \\end{aligned} \\] Properties of OLS It goes through the mean, \\((\\hat{y}, \\hat{x})\\) The mean and sum of the errors is zero 4.0.1 OLS is the weighted sum of outcomes In OLS, the coefficients are weighted averages of the dependent variables, \\[ \\hat{\\beta}_1 = \\sum_{i = 1}^n \\frac{(x_i - \\bar{x})}{{(x_i - \\bar{x})}^2} \\] Technical note Linear regression is linear not because \\(y = b_0 + b_2\\), but because the predictions can be represented as weighted sums of outcome, \\(\\hat{y} = w_i y_i\\). If we were to estimate \\(\\hat{\\beta}_0\\) or \\(\\hat{\\beta}_1\\) with a different objective function, then it would not longer be linear. "],
["covariance-and-correlation.html", "Chapter 5 Covariance and Correlation", " Chapter 5 Covariance and Correlation Plot of covariance and correlation The population covariance for random variables \\(X\\) and \\(Y\\) is, \\[ \\Cov(X, Y) = \\E[(X - \\mu_x)(Y - \\mu_y)] \\] The sample covariance for vectors \\(\\vec{x}\\) and \\(\\vec{y}\\) is, \\[ \\Cov(x_i, y_i) = \\frac{1}{n} \\sum_{i = 1}^n (x_i - \\bar{x})(y_i - \\bar{y}) \\] Some properties of covariance are: \\(\\Cov(X, X) = \\Var(X, X)\\). Why? \\(\\Cov(X, Y)\\) has a domain of \\((-\\infty, \\infty)\\). What would a covariance of \\(-\\infty\\) look like? of \\(\\infty\\)? of 0? Like variance is defined on the scale of the squared variable (\\(X^2\\)), the covariance is defined on the scale of the product of the variables (\\(X Y\\)), which makes it difficult to interpret. However unlike taking the square root of the variables, in correlation is standardized to a domain of \\([-1, 1]\\). \\[ \\Cor(X, Y) = \\frac{\\E[(X - \\mu_x)(Y - \\mu_y)]}{\\sigma_x \\sigma_y} \\] \\[ \\Cor(x_i, y_i) = \\frac{\\sum_{i = 1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{s_x s_y} \\] "],
["anscombe-quartet.html", "Chapter 6 Anscombe quartet", " Chapter 6 Anscombe quartet anscombe_tidy &lt;- anscombe %&gt;% mutate(obs = row_number()) %&gt;% gather(variable_dataset, value, -obs) %&gt;% separate(variable_dataset, c(&quot;variable&quot;, &quot;dataset&quot;), sep = c(1)) %&gt;% spread(variable, value) %&gt;% arrange(dataset, obs) What are summary statistics of the four anscombe datasets? ggplot(anscombe_tidy, aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + facet_wrap(~ dataset, ncol = 2) What are the mean, standard deviation, correlation coefficient, and regression coefficients of each line? anscombe_summ &lt;- anscombe_tidy %&gt;% group_by(dataset) %&gt;% summarise( mean_x = mean(x), mean_y = mean(y), sd_x = sd(x), sd_y = sd(y), cov = cov(x, y), cor = cor(x, y), coefs = list(coef(lm(y ~ x, data = .))) ) %&gt;% mutate( intercept = map_dbl(coefs, &quot;(Intercept)&quot;), slope = map_dbl(coefs, &quot;x&quot;) ) %&gt;% select(-coefs) WUT? They are the same? But they look so different. Of course that was the point … Since this all revolves around covariance, lets calculate the values of \\(x_i - \\bar{x}\\), \\(y_i - \\bar{y}\\), and \\((x_i - \\bar{x}) (y_i - \\bar{y})\\) for each obs for each variable. anscombe_tidy &lt;- anscombe_tidy %&gt;% group_by(dataset) %&gt;% mutate(mean_x = mean(x), diff_mean_x = x - mean_x, mean_y = mean(y), diff_mean_y = y - mean_y, diff_mean_xy = diff_mean_x * diff_mean_y, quadrant = if_else( diff_mean_x &gt; 0, if_else(diff_mean_y &gt; 0, 1, 2), if_else(diff_mean_y &gt; 0, 4, 3), )) ggplot(anscombe_tidy, aes(x = x, y = y, size = abs(diff_mean_xy), colour = factor(sign(diff_mean_xy)))) + geom_point() + geom_hline(data = anscombe_summ, aes(yintercept = mean_y)) + geom_vline(data = anscombe_summ, aes(xintercept = mean_x)) + facet_wrap(~ dataset, ncol = 2) ggplot(anscombe_tidy, aes(x = x, y = y, colour = factor(sign(diff_mean_xy)))) + geom_point() + geom_segment(mapping = aes(xend = mean_x, yend = mean_y)) + geom_hline(data = anscombe_summ, aes(yintercept = mean_y)) + geom_vline(data = anscombe_summ, aes(xintercept = mean_x)) + facet_wrap(~ dataset, ncol = 2) ggplot(anscombe_tidy, aes(x = x, y = y, colour = factor(sign(diff_mean_xy)))) + geom_point() + geom_segment(mapping = aes(xend = x, yend = mean_y)) + geom_segment(mapping = aes(xend = mean_x, yend = y)) + geom_hline(data = anscombe_summ, aes(yintercept = mean_y)) + geom_vline(data = anscombe_summ, aes(xintercept = mean_x)) + facet_wrap(~ dataset, ncol = 2) "],
["marginal-effects.html", "Chapter 7 Marginal Effects 7.1 References", " Chapter 7 Marginal Effects Let’s start with what it that we want to calculate. We want to calculate the “marginal effect” of changing the \\(j\\)th predictors while holding other predictors constant. \\[ ME_j = E( y | x_j, x_{-j}) - E( y | x_j + \\Delta x_j, x_{-j}) \\] or for a small change, \\(\\Delta x_j \\to 0\\) in a continuous \\(x_j\\), this is the partial derivative, \\[ ME_j = \\frac{\\partial E( y | x_j, x_{-j})}{\\partial x_j} \\] Now consider the linear regression with two predictors for a change in \\(x_1\\), \\[ \\begin{aligned}[t] ME_j &amp;= E(y | x_1, \\tilde{x}_2) - E(y | x_1 + \\Delta x_1, \\tilde{x}_2) \\end{aligned} \\] Since the linear regression equation is \\(E(y | x)\\), this simplifies to \\[ \\begin{aligned}[t] ME_j &amp;= (\\beta_0 + \\beta_1 x_1 + \\tilde{x}_2) - (\\beta_0 + \\beta_1 (x_1 + \\Delta x_1) \\tilde{x}_2) \\\\ &amp;= \\beta_1 \\Delta x_1 \\end{aligned} \\] or with \\(\\Delta \\to 0\\), \\[ \\begin{aligned}[t] ME_j &amp;= \\frac{\\partial E(y | x_1, x_2)}{\\partial x_1} \\\\ \\frac{\\partial (\\beta_0 + \\beta_1 x_1 + \\tilde{x}_2)}{\\partial x_1} &amp;= \\beta_1 \\end{aligned} \\] Note that those equations were on the population level \\[ \\widehat{ME}_j = \\hat{\\beta}_1 \\] So, for a linear regression, the marginal effect of \\(x_j\\), defined as the change in the expected value of \\(y\\) for a small a unit of \\(j\\) The equation presented above is not causal, it is simply a function derived from the population or estimated equation. If population equation is not the as the linear regressionthen \\(\\hat{\\beta_j}\\) can still be viewed as an estimator of \\(ME_j\\). It is like the \\(ME_j\\) weighted by observations with the most variation in \\(x_j\\), after accounting for the parts of \\(x_j\\) and \\(y\\) predicted by the other predictors. See the discussion in Agresti and Pischke. For regressions other than OLS, the coefficients are not the \\(ME_j\\). It is a luxury that the coefficients happen to have a nice interpretation in OLS. In most other regressions, the coefficients are not directly useful. The researcher should estimate what is of interest. Even for OLS, if \\(x_j\\) is included as part of a function, e.g. a polynomial or an interaction, then its coefficient cannot be interpreted as the marginal effect. Suppose the regression equation is, \\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_2, \\] then the marginal effect of \\(x_1\\) is, \\[ \\begin{aligned}[t] ME_j &amp;= \\frac{\\partial E(y | x_1, x_2)}{\\partial x_1} \\\\ \\frac{\\partial (\\beta_0 + \\beta_1 x_1 + \\beta_1 x_1^2 + \\beta_3 \\tilde{x}_2)}{\\partial x_1} &amp;= \\beta_1 + 2 \\beta_2 x_1 \\end{aligned} \\] Note that the marginal effect of \\(x_1\\) is not, \\(\\beta_1\\). That would require a change in \\(x_1\\) while holding \\(x_1 ^ 2\\) constant, which is a logical impossibility. Instead, the marginal effect of \\(x_1\\) depends on the value of \\(x_2\\) at which it is evaluated. Similarly is there is an interaction between \\(x_1\\) and \\(x_2\\), \\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1 + \\beta_3 x_1 x_2 \\] then the marginal effect of \\(x_1\\) is, \\[ \\begin{aligned}[t] ME_j &amp;= \\frac{\\partial E(y | x_1, x_2)}{\\partial x_1} \\\\ \\frac{\\partial (\\beta_0 + \\beta_1 x_1 + \\beta_1 x_1^2 + \\beta_2 \\tilde{x}_2)}{\\partial x_1} &amp;= \\beta_1 + \\beta_3 x_2 \\end{aligned} \\] Now the marginal effect of \\(x_1\\) id a function of the value(s) of \\(x_2\\). For marginal effects that are functions of the data, there are multiple ways to calculate them. They include, AME: Average Marginal Effect. Average the marginal effects at each observed \\(x\\). MEM: Marginal Effect at the mean. Calculate the marginal effect with all observations at their means or other central values. MER: Marginal Effect at a representative value. Similar to MEM but with another meaningful value. Of these, the AME is the preferred one; MEM is an approximation. When it is discrete change in \\(x\\), it is called a partial effect (APE) or a first difference. The difference in the expected value of y, given a change in \\(x_j\\) from \\(x^*\\) to \\(x^* + \\Delta\\) is \\(\\beta_j \\Delta\\), and the standard error can be calculated analytically by https://en.wikipedia.org/wiki/Delta_method, \\[ se(\\hat{beta}_j \\Delta) = \\sqrt{\\Var\\hat{\\beta_j} \\Delta^2} = \\se\\hat{\\beta_j} \\Delta . \\] The Delta method can be used to analytically derive approximations of the standard errors for other nonlinear functions and interaction in regression, but it scales poorly, and it is often easier to use bootstrapping or software than calculate it by hand. See the margins package. 7.1 References Cameron, Trivedi, “Microeconomics Using Stata” Ch 10. Agresti and Pischke etc. "],
["multiple-testing.html", "Chapter 8 Multiple Testing 8.1 Setup 8.2 Multiple Testing 8.3 Data snooping", " Chapter 8 Multiple Testing 8.1 Setup library(&quot;tidyverse&quot;) library(&quot;broom&quot;) library(&quot;stringr&quot;) library(&quot;magrittr&quot;) 8.2 Multiple Testing What happens if we run multiple regressions? What do p-values mean in that context? Simulate data where \\(Y\\) and \\(X\\) are all simulated from i.i.d. standard normal distributions, \\(Y_i \\sim N(0, 1)\\) and \\(X_{i,j} \\sim N(0, 1)\\). This means that \\(Y\\) and \\(X\\) are not associated. sim_reg_nothing &lt;- function(n, k, sigma = 1, .id = NULL) { .data &lt;- rnorm(n * k, mean = 0, sd = 1) %&gt;% matrix(nrow = n, ncol = k) %&gt;% set_colnames(str_c(&quot;X&quot;, seq_len(k))) %&gt;% as_tibble() .data$y &lt;- rnorm(n, mean = 0, sd = 1) # Run first regression .formula1 &lt;- as.formula(str_c(&quot;y&quot;, &quot;~&quot;, str_c(&quot;X&quot;, seq_len(k), collapse = &quot;+&quot;))) mod &lt;- lm(.formula1, data = .data, model = FALSE) df &lt;- tidy(mod) df[[&quot;.id&quot;]] &lt;- .id df } Here is an example with of running one regression: n &lt;- 1000 k &lt;- 19 results_sim &lt;- sim_reg_nothing(n, k) How many coefficients are significant at the 5% level? alpha &lt;- 0.05 arrange(results_sim, p.value) %&gt;% select(term, estimate, statistic, p.value) %&gt;% head(n = 20) ## term estimate statistic p.value ## 1 (Intercept) -0.057671023 -1.76761794 0.07743594 ## 2 X11 -0.048028515 -1.50903711 0.13161162 ## 3 X14 0.048992567 1.48746690 0.13721321 ## 4 X16 0.046164679 1.41286413 0.15801318 ## 5 X10 -0.044591056 -1.39512286 0.16329487 ## 6 X6 -0.034591848 -1.10784039 0.26820258 ## 7 X17 0.033580690 1.06105001 0.28892859 ## 8 X3 0.027898979 0.85598092 0.39221757 ## 9 X7 0.026048269 0.79321403 0.42784513 ## 10 X1 -0.021855838 -0.68917119 0.49087868 ## 11 X2 -0.021550954 -0.64034712 0.52209664 ## 12 X18 -0.020022613 -0.63555967 0.52521184 ## 13 X13 0.016805579 0.52445681 0.60007946 ## 14 X8 -0.015532376 -0.48393610 0.62853934 ## 15 X4 0.013266040 0.41097637 0.68117971 ## 16 X19 0.010020257 0.31736246 0.75103619 ## 17 X12 -0.010034560 -0.31199476 0.75511087 ## 18 X9 -0.009682185 -0.29624723 0.76710405 ## 19 X15 0.009669889 0.29186436 0.77045210 ## 20 X5 -0.002942765 -0.09249518 0.92632353 Is this surprising? No. Since the null hypothesis is true for all coefficients (\\(\\beta_j = 0\\)), a \\(p\\)-value of 5% means that 5% of the tests will be false positives (Type I error). Let’s confirm that with a larger number of simulations and also use it to calculate some other values. Run 1,024 simulations and save the results to a data frame. number_sims &lt;- 1024 sims &lt;- map_df(seq_len(number_sims), function(i) { sim_reg_nothing(n, k, .id = i) }) Calculate the number significant at the 5% level in each regression. n_sig &lt;- sims %&gt;% group_by(.id) %&gt;% summarise(num_sig = sum(p.value &lt; alpha)) %&gt;% count(num_sig) %&gt;% ungroup() %&gt;% mutate(p = n / sum(n)) Overall, we expect 5% to be significant at the 5 percent level. sims %&gt;% summarise(num_sig = sum(p.value &lt; alpha), n = n()) %&gt;% ungroup() %&gt;% mutate(p = num_sig / n) ## num_sig n p ## 1 1015 20480 0.04956055 What about the distribution of statistically significant coefficients in each regression? ggplot(n_sig, aes(x = num_sig, y = p)) + geom_bar(stat = &quot;identity&quot;) + scale_x_continuous(&quot;Number of significant coefs&quot;, breaks = unique(n_sig$num_sig)) + labs(y = &quot;Pr(reg has k signif coef)&quot;) What’s the probability that a regression will have no significant coefficients, \\(1 - (1 - \\alpha) ^ {k - 1}\\), (1 - (1 - alpha) ^ (k + 1)) ## [1] 0.6415141 What’s the take-away? Don’t be too impressed by statistical significance when many tests are run. Note that multiple hypothesis tests occur both within papers … and within literatures. 8.3 Data snooping A not-uncommon practice is to run a regresion, filter out variables with “insignificant” coefficients, and then run and report a regression with only the smaller number of “significant” variables. Most explicitly, this occurs with stepwise regression, the problems of which are well known (when used for inference). However, this can even occur in cases where the hypotheses are not specified in advance and there is no explicit stepwise function used. To see the issues with this method, let’s consider the worst case scenario, when there is no relationship between \\(Y\\) and \\(X\\). Suppose \\(Y_i\\) is sampled from a i.i.d. standard normal distributions, \\(Y_i \\sim N(0, 1)\\). Suppose that the design matrix, \\(\\mat{X}\\), consists of 50 variables, each sampled from i.i.d. standard normal distributions, \\(X_{i,k} \\sim N(0, 1)\\) for \\(i \\in 1:100\\), \\(k \\in 1:50\\). Given this, the \\(R^2\\) for these regressions should be approximately 0.50. As shown in the previous section, it will not be uncommon to have several “statistically” significant coefficients at the 5 percent level. The sim_datasnoop function simulates data, and runs two regressions: Regress \\(Y\\) on \\(X\\) Keep all variables in \\(X\\) with \\(p &lt; .25\\). Regress \\(Y\\) on the subset of \\(X\\), keeping only those variables that were significant in step 2. sim_datasnoop &lt;- function(n = 100, k = 50, p = 0.10) { .data &lt;- rnorm(n * k, mean = 0, sd = 1) %&gt;% matrix(nrow = n, ncol = k) %&gt;% set_colnames(str_c(&quot;X&quot;, seq_len(k))) %&gt;% as_tibble() .data$y &lt;- rnorm(n, mean = 0, sd = 1) # Run first regression .formula1 &lt;- as.formula(str_c(&quot;y&quot;, &quot;~&quot;, str_c(&quot;X&quot;, seq_len(k), collapse = &quot;+&quot;))) mod1 &lt;- lm(.formula1, data = .data, model = FALSE) # Select model with only significant values (ignoring intercept) signif_x &lt;- tidy(mod1) %&gt;% filter(p.value &lt; p, term != &quot;(Intercept)&quot;) %&gt;% `[[`(&quot;term&quot;) if (length(signif_x &gt; 0)) { .formula2 &lt;- str_c(str_c(&quot;y&quot;, &quot;~&quot;, str_c(signif_x, collapse = &quot;+&quot;))) mod2 &lt;- lm(.formula2, data = .data, model = FALSE) } else { mod2 &lt;- NULL } tibble(mod1 = list(mod1), mod2 = list(mod2)) } Now repeat this simulation 1,024 times, calculate the \\(R^2\\) and number of statistically signifcant coefficients at \\(\\alpha = .05\\). n_sims &lt;- 1024 alpha &lt;- 0.05 sims &lt;- rerun(n_sims, sim_datasnoop()) %&gt;% bind_rows() %&gt;% mutate( r2_1 = map_dbl(mod1, ~ glance(.x)$r.squared), r2_2 = map_dbl(mod2, function(x) if (is.null(x)) NA_real_ else glance(x)$r.squared), pvalue_1 = map_dbl(mod1, ~ glance(.x)$p.value), pvalue_2 = map_dbl(mod2, function(x) if (is.null(x)) NA_real_ else glance(x)$p.value), sig_1 = map_dbl(mod1, ~ nrow(filter(tidy(.x), term != &quot;(Intercept)&quot;, p.value &lt; alpha))), sig_2 = map_dbl(mod2, function(x) { if (is.null(x)) NA_real_ else nrow(filter(tidy(x), term != &quot;(Intercept)&quot;, p.value &lt; alpha)) }) ) select(sims, r2_1, r2_2, pvalue_1, pvalue_2, sig_1, sig_2) %&gt;% summarise_all(funs(mean(., na.rm = TRUE))) ## # A tibble: 1 × 6 ## r2_1 r2_2 pvalue_1 pvalue_2 sig_1 sig_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.5036307 0.1622974 0.5070183 0.03709078 2.523438 2.51002 While the average \\(R\\) squared of the second stage regressions are less, the average \\(p\\)-values of the F-test that all coefficients are zero are much less. The number of statistically significant coefficients in the first and second regressions are approximately the same, which the second regression being slightly What happens if the number of obs, number of variables, and filtering significance level are adjusted? So why are the significance levels of the overall \\(F\\) test incorrect? For a p-value to be correct, it has to have the correct sampling distribution of the observed data. Even though in this simulation we are sampling the data in the first stage from a model that satisfies the assumptions of the F-test, the second stage does not account for the original filtering. This example is known as Freedman’s Paradox [@Freedman1983a]. "],
["many-models.html", "Chapter 9 Many Models 9.1 Prerequisites 9.2 Programming with Formulas 9.3 Programming with Formulas", " Chapter 9 Many Models These notes build off of the topics discussed in the chapter Many Models in R for Data Science. It uses functionals (map() function) for iteration, string functions, and list columns in data frames. 9.1 Prerequisites library(&quot;tidyverse&quot;) library(&quot;stringr&quot;) library(&quot;broom&quot;) 9.2 Programming with Formulas In these examples, we’ll use the car dataset in the car package. Prestige &lt;- car::Prestige Each observation is an occupation, and contains the prestige score of the occupation from a survey, and the average education, income, percentage of women, and type of occumpation. glimpse(Prestige) ## Observations: 102 ## Variables: 6 ## $ education &lt;dbl&gt; 13.11, 12.26, 12.77, 11.42, 14.62, 15.64, 15.09, 15.... ## $ income &lt;int&gt; 12351, 25879, 9271, 8865, 8403, 11030, 8258, 14163, ... ## $ women &lt;dbl&gt; 11.16, 4.02, 15.70, 9.11, 11.68, 5.13, 25.65, 2.69, ... ## $ prestige &lt;dbl&gt; 68.8, 69.1, 63.4, 56.8, 73.5, 77.6, 72.6, 78.1, 73.1... ## $ census &lt;int&gt; 1113, 1130, 1171, 1175, 2111, 2113, 2133, 2141, 2143... ## $ type &lt;fctr&gt; prof, prof, prof, prof, prof, prof, prof, prof, pro... We will run several regressions with prestige as the outcome variable, and the over variables are explanatory variables. 9.3 Programming with Formulas In R, the formulas are objects (of class &quot;formula&quot;). That means we can program on them, and importantly, perhaps avoid excessive copying and pasting if we run multiple models. A formula object is created with the ~ operator: f &lt;- prestige ~ type + education class(f) ## [1] &quot;formula&quot; f ## prestige ~ type + education A useful function for working with formulas is update. The update function allows you to easiy # the . is replaced by the original formula values update(f, . ~ income) ## prestige ~ income update(f, income ~ .) ## income ~ type + education update(f, . ~ . + type + women) ## prestige ~ type + education + women Also note that many types of models have update method which will rerun the model with a new formula. Sometimes this can help computational time if the model is able to reuse some previous results or data. You can also create formulae from a character vector as.formula(&quot;prestige ~ income + education&quot;) ## prestige ~ income + education This means that you can create model formulae programmatically which is useful if you are running many models, or simply to keep the logic of your code clear. xvars &lt;- c(&quot;type&quot;, &quot;income&quot;, &quot;education&quot;) as.formula(str_c(&quot;prestige&quot;, &quot;~&quot;, str_c(xvars, collapse = &quot; + &quot;))) ## prestige ~ type + income + education Often you will need to run multiple models. Since most often the only thing that changes between models is the formula (the outcome or response variables), storing the formula in a list, and then running the models by iterating through the list is a clean strategy for estimating your models. xvar_list &lt;- list(c(&quot;type&quot;), c(&quot;income&quot;), c(&quot;education&quot;), c(&quot;type&quot;, &quot;income&quot;), c(&quot;type&quot;, &quot;income&quot;, &quot;education&quot;)) formulae &lt;- vector(&quot;list&quot;, length(xvar_list)) for (i in seq_along(xvar_list)) { formulae[[i]] &lt;- str_c(&quot;prestige ~ &quot;, str_c(xvar_list[[i]], collapse = &quot; + &quot;)) } formulae ## [[1]] ## [1] &quot;prestige ~ type&quot; ## ## [[2]] ## [1] &quot;prestige ~ income&quot; ## ## [[3]] ## [1] &quot;prestige ~ education&quot; ## ## [[4]] ## [1] &quot;prestige ~ type + income&quot; ## ## [[5]] ## [1] &quot;prestige ~ type + income + education&quot; Alternatively, create this list of formulae with a functional, make_mod_f &lt;- function(x) { str_c(&quot;prestige ~ &quot;, str_c(x, collapse = &quot; + &quot;)) } formulae &lt;- map(xvar_list, make_mod_f) Now that we have the various model formulae we want to run, we can Run a single model that returns a data frame with a single row and column: mod: a list column with lm object with the fitted model. I set model = FALSE because by default an lm model stores the data used to estimte it. This is convenient, but if you are estimating many models, it can start taking up space. run_reg &lt;- function(f) { mod &lt;- lm(f, data = Prestige, model = FALSE) data_frame(mod = list(mod)) } ret &lt;- run_reg(formulae[[1]]) ret[[&quot;mod&quot;]][[1]] ## ## Call: ## lm(formula = f, data = Prestige, model = FALSE) ## ## Coefficients: ## (Intercept) typeprof typewc ## 35.527 32.321 6.716 It doesn’t make much sense to store that as a data frame on its own, but with multiple inputs it will be useful. Now, run run_reg for each formula in formulae using map_df to return the results as a data frame with a list column, mod, containing the lm objects. prestige_fits &lt;- map_df(formulae, run_reg, .id = &quot;.id&quot;) prestige_fits ## # A tibble: 5 × 2 ## .id mod ## &lt;chr&gt; &lt;list&gt; ## 1 1 &lt;S3: lm&gt; ## 2 2 &lt;S3: lm&gt; ## 3 3 &lt;S3: lm&gt; ## 4 4 &lt;S3: lm&gt; ## 5 5 &lt;S3: lm&gt; From here, it is easy to extract parts of the models that. To extract the original formulas and add them to the data set, run formula() on each lm object using map, and then convert it to a character string using deparse: prestige_fits &lt;- prestige_fits %&gt;% mutate(formula = map_chr(mod, ~ deparse(formula(.x)))) prestige_fits$formula ## [1] &quot;prestige ~ type&quot; ## [2] &quot;prestige ~ income&quot; ## [3] &quot;prestige ~ education&quot; ## [4] &quot;prestige ~ type + income&quot; ## [5] &quot;prestige ~ type + income + education&quot; Get a data frame of the coefficients for all models using tidy and unnest: mutate(prestige_fits, x = map(mod, tidy)) %&gt;% unnest(x) ## # A tibble: 16 × 7 ## .id formula term estimate ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 prestige ~ type (Intercept) 35.527272727 ## 2 1 prestige ~ type typeprof 32.321114370 ## 3 1 prestige ~ type typewc 6.716205534 ## 4 2 prestige ~ income (Intercept) 27.141176368 ## 5 2 prestige ~ income income 0.002896799 ## 6 3 prestige ~ education (Intercept) -10.731981968 ## 7 3 prestige ~ education education 5.360877731 ## 8 4 prestige ~ type + income (Intercept) 27.997056941 ## 9 4 prestige ~ type + income typeprof 25.055473883 ## 10 4 prestige ~ type + income typewc 7.167155112 ## 11 4 prestige ~ type + income income 0.001401196 ## 12 5 prestige ~ type + income + education (Intercept) -0.622929165 ## 13 5 prestige ~ type + income + education typeprof 6.038970651 ## 14 5 prestige ~ type + income + education typewc -2.737230718 ## 15 5 prestige ~ type + income + education income 0.001013193 ## 16 5 prestige ~ type + income + education education 3.673166052 ## # ... with 3 more variables: std.error &lt;dbl&gt;, statistic &lt;dbl&gt;, ## # p.value &lt;dbl&gt; Get a data frame of model summary statistics for all models using glance, mutate(prestige_fits, x = map(mod, glance)) %&gt;% unnest(x) ## # A tibble: 5 × 14 ## .id mod formula r.squared ## &lt;chr&gt; &lt;list&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 &lt;S3: lm&gt; prestige ~ type 0.6976287 ## 2 2 &lt;S3: lm&gt; prestige ~ income 0.5110901 ## 3 3 &lt;S3: lm&gt; prestige ~ education 0.7228007 ## 4 4 &lt;S3: lm&gt; prestige ~ type + income 0.7764569 ## 5 5 &lt;S3: lm&gt; prestige ~ type + income + education 0.8348574 ## # ... with 10 more variables: adj.r.squared &lt;dbl&gt;, sigma &lt;dbl&gt;, ## # statistic &lt;dbl&gt;, p.value &lt;dbl&gt;, df &lt;int&gt;, logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, ## # BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt; "],
["coming-to-r-from-other-languages.html", "Chapter 10 Coming to R from other Languages 10.1 Stata", " Chapter 10 Coming to R from other Languages 10.1 Stata There are not as many of these as I would have expected. These are the only useful ones that I found. Matthieu Gomez “R for Stata Users”. http://www.princeton.edu/~mattg/statar/ EconometricsBySimulation. “Dictionary: Stata to R” https://github.com/EconometricsBySimulation/RStata/wiki/Dictionary:-Stata-to-R DataCamp course *R for SAS, SPSS and Stata Users“. https://www.datacamp.com/courses/r-for-sas-spss-and-stata-users-r-tutorial Muenchen, Robert A. and Joseph Hilbe. 2010. R for Stata Users http://link.springer.com/book/10.1007%2F978-1-4419-1318-0. Useful, although R has changed a surprisingly large amount in the last four years. For example, this does not include RStudio. Note that if you’re unaware, almost all Springer books are available through the library Oscar Torres-Reyna, “Getting Started in R~Stata Notes on Exploring Data” http://www.princeton.edu/~otorres/RStata.pdf. "],
["examples-part.html", "Chapter 11 Examples (PART)", " Chapter 11 Examples (PART) "],
["duncan-occupational-prestige.html", "Chapter 12 Duncan Occupational Prestige 12.1 Setup 12.2 Coefficients, Standard errors 12.3 Residuals, Fitted Values, 12.4 Broom 12.5 Plotting Fitted Regression Results", " Chapter 12 Duncan Occupational Prestige 12.1 Setup library(&quot;tidyverse&quot;) library(&quot;broom&quot;) This example makes use of the Duncan Occpuational prestige included in the car package. This is data from a classic sociology paper and contains data on the prestige and other characteristics of 45 U.S. occupations in 1950. data(&quot;Duncan&quot;, package = &quot;car&quot;) The dataset Duncan contains four variables: type, income, education, and prestige, glimpse(Duncan) ## Observations: 45 ## Variables: 4 ## $ type &lt;fctr&gt; prof, prof, prof, prof, prof, prof, prof, prof, wc,... ## $ income &lt;int&gt; 62, 72, 75, 55, 64, 21, 64, 80, 67, 72, 42, 76, 76, ... ## $ education &lt;int&gt; 86, 76, 92, 90, 86, 84, 93, 100, 87, 86, 74, 98, 97,... ## $ prestige &lt;int&gt; 82, 83, 90, 76, 90, 87, 93, 90, 52, 88, 57, 89, 97, ... You run a regression in R using the function lm. This runs a linear regression of occupational prestige on income, lm(prestige ~ income, data = Duncan) ## ## Call: ## lm(formula = prestige ~ income, data = Duncan) ## ## Coefficients: ## (Intercept) income ## 2.457 1.080 This estimates the linear regression \\[ \\mathtt{prestige} = \\beta_0 + \\beta_1 \\mathtt{income} \\] In R, \\(\\beta_0\\) is named (Intercept), and the other coefficients are named after the associated predictor. The function lm returns an lm object that can be used in future computations. Instead of printing the regression result to the screen, save it to the variable mod1, mod1 &lt;- lm(prestige ~ income, data = Duncan) We can print this object print(mod1) ## ## Call: ## lm(formula = prestige ~ income, data = Duncan) ## ## Coefficients: ## (Intercept) income ## 2.457 1.080 Somewhat counterintuitively, the summary function returns more information about a regression, summary(mod1) ## ## Call: ## lm(formula = prestige ~ income, data = Duncan) ## ## Residuals: ## Min 1Q Median 3Q Max ## -46.566 -9.421 0.257 9.167 61.855 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.4566 5.1901 0.473 0.638 ## income 1.0804 0.1074 10.062 7.14e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.4 on 43 degrees of freedom ## Multiple R-squared: 0.7019, Adjusted R-squared: 0.695 ## F-statistic: 101.3 on 1 and 43 DF, p-value: 7.144e-13 The summary function also returns an object that we can use later, summary_mod1 &lt;- summary(mod1) summary_mod1 ## ## Call: ## lm(formula = prestige ~ income, data = Duncan) ## ## Residuals: ## Min 1Q Median 3Q Max ## -46.566 -9.421 0.257 9.167 61.855 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.4566 5.1901 0.473 0.638 ## income 1.0804 0.1074 10.062 7.14e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.4 on 43 degrees of freedom ## Multiple R-squared: 0.7019, Adjusted R-squared: 0.695 ## F-statistic: 101.3 on 1 and 43 DF, p-value: 7.144e-13 Now lets estimate a multiple linear regression, mod2 &lt;- lm(prestige ~ income + education + type, data = Duncan) mod2 ## ## Call: ## lm(formula = prestige ~ income + education + type, data = Duncan) ## ## Coefficients: ## (Intercept) income education typeprof typewc ## -0.1850 0.5975 0.3453 16.6575 -14.6611 12.2 Coefficients, Standard errors Coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\): coef(mod2) ## (Intercept) income education typeprof typewc ## -0.1850278 0.5975465 0.3453193 16.6575134 -14.6611334 Variance-covariance matrix of the coefficients, \\(\\Var{\\hat{\\boldsymbol{\\beta}}}\\): vcov(mod2) ## (Intercept) income education typeprof typewc ## (Intercept) 13.7920916 -0.115636760 -0.257485549 14.0946963 7.9021988 ## income -0.1156368 0.007984369 -0.002924489 -0.1260105 -0.1090485 ## education -0.2574855 -0.002924489 0.012906986 -0.6166508 -0.3881200 ## typeprof 14.0946963 -0.126010517 -0.616650831 48.9021401 30.2138627 ## typewc 7.9021988 -0.109048528 -0.388119979 30.2138627 37.3171167 The standard errors of the coefficients, \\(\\se{\\hat{\\boldsymbol{\\beta}}}\\), are the square root diagonal of the vcov matrix, sqrt(diag(vcov(mod2))) ## (Intercept) income education typeprof typewc ## 3.7137705 0.0893553 0.1136089 6.9930065 6.1087737 This can be confirmed by comparing their values to those in the summary table, summary(mod2) ## ## Call: ## lm(formula = prestige ~ income + education + type, data = Duncan) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.890 -5.740 -1.754 5.442 28.972 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.18503 3.71377 -0.050 0.96051 ## income 0.59755 0.08936 6.687 5.12e-08 *** ## education 0.34532 0.11361 3.040 0.00416 ** ## typeprof 16.65751 6.99301 2.382 0.02206 * ## typewc -14.66113 6.10877 -2.400 0.02114 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.744 on 40 degrees of freedom ## Multiple R-squared: 0.9131, Adjusted R-squared: 0.9044 ## F-statistic: 105 on 4 and 40 DF, p-value: &lt; 2.2e-16 12.3 Residuals, Fitted Values, To get the fitted or predicted values (\\(\\hat{\\mathbf{y}} = \\mathbf{X} \\hat{\\boldsymbol\\beta}\\)) from a regression, mod1_fitted &lt;- fitted(mod1) head(mod1_fitted) ## accountant pilot architect author chemist minister ## 69.44073 80.24463 83.48580 61.87801 71.60151 25.14476 or mod1_predict &lt;- predict(mod1) head(mod1_predict) ## accountant pilot architect author chemist minister ## 69.44073 80.24463 83.48580 61.87801 71.60151 25.14476 The difference between predict and fitted is how they handle missing values in the data. Fitted values will not include predictions for missing values in the data, while predict will include values for Using predict, we can also predict values for new data. For example, create a data frame with each category of type, and in which income and education are set to their mean values. Duncan_at_means &lt;- data.frame(type = unique(Duncan$type), income = mean(Duncan$income), education = mean(Duncan$education)) Duncan_at_means ## type income education ## 1 prof 41.86667 52.55556 ## 2 wc 41.86667 52.55556 ## 3 bc 41.86667 52.55556 Now use this with the newdata argument, predict(mod2, newdata = Duncan_at_means) ## 1 2 3 ## 59.63821 28.31957 42.98070 To get the residuals (\\(\\hat{\\boldsymbol{\\epsilon}} = \\mathbf{y} - \\hat{\\mathbf{y}}\\)). mod1_resid &lt;- residuals(mod1) head(mod1_resid) ## accountant pilot architect author chemist minister ## 12.559266 2.755369 6.514200 14.121993 18.398486 61.855242 12.4 Broom The package broom has some functions that reformat the results of statistical modeling functions (t.test, lm, etc.) to data frames that work nicer with ggplot2, dplyr, and friends. The broom package has three main functions: glance: Information about the model. tidy: Information about the estimated parameters augment: The original data with estimates of the model. glance: Always return a one-row data.frame that is a summary of the model: e.g. R2, adjusted R2, etc. glance(mod2) ## r.squared adj.r.squared sigma statistic p.value df logLik ## 1 0.9130657 0.9043723 9.744171 105.0294 1.170871e-20 5 -163.6522 ## AIC BIC deviance df.residual ## 1 339.3045 350.1444 3797.955 40 tidy: Transforms into a ready-to-go data.frame the coefficients, SEs (and CIs if given), critical values, and p-values in statistical tests’ outputs tidy(mod2) ## term estimate std.error statistic p.value ## 1 (Intercept) -0.1850278 3.7137705 -0.0498221 9.605121e-01 ## 2 income 0.5975465 0.0893553 6.6873093 5.123720e-08 ## 3 education 0.3453193 0.1136089 3.0395443 4.164463e-03 ## 4 typeprof 16.6575134 6.9930065 2.3820246 2.206245e-02 ## 5 typewc -14.6611334 6.1087737 -2.4000125 2.114015e-02 augment: Add columns to the original data that was modeled. This includes predictions, estandard error of the predictions, residuals, and others. augment(mod2) %&gt;% head() ## .rownames prestige income education type .fitted .se.fit .resid ## 1 accountant 82 62 86 prof 83.21783 2.352262 -1.217831 ## 2 pilot 83 72 76 prof 85.74010 2.674659 -2.740102 ## 3 architect 90 75 92 prof 93.05785 2.755775 -3.057851 ## 4 author 76 55 90 prof 80.41628 2.589351 -4.416282 ## 5 chemist 90 64 86 prof 84.41292 2.360632 5.587076 ## 6 minister 87 21 84 prof 58.02779 4.260837 28.972214 ## .hat .sigma .cooksd .std.resid ## 1 0.05827491 9.866259 0.0002052803 -0.1287893 ## 2 0.07534370 9.857751 0.0013936701 -0.2924366 ## 3 0.07998300 9.855093 0.0018611391 -0.3271700 ## 4 0.07061418 9.841004 0.0033585648 -0.4701256 ## 5 0.05869037 9.825129 0.0043552315 0.5909809 ## 6 0.19120532 8.412639 0.5168053288 3.3061127 .fitted: the model predictions for all observations .se.fit: the estandard error of the predictions .resid: the residuals of the predictions (acual - predicted values) .sigma: is the standard error of the prediction. The other columns—.hat, .cooksd, and .std.resid are used in regression diagnostics. 12.5 Plotting Fitted Regression Results Consider the regression of prestige on income, mod3 &lt;- lm(prestige ~ income, data = Duncan) This creates a new dataset with the column income and 100 observations between the min and maximum observed incomes in the Duncan dataset. mod3_newdata &lt;- data_frame(income = seq(min(Duncan$income), max(Duncan$income), length.out = 100)) We will calculate fitted values for all these values of income. ggplot() + geom_point(data = Duncan, mapping = aes(x = income, y = prestige), colour = &quot;gray75&quot;) + geom_line(data = augment(mod3, newdata = mod3_newdata), mapping = aes(x = income, y = .fitted)) + ylab(&quot;Prestige&quot;) + xlab(&quot;Income&quot;) + theme_minimal() Now plot something similar, but for a regression with income interacted with type, mod4 &lt;- lm(prestige ~ income * type, data = Duncan) We want to create a dataset which has, (1) each value of type in the Duncan data, and (2) values spanning the range of income in the Duncan data. The function expand.grid creates a data frame with all combinations of vectors given to it (Cartesian product). mod4_newdata &lt;- expand.grid(income = seq(min(Duncan$income), max(Duncan$income), length.out = 100), type = unique(Duncan$type)) Now plot the fitted values evaluated at each of these values along wite original values in the data, ggplot() + geom_point(data = Duncan, mapping = aes(x = income, y = prestige, color = type)) + geom_line(data = augment(mod4, newdata = mod4_newdata), mapping = aes(x = income, y = .fitted, color = type)) + ylab(&quot;Prestige&quot;) + xlab(&quot;Income&quot;) + theme_minimal() Running geom_smooth with method = &quot;lm&quot; gives similar results. However, note that geom_smooth with run a separate regression for each group. ggplot(data = Duncan, aes(x = income, y = prestige, color = type)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + ylab(&quot;Prestige&quot;) + xlab(&quot;Income&quot;) + theme_minimal() "],
["yules-pauperism-data.html", "Chapter 13 Yule’s Pauperism Data 13.1 Setup 13.2 Examples", " Chapter 13 Yule’s Pauperism Data Example from @Yule1897a,@Yule1896b,@Yule1896a,@Plewis2017a. See @Stigler2016a,@Stigler1990a 13.1 Setup library(&quot;tidyverse&quot;) 13.2 Examples datums::pauperism_year %&gt;% filter(year == 1871) %&gt;% select(outratio, pauper2) %&gt;% drop_na() %&gt;% ggplot(aes(x = outratio, pauper2)) + geom_point() + geom_smooth(method = &quot;lm&quot;) datums::pauperism_year %&gt;% filter(year == 1871) %&gt;% select(outratio, pauper2) %&gt;% drop_na() %&gt;% cor() ## outratio pauper2 ## outratio 1.0000000 0.2105665 ## pauper2 0.2105665 1.0000000 Regressions for each year datums::pauperism_year %&gt;% group_by(year) %&gt;% summarise(mod = list(lm(outratio ~ pauper2, data = .))) ## # A tibble: 3 × 2 ## year mod ## &lt;int&gt; &lt;list&gt; ## 1 1871 &lt;S3: lm&gt; ## 2 1881 &lt;S3: lm&gt; ## 3 1891 &lt;S3: lm&gt; datums::pauperism_year %&gt;% filter(year == 1871) %&gt;% select(outratio, pauper2) %&gt;% drop_na() %&gt;% lm(pauper2 ~ outratio, data = .) ## ## Call: ## lm(formula = pauper2 ~ outratio, data = .) ## ## Coefficients: ## (Intercept) outratio ## 0.043637 0.001218 "],
["formatting-tables.html", "Chapter 14 Formatting Tables 14.1 Overview of Packages 14.2 Summary Statistic Table Example 14.3 Regression Table Example", " Chapter 14 Formatting Tables 14.1 Overview of Packages R has multiple packages and functions for directly producing formatted tables for LaTeX, HTML, and other output formats. Given the See the Reproducible Research Task View for an overview of various options. xtable is a general purpose package for creating LaTeX, HTML, or plain text tables in R. texreg is more specifically geared to regression tables. It also outputs results in LaTeX (texreg), HTML (texreg), and plain text. The packages stargazer and apsrtable are other popular packages for formatting regression output. However, they are less-well maintained and have less functionality than texreg. For example, apsrtable hasn’t been updated since 2012, stargazer since 2015. The texreg vignette is a good introduction to texreg, and also discusses the These blog posts by Will Lowe cover many of the options. Additionally, for simple tables, knitr, the package which provides the heavy lifting for R markdown, has a function knitr. knitr also has the ability to customize how R objects are printed with the knit_print function. Other notable packages are: pander creates output in markdown for export to other formats. tables uses a formula syntax to define tables ReportR has the most complete support for creating Word documents, but is likely too much. For a political science perspective on why automating the research process is important see: Nicholas Eubank Embrace Your Fallibility: Thoughts on Code Integrity, based on this article Matthew Gentzkow Jesse M. Shapiro.Code and Data for the Social Sciences: A Practitioner’s Guide. March 10, 2014. Political Methodologist issue on Workflow Management 14.2 Summary Statistic Table Example The xtable package has methods to convert many types of R objects to tables. library(&quot;gapminder&quot;) gapminder_summary &lt;- gapminder %&gt;% # Keep numeric variables select_if(is.numeric) %&gt;% # gather variables gather(variable, value) %&gt;% # Summarize by variable group_by(variable) %&gt;% # summarise all columns summarise(n = sum(!is.na(value)), `Mean` = mean(value), `Std. Dev.` = sd(value), `Median` = median(value), `Min.` = min(value), `Max.` = max(value)) gapminder_summary ## # A tibble: 4 × 7 ## variable n Mean `Std. Dev.` Median Min. ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 gdpPercap 1704 7.215327e+03 9.857455e+03 3531.8470 241.1659 ## 2 lifeExp 1704 5.947444e+01 1.291711e+01 60.7125 23.5990 ## 3 pop 1704 2.960121e+07 1.061579e+08 7023595.5000 60011.0000 ## 4 year 1704 1.979500e+03 1.726533e+01 1979.5000 1952.0000 ## # ... with 1 more variables: Max. &lt;dbl&gt; Now that we have a data frame with the table we want, use xtable to create it: library(&quot;xtable&quot;) foo &lt;- xtable(gapminder_summary, digits = 0) %&gt;% print(type = &quot;html&quot;, html.table.attributes = &quot;&quot;, include.rownames = FALSE, format.args = list(big.mark = &quot;,&quot;)) variable n Mean Std. Dev. Median Min. Max. gdpPercap 1,704 7,215 9,857 3,532 241 113,523 lifeExp 1,704 59 13 61 24 83 pop 1,704 29,601,212 106,157,897 7,023,596 60,011 1,318,683,096 year 1,704 1,980 17 1,980 1,952 2,007 Note that there we two functions to get HTML. The function xtable creates an xtable R object, and the function xtable (called as print()), which prints the xtable object as HTML (or LaTeX). The default HTML does not look nice, and would need to be formatted with CSS. If you are copy and pasting it into Word, you would do some post-processing cleanup anyways. Another alternative is the knitr function in the knitr package, which outputs R markdown tables. knitr::kable(gapminder_summary) variable n Mean Std. Dev. Median Min. Max. gdpPercap 1704 7.215327e+03 9.857455e+03 3531.8470 241.1659 1.135231e+05 lifeExp 1704 5.947444e+01 1.291711e+01 60.7125 23.5990 8.260300e+01 pop 1704 2.960121e+07 1.061579e+08 7023595.5000 60011.0000 1.318683e+09 year 1704 1.979500e+03 1.726533e+01 1979.5000 1952.0000 2.007000e+03 This is useful for producing quick tables. Finally, htmlTables package unsurprisingly produces HTML tables. library(&quot;htmlTable&quot;) htmlTable(txtRound(gapminder_summary, 0), align = &quot;lrrrr&quot;) variable n Mean Std. Dev. Median Min. Max. 1 gdpPercap 1704 7 10 3532 241 1 2 lifeExp 1704 6 1 61 24 8 3 pop 1704 3 1 7023596 60011 1 4 year 1704 2 2 1980 1952 2 It has more features for producing HTML tables than xtable, but does not output LaTeX. 14.3 Regression Table Example library(&quot;tidyverse&quot;) library(&quot;texreg&quot;) We will run several regression models with the Duncan data Prestige &lt;- car::Prestige Since I’m running several regressions, I will save them to a list. If you know that you will be creating multiple objects, and programming with them, always put them in a list. First, create a list of the regression formulas, formulae &lt;- list( prestige ~ type, prestige ~ income, prestige ~ education, prestige ~ type + education + income ) Write a function to run a single model, Now use map to run a regression with each of these formulae, and save them to a list, prestige_mods &lt;- map(formulae, ~ lm(.x, data = Prestige, model = FALSE)) This is a list of lm objects, map(prestige_mods, class) ## [[1]] ## [1] &quot;lm&quot; ## ## [[2]] ## [1] &quot;lm&quot; ## ## [[3]] ## [1] &quot;lm&quot; ## ## [[4]] ## [1] &quot;lm&quot; We can look at the first model, prestige_mods[[1]] ## ## Call: ## lm(formula = .x, data = Prestige, model = FALSE) ## ## Coefficients: ## (Intercept) typeprof typewc ## 35.527 32.321 6.716 Now we can format the regression table in HTML using htmlreg. The first argument of htmlreg is a list of models: htmlreg(prestige_mods) Statistical models Model 1 Model 2 Model 3 Model 4 (Intercept) 35.53*** 27.14*** -10.73** -0.62 (1.43) (2.27) (3.68) (5.23) typeprof 32.32*** 6.04 (2.23) (3.87) typewc 6.72** -2.74 (2.44) (2.51) income 0.00*** 0.00*** (0.00) (0.00) education 5.36*** 3.67*** (0.33) (0.64) R2 0.70 0.51 0.72 0.83 Adj. R2 0.69 0.51 0.72 0.83 Num. obs. 98 102 102 98 RMSE 9.50 12.09 9.10 7.09 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 By default, htmlreg() prints out HTML, which is exactly what I want in an R markdown document. To save the output to a file, specify a non-null file argument. For example, to save the table to the file prestige.html, htmlreg(prestige_mods, file = &quot;prestige.html&quot;) Since this function outputs HTML directly to the console, it can be hard to tell what’s going on. If you want to preview the table in RStudio while working on it, this snippet of code uses htmltools package to do so: library(&quot;htmltools&quot;) htmlreg(prestige_mods) %&gt;% HTML() %&gt;% browsable() The htmlreg function has many options to adjust the table formatting. Below, I clean up the table. I remove stars using stars = NULL. It is a growing convention to avoid the use of stars indicating significance in regression tables (see AJPS and Political Analysis guidelines). The arguments doctype, html.tag, head.tag, body.tag control what sort of HTML is created. Generally all these functions (whether LaTeX or HTML output) have some arguments that determine whether it is creating a standalone, complete document, or a fragment that will be copied into another dcoument. The arguments include.rsquared, include.adjrs, and include.nobs are passed to the function extract() which determines what information the texreg package extracts from a model to put into the table. I get rid of \\(R^2\\), but keep adjusted \\(R^2\\), and the number of observations. library(&quot;stringr&quot;) coefnames &lt;- c(&quot;Intercept&quot;, &quot;Professional&quot;, &quot;Working Class&quot;, &quot;Income&quot;, &quot;Education&quot;) note &lt;- &quot;OLS regressions with prestige as the response variable.&quot; htmlreg(prestige_mods, stars = NULL, custom.model.names = str_c(&quot;(&quot;, seq_along(prestige_mods), &quot;)&quot;), custom.coef.names = coefnames, custom.note = str_c(&quot;Note: &quot;, note), omit.coef = &quot;(Intercept)&quot;, caption.above = TRUE, caption = &quot;Regressions of Occupational Prestige&quot;, # better for markdown doctype = FALSE, html.tag = FALSE, head.tag = FALSE, body.tag = FALSE, # passed to extract() method for &quot;lm&quot; include.adjr = TRUE, include.rsquared = FALSE, include.rmse = FALSE, include.nobs = TRUE) Regressions of Occupational Prestige (1) (2) (3) (4) Professional 32.32 6.04 (2.23) (3.87) Working Class 6.72 -2.74 (2.44) (2.51) Income 0.00 0.00 (0.00) (0.00) Education 5.36 3.67 (0.33) (0.64) Adj. R2 0.69 0.51 0.72 0.83 Num. obs. 98 102 102 98 Note: OLS regressions with prestige as the response variable. Once you find a set of options that are common across your tables, make a function so you con’t need to retype them. my_reg_table &lt;- function(mods, ..., note = NULL) { htmlreg(mods, stars = NULL, custom.note = if (!is.null(note)) str_c(&quot;Note: &quot;, note) else NULL, caption.above = TRUE, # better for markdown doctype = FALSE, html.tag = FALSE, head.tag = FALSE) } my_reg_table(prestige_mods, custom.model.names = str_c(&quot;(&quot;, seq_along(prestige_mods), &quot;)&quot;), custom.coef.names = coefnames, note = note, # put intercept at the bottom reorder.coef = c(2, 3, 4, 5, 1), caption = &quot;Regressions of Occupational Prestige&quot;) Statistical models Model 1 Model 2 Model 3 Model 4 (Intercept) 35.53 27.14 -10.73 -0.62 (1.43) (2.27) (3.68) (5.23) typeprof 32.32 6.04 (2.23) (3.87) typewc 6.72 -2.74 (2.44) (2.51) income 0.00 0.00 (0.00) (0.00) education 5.36 3.67 (0.33) (0.64) R2 0.70 0.51 0.72 0.83 Adj. R2 0.69 0.51 0.72 0.83 Num. obs. 98 102 102 98 RMSE 9.50 12.09 9.10 7.09 Note: OLS regressions with prestige as the response variable. Note that I didn’t include every option in my_reg_table, only those arguments that will be common across tables. I use ... to pass arguments to htmlreg. Then when I call my_reg_table the only arguments are those specific to the content of the table, not the formatting, making it easier to understand what each table is saying. Of course, texreg also produces LaTeX output, with the function texreg. Almost all the options are the same as htmlreg. "],
["reproducible-research.html", "Chapter 15 Reproducible Research", " Chapter 15 Reproducible Research "],
["writing-resources.html", "Chapter 16 Writing Resources 16.1 Writing and Organizing Papers 16.2 Finding Research Ideas 16.3 Replications", " Chapter 16 Writing Resources 16.1 Writing and Organizing Papers Here are a few useful resources for writing papers: Chris Adolph. Writing Empirical Papers: 6 Rules &amp; 12 Recommendations Barry R. Weingast. 2015. Caltech Rules for Writing Papers: How to Structure Your Paper and Write an Introduction The Science of Scientific Writing American Scientist Deidre McCloskey. Economical Writing William Thompson. A Guide for the Young Economist. “Chapter 2: Writing Papers.” Stephen Van Evera. Guide to Methods for Students of Political Science. Appendix. Joseph M. Williams and Joseph Bizup. Style: Lessons in Clarity and Grace Strunk and White. The Elements of Style Chicago Manual of Style and APSA Style Manual for Political Science for editorial and style issues. How to construct a Nature summary paragraph. Though specifi to Nature is good advice for structuring abstracts or introductions. Ezra Klein. How researchers are terrible communications, and how they can do better. The advice in the AJPS Instructions for Submitting Authors is a concise description of how to write an abstract: The abstract should provide a very concise descriptive summary of the research stream to which the manuscript contributes, the specific research topic it addresses, the research strategy employed for the analysis, the results obtained from the analysis, and the implications of the findings. Concrete Advice for Writing Informative Abstracts and pHow to Carefully Choose Useless Titles for Academic Writing](http://www.socialsciencespace.com/2014/03/how-to-carefully-choose-useless-titles-for-academic-writing/) 16.2 Finding Research Ideas Paul Krugman How I Work Hal Varian. How to build an Economic Model in your spare time Greg Mankiw, My Rules of Thumb: links in Advice for Grad Students 16.3 Replications Gary King has advice on how to turn a replication into a publishable paper: Gary King How to Write a Publishable Paper as a Class Project Gary King. 2006. “Publication, Publication.” PS: Political Science and Politics. Political Science Should Not Stop Young Researchers from Replicating from the Political Science Replication blog. And see the examples of students replications from his Harvard course at https://politicalsciencereplication.wordpress.com/ Famous replications. David Broockman, Joahua Kalla, and Peter Aronow. 2015. Irregularities in LaCour (2014). Homas Herndon, Michael Ash &amp; Robert Pollin (2013). Does High Public Debt Consistently Stifle Economic Growth? A Critique of Reinhart and Rogoff. Working Paper Series 322. Political Economy Research Institute. [URL] However, although those replications are famous for finding fraud or obvious errors in the analysis, replications can lead to extensions and generate new ideas. This was the intent of Brookman, Kalla, and Aronow when starting the replication. "]
]
