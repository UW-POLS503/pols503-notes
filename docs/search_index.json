[
["index.html", "Data Analysis Notes Chapter 1 Introduction", " Data Analysis Notes Jeffrey B. Arnold 2017-04-21 Chapter 1 Introduction Notes used when teaching “POLS/CS&amp;SS 501: Advanced Political Research Design and Analysis” and “POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology” at the University of Washington. \\[ \\] "],
["bivariate-ols.html", "Chapter 2 Bivariate OLS", " Chapter 2 Bivariate OLS library(&quot;tidyverse&quot;) library(&quot;broom&quot;) library(&quot;modelr&quot;) ## ## Attaching package: &#39;modelr&#39; ## The following object is masked from &#39;package:broom&#39;: ## ## bootstrap Given two vectors, \\(\\vec{y} = (y_1, y_2, ..., y_n)\\), and \\(\\vec{x} = (x_1, x_2, ..., x_n)\\), the regression line is, \\[ \\E(y | y) = \\hat{y_i} = \\hat\\beta_0 + \\hat{\\beta} x_i \\] where \\(\\hat{y_i}\\) is called the fitted value, and the residual is \\[ \\hat{\\epsilon}_i = y_i - \\hat{y}_i \\] The OLS estimator for \\(\\hat{\\vec{\\beta}}\\) finds the values of the intercept, \\(\\hat{\\beta}_0\\), and slope, \\(\\hat{\\beta}_1\\), that minimize the sum of squared residuals, \\[ \\hat{\\beta}_0, \\hat{\\beta}_1 = \\argmin_{b_0, b_1}\\sum_{i = 1}^n {( y_i - b_0 - b_1 x_i )}^2 = \\argmin_{b_0, b_1}\\sum_{i = 1}^n \\hat{\\epsilon}_i^2 \\] Terminology esitmated coefficients: \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\) estimated intercept: \\(\\hat\\beta_0\\) estimated slope: \\(\\hat\\beta_1\\) predicted values, fitted values: \\(\\hat\\y_i\\) residuals, prediction errors: \\(\\hat\\epsilon_i = y_i - \\hat{y}_i\\) The solution to that minimization problem is the following closed-form solution1, \\[ \\begin{aligned} \\hat{\\beta}_0 &amp;= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\ \\hat{\\beta}_1 &amp;= \\frac{\\sum_{i = 1}^N (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum_{i = 1}^n (x_i - \\bar{x})^2} \\end{aligned} \\] Properties of OLS The regression line goes through the means of \\(X\\) and \\(Y\\), \\((\\hat{y}, \\hat{x})\\). The sum (and mean) of the errors are zero, \\[ 0 = \\sum_{i =1}^n \\hat\\epsilon_i = \\sum_{i =1}^n (y_i - \\hat\\beta_0 - \\hat\\beta_1 x_i) \\] This is a mechanical property and a direct consequence of finding the minimum sum of squared errors. The slope coefficient is the ratio of the covariance of \\(X\\) and \\(Y\\) to the variance of \\(X\\), \\[ \\begin{aligned}[t] \\hat{\\beta}_1 &amp;= \\frac{\\sum_{i = 1}^N (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum_{i = 1}^n (x_i - \\bar{x})^2} \\\\ &amp;= \\frac{\\Cov(x_i, y_i)}{\\Var(x_i)} \\end{aligned} \\] The slope coefficient is the scaled correlation between \\(X\\) and \\(Y\\), \\[ \\begin{aligned}[t] \\hat{\\beta}_1 &amp;= \\frac{\\Cov(x_i, y_i)}{\\Var(x_i)} \\\\ &amp;= \\frac{\\sd_x \\sd_y \\Cor(x, y)}{\\Var(x_i)} \\\\ &amp;= \\frac{\\sd_y}{\\sd_x} \\Cor(x, y) \\end{aligned} \\] 2.0.1 OLS is the weighted sum of outcomes In OLS, the coefficients are weighted averages of the dependent variables, \\[ \\hat{\\beta}_1 = \\frac{\\Cov(\\vec{x}, \\vec{y})}{\\Var(\\vec{x})} = \\sum_{i = 1}^n \\frac{(x_i - \\bar{x}) (y - \\bar{y})}{{(x_i - \\bar{x})}^2} = \\sum_{i = 1}^{n} w_i y_i \\] where the weights are, \\[ w_i = \\frac{x_i - \\bar{x}}{\\sum_{i = 1}^n (x_i - \\bar{x})^2} . \\] In other words, the slope of the regression line gives more weight to the values of observations that are Alternatively, we can rewrite the estimation error (difference between the parameter, \\(\\beta\\), and the estimate, \\(\\hat{\\beta}\\)) as a weighted sum of the errors, \\[ \\hat\\beta_1 - \\beta_1 = \\sum_{i = 1}^n w_i \\epsilon_i \\] Note Linear regression is linear not because \\(y = b_0 + b_2\\), but because the predicted values can be represented as weighted sums of outcome, \\(\\hat{y} = w_i y_i\\). If we were to estimate \\(\\hat{\\beta}_0\\) or \\(\\hat{\\beta}_1\\) with a different objective function, e.g. minimizing the absolute value of the errors, then it would not longer be linear. A closed form solution is one that can be expressed in terms of simple functions. Many other statistical estimators do not have closed-form solutions and must be solved numerically.↩ "],
["goodness-of-fit.html", "Chapter 3 Goodness of Fit 3.1 Root Mean Squared Error and Standard Error 3.2 R squared 3.3 Maximum Likelihood 3.4 Regression Line", " Chapter 3 Goodness of Fit What measures do we have for how well a line fits the data? 3.1 Root Mean Squared Error and Standard Error The first is the mean squared error, \\[ MSE = \\frac{1}{n} \\sum_{i = 1}^n \\hat\\epsilon_i \\] or root mean squared error, \\[ RMSE = \\sqrt{\\frac{1}{n} \\sum_{i = 1}^n \\hat\\epsilon_i} \\] The standard error, \\(\\hat\\sigma\\) is similar, but is an estimator of the standard deviation of the population errors, \\(\\epsilon_i \\sim N(0, \\sigma)\\) \\[ \\hat\\sigma = MSE = \\sqrt{\\frac{1}{n - k - 1} \\sum_{i = 1}^n \\hat\\epsilon_i} \\] where \\(k + 1\\) is the number of coefficients (including the intercept) in the regression. In the simple (bivariate) regression model \\(k = 1\\), since there is one variable in addition to the intercept. The only difference between the \\(RMSE\\) and \\(\\sigma\\hat\\) is the denominator; \\(\\sigma\\hat\\) adjusts for the degrees of freedom. As the sample size gets large relative to the number of variables, \\(n - k \\to \\infty\\), the standard error of the regression approaches the MSE, since \\(1 / (n - k - 1) \\to 1 / n\\). 3.2 R squared The coefficient of determination, \\(R^2\\), If \\(R^2 = 1\\), then \\(SSE = 0\\), and all points of \\(y_i\\) fall on a straight line. However, if all values of \\(y_i\\) are equal (\\(SSE = SST = 0\\)), then the \\(R^2\\) is undefined. If \\(R^2 = 0\\), then there is no relationship. \\(SSE = SST\\), meaning that including \\(x_i\\) does not reduce the residuals any more than using the mean of \\(\\vec{y}\\). In the bivariate regression case, \\[ R^2 = \\cor(x, y)^2 , \\] hence its name (since \\(r\\) is the letter usually used to indicate correlation). In the more general case, \\(R^2\\) is the squared correlation between the outcome and the fitted values of the regression, \\[ R^2 = \\cor(\\vec{y}, \\hat{\\vec{y}}). \\] The common interpretation of \\(R^2\\) is “the fraction of variation in \\(y_i\\) that is explained by the regression (\\(x_i\\)).” In this context, “explained” should not be interpreted a “caused.” Consider the \\[ Y = a X + \\epsilon \\] The variance of \\(Y\\) is \\(a^2 \\Var(X) + \\Var(\\epsilon)\\) (supposing \\(\\cor(X, \\epsilon) = 0\\) ). The “variance explained” by the regression is simply \\(a^2 \\var(x)\\), and \\[ R^2 = \\frac{a^2 \\var(X)}{a^2 \\var(X) + \\var(\\epsilon)} \\] As the variation in \\(X\\) gets large, \\(\\var(X) \\to \\infty\\), then the regression “explains” everything, \\(R^2 \\to 1\\), and as the variance in \\(X\\) gets small, \\(\\var(X) \\to 0\\), then the regression explains nothing, \\(R^2 \\to 0\\). 3.3 Maximum Likelihood The OLS estimator of linear regression finds \\(\\beta_0\\) and \\(\\beta_1\\) by minimizing the squared error. One nice property of this estimator is that it agrees with the maximum likelihood estimator of the coefficients.2 Maximum likelihood estimation (MLE) is a general statistical estimator. It finds parameters by doing what its name says, maximizing a likelihood function. A likelihood function, \\(f(\\vec{y} | \\vec{\\theta})\\), is the probability of observing the data, \\(\\vec{x}\\), given parameter values, \\(\\vec{y}\\). The MLE value of the parameters, \\(\\hat{\\vec{\\theta}}\\), is the value of the parameters that maximizes the probability of observing the data. While no distributional assumptions were needed to calculate the OLS estimator (though some are needed for inference in small samples), the MLE requires specifying distributions for the data. In linear regression, we assume the following model, \\[ \\begin{aligned}[t] Y_i &amp;= \\beta_0 + \\beta_1 X_i + \\epsilon_i \\end{aligned} \\] where \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). That must assume the errors are distributed normally in order to calculate the estimates of \\(\\vec{\\beta}\\) is different than OLS. Then the MLE function is, \\[ \\begin{aligned}[t] \\hat\\beta_0^{(MLE)}, \\hat\\beta_1^{(MLE)}, \\hat\\sigma^{(MLE)} &amp;= \\argmax_{\\beta_0, \\beta_1, \\sigma} \\sum_{i = 1}^n \\frac{1}{\\sqrt{2 \\sigma^2 \\pi}} \\exp \\left( - \\frac{1}{2 \\sigma^2} (y_i - \\beta_0 - \\beta_1 x_i)^2 \\right) \\\\ &amp;= \\argmax_{\\beta_0, \\beta_1, \\sigma} \\sum_{i = 1}^n \\frac{1}{\\sqrt{2 \\sigma^2 \\pi}} \\exp \\left( - \\frac{1}{2 \\sigma^2} \\cdot \\epsilon^2 \\right) \\end{aligned} \\] For numerical reasons,3 in practice, the log-likelihood maximized instead of the likelihood, \\[ \\begin{aligned}[t] \\hat\\beta_0^{(MLE)}, \\hat\\beta_1^{(MLE)}, \\hat\\sigma^{(MLE)} &amp;= \\argmax_{\\beta_0, \\beta_1, \\sigma} -\\frac{n}{2} \\log \\sigma^2 - \\sum_{i = 1}^n \\left( - \\frac{1}{2 \\sigma^2} (y_i - \\beta_0 - \\beta_1 x_i)^2 \\right) , \\\\ &amp;= \\argmax_{\\beta_0, \\beta_1, \\sigma} -\\frac{n}{2} \\log \\sigma^2 - \\sum_{i = 1}^n \\left( - \\frac{1}{2 s^2} \\cdot \\epsilon^2 \\right) . \\end{aligned} \\] Even though the estimators are different, both MLE and OLS will produce the same linear regression estimates for \\(\\beta_0\\) and \\(\\beta_1\\), \\[ \\hat\\beta_0^{(MLE)} = \\hat\\beta_0^{(OLS)} \\text{ and } \\hat\\beta_1^{(MLE)} = \\hat\\beta_1^{(OLS)} . \\] Some intuition as to why the OLS and MLE estimates agree can be gained from noticing that the likelihood function of the normal distribution includes the negative sum of squared errors, so maximizing the likelihood, minimizes the squared errors. That is all that will be said about MLE for now, since it is not necessary for most of the material on linear models. But MLE is perhaps the most commonly used to estimator and will reappear many times, notably with generalized linear models, e.g. logit, probit, binomial, Poisson models. 3.4 Regression Line filter &lt;- dplyr::filter library(&quot;HistData&quot;) heights &lt;- dplyr::filter(PearsonLee, par == &quot;Father&quot;, chl == &quot;Son&quot;) heights %&gt;% ggplot(aes(x = parent, y = child)) + geom_jitter() height_summary &lt;- heights %&gt;% group_by(parent) %&gt;% summarise(child = mean(child)) ggplot() + geom_jitter(data = heights, aes(x = parent, y = child), alpha = 0.3) + geom_smooth(data = heights, aes(x = parent, y = child), se = FALSE, method = &quot;lm&quot;, colour = &quot;red&quot;) + geom_point(data = height_summary, aes(x = parent, y = child), colour = &quot;black&quot;, size = 2) “Corelation measures the extent to which a scatter diagram is packed around a line” - Freedman p. 21 \\[ MSE = (1 - r^2) \\Var(y) \\] Among all lines, the regression line has the smallest RMSE estimates aren’t parameters, and residuals aren’t random errors in a regression model, the data are observed values of random variables. observed values are called “realizations” heights %&gt;% add_predictions(lm(parent ~ child, data = .), var = &quot;pred_parent&quot;) %&gt;% add_predictions(lm(child ~ parent, data = .), var = &quot;pred_child&quot;) %&gt;% mutate(sd_y = sd(child), sd_x = sd(parent), mean_y = mean(child), mean_x = mean(parent), cor_xy = cor(parent, child), cor_line = mean_y + sign(cor_xy) * (sd_y / sd_x) * (parent - mean_x)) %&gt;% ggplot() + geom_jitter(mapping = aes(x = parent, y = child), alpha = 0.3) + geom_line(mapping = aes(x = parent, y = pred_child), colour = &quot;blue&quot;) + geom_line(mapping = aes(x = pred_parent, y = child), colour = &quot;red&quot;) + geom_line(mapping = aes(x = parent, y = cor_line)) The correlation line is: \\[ y = \\bar{y} + \\text{sign}(r_{xy}) \\left( \\frac{s_y}{s_x} \\right) (x - \\bar{x}) \\] The correlation is the same as the regression line if both \\(y\\) and \\(x\\) are standardized to have mean zero and standard deviation one. However, the MLE estimator of the regression standard error is not the same as the OLS estimator, \\(\\hat\\sigma_{MLE} \\neq \\hat\\sigma_{OLS}\\).↩ Probabilities can get quite small, so multiplying them together can result in numbers too small to represent as different than zero. Adding the logarithms of probabilities can represent much smaller floating point numbers.↩ "],
["multiple-regression.html", "Chapter 4 Multiple Regression", " Chapter 4 Multiple Regression We can write a multiple linear regression, \\[ y_i = \\beta_0 + \\x_{i,1} \\beta_1 + x_{i,2} \\beta_2 + \\cdots + x_{i,k} \\beta_k + \\epsilon_i \\] The coefficient \\(\\beta_1\\) is the effect of a one-unit change in \\(x_{i,1}\\) conditional on all other \\(x_{ij}\\). This is called: partial effect, ceterbis paribus, all else equal, conditional on the covariates. "],
["what-is-regression.html", "Chapter 5 What is Regression? 5.1 What is Regression and What is it Used For ? 5.2 Joint vs. Conditional models 5.3 Conditional expectation function", " Chapter 5 What is Regression? library(&quot;tidyverse&quot;) library(&quot;modelr&quot;) library(&quot;stringr&quot;) library(&quot;HistData&quot;) 5.1 What is Regression and What is it Used For ? Regression can be used to summarize data predict the future predict results of interventions Causal inferences made from observational studies natural experiments randomized controlled experiments When using observational data the key problem is “confounding”. There are two general approaches to accounting for differences in confounding in observational data: stratification or cross-tabulation (matching) modeling (regression) Counfounding is a difference between treatment and control groups, other than the treatment, which affects the response. 5.2 Joint vs. Conditional models In most problems, the researcher is concerned with relationships between multiple variables. For example, suppose that we want to model the relationship between two variables, \\(Y\\) and \\(X\\). There are two main approaches to modeling this relationship. joint model: Jointly model \\(Y\\) and \\(X\\) as \\(f(Y, X)\\). For example, we can model \\(Y\\) and \\(X\\) as coming from a bivariate normal distribution.4 conditional model: Model \\(Y\\) as a conditional function of \\(X\\). This means we calculate a function of \\(Y\\) for each value of \\(X\\).5 Most often we focus on modeling a conditional statistic of \\(Y\\), and linear regression will focus on modeling the conditional mean of \\(Y\\), \\(\\E(Y | X)\\). regression (conditional) model \\(p(y | x_1, \\dots, x_k) = f(x_1, \\dots, x_k)\\) and \\(x_1, \\dots, x_k\\) are given. joint model \\(p(y, x_1, \\dots, x_k) = f(y, x_1, \\dots, x_k)\\) If we knew the joint model we could calculate the conditional model, \\[ (y | x_1, \\dots, x_k) = \\frac{p(y, x_1, \\dots, x_k)}{p(x_1, \\dots, x_k)} . \\] However, especially when there are specific outcome variables of interest, the conditional model, i.e. regression, is easier because the analyst can focus on modeling how \\(Y\\) varies with respect to \\(X\\), without necessarily having to model the process by which \\(X\\) is generated. However, that very convenience of not modeling the process which generates \\(X\\) will be the problem when regression is used for causal inference on observational data. At its most general, “regression analysis, broadly construed, traces the distribution of a response variable (denoted by \\(Y\\))—or some characteristic of this distribution (such as its mean)—as a function of one of more explanatory variables (Fox 2016, 15).” is a procedure that is used to summarize conditional relationships. That is, the average value of an outcome variable conditional on different values of one or more explanatory variables. While this section will generally cover linear regression, it is not the only form of regression. So let’s start with a definition of regression. Most generally, a regression represents a function of a variable, \\(Y\\) as a function of another variable or variables, \\(X\\), and and error. \\[ g(Y_i) = f(X_i) + \\text{error}_i \\] The conditional expectation function (CEF) or regression function of \\(Y\\) given \\(X\\) is denoted, \\[ \\mu(x) = \\E\\left[Y | X = x\\right] \\] But if regression represents \\(Y\\) as a function of \\(X\\), what’s the alternative? Instead of modeling \\(Y\\) as a function of \\(X\\), we could jointly model both \\(Y\\) and \\(X\\). A regression model of \\(Y\\) and \\(X\\) would be multivariate function, \\(f(Y, X)\\). In machine learning these approaches are sometimes called descriminative (regression) and generative (joint models) models. 5.3 Conditional expectation function 5.3.1 Discrete Covariates Before turning to considering continuous variable, it is useful to consider the conditional expectation function for a discrete \\(Y\\) and \\(X\\). Consider the datasets dataset included in the recommended R package datasets. It is a cross-tabulation of the survival of the 2,201 passengers in the sinking of the Titanic in 1912, as well as characteristics of those passengers: passenger class, gender, and age. Titanic &lt;- as_tibble(datasets::Titanic) %&gt;% mutate(Survived = (Survived == &quot;Yes&quot;)) The proportion of passengers who survived was summarise(Titanic, prop_survived = sum(n * Survived) / sum(n)) ## # A tibble: 1 × 1 ## prop_survived ## &lt;dbl&gt; ## 1 0.323035 Since Survived is a A conditional expectation function is a function that calculates the mean of Y for different values of X. For example, the conditional expectation function for Calculate the CEF for Survived conditional on Age, Titanic %&gt;% group_by(Age) %&gt;% summarise(prop_survived = sum(n * Survived) / sum(n)) ## # A tibble: 2 × 2 ## Age prop_survived ## &lt;chr&gt; &lt;dbl&gt; ## 1 Adult 0.3126195 ## 2 Child 0.5229358 conditional on Sex, Titanic %&gt;% group_by(Sex) %&gt;% summarise(prop_survived = sum(n * Survived) / sum(n)) ## # A tibble: 2 × 2 ## Sex prop_survived ## &lt;chr&gt; &lt;dbl&gt; ## 1 Female 0.7319149 ## 2 Male 0.2120162 conditional on Class, Titanic %&gt;% group_by(Class) %&gt;% summarise(prop_survived = sum(n * Survived) / sum(n)) ## # A tibble: 4 × 2 ## Class prop_survived ## &lt;chr&gt; &lt;dbl&gt; ## 1 1st 0.6246154 ## 2 2nd 0.4140351 ## 3 3rd 0.2521246 ## 4 Crew 0.2395480 finally, conditional on all combinations of the other variables (Age, Sex, Class), titanic_cef_3 &lt;- Titanic %&gt;% group_by(Class, Age, Sex) %&gt;% summarise(prop_survived = sum(n * Survived) / sum(n)) titanic_cef_3 ## Source: local data frame [16 x 4] ## Groups: Class, Age [?] ## ## Class Age Sex prop_survived ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1st Adult Female 0.97222222 ## 2 1st Adult Male 0.32571429 ## 3 1st Child Female 1.00000000 ## 4 1st Child Male 1.00000000 ## 5 2nd Adult Female 0.86021505 ## 6 2nd Adult Male 0.08333333 ## 7 2nd Child Female 1.00000000 ## 8 2nd Child Male 1.00000000 ## 9 3rd Adult Female 0.46060606 ## 10 3rd Adult Male 0.16233766 ## 11 3rd Child Female 0.45161290 ## 12 3rd Child Male 0.27083333 ## 13 Crew Adult Female 0.86956522 ## 14 Crew Adult Male 0.22273782 ## 15 Crew Child Female NaN ## 16 Crew Child Male NaN The CEF can be used to predict outcome variables given \\(X\\) variables. What is the predicted probability of survival for each of these characters from the movie Titanic? Rose (Kate Winslet): 1st class, adult female (survived) Jack (Leonardo DiCaprio): 3rd class, adult male (did not survive) Cal (Billy Zane) : 1st class, adult male (survived) titanic_chars &lt;- tribble( ~ name, ~ Class, ~ Age, ~ Sex, ~ Survived, &quot;Rose&quot;, &quot;1st&quot;, &quot;Adult&quot;, &quot;Female&quot;, TRUE, &quot;Jack&quot;, &quot;3rd&quot;, &quot;Adult&quot;, &quot;Male&quot;, FALSE, &quot;Cal&quot;, &quot;1st&quot;, &quot;Adult&quot;, &quot;Male&quot;, TRUE ) left_join(titanic_chars, titanic_cef_3, by = c(&quot;Class&quot;, &quot;Age&quot;, &quot;Sex&quot;)) ## # A tibble: 3 × 6 ## name Class Age Sex Survived prop_survived ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 Rose 1st Adult Female TRUE 0.9722222 ## 2 Jack 3rd Adult Male FALSE 0.1623377 ## 3 Cal 1st Adult Male TRUE 0.3257143 Rose was predicted to survive 97% of 1st class adult females survived, and she did. Jack was not predicted to survive (only 16% of 3rd class adult males survived, and he did not.6 Cal was not predicted to survive (33% of 1st class adult males survived), but he did, though through less than honorable means in the movie. Note that we haven’t made any assumptions about distributions of the variables. In this case, the outcome variable in the CEF was a binary variable, and we calculated a proportion. However, the proportion is the expected value (mean) of a binary variable, so the calculation of the CEF wouldn’t change. If we continued to condition on more discrete variables, the number of observed cell sizes would get smaller and smaller (and possibly zero), with larger standard errors. 5.3.2 Continuous Covariates But what happens if the conditioning variables are continuous? Galton (1886) examined the joint distribution of the heights of parents and their children. He was estimating the average height of children conditional upon the height of their parents. He found that this relationship was approximately linear with a slope of 2/3. This means that on average taller parents had taller children, but the children of taller parents were on average shorter than they were, and the children of shorter parents were on average taller than they were. In other words, the children’s height was more average than parent’s height. This phenomenon was called regression to the mean, and the term regression is now used to describe conditional relationships (Hansen 2010). His key insight was that if the marginal distributions of two variables are the same, then the linear slope will be less than one. He also found that when the variables are standardized, the slope of the regression of \\(y\\) on \\(x\\) and \\(x\\) on \\(y\\) are the same. They are both the correlation between \\(x\\) and \\(y\\), and they both show regression to the mean. Galton &lt;- as_tibble(Galton) Galton ## # A tibble: 928 × 2 ## parent child ## &lt;dbl&gt; &lt;dbl&gt; ## 1 70.5 61.7 ## 2 68.5 61.7 ## 3 65.5 61.7 ## 4 64.5 61.7 ## 5 64.0 61.7 ## 6 67.5 62.2 ## 7 67.5 62.2 ## 8 67.5 62.2 ## 9 66.5 62.2 ## 10 66.5 62.2 ## # ... with 918 more rows Calculate the regression of children’s heights on parents. Interpret the regression. child_reg &lt;- lm(child ~ parent, data=Galton) child_reg ## ## Call: ## lm(formula = child ~ parent, data = Galton) ## ## Coefficients: ## (Intercept) parent ## 23.9415 0.6463 Calculate the regression of parent’s heights on children’s heights. Interpret the regression. parent_reg &lt;- lm(parent ~ child, data=Galton) parent_reg ## ## Call: ## lm(formula = parent ~ child, data = Galton) ## ## Coefficients: ## (Intercept) child ## 46.1353 0.3256 Regression calculates the conditional expectation function, \\(f(Y, X) = \\E(Y | X) + \\epsilon\\), but we could instead jointly model \\(Y\\) and \\(X\\). This is a topic for multivariate statistics (principal components, factor analysis, clustering). In this case, an alternative would be to model the heights of fathers and sons as a bivariate normal distribution. ggplot(Galton, aes(y = child, x = parent)) + geom_jitter() + geom_density2d() # covariance matrix Galton_mean &lt;- c(mean(Galton$parent), mean(Galton$child)) # variance covariance matrix Galton_cov &lt;- cov(Galton) Galton_cov ## parent child ## parent 3.194561 2.064614 ## child 2.064614 6.340029 var(Galton$parent) ## [1] 3.194561 var(Galton$child) ## [1] 6.340029 cov(Galton$parent, Galton$child) ## [1] 2.064614 Calculate density for a multivariate normal distribution library(&quot;mvtnorm&quot;) Galton_mvnorm &lt;- function(parent, child) { # mu and Sigma will use the values calculated earlier dmvnorm(cbind(parent, child), mean = Galton_mean, sigma = Galton_cov) } Galton_mvnorm(Galton$parent[1], Galton$child[1]) ## [1] 4.272599e-05 Galton_dist &lt;- Galton %&gt;% modelr::data_grid(parent = seq_range(parent, 50), child = seq_range(child, 50)) %&gt;% mutate(dens = map2_dbl(parent, child, Galton_mvnorm)) Why don’t I calculate the mean and density using the data grid? library(&quot;viridis&quot;) ggplot(Galton_dist, aes(x = parent, y = child)) + geom_raster(mapping = aes(fill = dens)) + #geom_contour(mapping = aes(z = dens), colour = &quot;white&quot;, alpha = 0.3) + #geom_jitter(data = Galton, colour = &quot;white&quot;, alpha = 0.2) + scale_fill_viridis() + theme_minimal() + theme(panel.grid = element_blank()) + labs(y = &quot;Parent height (in)&quot;, x = &quot;Child height (in)&quot;) Using the plotly library we can make an interactive 3D plot: x &lt;- unique(Galton_dist$parent) y &lt;- unique(Galton_dist$child) z &lt;- Galton_dist %&gt;% arrange(child, parent) %&gt;% spread(parent, dens) %&gt;% select(-child) %&gt;% as.matrix() plotly::plot_ly(z = z, type = &quot;surface&quot;) But with regression we are calculating only one margin. Galton_means &lt;- Galton %&gt;% group_by(parent) %&gt;% summarise(child = mean(child)) ggplot(Galton, aes(x = factor(parent), y = child)) + geom_jitter(width = 0) + geom_point(data = Galton_means, colour = &quot;red&quot;) Note that in this example, it doesn’t really matter since a bivariate normal distribution happens to describe the data very well. This is not true in general, and we are simplifying our analysis by calculating the CEF rather than jointly modeling both. References "],
["interpreting-coefficients.html", "Chapter 6 Interpreting Coefficients 6.1 Interactions and Polynomials 6.2 Average Marginal Effects 6.3 Standardized Coefficients", " Chapter 6 Interpreting Coefficients That the coefficients are the marginal effects of each predictor makes linear regression particularly easy to interpret. However, this interpretation of predictors becomes more complicated once a variable is included in multiple terms through interactions or nonlinear functions, such as polynomials. Consider the regression, \\[ Y_i = \\beta_0 + \\beta_1 X + \\beta_2 Z + \\varepsilon \\] The regression coefficient \\(\\beta_1\\) it the change in the expected value of \\(Y\\) associated with a one-unit change in \\(X\\) holding \\(Z\\) constant, \\[ \\begin{aligned}[t] E(Y | X = x, Z = z) - E(Y | X = x + 1, Z = z) &amp;= (\\beta_0 + \\beta_1 X + \\beta_2 z) - (\\beta_0 + \\beta_1 (x + 1) + \\beta_2 z) \\\\ &amp;= \\beta_1 x - \\beta_1 (x + 1) \\\\ &amp;= \\beta_1 (x - x + 1) \\\\ &amp;= \\beta_1 \\end{aligned} \\] More formally, the coefficient \\(\\beta_k\\) is the partial derivative of \\(E(Y | X)\\) with respect to \\(X_k\\), \\[ \\begin{aligned}[t] \\frac{\\partial\\,E(Y | X)}{\\partial\\,X_k} = \\frac{\\partial}{\\partial\\,X_k} \\left( \\beta_0 + \\sum_{k = 1}^K \\beta_k X_k) \\right) = \\beta_k \\end{aligned} \\] Implications: If \\(X\\) is multiplied by a constant scalar \\(a\\), \\[ E(Y | X) = \\tilde{\\beta}_0 + \\tilde{\\beta}_1 a X = \\beta_0 + (a \\beta_1) X . \\] If \\(X_k\\) has a scalar \\(a\\) added to it, \\[ E(Y | X) = \\tilde{\\beta}_0 + \\tilde{\\beta}_1 (X + a) = (\\beta_0 + \\tilde{\\beta}_1 a) + \\tilde{\\beta}_1 X \\] Thus, \\(\\tilde{\\beta}_0 = (\\beta_0 + \\beta_1 a)\\) and \\(\\tilde{\\beta}_1 = \\beta_1\\). Consider the regression model \\[ \\vec{Y} = \\vec{\\beta} \\mat{X} + \\vec{\\epsilon} \\] Rather than staring by asking what do the regression coefficients, \\(\\vec{\\beta}\\), mean, we should start by asking what we want to estimate (i.e. the estimand) and then figure out how to extract that from the regression model. Let’s start with what it that we want to calculate. We want to calculate the “marginal effect” of changing the \\(j\\)th predictors while holding other predictors constant. In particular, one common estimand is the predicted change in the expected value of \\(Y\\) from a change in the \\(j\\)th predictor variable while holding the other predictors constant. The regression model is a model of the expected value of \\(Y\\) as a function of \\(\\mat{X}\\), \\[ \\hat{\\vec{y}} = \\E(Y) = \\hat{\\vec{beta}} \\mat{X} \\] For a continuous variable, \\(\\vec{x}_j\\), this is called the “marginal effect” and it is the partial derivative of the regression line with respect to \\(\\vec{x}_j\\), \\[ ME_{i,j} = \\frac{\\partial \\E( Y_i | x_{i,j}, x_{i,-j})}{\\partial x_{i,j}} \\] For a discrete change in \\(x_j\\), this is called the “partial effect” or “first difference”, and is simply the difference of predicted values, \\[ ME_{i,j} = \\E(Y_i | x_{i,j}, x_{i,-j}) - \\E(Y_i | x_{i,j} + \\Delta x_{i,j}, x_{i,-j}) \\] Now consider the linear regression with two predictors for a change in \\(x_1\\), \\[ \\begin{aligned}[t] ME_j &amp;= E(y | x_1, \\tilde{x}_2) - E(y | x_1 + \\Delta x_1, \\tilde{x}_2) \\end{aligned} \\] Since the linear regression equation is \\(E(y | x)\\), this simplifies to \\[ \\begin{aligned}[t] ME_j &amp;= (\\beta_0 + \\beta_1 x_1 + \\tilde{x}_2) - (\\beta_0 + \\beta_1 (x_1 + \\Delta x_1) \\tilde{x}_2) \\\\ &amp;= \\beta_1 \\Delta x_1 \\end{aligned} \\] or as \\(\\Delta x_1 \\to 0\\), this simplifies to the coefficient itself. \\[ \\begin{aligned}[t] ME_j &amp;= \\frac{\\partial E(y | x_1, x_2)}{\\partial x_1} \\\\ \\frac{\\partial (\\beta_0 + \\beta_1 x_1 + \\tilde{x}_2)}{\\partial x_1} &amp;= \\beta_1 \\end{aligned} \\] All of the previous equations were at the population level. The sample estimate for the marginal effect is \\[ \\widehat{ME}_j = \\hat{\\beta}_1 \\] So, for a linear regression, the marginal effect of \\(x_j\\), defined as the change in the expected value of \\(y\\) for a small a unit of \\(j\\) The equation presented above is not causal, it is simply a function derived from the population or estimated equation. If population equation is not the as the linear regression, \\(\\hat{\\beta_j}\\) can still be viewed as an estimator of \\(ME_j\\). In OLS, the \\(ME_j\\) is weighted by observations with the most variation in \\(x_j\\), after accounting for the parts of \\(x_j\\) and \\(y\\) predicted by the other predictors. See the discussion Angrist and Pischke (2009) and Aronow and Samii (2015). For regressions other than OLS, the coefficients are not the \\(ME_j\\). It is a luxury that the coefficients happen to have a nice interpretation in OLS. In most other regressions, the coefficients are not directly useful. This is yet another reason to avoid the mindless presentation of tables and star-worshiping. The researcher should focus on inference about the research quantity of interest, whether or not that happens to be conveniently provided as a parameter of the model that was estimated. 6.1 Interactions and Polynomials Even for OLS, if \\(x_j\\) is included as part of a function, e.g. a polynomial or an interaction, then its coefficient cannot be interpreted as the marginal effect. Suppose that the regression equation is \\[ \\vec{y} = \\vec{\\beta}_0 + \\vec{\\beta}_1 x_1 + \\vec{\\beta}_2 x_1^2 + \\vec{\\beta}_3 x_2, \\] then the marginal effect of \\(x_1\\) is, \\[ \\begin{aligned}[t] ME_j &amp;= \\frac{\\partial E(y | x_1, x_2)}{\\partial x_1} \\\\ &amp;= \\frac{\\partial (\\beta_0 + \\beta_1 x_1 + \\beta_1 x_1^2 + \\beta_3 \\tilde{x}_2)}{\\partial x_1} \\\\ &amp;= \\beta_1 + 2 \\beta_2 x_1 \\end{aligned} \\] Note that the marginal effect of \\(x_1\\) is not, \\(\\beta_1\\). That would require a change in \\(x_1\\) while holding \\(x_1 ^ 2\\) constant, which is a logical impossibility. Instead, the marginal effect of \\(x_1\\) depends on the value of \\(x_2\\) at which it is evaluated, and, thus, observations will have different marginal effects. Similarly, if there is an interaction between \\(x_1\\) and \\(x_2\\), the coefficient of a predictor is not its marginal effect. For example, in \\[ y = \\vec{\\beta}_0 + \\vec{\\beta}_1 x_1 + \\vec{\\beta}_2 x_1 + \\vec{\\beta}_3 x_1 x_2 \\] the marginal effect of \\(x_1\\) is \\[ \\begin{aligned}[t] ME_j &amp;= \\frac{\\partial E(y | x_1, x_2)}{\\partial x_1} \\\\ &amp;= \\frac{\\partial (\\beta_0 + \\beta_1 x_1 + \\beta_1 x_1^2 + \\beta_2 \\tilde{x}_2)}{\\partial x_1} \\\\ &amp;= \\beta_1 + \\beta_3 x_2 \\end{aligned} \\] Now the marginal effect of \\(x_1\\) is a function of another variable \\(x_2\\). 6.2 Average Marginal Effects For marginal effects that are functions of the data, there are multiple ways to calculate them. They include, AME: Average Marginal Effect. Average the marginal effects at each observed \\(x\\). MEM: Marginal Effect at the mean. Calculate the marginal effect with all observations at their means or other central values. MER: Marginal Effect at a representative value. Similar to MEM but with another meaningful value. Of these, the AME is the preferred one; marginal effects should be calculated for all observations, and then averaged (Hanmer and Kalkan 2012). When it is discrete change in \\(x\\), it is called a partial effect (APE) or a first difference. The difference in the expected value of y, given a change in \\(x_j\\) from \\(x^*\\) to \\(x^* + \\Delta\\) is \\(\\beta_j \\Delta\\), and the standard error can be calculated analytically by the Delta method, \\[ \\se(\\hat{\\beta}_j \\Delta x_j) = \\sqrt{\\Var\\hat{\\beta}_j (\\Delta x_j)^2} = \\se\\hat{\\beta}_j \\Delta x_j. \\] The Delta method can be used to analytically derive approximations of the standard errors for other nonlinear functions and interaction in regression, but it scales poorly, and it is often easier to use bootstrapping or software than calculate it by hand. See the margins package. 6.3 Standardized Coefficients A standardized coefficient is the coefficient on \\(X\\), when \\(X\\) is standardized so that \\(\\mean(X) = 0\\) and \\(\\Var(X) = 1\\). In that case, \\(\\beta_1\\) is the change in \\(\\E(Y)\\) associated with a one standard deviation change in \\(X\\). Additionally, if all predictors are set so that \\(\\mean(X) = 0\\), \\(\\beta_0\\) is the expected value of \\(Y\\) when all \\(X\\) are at their means. However, if any variables appear in multiple terms, then the standardized coefficients are not particularly useful. Standardized coefficients are generally not used in political science. (King How Not to Lie with Statistics, p. 669) More often, the effects of variables are compared by the first difference between the value of the variable at the mean, and a one standard deviation change. While, this is equivalent to the standardized coefficient Note, that standardizing variables can help computationally in some cases. In OLS, there is a closed-form solution, so iterative optimization algorithms are not needed in to find the best parameters. However, in more complicated models which require iterative optimization, standardizing variables can often improve the performance of the optimization. Thus standardizing variables before analysis is common in machine learning. However, the purpose is for ease of computation, not for ease of interpretation. References "],
["regression-inference.html", "Chapter 7 Regression Inference 7.1 Prerequisites 7.2 Sampling Distribution and Standard Errors of Coefficients 7.3 Single Coefficient 7.4 Multiple Coefficients 7.5 Linear and Non-Linear Confidence Intervals 7.6 Multiple Testing 7.7 Data snooping 7.8 Power", " Chapter 7 Regression Inference 7.1 Prerequisites library(&quot;tidyverse&quot;) library(&quot;broom&quot;) library(&quot;stringr&quot;) library(&quot;magrittr&quot;) 7.2 Sampling Distribution and Standard Errors of Coefficients The standard error of a single regression coefficient is (Fox 2008, 107) \\[ \\widehat{\\se}(\\hat{\\beta}_j) = \\frac{1}{\\sqrt{1 - R_j^2}} \\times \\frac{\\hat{\\sigma}^2}{\\sum_i (x_{ij} - \\bar{x}_j)^2} , \\] where \\(R_j^2\\) is the \\(R^2\\) of the linear regression of \\(x_j\\) on all the other predictors except \\(x_j\\). The first term, \\(1 / \\sqrt{1 - R_j}\\), is named the variance inflation factor (VIF) for variable \\(j\\). It ranges from \\(\\inf\\) when \\(x_j\\) is completely “explained” (is a linear function of) the other predictors (\\(R_j^2 = 1\\)), to \\(0\\), when \\(x_j\\) is uncorrelated with the other variables (\\(R_j^2 = 0\\)) The term \\(\\hat{\\sigma}^2\\) is the standard error of the regression, \\(\\hat{\\sigma}^2 = \\sum_i \\hat{\\epsilon}^2 / (n - k - 1)\\). The variance-covariance matrix of the regression coefficients is (Fox 2008, 199) \\[ \\widehat{\\Cov}(\\hat{\\vec{\\beta}}) = \\hat{\\sigma}^2 (\\mat{X}\\T \\mat{X})^{-1} . \\] 7.3 Single Coefficient Consider these hypothesis about a single \\(\\beta_k\\) coefficient: \\[ \\begin{aligned}[t] H_0:&amp; \\beta_k = \\beta_0 \\\\ H_a:&amp; \\beta_k \\neq \\beta_0 \\\\ \\end{aligned} \\] The test statistic is, \\[ t = \\frac{\\hat{\\beta}_k - \\beta_0}{\\widehat{\\se}(\\hat{\\beta}_k)} \\] which is distributed \\(t_{n - (k + 1)}\\). The \\(p\\) value \\(p = \\Pr(T &lt; t) + \\Pr(T &gt; t)\\) where \\(t\\) is the test statistic, and \\(T\\) is a random variable distributed Student’s t. The most common null hypothesis, and the default null hypothesis reported in regression tables and regression software output is that the coefficient is zero, i.e. \\(\\beta_0 = 0\\). This simplifies the test statistic to 0, \\[ t = \\frac{\\hat\\beta_k}{\\widehat{\\se}(\\hat{\\beta_k})} \\] Since the critical value for a two-sided p-value with a normal distribution is 1.96, this yields the rule of thumb that \\(\\hat\\beta\\) is significant at the 5% level if \\(t &lt; 2\\). For a one sided hypothesis, such as \\[ \\begin{aligned}[t] H_0:&amp; \\beta_k &lt; \\beta_0 \\\\ H_a:&amp; \\beta_k \\neq \\beta_0 \\\\ \\end{aligned} \\] use the same test statistic as above, but halve the \\(p\\)-value since \\(p = \\Pr(T &lt; t)\\). 7.3.1 Confidence Intervals The \\(1 - \\alpha\\) confidence interval for a single regression coefficient is \\[ CI(\\hat\\beta_k, \\alpha) = \\hat\\beta_k \\pm t^*_{\\alpha / 2} \\hat{se}(\\hat\\beta) \\] where \\(t^*_{\\alpha / 2}\\) is the quantile of the Student’s \\(t\\) distribution with \\(n - k - 1\\) degrees of freedom, \\(t^*_{\\alpha/2} = t s.t. \\Pr(t &lt; T) = 1 - (1 - \\alpha / 2)\\). 7.4 Multiple Coefficients We can consider several common confidence intervals and NHST for multiple coefficients. 7.4.1 F-test Consider a multiple regression model: \\[ Y_i = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 \\] Consider the null hypothesis is that all the coefficients are equal to zero, and the alternative that at least one coefficient is not zero: \\[ \\begin{aligned}[t] H_0 :&amp; \\text{$\\beta_1 = 0$ and $\\beta_2 = 0$} \\\\ H_a :&amp; \\text{$\\beta_1 \\neq 0$ or $\\beta_2 \\neq 0$} \\end{aligned} \\] To test this hypothesis, compare the fit (residuals) of the model under the null and alternative hypthesis. Note that these hypothesese are really about a model comparison. Does the model with variables \\(\\beta_1\\) and \\(\\beta_2\\) fit better than the model without them. The model without those predictors is called the restricted model and the model with those predictors is the unrestricted model. Unrestricted model (Long model): The model if \\(H_a\\) is true: \\[ Y_i = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 \\] with estimates \\[ \\hat{Y}_i = \\beta_0 + \\hat\\beta_1 X_1 + \\hat\\beta_2 X_2 + \\hat\\beta_3 X_3 \\] and sum of squared residuals, \\[ SSR_u = \\sum_{i = 1}^n (Y_i - \\hat{Y}_i)^2 \\] Restricted model (short model): The model if the null is true, \\(\\beta_2 = \\beta_3 = 0\\) \\[ Y_i = \\beta_0 + \\beta_1 X_1 \\] with estimates, \\[ \\tilde{Y}_i = \\tilde\\beta_0 + \\tilde\\beta_1 X_1 \\] and sum of squared residuals, \\[ SSR_r = \\sum_{i = 1}^n (Y_i - \\tilde{Y}_i)^2 \\] Note that the variance of the errors in the unrestricted model has to be smaller than the variances in the restricted model, \\(SSR_r \\leq SSR_u\\). This is because the unrestricted model has all the variables in the restricted model plus some more, so it can’t fit any worse than the restricted model. Remember that variables to a linear model cannot worsen its in-sample fit. If the null is true, then we would expect that \\(SSR_r = SSR_u\\) apart from sampling variation. The bigger the difference \\(SSR_r - SSR_u\\), the less plausible the null hypothesis is. F-statistic: The F-statistic is \\[ F = \\frac{(SSR_r - SSR_u) / q}{SSR_u / (n - k - 1)} , \\] where, \\(SSR_r - SSR_u\\): increase in variation explationed (decrease in in-sample fit) when the new variables are removed \\(q\\) : number of restrictions (number of variables hypothesized to be equal to 0 in the null hypothesis) \\(n - k - 1\\): denominator/unrestricted degrees of freedom. Intuition \\[ \\frac{\\text{increase in prediction error}}{\\text{original prediction error}} \\] where each of these prediction errors is scaled by its degrees of freedom. The sampling distribution of the test statistic, \\(F\\) is the unsurprisingly named \\(F\\)-distribution. The F-distribution is the ratio of two \\(\\chi^2\\) (Chi-squared) distributions. \\[ F = \\frac{(SSR_r - SSR_u) / q}{SSR_u / (n - k - 1)} \\sim F_{} \\] Connection to t-test But isn’t the \\(t\\)-test a special case of a multiple hypothesis test in which only the null hypothesis only has one coefficient set to 0. Yes, yes, it is. The F-statitic for a single restriction is a square of the t-statistic: \\[ F = t^2 = {\\left( \\frac{\\hat{\\beta}_1}{\\widehat{\\se}(\\hat{\\beta}_1)} \\right)}^2 \\] TODO Simulate this test to show its sampling distribution 7.4.2 Confidence Regions A confidence ellipses is the multivariate generalization of a confidence interval. A \\(1 - \\alpha\\)% Confidence intervals computed on repeated i.i.d. samples will contain the vector of true parameter values in \\(1 - \\alpha\\) of those samples. See Fox (2016) for the derivation of the OLS confidence ellipse. The joint confidence region for the \\(q\\) parameters \\(\\beta^*\\) is the region where the \\(F\\) test of a joint hypothesis given them is not rejected at the \\(1 - \\alpha\\) level of significance. It is given by \\[ (\\hat\\beta^* - \\beta^*)&#39; \\widehat\\Cov(\\hat\\beta^*) (\\hat\\beta^* - \\beta^*) \\leq q \\hat\\sigma^2 F_{\\alpha, q, n - k - 1} \\] where \\(F_{\\alpha, q, n - k - 1}\\) is the quantile of the \\(F\\) distribution with \\(q\\) and \\(n - k - 1\\) degrees of freedom where \\(\\Pr(x &gt; X) = \\alpha\\), and \\(\\hat\\beta^*\\) is the The diagrams in Fox (2016) are particularly useful. The car function car calculates the confidence ellipse. See its help page for examples. 7.4.3 Linear Hypothesis Tests The null hypothesis in a general linear hypothesis is \\[ H_0: \\underbrace{\\mat{L}}_{q \\times k + 1} \\underbrace{\\vec\\beta}_{k +1 \\times 1} = \\underbrace{\\vec{c}}_{q \\times 1} \\] where \\(\\mat{L}\\) and \\(\\vec{c}\\) are constants that are specified in the hypothesis. Example: For \\(H_0: \\beta_1 = \\beta_2 = 0\\), \\[ \\begin{aligned} \\mat{L} &amp;= \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} &amp; \\vec{c} &amp;= \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\end{aligned} \\] Example: For \\(H_0: \\beta_1 = \\beta_2\\) or \\(H_0: \\beta_1 - \\beta_2 = 0\\), \\[ \\begin{aligned} \\mat{L} &amp;= \\begin{bmatrix} 0 &amp; 1 &amp; -1 \\\\ \\end{bmatrix} &amp; \\vec{c} &amp;= \\begin{bmatrix} 0 \\end{bmatrix} \\end{aligned} \\] The test statistic for this is distributed \\(F\\) under the null hypothesis. See Fox (2016) for a discussion. See car function for an implementation. 7.5 Linear and Non-Linear Confidence Intervals For a single coefficient, a confidence interval for a linear function of \\(\\hat\\beta\\), \\[ CI(a + c \\hat\\beta_k) = a + c CI(\\hat\\beta_k) \\] For non-linear confidence intervals the easiest way to calculate the confidence intervals is using a bootstrap (see Bootstrapping) or simulation (King, Tomz, and Wittenberg 2000). Non-linear confidence intervals are easiest to construct with bootstrapping. However, the Delta method can also be used (see car. 7.6 Multiple Testing What happens if we run multiple regressions? What do p-values mean in that context? Simulate data where \\(Y\\) and \\(X\\) are all simulated from i.i.d. standard normal distributions, \\(Y_i \\sim N(0, 1)\\) and \\(X_{i,j} \\sim N(0, 1)\\). This means that \\(Y\\) and \\(X\\) are not associated. sim_reg_nothing &lt;- function(n, k, sigma = 1, .id = NULL) { .data &lt;- rnorm(n * k, mean = 0, sd = 1) %&gt;% matrix(nrow = n, ncol = k) %&gt;% set_colnames(str_c(&quot;X&quot;, seq_len(k))) %&gt;% as_tibble() .data$y &lt;- rnorm(n, mean = 0, sd = 1) # Run first regression .formula1 &lt;- as.formula(str_c(&quot;y&quot;, &quot;~&quot;, str_c(&quot;X&quot;, seq_len(k), collapse = &quot;+&quot;))) mod &lt;- lm(.formula1, data = .data, model = FALSE) df &lt;- tidy(mod) df[[&quot;.id&quot;]] &lt;- .id df } Here is an example with of running one regression: n &lt;- 1000 k &lt;- 19 results_sim &lt;- sim_reg_nothing(n, k) How many coefficients are significant at the 5% level? alpha &lt;- 0.05 arrange(results_sim, p.value) %&gt;% select(term, estimate, statistic, p.value) %&gt;% head(n = 20) ## term estimate statistic p.value ## 1 X7 -0.078653549 -2.42975650 0.01528771 ## 2 X3 0.074319930 2.31981338 0.02055568 ## 3 X2 0.073479210 2.26682346 0.02361823 ## 4 X18 0.060116549 1.91166357 0.05621077 ## 5 X15 -0.063182860 -1.90934736 0.05650909 ## 6 X4 0.055228170 1.70043102 0.08936716 ## 7 X16 0.037261288 1.17882830 0.23875268 ## 8 X12 -0.031195077 -1.02930126 0.30359210 ## 9 X6 -0.030642570 -0.98608544 0.32433457 ## 10 X11 -0.031623879 -0.96449749 0.33503450 ## 11 X5 -0.027538547 -0.85419387 0.39320633 ## 12 X9 -0.017672658 -0.55548574 0.57868924 ## 13 X1 -0.016570940 -0.50298851 0.61508537 ## 14 X17 -0.013440731 -0.42137369 0.67357463 ## 15 (Intercept) -0.011582072 -0.36791210 0.71301823 ## 16 X10 0.008910153 0.27966163 0.77979613 ## 17 X8 0.008178612 0.26544095 0.79072562 ## 18 X13 -0.004758974 -0.14797090 0.88239617 ## 19 X19 0.003901157 0.12131962 0.90346275 ## 20 X14 0.001949713 0.06442449 0.94864537 Is this surprising? No. Since the null hypothesis is true for all coefficients (\\(\\beta_j = 0\\)), a \\(p\\)-value of 5% means that 5% of the tests will be false positives (Type I error). Let’s confirm that with a larger number of simulations and also use it to calculate some other values. Run 1,024 simulations and save the results to a data frame. number_sims &lt;- 1024 sims &lt;- map_df(seq_len(number_sims), function(i) { sim_reg_nothing(n, k, .id = i) }) Calculate the number significant at the 5% level in each regression. n_sig &lt;- sims %&gt;% group_by(.id) %&gt;% summarise(num_sig = sum(p.value &lt; alpha)) %&gt;% count(num_sig) %&gt;% ungroup() %&gt;% mutate(p = n / sum(n)) Overall, we expect 5% to be significant at the 5 percent level. sims %&gt;% summarise(num_sig = sum(p.value &lt; alpha), n = n()) %&gt;% ungroup() %&gt;% mutate(p = num_sig / n) ## num_sig n p ## 1 988 20480 0.04824219 What about the distribution of statistically significant coefficients in each regression? ggplot(n_sig, aes(x = num_sig, y = p)) + geom_bar(stat = &quot;identity&quot;) + scale_x_continuous(&quot;Number of significant coefs&quot;, breaks = unique(n_sig$num_sig)) + labs(y = &quot;Pr(reg has k signif coef)&quot;) What’s the probability that a regression will have no significant coefficients, \\(1 - (1 - \\alpha) ^ {k - 1}\\), (1 - (1 - alpha) ^ (k + 1)) ## [1] 0.6415141 What’s the take-away? Don’t be too impressed by statistical significance when many tests are run. Note that multiple hypothesis tests occur both within papers and within literatures. TODO Familywise Error Rate Familywise Discovery Rate R function stats will adjust p-values for multiple testing: Bonferroni, Holm, Hochberg, etc. 7.7 Data snooping A not-uncommon practice is to run a regression, filter out variables with “insignificant” coefficients, and then run and report a regression with only the smaller number of “significant” variables. Most explicitly, this occurs with stepwise regression, the problems of which are well known (when used for inference). However, this can even occur in cases where the hypotheses are not specified in advance and there is no explicit stepwise function used. To see the issues with this method, let’s consider the worst case scenario, when there is no relationship between \\(Y\\) and \\(X\\). Suppose \\(Y_i\\) is sampled from a i.i.d. standard normal distributions, \\(Y_i \\sim N(0, 1)\\). Suppose that the design matrix, \\(\\mat{X}\\), consists of 50 variables, each sampled from i.i.d. standard normal distributions, \\(X_{i,k} \\sim N(0, 1)\\) for \\(i \\in 1:100\\), \\(k \\in 1:50\\). Given this, the \\(R^2\\) for these regressions should be approximately 0.50. As shown in the previous section, it will not be uncommon to have several “statistically” significant coefficients at the 5 percent level. The sim_datasnoop function simulates data, and runs two regressions: Regress \\(Y\\) on \\(X\\) Keep all variables in \\(X\\) with \\(p &lt; .25\\). Regress \\(Y\\) on the subset of \\(X\\), keeping only those variables that were significant in step 2. sim_datasnoop &lt;- function(n = 100, k = 50, p = 0.10) { .data &lt;- rnorm(n * k, mean = 0, sd = 1) %&gt;% matrix(nrow = n, ncol = k) %&gt;% set_colnames(str_c(&quot;X&quot;, seq_len(k))) %&gt;% as_tibble() .data$y &lt;- rnorm(n, mean = 0, sd = 1) # Run first regression .formula1 &lt;- as.formula(str_c(&quot;y&quot;, &quot;~&quot;, str_c(&quot;X&quot;, seq_len(k), collapse = &quot;+&quot;))) mod1 &lt;- lm(.formula1, data = .data, model = FALSE) # Select model with only significant values (ignoring intercept) signif_x &lt;- tidy(mod1) %&gt;% filter(p.value &lt; p, term != &quot;(Intercept)&quot;) %&gt;% `[[`(&quot;term&quot;) if (length(signif_x &gt; 0)) { .formula2 &lt;- str_c(str_c(&quot;y&quot;, &quot;~&quot;, str_c(signif_x, collapse = &quot;+&quot;))) mod2 &lt;- lm(.formula2, data = .data, model = FALSE) } else { mod2 &lt;- NULL } tibble(mod1 = list(mod1), mod2 = list(mod2)) } Now repeat this simulation 1,024 times, calculate the \\(R^2\\) and number of statistically significant coefficients at \\(\\alpha = .05\\). n_sims &lt;- 1024 alpha &lt;- 0.05 sims &lt;- rerun(n_sims, sim_datasnoop()) %&gt;% bind_rows() %&gt;% mutate( r2_1 = map_dbl(mod1, ~ glance(.x)$r.squared), r2_2 = map_dbl(mod2, function(x) if (is.null(x)) NA_real_ else glance(x)$r.squared), pvalue_1 = map_dbl(mod1, ~ glance(.x)$p.value), pvalue_2 = map_dbl(mod2, function(x) if (is.null(x)) NA_real_ else glance(x)$p.value), sig_1 = map_dbl(mod1, ~ nrow(filter(tidy(.x), term != &quot;(Intercept)&quot;, p.value &lt; alpha))), sig_2 = map_dbl(mod2, function(x) { if (is.null(x)) NA_real_ else nrow(filter(tidy(x), term != &quot;(Intercept)&quot;, p.value &lt; alpha)) }) ) select(sims, r2_1, r2_2, pvalue_1, pvalue_2, sig_1, sig_2) %&gt;% summarise_all(funs(mean(., na.rm = TRUE))) ## # A tibble: 1 × 6 ## r2_1 r2_2 pvalue_1 pvalue_2 sig_1 sig_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.5022612 0.1605037 0.5094306 0.03952331 2.47168 2.46802 While the average \\(R\\) squared of the second stage regressions are less, the average \\(p\\)-values of the F-test that all coefficients are zero are much less. The number of statistically significant coefficients in the first and second regressions are approximately the same, which the second regression being slightly What happens if the number of obs, number of variables, and filtering significance level are adjusted? So why are the significance levels of the overall \\(F\\) test incorrect? For a p-value to be correct, it has to have the correct sampling distribution of the observed data. Even though in this simulation we are sampling the data in the first stage from a model that satisfies the assumptions of the F-test, the second stage does not account for the original filtering. This example is known as Freedman’s Paradox (Freedman 1983). 7.8 Power See Gelman and Hill (2007 Ch. 20.5). References "],
["omitted-variable-bias.html", "Chapter 8 Omitted Variable Bias 8.1 Prerequisites 8.2 Simpson’s Paradox 8.3 Omitted Variable Bias 8.4 Measurement Error 8.5 More Information", " Chapter 8 Omitted Variable Bias 8.1 Prerequisites This chapter uses the car data set in the car package. library(&quot;car&quot;) library(&quot;tidyverse&quot;) 8.2 Simpson’s Paradox Before considering the more general phenomena of omitted variable bias, we’ll discuss Simpson’s Paradox.7 This when a trend or relationship appears when data is disaggregated into groups, but that trend or relationship either disappears or reverses when the data are aggregated. A famous real-world case of this Bickel, Hammel, and O’Connell (1975), which analyzes a claim of sex bias in graduate admissions against UC-Berkeley in the 1970s. In 1973, 8,442 men and 4,321 women applied for admission to Berkeley graduate programs. In aggregate, UC Berkeley admitted 44% of men and 35% of women applicants, seemingly supporting that claim. However, when the admissions rates were disaggregated by graduate department, the acceptance rates by department were not, on average, different. What is going on? On average, more women applied to more selective (higher rejection rate) departments than men. The dataset datasets in the datasets package contains data for the largest 6 programs. data(&quot;UCBAdmissions&quot;, package = &quot;datasets&quot;) admissions &lt;- as_tibble(UCBAdmissions) %&gt;% spread(Admit, n) %&gt;% mutate(applicants = Admitted + Rejected, accepted = Admitted / applicants) ggplot(admissions, aes(x = Dept, y = accepted, size = applicants, colour = Gender)) + geom_point() select(admissions, Dept, Gender, applicants, accepted) %&gt;% arrange(Dept, Gender) ## # A tibble: 12 × 4 ## Dept Gender applicants accepted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A Female 108 0.82407407 ## 2 A Male 825 0.62060606 ## 3 B Female 25 0.68000000 ## 4 B Male 560 0.63035714 ## 5 C Female 593 0.34064081 ## 6 C Male 325 0.36923077 ## 7 D Female 375 0.34933333 ## 8 D Male 417 0.33093525 ## 9 E Female 393 0.23918575 ## 10 E Male 191 0.27748691 ## 11 F Female 341 0.07038123 ## 12 F Male 373 0.05898123 An interactive visualization is this UC Berkeley VUD Lab visualization: Simpson’s Paradox. Stop. Go to that link. Explore that visualization, and build your intution For some other examples see Moore (2005), Wagner (1982), Wikipedia, Julious and Mullee (1994) (Kidney stone treatment). 8.3 Omitted Variable Bias Suppose that the population model is, \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 Z_i + \\epsilon_i , \\] but given a sample, we run a regression with only \\(\\vec{x}\\) and not \\(\\vec{z}\\). \\[ y_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i + \\hat{\\epsilon}_i . \\] What is the relationship between \\(\\beta_1\\) and \\(\\hat{\\beta}_1\\)? Is \\(\\hat{\\beta}_1\\) an unbiased estimator of \\(\\beta_1\\) ? \\[ \\text{omitted variable bias} = (\\text{effect of $Z_i$ on $Y_i$}) \\times (\\text{effect of $X_i$ on $Z_i$}) \\] What does the omitted variable bias An irrelevant variable is one that is uncorrelated with \\(Y_i\\), meaning that its population coefficient is 0. Suppose \\(Z_i\\) is an irrelevant variable, \\[ Y_i = \\beta_0 + \\beta_1 X_i + 0 \\times Z_i = \\epsilon_i \\] In this case OLS is unbiased … \\[ \\begin{aligned}[t] \\E(\\hat\\beta_0) &amp;= \\beta_0 \\\\ \\E(\\hat\\beta_1) &amp;= \\beta_1 \\\\ \\E(\\hat\\beta_2) &amp;= 0 \\end{aligned} \\] However, including an irrelevant variable will increase the standard errors for \\(\\hat{\\beta}_1\\). Why? Consider the linear regression model, \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i. \\] What if we included \\(X_i\\) twice? \\[ Y_i = \\tilde\\beta_0 + \\tilde\\beta_1 X_i + \\tilde\\beta_2 X_i + \\epsilon_i. \\] Clearly, any combination of \\(\\tilde\\beta_1\\) and \\(\\tilde\\beta_2\\) where \\[ \\tilde\\beta_1 + \\tilde\\beta_2 =\\beta_1 \\] will fit the model as well as any other. Consider cases of bivariate OLS with “effective” number of observations continuous OLS \\(\\Cov(X_1, X_2)\\) \\(\\Cov(X_2, Y) &gt; 0\\) \\(\\Cov(X_2, Y) = 0\\) \\(\\Cov(X_2, Y) &lt; 0\\) \\(&gt; 0\\) \\(0\\) \\(&lt; 0\\) + 0 - 0 0 0 - 0 + In practice, this is the primary problem of many papers and papers. That is because it biases the coefficient of interest. Reviewers and discussants will often ask about whether you have considered “controlling” for insert variable here. Although these may be legitimate concerns, not all reviewerss understand the purpose of controls variables so some of these may not be legitimate, and in fact harmful. There two arguments to consider when controlling for a variable. The omitted variable has to plausibly be correlated with both the variable of interest and the outcome variable, and the burden is on the reviewers to provide at a confounding variable and plausible relationships. Simply stating that there could be an unobservable variable is trivially true, uninteresting, and not a fatal critique. That said, the plausibility of a causal claim would be higher if with methods less susceptible to unobserved confounders, such as experiments, instrumental variables, regression discontinuity, and difference-in-differences. The omitted variable should be not be “post treatment” variable. If the omitted variable should not be one of the causal pathways by which \\(X\\) affects \\(Y\\), it should not be controlled for. If \\(Z\\) affects the values of \\(X\\) and also affects \\(Y\\), then it needs to be controlled for. How to assess the potential magnitude of omitted variable bias? Informal method. This is the methods that you see in many empirical papers. They estimate the model including different control variables. The less sensitive the coefficient(s) of the variables of interest are to the inclusion of control variables, the more plausible it is that the variable of interest also not sensitive to unobserved confounders (Angrist and Pischke 2014). Oster (2016) states A common heuristic for evaluating the robustness of a result to omitted variable bias concerns is to look at the sensitivity of the treatment effect to inclusion of observed controls. In three top general interest economics journals in 2012, 75% of non-experimental empirical papers included such sensitivity analysis. The intuitive appeal of this approach lies in the idea that the bias arising from the observed controls is informative about the bias that arises from the unobserved ones. Note that what is important is that the magnitude of the coefficient is stable to the inclusion of controls, not that the coefficient remains statistically significant. Formal methods: Bellows and Miguel (2009) propose the following simple statistic to assess the magnitude of omitted variable bias: \\[ \\delta = \\frac{\\hat{\\beta}_F}{\\hat{\\beta}_R - \\hat{\\beta}_C}, \\] The statistic \\(\\delta\\) is interpreted as the magnitude of covariance between the unobserved part of the controls and the treatment variable necessary to explain away the entire treatment effect of \\(X\\) on \\(Y\\). A larger ratio suggests it is implausible that omitted variable bias could explain away the entire observed effect. See Bellows and Miguel (2009 Appendix A) for the derivation. Nunn and Wantchekon (2011) provides a clear explanation and application of the statistic. Often you will see works that add regressors sequentially and make some sort of implicit coefficient stability argument. That is not useful. The important comparison is between the coefficient when nothing (or only a small subset of covariates) is controlled for, and the full set of controls. Bellows and Miguel (2009) themselves generalize Altonji, Elder, and Taber (2005) from binary to continuous treatment variables. Oster (2016) further generalizes the estimator. Pei, Pischke, and Schwandt (2017) show that if the covariates are measured with error, a “balancing test” (regressing the confounder on the treatment) is more powerful. Methods such as matching, propensity scores, or inverse weighting still depend on assumptions about selection on observables. They may be less sensitive to “omitted variable bias” due to The differ from regression in the estimand or their sensitivity to model misspecification. The preference for “design based” inference is mostly driven by a desire to find situations (designs) where other assumptions can substitute for the nigh impossible to test “selection on observables” assumption. Apart from experiments, these include instrumental variables, regression discontinuity, and difference-in-differences. 8.4 Measurement Error 8.4.1 What’s the problem? It biases coefficients: Variable with measurement error: biases \\(\\beta\\) towards zero (attenuation bias) Other variables: Biases \\(\\beta\\) similarly to omitted variable bias. In other words, when a variable has measurement error it is an imperfect control. You can think of omitted variables as the limit of the effect of measurement error as it increases. 8.4.2 What to do about it? There’s no easy fix within the OLS framework. If the measurement error is in the variable of interest, then the variable will be biased towards zero, and your estimate is too large. Find better measures with lower measurement errors. If the variable is the variable of interest, then perhaps combine multiple variables into a single index. If the measurement error is in the control variables, then include several measures. That these measure correlate closely increases their standard errors, but the control variables are not the object of the inferential analysis. More complicated methods: errors in variable models, structural equation models, instrumental variable (IV) models, and Bayesian methods. Blackwell, Honaker, and King (2015) note that the easiest way to handle measurement error in the predictors is to treat them as missing data where you have extra information about their range. Suppose a covariate is observed as \\(x \\sim(x^*, \\delta^2)\\), where \\(x^*\\) is the true value, and \\(\\delta\\) is the scale of the measurement error. Then missing values are the case when \\(\\delta \\to \\infty\\). So missing values are special, extreme, case of measurement error. This means that we can use multiple imputation methods for dealing with missing values where we add additional information to restrict the plausible range of imputated values. The Amelia has built-in support for this, but the general idea could be adapted to other multiple imputation methods. 8.5 More Information 8.5.1 Simpson’s Paradox See Samuels (1993) for more discussion of Simpson’s Paradox Moore (2005) collects and succinctly describes several examples of Simpson’s Paradox An interactive visualization of the Simpson’s Paradox Horton. 2015. Fun with Simpson’s Paradox: Simulating Confounders Horton. 2012. Example 9.20: visualizing Simpson’s paradox See the Wikipedia Page US Median Wage by Education Level. Overall wages have risen, but within every group, the wage has fallen. Nielsen. Reinventing Explanation has a visual explanation of the Simpson’s paradox Gelman. Understanding Simpson’s Praradox Using a Graph. April 8, 2014. Discusses the Nielsen post, provides other visualizations, and notes how aggregation problems arise even in non-causal cases. Armstrong and Wattenberg (2014) introduce the Comet Chart for visualizing Simpson’s Paradoxes. See this page for code and examples, including an R implementation. References "],
["outliers.html", "Chapter 9 Outliers 9.1 Iver and Soskice Data 9.2 Influential Observations", " Chapter 9 Outliers This has absolutely nothing to do with the Malcolm Gladwell book. An outlier is an observation which has large regression errors \\(\\hat{\\epsilon}^2\\). It is distant from the other observations on the \\(y\\) dimension. It increases standard errors by increasing \\(\\hat{\\sigma}^2\\), but does not bias \\(\\beta\\) if it is has typical values of \\(x\\) There are two types of extreme values in a regression. Leverage point: extreme in the \\(x\\) direction Outlier : extreme in the \\(y\\) direction. The point has a large error (the regression line does not fit the point well) For a point to affect the results of a regression (influential) it must be both a high levarage point and an outlier. The points that are influential follows from the same calculations that were in the discussion of how the linear regression is a weighted averge of points. What does this mean? Are the outliers bad data? Are the data truly contaminated, meaning that they come from a different distribution. This means that you are fitting the wrong model to the DGP causing inefficiency and maybe bias. Hat matrix The hat matrix is named as such because it puts the “hat” on \\(Y\\), The hat matrix \\[ \\mat{H} = \\mat{X} (\\mat{X}\\T \\mat{X})^{-1} \\mat{X}\\T \\] \\[ \\begin{aligned}[t] \\hat{\\vec{\\epsilon}} &amp;= \\vec{y} - \\mat{X} \\hat{\\vec{\\beta}} \\\\ &amp;= \\vec{y} - \\mat{X} (\\mat{X}\\T \\mat{X})^{-1} \\mat{X} \\vec{y} \\\\ &amp;= \\vec{y} - \\mat{H} \\vec{y} \\\\ &amp;= (\\mat{I} - \\mat{H}) \\vec{y} \\end{aligned} \\] \\[ \\hat{\\vec{y}} = \\mat{H} \\vec{y} \\] Some notes: \\(\\mat{H}\\) is a \\(n \\times n\\) symmetric matrix \\(\\mat{H}\\) is idempotent: \\(\\mat{H} \\mat{H} = \\mat{H}\\) Since \\[ \\hat{\\vec{y}} = \\mat{X} \\widehat{\\vec{\\beta}} = \\mat{X} (\\mat{X}\\T \\mat{X})^{-1} \\mat{X}\\T \\vec{y} = \\mat{H} \\vec{y}, \\] for a particular observation \\(i\\), \\[ \\hat{y}_i = \\sum_{j = 1}^n h_{ij} y_j. \\] The equation above means that predicted value of every observation is a weighted value of the outcomes of other observations. The hat values \\(h_i = h_ij\\) are diagonal entries in the hat matrix. For a bivariate linear regresion, \\[ h_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j = 1}^n (x_j - \\bar{x})^2}, \\] meaning hat values are always at least \\(1 / n\\) hat values are a function of how far \\(i\\) is from the center of \\(\\mat{X}\\) distribution Rule of thumb: examine hat values greater than \\(2 (k + 1) / n\\). This example will use the following library(&quot;MASS&quot;) library(&quot;dplyr&quot;) library(&quot;tidyr&quot;) library(&quot;broom&quot;) library(&quot;boot&quot;) library(&quot;ggplot2&quot;) This ensures that we always use the select function from dplyr rather than the one from MASS. select &lt;- dplyr::select For the ggplot2 plots, we will the default theme settings here, so that we can reuse them for all plots, and also, if we feel like changing them, we only need to change them in one location. theme_local &lt;- theme_minimal 9.1 Iver and Soskice Data This is an example of from Iversen and Soskice (2003). That paper is interested in the relationship between party systems and redistributive efforts of the government. The party system is measured using the effective number of parties; the redistributive efforts of the government is measured as the percent people lifted from poverty by taxes and transfers First, let’s load the data iver &lt;- read.csv(&quot;http://uw-pols503.github.io/2015/data/iver.csv&quot;) glimpse(iver) ## Observations: 14 ## Variables: 8 ## $ cty &lt;fctr&gt; Australia, Belgium, Canada, Denmark, Finland, France... ## $ elec_sys &lt;fctr&gt; maj, pr, maj, pr, pr, maj, maj, pr, pr, pr, pr, unam... ## $ povred &lt;dbl&gt; 42.16, 78.79, 29.90, 71.54, 69.08, 57.91, 46.90, 42.8... ## $ enp &lt;dbl&gt; 2.38, 7.01, 1.69, 5.04, 5.14, 2.68, 3.16, 4.11, 3.49,... ## $ lnenp &lt;dbl&gt; 0.867100, 1.947340, 0.524729, 1.617410, 1.637050, 0.9... ## $ maj &lt;int&gt; 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1 ## $ pr &lt;int&gt; 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0 ## $ unam &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 The variables of interest are lnemp (log effective number of parties), and povred (poverty reduction). Let’s plot the relationship between them ggplot(iver, aes(x = lnenp, y = povred)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + xlab(&quot;log(Number of Effective parties)&quot;) + ylab(&quot;Poverty Reduction&quot;) + theme_local() 9.2 Influential Observations What are influential points in a regression? They are points that How much would the regression line change if we deleted a the point and reran the regression? iver_mod1 &lt;- lm(povred ~ lnenp, data = iver) iver_loo_regs &lt;- # Start with the iver data iver %&gt;% # Group by country group_by(cty) %&gt;% # For each country # Run the regression without that country and store the coefficient values do({ tidy(lm(povred ~ lnenp, data = filter(iver, cty != .$cty))) %&gt;% select(term, estimate) }) %&gt;% # Reshape the dataset so that each coefficient is in a column spread(term, estimate) %&gt;% # Calculate how much these slopes differ from the one with all the data mutate(diff_slope = lnenp - coef(iver_mod1)[&quot;lnenp&quot;], abs_diff_slope = abs(diff_slope)) %&gt;% # Sort by the difference in slopes arrange(- abs_diff_slope) iver_loo_regs ## Source: local data frame [14 x 5] ## Groups: cty [14] ## ## cty `(Intercept)` lnenp diff_slope abs_diff_slope ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Switzerland 11.96341 35.84168 11.67038124 11.67038124 ## 2 United States 33.00804 16.74422 -7.42708138 7.42708138 ## 3 Belgium 26.39803 19.48071 -4.69058913 4.69058913 ## 4 Denmark 23.62155 21.91104 -2.26025292 2.26025292 ## 5 United Kingdom 18.40976 26.35067 2.17937476 2.17937476 ## 6 Canada 24.46021 22.32840 -1.84289747 1.84289747 ## 7 Finland 23.22400 22.44224 -1.72905264 1.72905264 ## 8 Italy 21.22614 25.50988 1.33858580 1.33858580 ## 9 France 19.31982 25.43240 1.26110272 1.26110272 ## 10 Norway 19.66602 24.78567 0.61437307 0.61437307 ## 11 Netherlands 21.06469 23.82630 -0.34499536 0.34499536 ## 12 Sweden 20.93623 24.04618 -0.12511217 0.12511217 ## 13 Australia 21.96619 24.07283 -0.09847140 0.09847140 ## 14 Germany 22.08429 24.10787 -0.06343038 0.06343038 Switzerland looks particularly problematic. The effect of lnenp on povred is 7. We could also plot these lines against the original data, to get a more intuitive sense of how much dropping one observation affects the regression slopes. ggplot() + geom_abline(data = iver_loo_regs, aes(intercept = `(Intercept)`, slope = lnenp)) + geom_point(data = iver, aes(x = lnenp, y = povred)) + xlab(&quot;log(Number of Effective parties)&quot;) + ylab(&quot;Poverty Reduction&quot;) + theme_local() Conveniently, in linear regression we can find which observations will have the largest influence on regression lines without rerunning the regression. Three statistics are of interest: Cook’s distance: a single number that summarizes how much dropping an observation changes all the regression coefficients. Studentized residual: The scaled residual of the observation. Hat score: How far the observation is from the center of the data. Use the broom function augment to add residuals and other diagnostic data to the original regression data. See help(influence) for functions to get these diagnostics using base R. iver_mod1_aug &lt;- augment(iver_mod1) %&gt;% mutate(cty = iver$cty) glimpse(iver_mod1_aug) ## Observations: 14 ## Variables: 10 ## $ povred &lt;dbl&gt; 42.16, 78.79, 29.90, 71.54, 69.08, 57.91, 46.90, 42... ## $ lnenp &lt;dbl&gt; 0.867100, 1.947340, 0.524729, 1.617410, 1.637050, 0... ## $ .fitted &lt;dbl&gt; 42.75835, 68.86916, 34.48280, 60.89432, 61.36904, 4... ## $ .se.fit &lt;dbl&gt; 6.692466, 10.833460, 10.047244, 7.413818, 7.595310,... ## $ .resid &lt;dbl&gt; -0.5983544, 9.9208441, -4.5828034, 10.6456800, 7.71... ## $ .hat &lt;dbl&gt; 0.11973167, 0.31374085, 0.26985506, 0.14693340, 0.1... ## $ .sigma &lt;dbl&gt; 20.20023, 19.87581, 20.13632, 19.89997, 20.04234, 1... ## $ .cooksd &lt;dbl&gt; 7.394393e-05, 8.763926e-02, 1.420959e-02, 3.058498e... ## $ .std.resid &lt;dbl&gt; -0.03297382, 0.61918852, -0.27729691, 0.59593691, 0... ## $ cty &lt;fctr&gt; Australia, Belgium, Canada, Denmark, Finland, Fran... Oddly, augment calculates the standardized residual, \\[ \\mathtt{.std.resid} = E&#39;_i = \\frac{E_i}{S_E \\sqrt{1 - h_i}} \\] which divides by the regression residual standard error, which is itself a function of the residual of \\(i\\), \\(S_E = \\sqrt{\\frac{\\sum_j E_j}{n - k - 1}}\\). What we want is the studentized residual which divides by the standard error of the regression calculated omitting observation \\(i\\): \\[ \\mathtt{.resid / .sigma * sqrt(1 - .hat)} = E^*_i = \\frac{E_i}{S_{E_{(i)}} \\sqrt{1 - h_i}} \\] where \\(S_{E_(i)}\\) is the standard error of the regression run without observation \\(i\\). It is called the Studentized residual, because it is distributed Student’s \\(t\\); the standardized residual is not. Add a new variable called .student.resid, which we can calculate using the residual (.resid), standard error of the regression that omits that observation (.sigma), and the hat value (.hat): iver_mod1_aug &lt;- iver_mod1_aug %&gt;% mutate(.student.resid = .resid / .sigma * sqrt(1 - .hat)) In base R, the function rstudent calculates the Studentized residuals, and rstandard calculates the standardized residuals: setNames(rstudent(iver_mod1), iver$cty) ## Australia Belgium Canada Denmark Finland ## -0.03157146 0.60253131 -0.26634629 0.57920127 0.41834057 ## France Germany Italy Netherlands Norway ## 0.64999275 -0.13942917 -0.69795162 0.78818851 0.97001936 ## Sweden Switzerland United Kingdom United States ## 0.69123825 -4.39123120 0.49519482 -1.57878326 setNames(rstandard(iver_mod1), iver$cty) ## Australia Belgium Canada Denmark Finland ## -0.03297382 0.61918852 -0.27729691 0.59593691 0.43350755 ## France Germany Italy Netherlands Norway ## 0.66622163 -0.14550050 -0.71336214 0.80092982 0.97241536 ## Sweden Switzerland United Kingdom United States ## 0.70678751 -2.76425506 0.51154375 -1.48890165 This scatterplot weights observations by their hat score. Points further from the mean of lnenp have higher hat scores. ggplot(data = iver_mod1_aug, aes(x = lnenp, y = povred)) + geom_point(mapping = aes(size = .hat)) + geom_smooth(method = &quot;lm&quot;) + theme_local() This scatterplot weights observations by their absolute Studentized residuals. Those observations furthest from the regression line and high hat values, have the highest residuals. ggplot(data = iver_mod1_aug, aes(x = lnenp, y = povred)) + geom_point(mapping = aes(size = abs(.student.resid))) + geom_smooth(method = &quot;lm&quot;) + theme_local() Cook’s distance is a measure of the overall influence of points on the regression; the point’s effect on all the parameters. This plot weights points by their Cook’s distance. We can see that the two points on the bottom (Switzerland and the US) have the highest Cook’s distance. ggplot(data = iver_mod1_aug, aes(x = lnenp, y = povred)) + geom_point(mapping = aes(size = .cooksd)) + geom_smooth(method = &quot;lm&quot;) + theme_local() A standard plot to assess outliers is the Influence Plot. The x-axis is hat scores, the y-axis is Studentized residuals. The points are sized by Cook’s Distance. Rules of thumb lines are drawn at -2 and 2 for Studentized residuals, and \\(\\bar{h} + 2 sd(h)\\) and \\(\\bar{h} + 3 sd(h)\\) for hat scores. ggplot() + geom_point(data = iver_mod1_aug, mapping = aes(x = .hat, y = .student.resid, size = .cooksd)) + # add labels to points, but only those points that are flagged as outliers # for at least one of the diagnostics considered here geom_text(data = filter(iver_mod1_aug, .cooksd &gt; 4 / iver_mod1$df.residual | abs(.student.resid) &gt; 2 | .hat &gt; mean(.hat) + 2 * sd(.hat)), mapping = aes(x = .hat, y = .student.resid, label = cty), hjust = 0, size = 4, colour = &quot;red&quot;) + geom_hline(data = data.frame(yintercept = c(-2, 0, 2)), mapping = aes(yintercept = yintercept), colour = &quot;blue&quot;, alpha = 0.4) + geom_vline(data = data.frame(xintercept = mean(iver_mod1_aug$.hat) + sd(iver_mod1_aug$.hat) * c(2, 3)), mapping = aes(xintercept = xintercept), colour = &quot;blue&quot;, alpha = 0.4) + xlab(&quot;hat&quot;) + ylab(&quot;Studentized residuals&quot;) + scale_size_continuous(&quot;Cook&#39;s Distance&quot;) + theme_local() Instead of a plot, we could find the id Observations with high Cook’s distance (greater than \\(4 / (n - k - 1)\\)): filter(iver_mod1_aug, .cooksd &gt; (4 / iver_mod1$df.residual)) %&gt;% select(cty, .cooksd, lnenp) ## cty .cooksd lnenp ## 1 Switzerland 0.745124 1.66013 Observations with high hat scores (greater than 2 standard deviations than the mean hat score): filter(iver_mod1_aug, .hat &gt; mean(.hat) + 2 * sd(.hat)) %&gt;% select(cty, .hat, lnenp) ## cty .hat lnenp ## 1 Belgium 0.3137408 1.94734 Observations with high Studentized residuals (+/- 2): filter(iver_mod1_aug, abs(.student.resid) &gt; 2) %&gt;% select(cty, .student.resid, lnenp) ## cty .student.resid lnenp ## 1 Switzerland -3.674577 1.66013 Or combine these, filter(iver_mod1_aug, abs(.student.resid) &gt; 2 | .hat &gt; mean(.hat) + 2 * sd(.hat) | .cooksd &gt; 4 / iver_mod1$df.residual) %&gt;% select(cty, .cooksd, .hat, .student.resid, lnenp) ## cty .cooksd .hat .student.resid lnenp ## 1 Belgium 0.08763926 0.3137408 0.4134926 1.94734 ## 2 Switzerland 0.74512398 0.1632012 -3.6745770 1.66013 Also see influencePlot in car, and influencePlot in simcf for other implementations of this plot type. One feature of those implementations is that they allow for the ability to identify the points on the plot. Now that we’ve identified Switzerland as a problematic point, the question is what to do about it. Checking the Switzerland data, it appears that it is correct and is not the result of data entry issues. In general, we should avoid dropping points. Perhaps the issue is that we have not accounted for different electoral systems. Let’s try including iver_mod2 &lt;- lm(povred ~ lnenp + elec_sys, data = iver) iver_mod2 ## ## Call: ## lm(formula = povred ~ lnenp + elec_sys, data = iver) ## ## Coefficients: ## (Intercept) lnenp elec_syspr elec_sysunam ## 17.658 26.693 9.221 -48.952 iver_mod2_aug &lt;- augment(iver_mod2) %&gt;% mutate(.student.resid = .resid / (.sigma * sqrt(1 - .hat)), cty = iver$cty) However, by including a categorical variable for electoral system in which Switzerland is the only country with a unanamity government, we are effectively dropping Switzerland from the regression. This means that we cannot calculate Cook’s distance or studentized residuals, or hat scores for Switzerland since a regression estimated without switzerland cannot estimate a coefficient for the unam category, since Switzerland is the only member of that category. filter(iver_mod2_aug, abs(.student.resid) &gt; 2 | .hat &gt; mean(.hat) + 2 * sd(.hat) | .cooksd &gt; 4 / iver_mod1$df.residual) %&gt;% select(cty, .cooksd, .hat, .student.resid, lnenp) ## cty .cooksd .hat .student.resid lnenp ## 1 Italy 0.1548996 0.1455977 -2.267585 1.413420 ## 2 Switzerland NaN 1.0000000 -Inf 1.660130 ## 3 United States 0.2749067 0.1978831 -2.690288 0.667829 But now that we’ve ignored Switzerland, both Italy and the United States seem to be influential. This is because now that there are fewer observations per group, in some sense it is easier for observations to be influentia. But, although the US and Italy have high studentized residuals, neither of them exceed the rule of thumb for Cooks distance. filter(iver_mod2_aug, .cooksd &gt; 4 / iver_mod1$df.residual) %&gt;% select(cty, .cooksd, .hat, .student.resid, lnenp) ## [1] cty .cooksd .hat .student.resid ## [5] lnenp ## &lt;0 rows&gt; (or 0-length row.names) ggplot() + geom_point(data = filter(iver_mod2_aug, .cooksd &lt; Inf), mapping = aes(x = .hat, y = .student.resid, size = .cooksd)) + # add labels to points, but only those points that are flagged as outliers # for at least one of the diagnostics considered here geom_text(data = filter(iver_mod2_aug, .cooksd &gt; 4 / iver_mod2$df.residual | abs(.student.resid) &gt; 2 | .hat &gt; mean(.hat) + 2 * sd(.hat), .cooksd &lt; Inf), mapping = aes(x = .hat, y = .student.resid, label = cty), hjust = 0, size = 4, colour = &quot;red&quot;) + geom_hline(data = data.frame(yintercept = c(-2, 0, 2)), mapping = aes(yintercept = yintercept), colour = &quot;blue&quot;, alpha = 0.4) + geom_vline(data = data.frame(xintercept = mean(iver_mod2_aug$.hat) + sd(iver_mod2_aug$.hat) * c(2, 3)), mapping = aes(xintercept = xintercept), colour = &quot;blue&quot;, alpha = 0.4) + xlab(&quot;hat&quot;) + ylab(&quot;Studentized residuals&quot;) + scale_size_continuous(&quot;Cook&#39;s Distance&quot;) + theme_local() Although there are still a few observations with large residuals, and with a small dataset, it is almost inevitable that some observations will have outsized influence on the results, from an outlier perspective the new model seems less problematic. However, we accomplished this at the cost of effectively ignoring Switzerland. The model is able to estimate how different Switzerland is from what would be predicted, but by including a dummy variable that is only 1 for Switzerland, we are treating Switzerland as sui generis. Also note, that although the category is called unam, it would be inappropriate to interpret it as the effect of that type of government since Switzerland is the only country in that category. We cannot separate the effect of the government type from all the other things that make Switzerland unique. It would be more appropriate to call it the “Switzerland” category in this instance. "],
["problems-with-errors.html", "Chapter 10 Problems with Errors 10.1 Prerequisites 10.2 Heteroskedasticity 10.3 Non-normal Errors 10.4 Clustered Standard Errors", " Chapter 10 Problems with Errors 10.1 Prerequisites In addition to tidyverse pacakges, this chaper uses the sandwich and lmtest packages which provide robust standard errors and tests that use robust standard errors. library(&quot;sandwich&quot;) library(&quot;lmtest&quot;) library(&quot;tidyverse&quot;) library(&quot;broom&quot;) library(&quot;modelr&quot;) 10.2 Heteroskedasticity \\[ \\hat{\\beta} = (\\mat{X}\\T \\mat{X})^{-1} \\mat{X}\\T \\vec{y} \\] and \\[ \\Var(\\vec{\\epsilon}) = \\mat{\\Sigma} \\] is the variance-covariance matrix of the errors. Assumptions 1-4 give the expression for the sampling variance, \\[ \\Var(\\hat{\\beta}) = (\\mat{X}&#39;\\mat{X})^{-1} \\mat{X}\\T \\mat{\\Sigma} \\mat{X} (\\mat{X}\\T \\mat{X})^{-1} \\] under homoskedasticity, \\[ \\mat{\\Sigma} = \\sigma^2 \\mat{I}, \\] so the the variance-covariance matrix simplifies to \\[ \\Var(\\hat{\\beta} | X) = \\sigma^2 (\\mat{X}\\T \\mat{X})^{-1} \\] Homoskedastic: \\[ \\Var(\\vec{\\epsilon} | \\mat{X}) = \\sigma^2 I = \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma^2 &amp; 0 &amp; 0 &amp; \\cdots &amp; \\sigma^2 \\end{bmatrix} \\] Heteroskedastic \\[ \\Var(\\vec{\\epsilon} | \\mat{X}) = \\sigma^2 I = \\begin{bmatrix} \\sigma_1^2 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma_2^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma^2 &amp; 0 &amp; 0 &amp; \\cdots &amp; \\sigma_n^2 \\end{bmatrix} \\] - independent, since the only non-zero values are on the diagonal, meaning that there are no correlated errors between observations - non-identical, since the values on the diagonal are not equal, e.g. \\(\\sigma_1^2 \\neq \\sigma_2^2\\). - \\(\\Cov(\\epsilon_i, \\epsilon_j | \\mat{X}) = 0\\) - \\(\\Var(\\epsilon_i | \\vec{x}_i) = \\sigma^2_i\\) tibble( x = runif(100, 0, 3), `Homoskedastic` = rnorm(length(x), mean = 0, sd = 1), `Heteroskedasticity` = rnorm(length(x), mean = 0, sd = x) ) %&gt;% gather(type, `error`, -x) %&gt;% ggplot(aes(x = x, y = error)) + geom_hline(yintercept = 0, colour = &quot;white&quot;, size = 2) + geom_point() + facet_wrap(~ type, nrow = 1) Consequences \\(\\hat{\\vec{\\beta}}\\) are still unbiased and consistent estimators of \\(\\vec{\\beta}\\) Standard error estimates are biased, likely downward, meaning that the estimated standard errors will be smaller than the true standard errors (too optimistic). Test statstics won’t be distributed \\(t\\) or \\(F\\) \\(\\alpha\\)-level tests will have Type I errors \\(\\neq \\alpha\\) Coverage of confidence intervals will not be correct. OLS is not BLUE Visual diagnostics Plot residuals vs. fitted values Spread-location plot. y: square root of absolute value of residuals x: fitted values loess trend curve Dealing with NCV Transform the dependent variable Model the heteroskedasticity using WLS Use an estimator of \\(\\Var(\\hat{\\beta} | \\mat{X})\\) that is robust to heteroskedasticity Admit we are using the wrong model and use a different model The standard way to “fix” robust heteroskedasticity is to use so-called “robust” standard errors, more formally called Heteroskedasticity Consistent (HC), and heteroskedasticity and Autocorrelation Consistent standard errors. HC and HAC errors are implemented in the R package sandwich. See Zeileis (2006) and Zeileis2004a for succint discussion of the estimators themselves and examples of their usage. With robust standard errors, the coefficients of the model are estimated using lm(). Then a HC or HAC variance-covariance matrix is computed which corrects for heteroskedasticity (and autocorrelation). 10.2.1 Example: Duncan’s Occupation Data mod &lt;- lm(prestige ~ income + education + type, data = car::Duncan) The classic OLS variance covariance matrix is, vcov(mod) ## (Intercept) income education typeprof typewc ## (Intercept) 13.7920916 -0.115636760 -0.257485549 14.0946963 7.9021988 ## income -0.1156368 0.007984369 -0.002924489 -0.1260105 -0.1090485 ## education -0.2574855 -0.002924489 0.012906986 -0.6166508 -0.3881200 ## typeprof 14.0946963 -0.126010517 -0.616650831 48.9021401 30.2138627 ## typewc 7.9021988 -0.109048528 -0.388119979 30.2138627 37.3171167 and the standard errors are the diagonal of this matrix sqrt(diag(vcov(mod))) ## (Intercept) income education typeprof typewc ## 3.7137705 0.0893553 0.1136089 6.9930065 6.1087737 Now, use vcovHC to estimate the “robust” variance covariance matrix vcovHC(mod) ## (Intercept) income education typeprof typewc ## (Intercept) 15.2419440 -0.233347755 -0.255838779 25.6093353 12.4984902 ## income -0.2333478 0.023224098 -0.009806392 -0.6101496 -0.4039528 ## education -0.2558388 -0.009806392 0.019805541 -0.7730126 -0.4128297 ## typeprof 25.6093353 -0.610149584 -0.773012579 90.8056216 52.2164675 ## typewc 12.4984902 -0.403952792 -0.412829731 52.2164675 42.2001856 and the robust standard errors are the diagonal of the matrix sqrt(diag(vcovHC(mod))) ## (Intercept) income education typeprof typewc ## 3.9040932 0.1523945 0.1407322 9.5291984 6.4961670 Note that the robust standard errors are larger than the classic standard errors; this is almost always the case. If you need to use the robust standard errors to calculate t-statistics or p-values. coeftest(mod, vcovHC(mod)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.18503 3.90409 -0.0474 0.962436 ## income 0.59755 0.15239 3.9210 0.000337 *** ## education 0.34532 0.14073 2.4537 0.018589 * ## typeprof 16.65751 9.52920 1.7480 0.088128 . ## typewc -14.66113 6.49617 -2.2569 0.029547 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 TODO An example that uses vcovHAC() to calculate heteroskedasticity and autocorrelation consistent standard errors. 10.2.1.1 WLS vs. White’s esimator WLS: different estimator for \\(\\beta\\): \\(\\hat{\\beta}_{WLS} \\neq \\hat{\\beta}_{OLS}\\) With known weights: efficient \\(\\hat{\\se}(\\hat{\\beta}_{WLS})\\) are consistent If weights aren’t known … then biased for both \\(\\hat{\\beta}\\) and standard errors. White’s esimator (heteroskedasticity consistent standard errors): uses OLS estimator for \\(\\beta\\) consistent for \\(\\Var(\\hat{\\beta})\\) for any form of heteroskedasticity relies on consistency and large samples, and for small samples the performance may be poor. 10.2.2 Notes An additional use of robust standard errors is to diagnose potential model fit problems. The OLS line is still the minimum squared error of the population regression, but large differences may suggest that it is a poor approximation. King and Roberts (2015) suggest a formal test for this using the variance-covariance matrix. Note that there are other functions that have options to input variance-covariance matrices along with the lm object in order to use robust standard errors with that test or routine. Heteroskedastic consistent standard errors can be used with MLE models (White 1982). However, this is More generally, robust standard errors can be controversial: King and Roberts (2015) suggest using them to diagnose model fit problems. 10.3 Non-normal Errors This really isn’t an issue. Normal errors only affect the standard errors, and only if the sample size is small. Once there is a reasonably large residual degrees of freedom (observations minus parameters), the CLT kicks in and it doesn’t matter. If you are concerned about non-normal error it may be worth asking: Is the functional form, especially the form of the outcome varaible, correct? Is the conditional expected value (\\(Y | X\\)) really the best estimand? That’s what the regression is giving you, but the conditional median or other quantile may be more appropriate for your purposes. To diagnose use a qq-plot of the residuals against a normal distribution. 10.4 Clustered Standard Errors Clustering is when observations within groups are correlated. Suppose there are \\(J\\) equal sized clusters with \\(m\\) units from each cluster, and total sample size of \\(J m\\). The mean of a vector \\(y\\) is \\(\\hat{y}\\), and its standard error is (Gelman and Hill 2007, 447) \\[ \\se(\\bar{y}) = \\sqrt{\\sigma^2_y / n + \\sigma^2_{\\alpha} / J}, \\] where \\(\\sigma^2_{\\alpha}\\) is the variance of the cluster level means, and \\(\\sigma^2_{y}\\) is variance of the intra-cluster residuals. This can also be rewritten as, \\[ \\se(\\bar{y}) = \\sqrt{\\sigma^2_{total} / J(1 + (m - 1)) ICC}, \\] where \\(\\sigma^2_{total} = \\sigma^2_{\\alpha} + \\simga^2_y\\), and the \\(ICC\\) is the intra-class correlation, which is the fraction of total variance accounted for by the between group variation, \\[ ICC = \\frac{\\sigma_{\\alpha}^2}{\\sigma^2_{\\alpha} + \\sigma^2_{y}} . \\] How does the standard error of \\(\\bar{y}\\) change with the value of ICC? When ICC is 0? When ICC is 1? References "],
["weighted-regression.html", "Chapter 11 Weighted Regression 11.1 Weighted Least Squares (WLS) 11.2 When should you use WLS? 11.3 Correcting for Known Heteroskedasticity 11.4 Sampling Weights 11.5 References", " Chapter 11 Weighted Regression 11.1 Weighted Least Squares (WLS) Ordinary least squares estimates coefficients by finding the coefficients that minimize the sum of squared errors, \\[ \\hat{\\vec\\beta}_{OLS} = \\argmin_{\\vec{b}} \\sum_{i = 1}^N (y_i - \\vec{x}\\T \\vec{b})^2 . \\] Note that the objective function treats all observations equally; an error in one is as good as any other. However, there are several situations where we care more about minimizing some errors more than others. The next situation will discuss the reasons to use WLS, but Weighted least squares (WLS) requires only a small change to the OLS objective function. Each observation is given a weight, \\(w_i\\), and the weighted sum of squared errors is minimized, \\[ \\begin{aligned}[t] \\hat{\\vec\\beta}_{WLS} = \\argmin_{\\vec{b}} \\sum_{i = 1}^N w_i (y_i - \\vec{x}\\T \\vec{b})^2 \\end{aligned} \\hat{\\beta}_{WLS} = \\argmin_{\\vec{b}} \\sum_{i = 1}^N w_i (y_i - \\vec{x}\\T \\vec{b})^2 . \\] The weights \\(w_i\\) are provided by the analyst and are not estimated. Note that OLS is a special case of WLS where \\(w_i = 1\\) for all the observations. In order to minimize the errors, WLS will have to fit the line closer to observations with higher weights You can estimate WLS by using the weights argument to rdoc(&quot;stats&quot;, &quot;lm&quot;). 11.2 When should you use WLS? The previous section showed what WLS is, but when should you use weighted regression? It depends on the purpose of your analysis: If you are estimating population descriptive statistics, then weighting is needed to ensure that the sample is representative of the population. If you are concerned with causal inference, then weighting is more nuanced. You may or may not need to weight, and it will often be unclear which is better. There are three reasons for weighting in causal inference (Solon, Haider, and Wooldridge 2015): To correct standard errors for heteroskedasticity Get consistent estimates by correcting for endogenous sampling Identify average partial effects when there is unmodeled heterogeneity in the effects. Heteroskedasticity: Estimate OLS and WLS. If the model is misspecified or there is endogenous selection, then OLS and WLS have different probability limits. The contrast between OLS and WLS estimates is a diagnostic for model misspecification or endogenous sampling. Always use robust standard errors. Endogenous sampling: If the sample weights vary exogenously instead of endogenously, then weighting may be harmful for precision. The OLS still specifies the conditional mean. Sampling is exogenous if the sampling probabilities are independent of the error - e.g. if they are only functions of the explanatory variables. If the probabilities are a function of the dependent variable, then they are endogenous. if sampling rate is endogenous, weight by inverse selection. use robust standard errors. if the sampling rate is exogenous, then OLS and WLS are consistent. Use OLS and WLS as test of model misspecification. Heterogeneous effects: Identifying average partial effects. WLS estimates the linear regression of the population, but this is not the same as the average partial effects. But that is because OLS does not estimate the average partial effect, but weights according to the variance in X. Angrist and Pischke (2009, 92) suggest weighting when “they make it more likely that the regression you are estimaing is close to the population target you are trying to estimate”. sampling weights: yes grouped data (sums, averages): yes heteroskedasticity: no (just use robust standard errors) WLS can be more efficient than OLS if variance model is correct If \\(E(e_i | X)\\) is a poor approximation or measurements are noisy, WLS has bad finite sample properties If the CEF is not linear, then OLS and WLS are both wrong, but OLS still interpretable as the minimum mean squared error approximation of the CEF. The WLS is also an approx of CEF, but approx is a function of the weights. Advice of (Cameron and Trivedi 2010, 113). There are two approaches to using weights. Census parameter: Reweight regression to try to get the population regression estimates. Control function approach: Assuming that \\(\\E(\\epsilon_i | \\vec{x}_i) = 0\\), then weights are not needed. WLS is consistent for any weights, and OLS is more efficient. This means if we control for all covariates relevant to sampling probabilities, there is no need to weight. This works as long as sampling probabilities are a function of \\(x\\) and not of \\(y\\). It seems that Cameron and Trivedi (2010) “census parameter” approach is what Angrist and Pischke (2009) interprets it as, but it supports the model based approach. Weights should be used for predictions and computing average MEs. (Cameron and Trivedi 2010, 114–15). Fox (2016, 461): inverse probability weights are different than weights in heteroskedasticity, and WLS cannot be used. It will give the wrong SEs but correct point estimates. Seems to suggest using bootstrapping to get standard errors instead (Fox 2016, 661, 666). 11.3 Correcting for Known Heteroskedasticity Most generally, heteroskedasticity is “unknown” and robust standard errors should be used. However, there are some cases where heteroskedasticity is “known”. For example: The outcome variable consists of measurements with a given measurement error - perhaps they are estimates themselves. The error of the output depends on input variables in known ways. For example, the sampling error of polls. Examples: \\(\\E(\\epsilon)_i^2 \\propto z_i^2\\) where \\(a\\) is some observated variable. Then \\(w_i = z_i\\). \\(\\E(\\epsilon)_^2\\) is an average of values. Then \\(\\sigma^2_i = \\omega^2 / n_i\\). In WLS, \\(w_i = 1 / \\sqrt{n_i}\\). \\(\\E(\\epsilon)_^2\\) is the sum of values. Then $^2_i = n_i ^2 $. In WLS, \\(w_i = \\sqrt{n_i}\\). If \\(p_i^{-1}\\) is the inverse-sampling probability weight, then weight by \\(w_i\\) Suppose that the heteroskedasticity is known up to a multiplicative constant, \\[ \\Var(\\varepsilon_i | \\mat{X}) = a_i \\sigma^2 , \\] where \\(a_i = a_i \\vec{x}_i\\T\\) is a positive and known function of \\(\\vec{x}_i\\). Define the weighting matrix, \\[ \\mat{W} = \\begin{bmatrix} 1 / \\sqrt{a_1} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 / \\sqrt{a_2} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; 0 \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 / \\sqrt{a_N} \\end{bmatrix}, \\] and run the regression, \\[ \\begin{aligned}[t] \\mat{W} y &amp;= \\mat{W} \\mat{X} \\vec{\\beta} + \\mat{W} \\vec\\varepsilon \\\\ \\vec{y}^* &amp;= \\mat{X}^* \\vec{\\beta} + \\vec{\\varepsilon}^* . \\end{aligned} \\] Run the regression of \\(\\vec{y}^*\\) on \\(\\mat{X}^*\\), and the Gauss-Markov assumptions are satisfied. Then using the usual OLS formula, \\[ \\hat{\\vec\\beta}_{WLS} = ((\\mat{X}^*)&#39; \\mat{X}^*) (\\mat{X}^*)&#39; \\vec{y}^* = (\\mat{X}&#39; \\mat{W}&#39; \\mat{W} \\mat{X})^{-1} \\mat{X}&#39; \\mat{W}&#39; \\mat{W} \\vec{y} . \\] 11.4 Sampling Weights Sampling weights are the inverse probabilities of selection that are used to weight a sample to be representative of population (as if were a random draw from the population). In this situation, whether to use sampling weights depends on whether you are calculating If you are calculating a descriptive statistic from the sample as an estimator of a population parameter, you need to use weights if sample weights are a function of \\(X\\) only, estimates are unbiased and more efficient without weighting if the sample weights are a function of \\(Y | X\\), then use the weights With fixed \\(X\\), regression does not require random sampling, so the sampling weights of the \\(X\\) are irrelevant. If the original unweighted data are homoskedastic, then sampling weights induces heteroskedasticity. Suppose the true model is, \\[ Y_i = \\vec{x}\\T \\vec{\\beta} + \\varepsilon_i \\] where \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\). Then the weighted model is, \\[ \\sqrt{w_i} Y_i = \\sqrt{w_i} \\vec{x}\\T \\vec{\\beta} + \\sqrt{w_i} \\varepsilon_i \\] and now \\(\\sqrt{w_i} \\varepsilon_i \\sim N(0, w_i \\sigma^2)\\). If the sampling weights are only a function of the \\(X\\), then controlling for \\(X\\) is sufficient. In fact, OLS is preferred to WLS, and will produce unbiased and efficient estimates. The choice between OLS and WLS is a choice between different distributions of \\(\\mat{X}\\). However, if the model is specified correctly the coefficients should be the same, regardless of the distribution of \\(\\mat{X}\\). Thus, if the estimates of OLS and WLS differ, then it is evidence that the model is misspecified. Winship and Radbill (1994) suggest using the method of Dumouchel and Duncan (1983) to test whether the OLS and WLS are difference. Estimate \\(E(Y) = \\mat{X} \\beta\\) Estimate \\(E(Y) = \\mat{X} \\beta + \\delta \\vec{w} + \\vec{\\gamma} \\vec{w} \\mat{X}\\), where all \\(X\\) Test regression 1 vs. regression 2 using an F test. If the F-test is significant, then the weights are not simply a function of \\(X\\). Either try to respecify the model or use WLS with robust standard errors. If the F-test is insignificant, then the weights are simply a function of \\(X\\). Use OLS. Modern survey often use complex multi-stage sampling designs. Like clustering generally, this will affect the standard errors of these regressions. Clustering by primary sampling units is a good approximation of the standard errors from multistage sampling. 11.5 References The WLS derivation can be found in Fox (2016, 304–6, 335–36, 461). Other textbook discussions: Angrist and Pischke (2009, 91–94), Angrist and Pischke (2014), [p. 202-203], Davidson and MacKinnon (2004, 261–62), Wooldridge (2012, 409–13). Solon, Haider, and Wooldridge (2015) is a good (and recent) overview with practical advice of when to weight and when not-to weight linear regressions. Also see the advice from the World Bank blog. See also Deaton (1997), Dumouchel and Duncan (1983), and Wissoker (1999). Gelman (2007b), in the context of post-stratification, proposes controlling for variables related to selection into the sample instead of using survey weights; also see the responses (Bell and Cohen 2007; Breidt and Opsomer 2007; Little 2007; Pfeffermann 2007), and rejoinder (Gelman 2007a) and blog post. Gelman’s approach is similar to that earlier suggested by Winship and Radbill (1994). For survey weighting, see the R package survey. References "],
["discrete-outcome-variables.html", "Chapter 12 Discrete Outcome Variables 12.1 Linear Probability Model 12.2 Logit Model", " Chapter 12 Discrete Outcome Variables The linear regression model is: \\[ Y_i = \\beta X_i + \\epsilon_i \\] But what if the outcome, \\(Y_i\\), is binary \\(Y_i \\in \\{0, 1}\\)? When OLS is used to estimate a binary outcome variable it is called the linear probability model). This chapter considers the properties of OLS estimation with a binary outcome variable, and introduces logit, a generalized linear model (GLM), for binary outcome variables. 12.1 Linear Probability Model 12.2 Logit Model The logit model is a model of the probability that \\(y = 1\\), \\[ \\Pr(y_i = 1) = \\logit^{-1}(\\vec{x}_i\\T \\vec{\\beta}) , \\] with the assumption that \\(y_i\\) are independent given these probabilities. The value \\(\\vec{x}_i\\T \\vec{\\beta}\\) is called the linear predictor. The logit function transforms values between 0 and 1 to be between \\(-\\infty\\) and \\(\\+infty\\), \\[ \\logit(p) = \\log\\left( \\frac{p}{1 - p} \\right) = \\log(p) - \\log(1 - p) . \\] The logistic function or inverse logit function transforms continuous values \\((-\\infty, +\\infty)\\) to be between 0 and 1, \\[ \\logit^{-1}(x) = \\frac{e^x}{1 + e^x} = \\frac{1}{1 + e^{-x}} . \\] The logistic function is the inverse of the logit function, so, \\[ \\logit^{-1}(\\logit(p)) = p , \\] and \\[ \\logit(\\logit^{-1}(x)) = x . \\] TODO Plots of logit and logistic functions. Thus, in logit functions To estimate the logit model, minimize the following objective function, \\[ \\begin{aligned}[t] \\hat{\\vec{beta}} &amp;= \\argmax_{\\vec{b}} \\sum_i (y_i \\log \\Pr(y_i = 1) + (1 - y_i) \\Pr(y_i = 0) \\\\ &amp;= \\argmax_{\\vec{b}} \\sum_i y_i \\log(logit^{-1}(\\vec{x}_i\\T \\vec{b})) + (1 - y_i) \\log(1 - logit^{-1}(\\vec{x}_i\\T \\vec{b})) . \\end{aligned} \\] While OLS minimizes squared errors, the logit model minimizes a different objective function, which minimizes the log difference between the obeserved value (\\(y_i\\)) and the predicted probability of that value, \\(\\Pr(y_i = 1)\\) or \\(\\Pr(y_i = 0)\\). There are several methods fo find the values of \\(\\hat{\\beta}\\), including iterated least squares (cite), but these details are handled by the software. While the coefficients of OLS are (usually) directly interpretable as the marginal effect of \\(x\\) on \\(E(y)\\), those of logit regression models are not. The marginal effect of a logit model is, \\[ \\frac{\\partial\\,\\Pr(y = 1)}{\\partial x_k} = \\frac{1}{1 + e^{- \\vec{x}\\T \\vec{\\beta}}} = \\beta_k \\frac{e^{-\\vec{\\beta} \\vec{x}}}{(1 + e^{- \\vec{x} \\vec{\\beta}})^2}\\T = \\beta_k \\Pr(y = 1) \\Pr(y = 0) . \\] In an OLS model, the marginal effect of \\(x\\) does not depend on the the specific values of \\(x_i\\). In a logit model it does. Interpretation short-cuts Calculate the Average Marginal Effects or the Marginal Effects at the Mean (or other Representative Values). CITES The divide by four rule Divide \\(\\beta\\) by four to get an upper bound on the marginal effect (in probability) of \\(x\\). The slope of the logistic curve is steepest at its center where \\(\\logit^{-1}(\\alpha + \\beta x) = 0.5\\) and \\(\\alpha + \\beta x = 0\\). The slope of this curve at this point is \\(\\beta e^0 / (1 + e^0)^2 = \\beta / 4\\) (Gelman and Hill 2007, 82). See [this blog post] (http://www.r-bloggers.com/divide-by-4-rule-for-marginal-effects/) for some examples. log-odds While the marginal effects of the logit model are non-linear on the probability scale, they are linear on the log-odds scale. A one unit change in \\(x\\) is associated with a \\(\\beta\\) change in the log-odds \\(\\log(\\Pr(y = 1) / Pr(y = 0))\\). TODO Inference: standard errors, confidence intervals, statistical significance. Predictions: For each observation, there is a predictive probability, \\[ \\tilde{p}_i = Pr(\\tilde{y}_i = 1) = \\logit^{-1}(\\tilde{\\vec{x}}_i\\T \\hat{\\vec{\\beta}}) . \\]n TODO Diagnostics: leverage, outliers, what isn’t relevant from OLS. References "],
["robust-regression.html", "Chapter 13 Robust Regression 13.1 Prerequites 13.2 Examples 13.3 Notes", " Chapter 13 Robust Regression Consider a more general formulation of linear regression estimation, \\[ \\hat{\\vec{\\beta}} = \\argmin_{b} \\sum_i f(y_i - \\vec{x}_i\\T \\vec{b}) = \\argmin_{b} \\sum_i f(\\hat{\\epsilon}_i) \\] where \\(f\\) is some function of the errors to be minimized. OLS minimizes the squared errors, \\[ f(\\hat{\\epsilon}_i) = \\hat{\\epsilon}_i^2 \\] However, other objective functions could be used. Some of these are less influenced by outliers, and these are called robust or resistent methods (these are slightly different properties of the estimator). One example is least median squares, which minimizes the absolute \\[ f(\\hat{\\epsilon}_i) = |\\hat{\\epsilon}| \\] 13.1 Prerequites Many forms of robust regression are available through the **MASS* library: library(&quot;MASS&quot;) library(&quot;tidyverse&quot;) library(&quot;modelr&quot;) 13.2 Examples Methods of dealing with outliers include robust and resistant regression methods. These methods will weigh observations far from the regression line less, which makes them less influential on the regression line. The functions lqs (least quantile squares) and rls (robust least squares) in MASS implement several roubst and resistant methods. These include least median squares: models &lt;- list() models[[&quot;lms&quot;]] &lt;- lqs(prestige ~ type + income + education, data = car::Duncan, method = &quot;lms&quot;) models[[&quot;lms&quot;]] ## Call: ## lqs.formula(formula = prestige ~ type + income + education, data = car::Duncan, ## method = &quot;lms&quot;) ## ## Coefficients: ## (Intercept) typeprof typewc income education ## -3.8565 -3.2120 -27.2139 0.7298 0.4953 ## ## Scale estimates 4.678 6.263 least trimmed squares, models[[&quot;lqs&quot;]] &lt;- lqs(prestige ~ type + income + education, data = car::Duncan, method = &quot;lts&quot;) models[[&quot;lqs&quot;]] ## Call: ## lqs.formula(formula = prestige ~ type + income + education, data = car::Duncan, ## method = &quot;lts&quot;) ## ## Coefficients: ## (Intercept) typeprof typewc income education ## -8.4158 -0.2083 -22.8542 0.7292 0.5000 ## ## Scale estimates 6.20 6.78 M-method with Huber weighting, models[[&quot;m_huber&quot;]] &lt;- rlm(prestige ~ type + income + education, data = car::Duncan, method = &quot;M&quot;, scale.est = &quot;Huber&quot;) models[[&quot;m_huber&quot;]] ## Call: ## rlm(formula = prestige ~ type + income + education, data = car::Duncan, ## scale.est = &quot;Huber&quot;, method = &quot;M&quot;) ## Converged in 8 iterations ## ## Coefficients: ## (Intercept) typeprof typewc income education ## -1.2114545 15.8670992 -14.7744856 0.6663791 0.3024461 ## ## Degrees of freedom: 45 total; 40 residual ## Scale estimate: 9.03 MM-methods with Huber weighting, models[[&quot;mm_huber&quot;]] &lt;- rlm(prestige ~ type + income + education, data = car::Duncan, method = &quot;MM&quot;, scale.est = &quot;Huber&quot;) models[[&quot;mm_huber&quot;]] ## Call: ## rlm(formula = prestige ~ type + income + education, data = car::Duncan, ## scale.est = &quot;Huber&quot;, method = &quot;MM&quot;) ## Converged in 7 iterations ## ## Coefficients: ## (Intercept) typeprof typewc income education ## -2.0191196 14.3261207 -15.6664693 0.7196171 0.2803928 ## ## Degrees of freedom: 45 total; 40 residual ## Scale estimate: 8.21 13.3 Notes See the Fox (2016) chapter on Robust Regression See Western (1995) for discussion of robust regression in the context of political science See the MASS package for implementations of many of these methods. References "],
["bootstrapping.html", "Chapter 14 Bootstrapping", " Chapter 14 Bootstrapping library(&quot;tidyverse&quot;) library(&quot;broom&quot;) library(&quot;modelr&quot;) ## ## Attaching package: &#39;modelr&#39; ## The following object is masked from &#39;package:broom&#39;: ## ## bootstrap Non-parametric bootstrapping estimates standard errors and confidence intervals by resampling the observations in the data. The modelr function in modelr implements simple non-parametric bootstrapping.8 It generates n bootstrap replicates. bsdata &lt;- modelr::bootstrap(car::Duncan, n = 1024) glimpse(bsdata) ## Observations: 1,024 ## Variables: 2 ## $ strap &lt;list&gt; [&lt;2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2,... ## $ .id &lt;chr&gt; &quot;0001&quot;, &quot;0002&quot;, &quot;0003&quot;, &quot;0004&quot;, &quot;0005&quot;, &quot;0006&quot;, &quot;0007&quot;, ... It returns a data frame with two columns an id, and list column, strap containing modelr objects. The resample objects consist of two elements: data, the data frame; idx, the indexes of the data in the sample. bsdata[[&quot;strap&quot;]][[1]] ## &lt;resample [45 x 4]&gt; 13, 34, 35, 20, 11, 10, 6, 7, 39, 32, ... Since the data object hasn’t changed it doesn’t take up any additional memory until subsets are created, allowing for the creation of lazy subsamples of a dataset. A resample object can be turned into a data frame with as.data.frame: as.data.frame(bsdata[[&quot;strap&quot;]][[1]]) ## type income education prestige ## physician prof 76 97 97 ## taxi.driver bc 9 19 10 ## truck.driver bc 21 15 13 ## banker prof 78 82 92 ## undertaker prof 42 74 57 ## engineer prof 72 86 88 ## minister prof 21 84 87 ## professor prof 64 93 93 ## shoe.shiner bc 9 17 3 ## coal.miner bc 7 7 15 ## machine.operator bc 21 20 24 ## physician.1 prof 76 97 97 ## coal.miner.1 bc 7 7 15 ## truck.driver.1 bc 21 15 13 ## dentist prof 80 100 90 ## streetcar.motorman bc 42 26 19 ## bartender bc 16 28 7 ## factory.owner prof 60 56 81 ## banker.1 prof 78 82 92 ## coal.miner.2 bc 7 7 15 ## welfare.worker prof 41 84 59 ## accountant prof 62 86 82 ## factory.owner.1 prof 60 56 81 ## insurance.agent wc 55 71 41 ## waiter bc 8 32 10 ## RR.engineer bc 81 28 67 ## electrician bc 47 39 53 ## RR.engineer.1 bc 81 28 67 ## minister.1 prof 21 84 87 ## author prof 55 90 76 ## plumber bc 44 25 29 ## physician.2 prof 76 97 97 ## RR.engineer.2 bc 81 28 67 ## welfare.worker.1 prof 41 84 59 ## carpenter bc 21 23 33 ## truck.driver.2 bc 21 15 13 ## dentist.1 prof 80 100 90 ## lawyer prof 76 98 89 ## engineer.1 prof 72 86 88 ## waiter.1 bc 8 32 10 ## conductor wc 76 34 38 ## accountant.1 prof 62 86 82 ## physician.3 prof 76 97 97 ## cook bc 14 22 16 ## shoe.shiner.1 bc 9 17 3 To generate standard errors for a statistic, estimate it on each bootstrap replicate. Suppose, we’d like to calculate robust standard errors for the regression coefficients in this regresion: mod &lt;- lm(prestige ~ type + income + education, data = car::Duncan) mod ## ## Call: ## lm(formula = prestige ~ type + income + education, data = car::Duncan) ## ## Coefficients: ## (Intercept) typeprof typewc income education ## -0.1850 16.6575 -14.6611 0.5975 0.3453 Since we are interested in the coefficients, we need to re-run the regression with lm, extract the coefficients to a data frame using tidy, and return it all as a large data frame. For one bootstrap replicate this looks like, lm(prestige ~ type + income + education, data = as.data.frame(bsdata$strap[[1]])) %&gt;% tidy() %&gt;% select(term, estimate) ## term estimate ## 1 (Intercept) 3.3880286 ## 2 typeprof 27.2634521 ## 3 typewc -13.2084788 ## 4 income 0.5971872 ## 5 education 0.1943750 Note that the coefficients on this regression are slightly different than those in the original regression. bs_coef &lt;- map_df(bsdata$strap, function(dat) { lm(prestige ~ type + income + education, data = dat) %&gt;% tidy() %&gt;% select(term, estimate) }) There are multiple methods to estimate standard errors and confidence intervals using the bootstrap replicate estimates. Two simple ones are are Use the standard deviation of the boostrap estimates as \\(\\hat{se}(\\hat{\\beta})\\) instead of those produces by OLS. The confidence intervals are generated using the OLS coefficient estimate and the bootstrap standard errors, \\(\\hat{\\beta}_{OLS} \\pm t_{df,\\alpha/2}^* \\hat{se}_{boot}(\\hat{\\beta})\\) Use the quantiles of the bootstrap estimates as the endpoints of the confidence interval. E.g. the 95% confidence interval uses the 2.5th and 97.5th quantiles of the bootstrap estimates. The first (standard error) method requires less bootstrap replicates. The quantile method allows for asymmetric confidence intervals, but is noisier (the 5th and 95th quantiles vary more by samples) and requires more bootstrap replicates to get an accurate estimate. The bootstrap standard error confidence intervals: alpha &lt;- 0.95 tstar &lt;- qt(1 - (1 - alpha / 2), df = mod$df.residual) bs_est_ci1 &lt;- bs_coef %&gt;% group_by(term) %&gt;% summarise(std.error = sd(estimate)) %&gt;% left_join(select(tidy(mod), term, estimate, std.error_ols = std.error), by = &quot;term&quot;) %&gt;% mutate( conf.low = estimate - tstar * std.error, conf.high = estimate + tstar * std.error ) select(bs_est_ci1, term, conf.low, estimate, conf.high) ## # A tibble: 5 × 4 ## term conf.low estimate conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.03240667 -0.1850278 -0.4024623 ## 2 education 0.35311336 0.3453193 0.3375253 ## 3 income 0.60631732 0.5975465 0.5887757 ## 4 typeprof 17.19855410 16.6575134 16.1164727 ## 5 typewc -14.25928277 -14.6611334 -15.0629840 select(bs_est_ci1, term, std.error, std.error_ols) ## # A tibble: 5 × 3 ## term std.error std.error_ols ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 3.4457925 3.7137705 ## 2 education 0.1235159 0.1136089 ## 3 income 0.1389956 0.0893553 ## 4 typeprof 8.5741408 6.9930065 ## 5 typewc 6.3683260 6.1087737 The quantile confidence intervals: alpha &lt;- 0.95 bs_coef %&gt;% group_by(term) %&gt;% summarise( conf.low = quantile(estimate, (1 - alpha) / 2), conf.high = quantile(estimate, 1 - (1 - alpha) / 2) ) %&gt;% left_join(select(tidy(mod), term, estimate), by = &quot;term&quot;) %&gt;% select(term, estimate) ## # A tibble: 5 × 2 ## term estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) -0.1850278 ## 2 education 0.3453193 ## 3 income 0.5975465 ## 4 typeprof 16.6575134 ## 5 typewc -14.6611334 See the boot package (and other cites TODO) for more sophisticated methods of generating standard errors and quantiles. The package resamplr includes more methods using resampler objects. The package boot implements many more bootstrap methods (Canty 2002). References "],
["prediction-and-model-comparison.html", "Chapter 15 Prediction and Model Comparison 15.1 Prerequisites 15.2 Measures of Prediction 15.3 Model Comparison 15.4 Example: Predicting the Price of Wine 15.5 Cross-Validation 15.6 Out of Sample Error 15.7 Analytic Covariance Methods 15.8 Further Resources", " Chapter 15 Prediction and Model Comparison 15.1 Prerequisites library(&quot;tidyverse&quot;) library(&quot;broom&quot;) library(&quot;modelr&quot;) ## ## Attaching package: &#39;modelr&#39; ## The following object is masked from &#39;package:broom&#39;: ## ## bootstrap library(&quot;stringr&quot;) 15.2 Measures of Prediction Ideally the prediction measure should be derived from the problem at hand; There is no uniformly correct measure of accuracy, so absent other costs the the analysis should include the costs of outcomes to the [analyst(https://en.wikipedia.org/wiki/Decision_theory). Categorical variables Accuracy: (True Positive) + (True Negative) / (all observations) Precision: (True Positives) / (Classifier Positives) Sensitivity (recall): (True Positive) / (All positives) Specificity: (True negative) / (Classifier negatives) F1 score which balances precision (TP / (TP + FP)) and recall (TP / (TP + FN)) as (precision * recall) / (precision + recall) Continuous Variables Root Mean Squared Error (RMSE): \\(\\frac{1}{m} \\sum_i (\\hat{y}_i - y_i)^2\\). This is weights large errors heavily since it uses “quadratic errors”. Mean Absolute Devision (MAD): \\(\\frac{1}{m} \\sum_i \\|\\hat{y}_i - y_i \\|\\). This is robust to large errors, but sensitive to the scale of the forecasts. Mean Absolute Percentage Error (MAPE): \\(\\frac{1}{m} \\sum_i \\| (\\hat{y}_i - y_i) / y_i \\|\\). 15.3 Model Comparison For comparing models in terms of prediction we want to compare them on their expected error on future data, not their in-sample error. It is easy to minimize in-sample error, use the every-observation-is-special model—have a predictor for each observation. However, that model will have no ability to predict future observations. The fundamental problem to estimating the expected error of the model is that we don’t have the future data to evaluate it. Even if we acquire new data that did not exist at the time of fitting the model, all that can be done is to retrospectively evaluate the model performance, perhaps giving a better estimate of the expected error of the model in the future. Yet, any errors of the model with respect to any future data will still be unknown. For example the errors of model based forecasts of the popular vote share, electoral votes, or winner of the U.S. presidential election of 2016 can be transparently evaluated since they can be made prior to the data, and short of access to a time machine, the models cannot overfit or peak on future data. After the election the models can be evaluated. However, at that point it is their expected error in the next election in 2020 which is of interest, and that is still unknown. There are two main approaches to estimating the prediction error cross validation: Split the data into test and training subsets. The model is fit on the training data, and predictions are made on the test data. This is often done repeatedly. covariance estimates: These are analytic estimates of the expected error, usually restricted to either linear models and/or only asymptotically valid. But since they do not require resampling, they are fast. So when do these approaches work? When do measures based on in-sample data extrapolate to non-sample errors? Like pretty much every statistical method, they work when the sample used to fit the model resembles the data on which which inference is being made. 15.4 Example: Predicting the Price of Wine The bordeaux dataset contains the prices of vintages of Bordeaux wines sold at auction in New York, as well as the the weather and rainfall for the years of the wine. This data was used by economist, Orley Ashenfelter, to show that the quality of a wine vintage can be as measured by its price, can largely be predicted by its age, and the weather (temperature and rainfall) of its vintage year. At the time, these prediction were not taken kindly by the wine conoisseurs.[^wine] # devtools::install_github(&quot;jrnold/datums&quot;) bordeaux &lt;- datums::bordeaux %&gt;% mutate(lprice = log(price / 100)) %&gt;% dplyr::filter(!is.na(price)) The data contains 27 prices of Bordeaux wines sold at auction in New York in 1993 for vintages from 1952 to 1980; the years 1952 and 1954 were excluded because they were rarely sold. Prices are measured as an index where 100 is the price of the 1961. Since prices are 7 The dataset also includes three predictors of the price of each vintage: time_sv: Time since the vintage, where 0 is 1983. wrain: Winter (October–March) rain hrain: Harvest (August–September) rain degrees: Average temperature in degrees centigrade from April to September in the vintage year. The first variable to consider is the age of the vintage and the price: ggplot(filter(bordeaux, !is.na(price), !is.na(vint)), aes(y = log(price), x = vint)) + geom_point() + geom_rug() + geom_smooth(method = &quot;lm&quot;) Ashenfleter, Ashmore, and Lalonde (1995) run two models. All models were estimated using OLS with log-price as the outcome variable. The predictors in the models were: vintage age vintage age, winter rain, harvest rain We’ll start by considering these models. Since we are running several models, we’ll define the model formulae in a list mods_f &lt;- list(lprice ~ time_sv, lprice ~ time_sv + wrain + hrain + degrees) and, run each model and store the results in a data frame as list column of lm objects: mods_res &lt;- tibble( model = seq_along(mods_f), formula = map_chr(mods_f, deparse), mod = map(mods_f, ~ lm(.x, data = bordeaux)) ) mods_res ## # A tibble: 2 × 3 ## model formula mod ## &lt;int&gt; &lt;chr&gt; &lt;list&gt; ## 1 1 lprice ~ time_sv &lt;S3: lm&gt; ## 2 2 lprice ~ time_sv + wrain + hrain + degrees &lt;S3: lm&gt; Now that we have these models, extract the coefficients into a data frame with the broom function tidy: mods_coefs &lt;- mods_res %&gt;% # Add column with the results of tidy for each model # conf.int = TRUE adds confidence intervals to the data mutate(tidy = map(mod, tidy, conf.int = TRUE)) %&gt;% # use unnest() to expand the data frame to one row for each row in the tidy # elements unnest(tidy, .drop = TRUE) glimpse(mods_coefs) ## Observations: 7 ## Variables: 9 ## $ model &lt;int&gt; 1, 1, 2, 2, 2, 2, 2 ## $ formula &lt;chr&gt; &quot;lprice ~ time_sv&quot;, &quot;lprice ~ time_sv&quot;, &quot;lprice ~ ti... ## $ term &lt;chr&gt; &quot;(Intercept)&quot;, &quot;time_sv&quot;, &quot;(Intercept)&quot;, &quot;time_sv&quot;, ... ## $ estimate &lt;dbl&gt; -2.025199170, 0.035429560, -12.145333577, 0.02384741... ## $ std.error &lt;dbl&gt; 0.2472286519, 0.0136624941, 1.6881026571, 0.00716671... ## $ statistic &lt;dbl&gt; -8.191604, 2.593199, -7.194665, 3.327521, 2.420525, ... ## $ p.value &lt;dbl&gt; 1.522111e-08, 1.566635e-02, 3.278791e-07, 3.055739e-... ## $ conf.low &lt;dbl&gt; -2.534376e+00, 7.291126e-03, -1.564624e+01, 8.984546... ## $ conf.high &lt;dbl&gt; -1.516022230, 0.063567993, -8.644422940, 0.038710280... walk(mods_res$mod, ~ print(summary(.x))) ## ## Call: ## lm(formula = .x, data = bordeaux) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.8545 -0.4788 -0.0718 0.4562 1.2457 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.02520 0.24723 -8.192 1.52e-08 *** ## time_sv 0.03543 0.01366 2.593 0.0157 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5745 on 25 degrees of freedom ## Multiple R-squared: 0.212, Adjusted R-squared: 0.1804 ## F-statistic: 6.725 on 1 and 25 DF, p-value: 0.01567 ## ## ## Call: ## lm(formula = .x, data = bordeaux) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.46027 -0.23864 0.01347 0.18600 0.53446 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.215e+01 1.688e+00 -7.195 3.28e-07 *** ## time_sv 2.385e-02 7.167e-03 3.328 0.00306 ** ## wrain 1.167e-03 4.820e-04 2.421 0.02420 * ## hrain -3.861e-03 8.075e-04 -4.781 8.97e-05 *** ## degrees 6.164e-01 9.518e-02 6.476 1.63e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2865 on 22 degrees of freedom ## Multiple R-squared: 0.8275, Adjusted R-squared: 0.7962 ## F-statistic: 26.39 on 4 and 22 DF, p-value: 4.058e-08 Likewise, extract model statistics such as, \\(R^2\\), adjusted \\(R^2\\), and \\(\\hat{\\sigma}\\): mods_glance &lt;- mutate(mods_res, .x = map(mod, glance)) %&gt;% unnest(.x, .drop = TRUE) mods_glance %&gt;% select(formula, r.squared, adj.r.squared, sigma) ## # A tibble: 2 × 4 ## formula r.squared adj.r.squared ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lprice ~ time_sv 0.2119700 0.1804488 ## 2 lprice ~ time_sv + wrain + hrain + degrees 0.8275278 0.7961692 ## # ... with 1 more variables: sigma &lt;dbl&gt; 15.5 Cross-Validation o compare predictive models, we want to compare how well it predicts (duh), which means estimating how well it will work on new data. The problem with this is that new data is just that, …, new. The trick is to resuse the sample data to get an estimate of how well the model will work on new data. This is done by fitting the model on a subset of the data, and predicting another subset of the data which was not used to fit the model; often this is done repeatedly. There are a variety of ways to do this, depending on the nature of the data and the predictive task. However, they all implictly assume that the sample of data that was used to fit the model is representative of future data. … model validation is a good, simple, broadly aplicable procedure that is rarely used in social research (Fox, p. 630) The simple idea of splitting a sample into two and then developing the hypothesis on the basis of one part and testing it on the remainder may perhaps be said to be one of the most seriously neglected ideas in statistics, if we measure the degree of neglect by the ratio of the number of cases in where a method could give help to the number where it was actually used. (Barnard 1974, p. 133, quoted in Fox, p. 630) 15.6 Out of Sample Error k &lt;- 20 f &lt;- map(seq_len(k), ~ as.formula(str_c(&quot;lprice ~ poly(time_sv, &quot;, .x, &quot;)&quot;))) names(f) &lt;- seq_len(k) mods_overfit &lt;- map(f, ~ lm(.x, data = bordeaux)) fits &lt;- map_df(mods_overfit, glance, .id = &quot;.id&quot;) fits %&gt;% select(.id, r.squared, adj.r.squared, df.residual) %&gt;% gather(stat, value, -.id) %&gt;% mutate(.id = as.integer(.id)) %&gt;% ggplot(aes(x = .id, y = value)) + geom_point() + geom_line() + facet_wrap(~ stat, ncol = 2, scales = &quot;free&quot;) The larger the polynomial, the more wiggly the line. library(&quot;modelr&quot;) invoke(gather_predictions, .x = c(list(data = bordeaux), mods_overfit)) %&gt;% ggplot(aes(x = vint)) + geom_point(aes(y = lprice)) + geom_line(aes(y = pred, group = model, colour = as.numeric(model))) Intuitively it seems that as we increase the flexibility of the model by increasing the number of variables the model is overfitting the data, but what does it actually mean to overfit? If we use \\(R^2\\) as the “measure of fit”, more variables always leads to better fit. Adjusted \\(R^2\\) does not increase, because the decrease in errors is offset by the increase in the degrees of freedom. However, there is little justification for the specific formula of \\(R^2\\). The problem with over-fitting is that the model starts to fit pecularities of the sample (errors) rather than the underlying model. We’ll never know the underlying model, but what we can see is if the model predicts new data. wine_mods_f &lt;- list( lprice ~ time_sv, lprice ~ poly(time_sv, 2), lprice ~ wrain, lprice ~ hrain, lprice ~ degrees, lprice ~ wrain + hrain + degrees, lprice ~ time_sv + wrain + hrain + degrees, lprice ~ time_sv + wrain * hrain * degrees, lprice ~ time_sv * (wrain + hrain + degrees), lprice ~ time_sv * wrain * hrain * degrees, lprice ~ time_sv * wrain * hrain * degrees + I(time_sv ^ 2) ) 15.6.1 Held-out data A common rule of thumb is to use 70% of the data for training, and 30% of the data for testing. In this case, let’s partition the data to use the first 70% of the observations as training data, and the remaining 30% of the data as testing. n_test &lt;- round(0.3 * nrow(bordeaux)) n_train &lt;- nrow(bordeaux) - n_test mod_train &lt;- lm(lprice ~ time_sv + wrain + hrain + degrees, data = head(bordeaux, n_train)) mod_train ## ## Call: ## lm(formula = lprice ~ time_sv + wrain + hrain + degrees, data = head(bordeaux, ## n_train)) ## ## Coefficients: ## (Intercept) time_sv wrain hrain degrees ## -1.080e+01 1.999e-02 9.712e-04 -4.461e-03 5.533e-01 # in-sample RMSE sqrt(mean(mod_train$residuals ^ 2)) ## [1] 0.2280059 The out-of-sample RMSE is higher than the in-sample RMSE. outsample &lt;- augment(mod_train, newdata = tail(bordeaux, n_test)) sqrt(mean((outsample$lprice - outsample$.fitted) ^ 2)) ## [1] 0.351573 This is common, but not necessarily the case. But note that this value is highly dependent on the subset of data used for testing. In some sense, we may choose as model that “overfits” the testing data. 15.6.2 Leave-One-Out Cross-Validation For each \\(i \\in 1, \\dots, n\\) Estimate the model using all observations but \\(i\\) Predict \\(\\hat{y}_i\\) using that model Calculate some measure(s) of model fit Let’s create a function to fit the model on a dataset dropping a single observation. i &lt;- 1 f &lt;- lprice ~ time_sv + wrain + hrain + degrees mod &lt;- lm(f, data = bordeaux) mod ## ## Call: ## lm(formula = f, data = bordeaux) ## ## Coefficients: ## (Intercept) time_sv wrain hrain degrees ## -12.145334 0.023847 0.001167 -0.003861 0.616392 mod_loo1 &lt;- lm(f, data = bordeaux[-i, ]) mod_loo1 ## ## Call: ## lm(formula = f, data = bordeaux[-i, ]) ## ## Coefficients: ## (Intercept) time_sv wrain hrain degrees ## -12.336504 0.025916 0.001188 -0.003832 0.625560 Unsurprisingly the fits of models fit with and without the first observation are similar, since they were fit using \\(n - 1\\) observations. Now use the model fit without the first observation to yhat_loo1 &lt;- predict(mod_loo1, newdata = bordeaux[1, ]) yhat_loo1 ## 1 ## -0.7262297 bordeaux$lprice[1] - yhat_loo1 ## 1 ## -0.2724503 Now we want to repeat this for all observations, fit_loo &lt;- function(i, formula, data) { # fit without i m &lt;- lm(formula, data = data[-i, ]) # predict i yhat &lt;- predict(m, newdata = data[i, ]) tibble(i = i, pred = yhat, resid = yhat - data[[&quot;lprice&quot;]][[i]]) } cv_loo &lt;- map_df(seq_len(nrow(bordeaux)), fit_loo, formula = f, data = bordeaux) sqrt(mean(cv_loo$resid ^ 2)) ## [1] 0.3230043 15.6.3 k-fold Cross-validation The most common approach is to to partition the data into k-folds, and use each fold once as the testing subset, where the model is fit on the other \\(k - 1\\) folds. [Cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics) is a non-parametric method that splits the data into training and test subsets. The data is fit on the training set and then used the predict the test set. The most common form of cross validation is 5- or 10-fold cross validation? Why this number of folds? See ISLR 5.1.4 “Bias-Variance Trade-Off for k-Fold Cross Validation” Larger number of folds requires more computation: a \\(k\\)-fold cross validation requires running the model \\(k\\) times A large number of folds has low bias because the result of \\((n - 1)\\) observations is approximately the same as the result of \\(n\\) observations But larger folds results in higher variance. LOOCV is averaging \\(n\\) models, but all those models will be similar, because they share almost all the same observations. But with fewer folds the models are fit with fewer overlapping observations and thus will have less correlated results. cv_fold5 &lt;- modelr::crossv_kfold(bordeaux, k = 5) glimpse(cv_fold5) ## Observations: 5 ## Variables: 3 ## $ train &lt;list&gt; [&lt;1952.00000, 1953.00000, 1955.00000, 1957.00000, 1958.... ## $ test &lt;list&gt; [&lt;1952.00000, 1953.00000, 1955.00000, 1957.00000, 1958.... ## $ .id &lt;chr&gt; &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot; cv_fold5$train[[1]] ## &lt;resample [21 x 7]&gt; 1, 2, 6, 7, 8, 9, 10, 11, 12, 13, ... cv_fold5$test[[1]] ## &lt;resample [6 x 7]&gt; 3, 4, 5, 15, 22, 26 as.data.frame(cv_fold5$train[[1]]) ## # A tibble: 21 × 7 ## vint price wrain degrees hrain time_sv lprice ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1952 36.83654 600 17.1167 160 31 -0.99868 ## 2 1953 63.48288 690 16.7333 80 30 -0.45440 ## 3 1959 65.83622 485 17.4833 187 24 -0.41800 ## 4 1960 13.87738 763 16.4167 290 23 -1.97491 ## 5 1961 100.00000 830 17.3333 38 22 0.00000 ## 6 1962 33.09725 697 16.3000 52 21 -1.10572 ## 7 1963 16.84730 608 15.7167 155 20 -1.78098 ## 8 1964 30.59450 402 17.2667 96 19 -1.18435 ## 9 1965 10.62522 602 15.3667 267 18 -2.24194 ## 10 1966 47.26359 819 16.5333 86 17 -0.74943 ## # ... with 11 more rows as.data.frame(cv_fold5$test[[1]]) ## # A tibble: 6 × 7 ## vint price wrain degrees hrain time_sv lprice ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1955 44.57665 502 17.1500 130 28 -0.80796 ## 2 1957 22.10735 420 16.1333 110 26 -1.50926 ## 3 1958 17.96850 582 16.4167 187 25 -1.71655 ## 4 1968 10.53803 610 16.2000 292 15 -2.25018 ## 5 1975 30.06886 572 16.9500 171 8 -1.20168 ## 6 1979 21.44669 717 16.1667 122 4 -1.53960 In k-fold cross-validation, each observation appears in the test-set in one fold, and is in the the training set in the remaining \\(k - 1\\) folds. The following plot shows this, cv_fold5_obs &lt;- cv_fold5 %&gt;% rowwise() %&gt;% do(tibble(vint = c(as.data.frame(.$train)$vint, as.data.frame(.$test)$vint), fold = .$.id, set = c(rep(&quot;train&quot;, dim(.$train)[1]), rep(&quot;test&quot;, dim(.$test)[1])))) ggplot(cv_fold5_obs, aes(y = factor(vint), x = fold, fill = set)) + geom_raster() + scale_fill_manual(values = c(&quot;train&quot; = &quot;black&quot;, &quot;test&quot; = &quot;gray&quot;)) + theme_minimal() + theme(axis.ticks = element_blank(), legend.position = &quot;bottom&quot;) + labs(x = &quot;vint&quot;, y = &quot;CV fold&quot;, fill = &quot;&quot;) Example with one fold fit_train &lt;- lm(f, data = as.data.frame(cv_fold5$train[[1]])) fit_train ## ## Call: ## lm(formula = f, data = as.data.frame(cv_fold5$train[[1]])) ## ## Coefficients: ## (Intercept) time_sv wrain hrain degrees ## -1.131e+01 3.114e-02 9.724e-04 -3.965e-03 5.662e-01 fit_test &lt;- augment(fit_train, newdata = as.data.frame(cv_fold5$test[[1]])) %&gt;% select(vint, lprice, .fitted) %&gt;% mutate(.resid = .fitted - lprice) fit_test ## vint lprice .fitted .resid ## 1 1955 -0.80796 -0.7542891 0.05367086 ## 2 1957 -1.50926 -1.3926673 0.11659270 ## 3 1958 -1.71655 -1.4111139 0.30543610 ## 4 1968 -2.25018 -2.2342730 0.01590699 ## 5 1975 -1.20168 -1.5847434 -0.38306339 ## 6 1979 -1.53960 -1.8175094 -0.27790943 # could also use modelr::rmse() mod_rmse &lt;- sqrt(mean(sum(fit_test$.resid ^ 2))) mod_rmse ## [1] 0.5779186 Let’s apply that to each row using map. That way we keep the results together in the same data frame. fit_train &lt;- lm(f, data = as.data.frame(cv_fold5$train[[1]])) fit_train ## ## Call: ## lm(formula = f, data = as.data.frame(cv_fold5$train[[1]])) ## ## Coefficients: ## (Intercept) time_sv wrain hrain degrees ## -1.131e+01 3.114e-02 9.724e-04 -3.965e-03 5.662e-01 fit_test &lt;- augment(fit_train, newdata = as.data.frame(cv_fold5$test[[1]])) %&gt;% select(vint, lprice, .fitted) %&gt;% mutate(.resid = .fitted - lprice) fit_test ## vint lprice .fitted .resid ## 1 1955 -0.80796 -0.7542891 0.05367086 ## 2 1957 -1.50926 -1.3926673 0.11659270 ## 3 1958 -1.71655 -1.4111139 0.30543610 ## 4 1968 -2.25018 -2.2342730 0.01590699 ## 5 1975 -1.20168 -1.5847434 -0.38306339 ## 6 1979 -1.53960 -1.8175094 -0.27790943 # could also use modelr::rmse() mod_rmse &lt;- sqrt(mean(sum(fit_test$.resid ^ 2))) mod_rmse ## [1] 0.5779186 fit_cv_fold5 &lt;- cv_fold5 %&gt;% mutate( # fit each row using train as the data fit_train = map(train, ~ lm(f, data = as.data.frame(.x))), # predicted values predict_test = map(train, ~ predict(fit_train, newdata = as.data.frame(.x))), # calculate out-of-sample RMSE rmse = map2_dbl(train, predict_test, ~ sqrt(mean((as.data.frame(.x)$lprice - .y) ^ 2)))) Let’s apply this to all the models. In the previous steps we kept a lot of extra information in order to understand how cross-validation worked. But really, all we care about is the average RMSE. mod_rmse_fold &lt;- function(f, train, test) { fit &lt;- lm(f, data = as.data.frame(train)) # sqrt(mean(residuals(fit, newdata = as.data.frame(test)) ^ 2)) test_data &lt;- as.data.frame(test) err &lt;- test_data$lprice - predict(fit, newdata = test_data) sqrt(mean(err ^ 2)) } mod_rmse_fold(f, cv_fold5$train[[1]], cv_fold5$test[[1]]) ## [1] 0.2359343 Now calculate this for a single model formula, averaging over all folds: mod_rmse &lt;- function(f, data) { map2_dbl(data$train, data$test, function(train, test) { mod_rmse_fold(f, train, test) }) %&gt;% mean() } Now we can apply this to all the models: mod_comparison &lt;- tibble(formula = wine_mods_f, .name = map_chr(formula, deparse), .id = seq_along(wine_mods_f), rmse = map_dbl(wine_mods_f, mod_rmse, data = cv_fold5)) mod_comparison %&gt;% select(.name, rmse) ## # A tibble: 11 × 2 ## .name rmse ## &lt;chr&gt; &lt;dbl&gt; ## 1 lprice ~ time_sv 0.6086064 ## 2 lprice ~ poly(time_sv, 2) 0.6100899 ## 3 lprice ~ wrain 0.6918896 ## 4 lprice ~ hrain 0.5925636 ## 5 lprice ~ degrees 0.5025091 ## 6 lprice ~ wrain + hrain + degrees 0.3733184 ## 7 lprice ~ time_sv + wrain + hrain + degrees 0.3250124 ## 8 lprice ~ time_sv + wrain * hrain * degrees 0.4522841 ## 9 lprice ~ time_sv * (wrain + hrain + degrees) 0.3926909 ## 10 lprice ~ time_sv * wrain * hrain * degrees 1.0599623 ## 11 lprice ~ time_sv * wrain * hrain * degrees + I(time_sv^2) 1.4167142 15.7 Analytic Covariance Methods For some models, notably linear regression, analytical approximations to the expected out of sample error can be made. Each of these approximations will make some slightly different assumptions to plug in some unknown values. Adjusted \\(R^2\\) is most often seen in statistical software and in papers (though often never interpreted). It intuitively penalizes a regression for a higher number of predictors; however apart from that intuitive appeal, and unlike the other measures presented here, there is no deeper justification for it (Fox, p. 609): \\[ \\mathrm{adj}\\,R^2 = 1 - \\frac{\\hat{\\sigma}^2}{\\Var{(Y)}} = 1 - \\frac{n - 1}{n - k - 1} \\times \\frac{\\sum (y_i - \\hat{y}_i^2)}{\\sum (y_i - \\bar{y})^2} \\] In linear regression, the LOO-CV MSE can be calculated analytically, and without simulation. It is (ISLR, p. 180): \\[ \\text{LOO-CV} = \\frac{1}{n} \\sum_{i = 1}^n {\\left(\\frac{y_i - \\hat{y}_i}{1 - h_i} \\right)}^2 = \\frac{1}{n} \\sum_{i = 1}^n {\\left(\\frac{\\hat{\\epsilon}_i}{1 - h_i} \\right)}^2 = \\frac{1}{n} \\times \\text{PRESS} \\] where PRESS is the predictive residual sum of squares, and \\(h_i\\) is the hat-value of observation \\(i\\) (Fox, p. 270, 289) \\[ h_i = \\mat{X}(\\mat{X}&#39; \\mat{X})^{-1} \\mat{X}&#39; \\] loocv &lt;- function(x) { mean((residuals(x) / (1 - hatvalues(x))) ^ 2) } An alternative approximation of the expected out-of-sample error is the generalized cross-validation criterion (GCV) is (Fox, 673) \\[ GCV = \\frac{n}{df_{res}^2} \\times RSS = \\frac{n}{(n - k - 1)^2} \\times \\sum \\hat{\\epsilon}^2_i \\] gcv &lt;- function(x) { err2 &lt;- residuals(x) ^ 2 n &lt;- length(err2) (n / x[[&quot;df.residual&quot;]] ^ 2) * sum(err2) } Since we generated the LOO data manually using a loop, create a LOO cross validation data frame using crossv_kfold: cv_loo &lt;- crossv_kfold(bordeaux, nrow(bordeaux)) mod_comparison &lt;- tibble(formula = wine_mods_f, .name = map_chr(formula, deparse), .id = seq_along(wine_mods_f), rmse_5fold = map_dbl(wine_mods_f, mod_rmse, data = cv_fold5), rmse_loo = map_dbl(wine_mods_f, mod_rmse, data = cv_loo), mod = map(formula, lm, data = bordeaux), gcv = sqrt(map_dbl(mod, gcv)), loocv = sqrt(map_dbl(mod, loocv)) ) mod_comparison %&gt;% select(.name, rmse_loo, rmse_5fold, gcv, loocv) ## # A tibble: 11 × 5 ## .name rmse_loo ## &lt;chr&gt; &lt;dbl&gt; ## 1 lprice ~ time_sv 0.5194518 ## 2 lprice ~ poly(time_sv, 2) 0.5312953 ## 3 lprice ~ wrain 0.5703790 ## 4 lprice ~ hrain 0.4635317 ## 5 lprice ~ degrees 0.4031840 ## 6 lprice ~ wrain + hrain + degrees 0.3092453 ## 7 lprice ~ time_sv + wrain + hrain + degrees 0.2767282 ## 8 lprice ~ time_sv + wrain * hrain * degrees 0.3199831 ## 9 lprice ~ time_sv * (wrain + hrain + degrees) 0.3128323 ## 10 lprice ~ time_sv * wrain * hrain * degrees 0.7493031 ## 11 lprice ~ time_sv * wrain * hrain * degrees + I(time_sv^2) 0.7287324 ## # ... with 3 more variables: rmse_5fold &lt;dbl&gt;, gcv &lt;dbl&gt;, loocv &lt;dbl&gt; Other measures that are also equivalent to some form of an estimate of the out-of-sample error are the AIC and BIC. 15.8 Further Resources Fox (2016) Chapter 22: “Model Selection, Averaging, and Validation”, p. 669. James et al. (2013), Ch. 5. “Resampling Methods” Hastie, Tibshirani, and Friedman (2009), Ch. 7. “Model Assessment and Selection” Rob Hyndman’s blog posts on cross validation, time series cross validation, and leave-one-out CV in linear models. References "],
["miscellaneous-regression-stuff.html", "Chapter 16 Miscellaneous Regression Stuff 16.1 Anscombe quartet 16.2 Correlation Plots", " Chapter 16 Miscellaneous Regression Stuff library(&quot;tidyverse&quot;) library(&quot;stringr&quot;) 16.1 Anscombe quartet anscombe_tidy &lt;- anscombe %&gt;% mutate(obs = row_number()) %&gt;% gather(variable_dataset, value, -obs) %&gt;% separate(variable_dataset, c(&quot;variable&quot;, &quot;dataset&quot;), sep = c(1)) %&gt;% spread(variable, value) %&gt;% arrange(dataset, obs) What are summary statistics of the four Anscombe datasets? ggplot(anscombe_tidy, aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + facet_wrap(~ dataset, ncol = 2) What are the mean, standard deviation, correlation coefficient, and regression coefficients of each line? anscombe_summ &lt;- anscombe_tidy %&gt;% group_by(dataset) %&gt;% summarise( mean_x = mean(x), mean_y = mean(y), sd_x = sd(x), sd_y = sd(y), cov = cov(x, y), cor = cor(x, y), coefs = list(coef(lm(y ~ x, data = .))) ) %&gt;% mutate( intercept = map_dbl(coefs, &quot;(Intercept)&quot;), slope = map_dbl(coefs, &quot;x&quot;) ) %&gt;% select(-coefs) WUT? They are the same? But they look so different. Of course that was the point … Since this all revolves around covariance, lets calculate the values of \\(x_i - \\bar{x}\\), \\(y_i - \\bar{y}\\), and \\((x_i - \\bar{x}) (y_i - \\bar{y})\\) for each obs for each variable. anscombe_tidy &lt;- anscombe_tidy %&gt;% group_by(dataset) %&gt;% mutate(mean_x = mean(x), diff_mean_x = x - mean_x, mean_y = mean(y), diff_mean_y = y - mean_y, diff_mean_xy = diff_mean_x * diff_mean_y, quadrant = if_else( diff_mean_x &gt; 0, if_else(diff_mean_y &gt; 0, 1, 2), if_else(diff_mean_y &gt; 0, 4, 3), )) ggplot(anscombe_tidy, aes(x = x, y = y, size = abs(diff_mean_xy), colour = factor(sign(diff_mean_xy)))) + geom_point() + geom_hline(data = anscombe_summ, aes(yintercept = mean_y)) + geom_vline(data = anscombe_summ, aes(xintercept = mean_x)) + facet_wrap(~ dataset, ncol = 2) ggplot(anscombe_tidy, aes(x = x, y = y, colour = factor(sign(diff_mean_xy)))) + geom_point() + geom_segment(mapping = aes(xend = mean_x, yend = mean_y)) + geom_hline(data = anscombe_summ, aes(yintercept = mean_y)) + geom_vline(data = anscombe_summ, aes(xintercept = mean_x)) + facet_wrap(~ dataset, ncol = 2) ggplot(anscombe_tidy, aes(x = x, y = y, colour = factor(sign(diff_mean_xy)))) + geom_point() + geom_segment(mapping = aes(xend = x, yend = mean_y)) + geom_segment(mapping = aes(xend = mean_x, yend = y)) + geom_hline(data = anscombe_summ, aes(yintercept = mean_y)) + geom_vline(data = anscombe_summ, aes(xintercept = mean_x)) + facet_wrap(~ dataset, ncol = 2) 16.2 Correlation Plots "],
["rs-forumula-syntax.html", "Chapter 17 R’s Forumula Syntax 17.1 Setup 17.2 Introduction to Formula Objects 17.3 Programming with Formulas", " Chapter 17 R’s Forumula Syntax These notes build off of the topics discussed in the chapter Many Models in R for Data Science. It uses the functionals (map() function) for iteration, string functions, and list columns in data frames. 17.1 Setup data(&quot;Duncan&quot;, package = &quot;car&quot;) library(&quot;tidyverse&quot;) library(&quot;stringr&quot;) library(&quot;broom&quot;) 17.2 Introduction to Formula Objects Many R functions, especially those for statistical models such as lm(), use a convenient syntax to compactly specify the outcome and predictor variables. This format is described in more detail in the stats. Formula objects in R are created with the tilde operator (~), and can be either one- or two-sided. prestige ~ type + income * education ~ prestige + type + income Formula are used in a variety of different contexts in R, e.g. ggplot2, but are commonly associated with statistical modeling functions as a compact way to specify the outcome and design matrix. lm(formula = prestige ~ type + income * education, data = Duncan) estimates this, \\[ \\mathtt{prestige} = \\beta_0 + \\beta_1 \\mathtt{income} + \\beta_2 \\mathtt{education} + \\beta_3 \\mathtt{income} \\times \\mathtt{education} . \\] Note that in a formula, the operators + and * do not refer to addition or multiplication. Instead adding + adds terms to the regression, and * creates interactions. Symbol Example Meaning + +x Include \\(x\\) - -x Exclude \\(x\\) : x : z Include \\(x z\\) as a predictor * x * z Include \\(x\\), \\(z\\), and their interaction \\(x z\\) as predictors ^ (x + w + z) ^ 3 Include variables and interactions up to three way: \\(x\\), \\(z\\), \\(w\\), \\(xz\\), \\(xw\\), \\(zw\\), \\(xzw\\). I I(x ^ 2) as is: include a new variable with \\(x ^ 2\\). 1 -1 Intercept; Use -1 to delete, and +1 to include Formula Equation y ~ x + y + z \\(y = \\beta_0 + beta_1 x + beta_2 y + beta_3 z\\) y ~ x + y - 1 \\(y = \\beta_1 x + \\beta_2 y\\) y ~ 1 \\(y = \\beta_0\\) y ~ x:z \\(y = \\beta_0 + \\beta_1 xz\\) y ~ x*z \\(y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 xz\\) y ~ x*z - x \\(y = \\beta_0 + \\beta_1 z + \\beta_2 xz\\) y ~ (x + z)^2 \\(y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3\\) y ~ (x + z + w)^2 \\(y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 w + \\beta_4 w + \\beta_5 xz + \\beta_6 xw + \\beta_7 zw\\) y ~ (x + z + w)^2 \\(y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 w + \\beta_4 w + \\beta_5 xz + \\beta_6 xw + \\beta_7 zw + \\beta_8 xzw\\) y ~ x + I(x ^ 2) \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\) y ~ poly(x, 2) \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\) y ~ I(x * z) \\(y = \\beta_0 + \\beta_1 xz\\) y ~ I(x + z) + I(x - z) \\(y = \\beta_0 + \\beta_1 (x + z) + \\beta_2 (x - z)\\) y ~ log(x) \\(y = \\beta_0 + \\beta_1 \\log(x)\\) y ~ x / z \\(y = \\beta_0 + \\beta_1 (x / z)\\) y ~ x + 0 \\(y = \\beta_1 x\\) Formula objects provide a convenient means of specifying the statistical model and also generating temporary variables that we only needed in the model. For example, if we want to include \\(\\log(x)\\), \\(z\\), their interaction, and the square of \\(z\\), we could write y ~ log(x) * y + I(y ^ 2) instead of of having to create new columns with the log, square, and interaction variables before running the regression. This becomes especially useful if you need to include large polynomials, splines, interaction, or similarly complicated functional forms. Warning: Often you will want to include polynomials in a regression. However, ^ already has a special meaning in R’s syntax, so you cannot use x ^ 2. There are two ways to specify polynomials. - Use the as is operator, I(). For example, I(x ^ 2) includes \\(x^2\\) in the regression. - Use the poly function to generate polynomials. For example, I(x ^ 2) will include \\(x\\) and \\(x^2\\) in the regression. This can be more convenient and clear than writing x + I(x ^ 2)$. Warning: R’s formula syntax is flexible and includes more features than what was covered in this section. What was described in this section were features that lm() uses. Other functions may have more features or the parts will have different meanings. However, they will be mostly be similar to what was described here. An example of a function that has slightly different syntax for its formula is lme4 which adds notation for random and fixed effects. Some functions like stats use a formula syntax even though they aren’t statistical modeling functions. The package Formula provides an even more powerful Formula syntax than that included with base R. If you have to write a function or package that uses a formula, use that package. Inspired by this handout from Richard Hahn and the handout form the NPS. 17.3 Programming with Formulas In these examples, we’ll use the car dataset in the car package. Prestige &lt;- car::Prestige Each observation is an occupation, and contains the prestige score of the occupation from a survey, and the average education, income, percentage of women, and type of occupation. glimpse(Prestige) ## Observations: 102 ## Variables: 6 ## $ education &lt;dbl&gt; 13.11, 12.26, 12.77, 11.42, 14.62, 15.64, 15.09, 15.... ## $ income &lt;int&gt; 12351, 25879, 9271, 8865, 8403, 11030, 8258, 14163, ... ## $ women &lt;dbl&gt; 11.16, 4.02, 15.70, 9.11, 11.68, 5.13, 25.65, 2.69, ... ## $ prestige &lt;dbl&gt; 68.8, 69.1, 63.4, 56.8, 73.5, 77.6, 72.6, 78.1, 73.1... ## $ census &lt;int&gt; 1113, 1130, 1171, 1175, 2111, 2113, 2133, 2141, 2143... ## $ type &lt;fctr&gt; prof, prof, prof, prof, prof, prof, prof, prof, pro... We will run several regressions with prestige as the outcome variable, and the over variables are explanatory variables. In R, the formulas are objects (of class &quot;formula&quot;). That means we can program on them, and importantly, perhaps avoid excessive copying and pasting if we run multiple models. A formula object is created with the ~ operator: f &lt;- prestige ~ type + education class(f) ## [1] &quot;formula&quot; f ## prestige ~ type + education A useful function for working with formulas is update. The update function allows you to easily update a formula object # the . is replaced by the original formula values update(f, . ~ income) ## prestige ~ income update(f, income ~ .) ## income ~ type + education update(f, . ~ . + type + women) ## prestige ~ type + education + women Also note that many types of models have update method which will rerun the model with a new formula. Sometimes this can help computational time if the model is able to reuse some previous results or data. You can also create formula objects from a character vector as.formula(&quot;prestige ~ income + education&quot;) ## prestige ~ income + education This means that you can create model formula objects programmatically which is useful if you are running many models, or simply to keep the logic of your code clear. xvars &lt;- c(&quot;type&quot;, &quot;income&quot;, &quot;education&quot;) as.formula(str_c(&quot;prestige&quot;, &quot;~&quot;, str_c(xvars, collapse = &quot; + &quot;))) ## prestige ~ type + income + education Often you will need to run multiple models. Since most often the only thing that changes between models is the formula (the outcome or response variables), storing the formula in a list, and then running the models by iterating through the list is a clean strategy for estimating your models. xvar_list &lt;- list(c(&quot;type&quot;), c(&quot;income&quot;), c(&quot;education&quot;), c(&quot;type&quot;, &quot;income&quot;), c(&quot;type&quot;, &quot;income&quot;, &quot;education&quot;)) formulae &lt;- vector(&quot;list&quot;, length(xvar_list)) for (i in seq_along(xvar_list)) { formulae[[i]] &lt;- str_c(&quot;prestige ~ &quot;, str_c(xvar_list[[i]], collapse = &quot; + &quot;)) } formulae ## [[1]] ## [1] &quot;prestige ~ type&quot; ## ## [[2]] ## [1] &quot;prestige ~ income&quot; ## ## [[3]] ## [1] &quot;prestige ~ education&quot; ## ## [[4]] ## [1] &quot;prestige ~ type + income&quot; ## ## [[5]] ## [1] &quot;prestige ~ type + income + education&quot; Alternatively, create this list of formula objects with a functional, make_mod_f &lt;- function(x) { str_c(&quot;prestige ~ &quot;, str_c(x, collapse = &quot; + &quot;)) } formulae &lt;- map(xvar_list, make_mod_f) Now that we have a list with formula objects for each model that we want to run, we can loop over the list and run each model. But first, we need to create a function that runs a single model that returns a data frame with a single row and a column named mod, which is a list column with an lm object containing the fitted model. In this function, I set model = FALSE because by default an lm model stores the data used to estimate. This is convenient, but if you are estimating many models, this can consume much memory. run_reg &lt;- function(f) { mod &lt;- lm(f, data = Prestige, model = FALSE) data_frame(mod = list(mod)) } ret &lt;- run_reg(formulae[[1]]) ret[[&quot;mod&quot;]][[1]] ## ## Call: ## lm(formula = f, data = Prestige, model = FALSE) ## ## Coefficients: ## (Intercept) typeprof typewc ## 35.527 32.321 6.716 Since each data frame has only one row, it is not particularly useful on its own, but it will be convenient to keep all the models in a data frame. Now, run run_reg for each formula in formulae using map_df to return the results as a data frame with a list column, mod, containing the lm objects. prestige_fits &lt;- map_df(formulae, run_reg, .id = &quot;.id&quot;) prestige_fits ## # A tibble: 5 × 2 ## .id mod ## &lt;chr&gt; &lt;list&gt; ## 1 1 &lt;S3: lm&gt; ## 2 2 &lt;S3: lm&gt; ## 3 3 &lt;S3: lm&gt; ## 4 4 &lt;S3: lm&gt; ## 5 5 &lt;S3: lm&gt; To extract the original formulas and add them to the data set, run formula() on each lm object using map, and then convert it to a character string using deparse: prestige_fits &lt;- prestige_fits %&gt;% mutate(formula = map_chr(mod, ~ deparse(formula(.x)))) prestige_fits$formula ## [1] &quot;prestige ~ type&quot; ## [2] &quot;prestige ~ income&quot; ## [3] &quot;prestige ~ education&quot; ## [4] &quot;prestige ~ type + income&quot; ## [5] &quot;prestige ~ type + income + education&quot; Get a data frame of the coefficients for all models using tidy and tidyr: mutate(prestige_fits, x = map(mod, tidy)) %&gt;% unnest(x) ## # A tibble: 16 × 7 ## .id formula term estimate ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 prestige ~ type (Intercept) 35.527272727 ## 2 1 prestige ~ type typeprof 32.321114370 ## 3 1 prestige ~ type typewc 6.716205534 ## 4 2 prestige ~ income (Intercept) 27.141176368 ## 5 2 prestige ~ income income 0.002896799 ## 6 3 prestige ~ education (Intercept) -10.731981968 ## 7 3 prestige ~ education education 5.360877731 ## 8 4 prestige ~ type + income (Intercept) 27.997056941 ## 9 4 prestige ~ type + income typeprof 25.055473883 ## 10 4 prestige ~ type + income typewc 7.167155112 ## 11 4 prestige ~ type + income income 0.001401196 ## 12 5 prestige ~ type + income + education (Intercept) -0.622929165 ## 13 5 prestige ~ type + income + education typeprof 6.038970651 ## 14 5 prestige ~ type + income + education typewc -2.737230718 ## 15 5 prestige ~ type + income + education income 0.001013193 ## 16 5 prestige ~ type + income + education education 3.673166052 ## # ... with 3 more variables: std.error &lt;dbl&gt;, statistic &lt;dbl&gt;, ## # p.value &lt;dbl&gt; Get a data frame of model summary statistics for all models using glance, mutate(prestige_fits, x = map(mod, glance)) %&gt;% unnest(x) ## # A tibble: 5 × 14 ## .id mod formula r.squared ## &lt;chr&gt; &lt;list&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 &lt;S3: lm&gt; prestige ~ type 0.6976287 ## 2 2 &lt;S3: lm&gt; prestige ~ income 0.5110901 ## 3 3 &lt;S3: lm&gt; prestige ~ education 0.7228007 ## 4 4 &lt;S3: lm&gt; prestige ~ type + income 0.7764569 ## 5 5 &lt;S3: lm&gt; prestige ~ type + income + education 0.8348574 ## # ... with 10 more variables: adj.r.squared &lt;dbl&gt;, sigma &lt;dbl&gt;, ## # statistic &lt;dbl&gt;, p.value &lt;dbl&gt;, df &lt;int&gt;, logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, ## # BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt; "],
["duncan-occupational-prestige.html", "Chapter 18 Duncan Occupational Prestige 18.1 Setup 18.2 Coefficients, Standard errors 18.3 Residuals, Fitted Values, 18.4 Broom 18.5 Plotting Fitted Regression Results", " Chapter 18 Duncan Occupational Prestige 18.1 Setup library(&quot;tidyverse&quot;) library(&quot;broom&quot;) This example makes use of the Duncan Occpuational prestige included in the car package. This is data from a classic sociology paper and contains data on the prestige and other characteristics of 45 U.S. occupations in 1950. data(&quot;Duncan&quot;, package = &quot;car&quot;) The dataset Duncan contains four variables: type, income, education, and prestige, glimpse(Duncan) ## Observations: 45 ## Variables: 4 ## $ type &lt;fctr&gt; prof, prof, prof, prof, prof, prof, prof, prof, wc,... ## $ income &lt;int&gt; 62, 72, 75, 55, 64, 21, 64, 80, 67, 72, 42, 76, 76, ... ## $ education &lt;int&gt; 86, 76, 92, 90, 86, 84, 93, 100, 87, 86, 74, 98, 97,... ## $ prestige &lt;int&gt; 82, 83, 90, 76, 90, 87, 93, 90, 52, 88, 57, 89, 97, ... You run a regression in R using the function lm. This runs a linear regression of occupational prestige on income, lm(prestige ~ income, data = Duncan) ## ## Call: ## lm(formula = prestige ~ income, data = Duncan) ## ## Coefficients: ## (Intercept) income ## 2.457 1.080 This estimates the linear regression \\[ \\mathtt{prestige} = \\beta_0 + \\beta_1 \\mathtt{income} \\] In R, \\(\\beta_0\\) is named (Intercept), and the other coefficients are named after the associated predictor. The function lm returns an lm object that can be used in future computations. Instead of printing the regression result to the screen, save it to the variable mod1, mod1 &lt;- lm(prestige ~ income, data = Duncan) We can print this object print(mod1) ## ## Call: ## lm(formula = prestige ~ income, data = Duncan) ## ## Coefficients: ## (Intercept) income ## 2.457 1.080 Somewhat counterintuitively, the summary function returns more information about a regression, summary(mod1) ## ## Call: ## lm(formula = prestige ~ income, data = Duncan) ## ## Residuals: ## Min 1Q Median 3Q Max ## -46.566 -9.421 0.257 9.167 61.855 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.4566 5.1901 0.473 0.638 ## income 1.0804 0.1074 10.062 7.14e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.4 on 43 degrees of freedom ## Multiple R-squared: 0.7019, Adjusted R-squared: 0.695 ## F-statistic: 101.3 on 1 and 43 DF, p-value: 7.144e-13 The summary function also returns an object that we can use later, summary_mod1 &lt;- summary(mod1) summary_mod1 ## ## Call: ## lm(formula = prestige ~ income, data = Duncan) ## ## Residuals: ## Min 1Q Median 3Q Max ## -46.566 -9.421 0.257 9.167 61.855 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.4566 5.1901 0.473 0.638 ## income 1.0804 0.1074 10.062 7.14e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.4 on 43 degrees of freedom ## Multiple R-squared: 0.7019, Adjusted R-squared: 0.695 ## F-statistic: 101.3 on 1 and 43 DF, p-value: 7.144e-13 Now lets estimate a multiple linear regression, mod2 &lt;- lm(prestige ~ income + education + type, data = Duncan) mod2 ## ## Call: ## lm(formula = prestige ~ income + education + type, data = Duncan) ## ## Coefficients: ## (Intercept) income education typeprof typewc ## -0.1850 0.5975 0.3453 16.6575 -14.6611 18.2 Coefficients, Standard errors Coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\): coef(mod2) ## (Intercept) income education typeprof typewc ## -0.1850278 0.5975465 0.3453193 16.6575134 -14.6611334 Variance-covariance matrix of the coefficients, \\(\\Var{\\hat{\\boldsymbol{\\beta}}}\\): vcov(mod2) ## (Intercept) income education typeprof typewc ## (Intercept) 13.7920916 -0.115636760 -0.257485549 14.0946963 7.9021988 ## income -0.1156368 0.007984369 -0.002924489 -0.1260105 -0.1090485 ## education -0.2574855 -0.002924489 0.012906986 -0.6166508 -0.3881200 ## typeprof 14.0946963 -0.126010517 -0.616650831 48.9021401 30.2138627 ## typewc 7.9021988 -0.109048528 -0.388119979 30.2138627 37.3171167 The standard errors of the coefficients, \\(\\se{\\hat{\\boldsymbol{\\beta}}}\\), are the square root diagonal of the vcov matrix, sqrt(diag(vcov(mod2))) ## (Intercept) income education typeprof typewc ## 3.7137705 0.0893553 0.1136089 6.9930065 6.1087737 This can be confirmed by comparing their values to those in the summary table, summary(mod2) ## ## Call: ## lm(formula = prestige ~ income + education + type, data = Duncan) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.890 -5.740 -1.754 5.442 28.972 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.18503 3.71377 -0.050 0.96051 ## income 0.59755 0.08936 6.687 5.12e-08 *** ## education 0.34532 0.11361 3.040 0.00416 ** ## typeprof 16.65751 6.99301 2.382 0.02206 * ## typewc -14.66113 6.10877 -2.400 0.02114 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.744 on 40 degrees of freedom ## Multiple R-squared: 0.9131, Adjusted R-squared: 0.9044 ## F-statistic: 105 on 4 and 40 DF, p-value: &lt; 2.2e-16 18.3 Residuals, Fitted Values, To get the fitted or predicted values (\\(\\hat{\\mathbf{y}} = \\mathbf{X} \\hat{\\boldsymbol\\beta}\\)) from a regression, mod1_fitted &lt;- fitted(mod1) head(mod1_fitted) ## accountant pilot architect author chemist minister ## 69.44073 80.24463 83.48580 61.87801 71.60151 25.14476 or mod1_predict &lt;- predict(mod1) head(mod1_predict) ## accountant pilot architect author chemist minister ## 69.44073 80.24463 83.48580 61.87801 71.60151 25.14476 The difference between predict and fitted is how they handle missing values in the data. Fitted values will not include predictions for missing values in the data, while predict will include values for Using predict, we can also predict values for new data. For example, create a data frame with each category of type, and in which income and education are set to their mean values. Duncan_at_means &lt;- data.frame(type = unique(Duncan$type), income = mean(Duncan$income), education = mean(Duncan$education)) Duncan_at_means ## type income education ## 1 prof 41.86667 52.55556 ## 2 wc 41.86667 52.55556 ## 3 bc 41.86667 52.55556 Now use this with the newdata argument, predict(mod2, newdata = Duncan_at_means) ## 1 2 3 ## 59.63821 28.31957 42.98070 To get the residuals (\\(\\hat{\\boldsymbol{\\epsilon}} = \\mathbf{y} - \\hat{\\mathbf{y}}\\)). mod1_resid &lt;- residuals(mod1) head(mod1_resid) ## accountant pilot architect author chemist minister ## 12.559266 2.755369 6.514200 14.121993 18.398486 61.855242 18.4 Broom The package broom has some functions that reformat the results of statistical modeling functions (t.test, lm, etc.) to data frames that work nicer with ggplot2, dplyr, and friends. The broom package has three main functions: glance: Information about the model. tidy: Information about the estimated parameters augment: The original data with estimates of the model. glance: Always return a one-row data.frame that is a summary of the model: e.g. R2, adjusted R2, etc. glance(mod2) ## r.squared adj.r.squared sigma statistic p.value df logLik ## 1 0.9130657 0.9043723 9.744171 105.0294 1.170871e-20 5 -163.6522 ## AIC BIC deviance df.residual ## 1 339.3045 350.1444 3797.955 40 tidy: Transforms into a ready-to-go data.frame the coefficients, SEs (and CIs if given), critical values, and p-values in statistical tests’ outputs tidy(mod2) ## term estimate std.error statistic p.value ## 1 (Intercept) -0.1850278 3.7137705 -0.0498221 9.605121e-01 ## 2 income 0.5975465 0.0893553 6.6873093 5.123720e-08 ## 3 education 0.3453193 0.1136089 3.0395443 4.164463e-03 ## 4 typeprof 16.6575134 6.9930065 2.3820246 2.206245e-02 ## 5 typewc -14.6611334 6.1087737 -2.4000125 2.114015e-02 augment: Add columns to the original data that was modeled. This includes predictions, estandard error of the predictions, residuals, and others. augment(mod2) %&gt;% head() ## .rownames prestige income education type .fitted .se.fit .resid ## 1 accountant 82 62 86 prof 83.21783 2.352262 -1.217831 ## 2 pilot 83 72 76 prof 85.74010 2.674659 -2.740102 ## 3 architect 90 75 92 prof 93.05785 2.755775 -3.057851 ## 4 author 76 55 90 prof 80.41628 2.589351 -4.416282 ## 5 chemist 90 64 86 prof 84.41292 2.360632 5.587076 ## 6 minister 87 21 84 prof 58.02779 4.260837 28.972214 ## .hat .sigma .cooksd .std.resid ## 1 0.05827491 9.866259 0.0002052803 -0.1287893 ## 2 0.07534370 9.857751 0.0013936701 -0.2924366 ## 3 0.07998300 9.855093 0.0018611391 -0.3271700 ## 4 0.07061418 9.841004 0.0033585648 -0.4701256 ## 5 0.05869037 9.825129 0.0043552315 0.5909809 ## 6 0.19120532 8.412639 0.5168053288 3.3061127 .fitted: the model predictions for all observations .se.fit: the estandard error of the predictions .resid: the residuals of the predictions (acual - predicted values) .sigma: is the standard error of the prediction. The other columns—.hat, .cooksd, and .std.resid are used in regression diagnostics. 18.5 Plotting Fitted Regression Results Consider the regression of prestige on income, mod3 &lt;- lm(prestige ~ income, data = Duncan) This creates a new dataset with the column income and 100 observations between the min and maximum observed incomes in the Duncan dataset. mod3_newdata &lt;- data_frame(income = seq(min(Duncan$income), max(Duncan$income), length.out = 100)) We will calculate fitted values for all these values of income. ggplot() + geom_point(data = Duncan, mapping = aes(x = income, y = prestige), colour = &quot;gray75&quot;) + geom_line(data = augment(mod3, newdata = mod3_newdata), mapping = aes(x = income, y = .fitted)) + ylab(&quot;Prestige&quot;) + xlab(&quot;Income&quot;) + theme_minimal() Now plot something similar, but for a regression with income interacted with type, mod4 &lt;- lm(prestige ~ income * type, data = Duncan) We want to create a dataset which has, (1) each value of type in the Duncan data, and (2) values spanning the range of income in the Duncan data. The function expand.grid creates a data frame with all combinations of vectors given to it (Cartesian product). mod4_newdata &lt;- expand.grid(income = seq(min(Duncan$income), max(Duncan$income), length.out = 100), type = unique(Duncan$type)) Now plot the fitted values evaluated at each of these values along wite original values in the data, ggplot() + geom_point(data = Duncan, mapping = aes(x = income, y = prestige, color = type)) + geom_line(data = augment(mod4, newdata = mod4_newdata), mapping = aes(x = income, y = .fitted, color = type)) + ylab(&quot;Prestige&quot;) + xlab(&quot;Income&quot;) + theme_minimal() Running geom_smooth with method = &quot;lm&quot; gives similar results. However, note that geom_smooth with run a separate regression for each group. ggplot(data = Duncan, aes(x = income, y = prestige, color = type)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + ylab(&quot;Prestige&quot;) + xlab(&quot;Income&quot;) + theme_minimal() "],
["formatting-tables.html", "Chapter 19 Formatting Tables 19.1 Overview of Packages 19.2 Summary Statistic Table Example 19.3 Regression Table Example", " Chapter 19 Formatting Tables 19.1 Overview of Packages R has multiple packages and functions for directly producing formatted tables for LaTeX, HTML, and other output formats. Given the See the Reproducible Research Task View for an overview of various options. xtable is a general purpose package for creating LaTeX, HTML, or plain text tables in R. texreg is more specifically geared to regression tables. It also outputs results in LaTeX (texreg), HTML (texreg), and plain text. The packages stargazer and apsrtable are other popular packages for formatting regression output. However, they are less-well maintained and have less functionality than texreg. For example, apsrtable hasn’t been updated since 2012, stargazer since 2015. The texreg vignette is a good introduction to texreg, and also discusses the These blog posts by Will Lowe cover many of the options. Additionally, for simple tables, knitr, the package which provides the heavy lifting for R markdown, has a function knitr. knitr also has the ability to customize how R objects are printed with the knit_print function. Other notable packages are: pander creates output in markdown for export to other formats. tables uses a formula syntax to define tables ReportR has the most complete support for creating Word documents, but is likely too much. For a political science perspective on why automating the research process is important see: Nicholas Eubank Embrace Your Fallibility: Thoughts on Code Integrity, based on this article Matthew Gentzkow Jesse M. Shapiro.Code and Data for the Social Sciences: A Practitioner’s Guide. March 10, 2014. Political Methodologist issue on Workflow Management 19.2 Summary Statistic Table Example The xtable package has methods to convert many types of R objects to tables. library(&quot;gapminder&quot;) gapminder_summary &lt;- gapminder %&gt;% # Keep numeric variables select_if(is.numeric) %&gt;% # gather variables gather(variable, value) %&gt;% # Summarize by variable group_by(variable) %&gt;% # summarise all columns summarise(n = sum(!is.na(value)), `Mean` = mean(value), `Std. Dev.` = sd(value), `Median` = median(value), `Min.` = min(value), `Max.` = max(value)) gapminder_summary ## # A tibble: 4 × 7 ## variable n Mean `Std. Dev.` Median Min. ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 gdpPercap 1704 7.215327e+03 9.857455e+03 3531.8470 241.1659 ## 2 lifeExp 1704 5.947444e+01 1.291711e+01 60.7125 23.5990 ## 3 pop 1704 2.960121e+07 1.061579e+08 7023595.5000 60011.0000 ## 4 year 1704 1.979500e+03 1.726533e+01 1979.5000 1952.0000 ## # ... with 1 more variables: Max. &lt;dbl&gt; Now that we have a data frame with the table we want, use xtable to create it: library(&quot;xtable&quot;) foo &lt;- xtable(gapminder_summary, digits = 0) %&gt;% print(type = &quot;html&quot;, html.table.attributes = &quot;&quot;, include.rownames = FALSE, format.args = list(big.mark = &quot;,&quot;)) variable n Mean Std. Dev. Median Min. Max. gdpPercap 1,704 7,215 9,857 3,532 241 113,523 lifeExp 1,704 59 13 61 24 83 pop 1,704 29,601,212 106,157,897 7,023,596 60,011 1,318,683,096 year 1,704 1,980 17 1,980 1,952 2,007 Note that there we two functions to get HTML. The function xtable creates an xtable R object, and the function xtable (called as print()), which prints the xtable object as HTML (or LaTeX). The default HTML does not look nice, and would need to be formatted with CSS. If you are copy and pasting it into Word, you would do some post-processing cleanup anyways. Another alternative is the knitr function in the knitr package, which outputs R markdown tables. knitr::kable(gapminder_summary) variable n Mean Std. Dev. Median Min. Max. gdpPercap 1704 7.215327e+03 9.857455e+03 3531.8470 241.1659 1.135231e+05 lifeExp 1704 5.947444e+01 1.291711e+01 60.7125 23.5990 8.260300e+01 pop 1704 2.960121e+07 1.061579e+08 7023595.5000 60011.0000 1.318683e+09 year 1704 1.979500e+03 1.726533e+01 1979.5000 1952.0000 2.007000e+03 This is useful for producing quick tables. Finally, htmlTables package unsurprisingly produces HTML tables. library(&quot;htmlTable&quot;) htmlTable(txtRound(gapminder_summary, 0), align = &quot;lrrrr&quot;) variable n Mean Std. Dev. Median Min. Max. 1 gdpPercap 1704 7 10 3532 241 1 2 lifeExp 1704 6 1 61 24 8 3 pop 1704 3 1 7023596 60011 1 4 year 1704 2 2 1980 1952 2 It has more features for producing HTML tables than xtable, but does not output LaTeX. 19.3 Regression Table Example library(&quot;tidyverse&quot;) library(&quot;texreg&quot;) We will run several regression models with the Duncan data Prestige &lt;- car::Prestige Since I’m running several regressions, I will save them to a list. If you know that you will be creating multiple objects, and programming with them, always put them in a list. First, create a list of the regression formulas, formulae &lt;- list( prestige ~ type, prestige ~ income, prestige ~ education, prestige ~ type + education + income ) Write a function to run a single model, Now use map to run a regression with each of these formulae, and save them to a list, prestige_mods &lt;- map(formulae, ~ lm(.x, data = Prestige, model = FALSE)) This is a list of lm objects, map(prestige_mods, class) ## [[1]] ## [1] &quot;lm&quot; ## ## [[2]] ## [1] &quot;lm&quot; ## ## [[3]] ## [1] &quot;lm&quot; ## ## [[4]] ## [1] &quot;lm&quot; We can look at the first model, prestige_mods[[1]] ## ## Call: ## lm(formula = .x, data = Prestige, model = FALSE) ## ## Coefficients: ## (Intercept) typeprof typewc ## 35.527 32.321 6.716 Now we can format the regression table in HTML using htmlreg. The first argument of htmlreg is a list of models: htmlreg(prestige_mods) Statistical models Model 1 Model 2 Model 3 Model 4 (Intercept) 35.53*** 27.14*** -10.73** -0.62 (1.43) (2.27) (3.68) (5.23) typeprof 32.32*** 6.04 (2.23) (3.87) typewc 6.72** -2.74 (2.44) (2.51) income 0.00*** 0.00*** (0.00) (0.00) education 5.36*** 3.67*** (0.33) (0.64) R2 0.70 0.51 0.72 0.83 Adj. R2 0.69 0.51 0.72 0.83 Num. obs. 98 102 102 98 RMSE 9.50 12.09 9.10 7.09 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 By default, htmlreg() prints out HTML, which is exactly what I want in an R markdown document. To save the output to a file, specify a non-null file argument. For example, to save the table to the file prestige.html, htmlreg(prestige_mods, file = &quot;prestige.html&quot;) Since this function outputs HTML directly to the console, it can be hard to tell what’s going on. If you want to preview the table in RStudio while working on it, this snippet of code uses htmltools package to do so: library(&quot;htmltools&quot;) htmlreg(prestige_mods) %&gt;% HTML() %&gt;% browsable() The htmlreg function has many options to adjust the table formatting. Below, I clean up the table. I remove stars using stars = NULL. It is a growing convention to avoid the use of stars indicating significance in regression tables (see AJPS and Political Analysis guidelines). The arguments doctype, html.tag, head.tag, body.tag control what sort of HTML is created. Generally all these functions (whether LaTeX or HTML output) have some arguments that determine whether it is creating a standalone, complete document, or a fragment that will be copied into another dcoument. The arguments include.rsquared, include.adjrs, and include.nobs are passed to the function extract() which determines what information the texreg package extracts from a model to put into the table. I get rid of \\(R^2\\), but keep adjusted \\(R^2\\), and the number of observations. library(&quot;stringr&quot;) coefnames &lt;- c(&quot;Professional&quot;, &quot;Working Class&quot;, &quot;Income&quot;, &quot;Education&quot;) note &lt;- &quot;OLS regressions with prestige as the response variable.&quot; htmlreg(prestige_mods, stars = NULL, custom.model.names = str_c(&quot;(&quot;, seq_along(prestige_mods), &quot;)&quot;), omit.coef = &quot;\\\\(Intercept\\\\)&quot;, custom.coef.names = coefnames, custom.note = str_c(&quot;Note: &quot;, note), caption.above = TRUE, caption = &quot;Regressions of Occupational Prestige&quot;, # better for markdown doctype = FALSE, html.tag = FALSE, head.tag = FALSE, body.tag = FALSE, # passed to extract() method for &quot;lm&quot; include.adjr = TRUE, include.rsquared = FALSE, include.rmse = FALSE, include.nobs = TRUE) Regressions of Occupational Prestige (1) (2) (3) (4) Professional 32.32 6.04 (2.23) (3.87) Working Class 6.72 -2.74 (2.44) (2.51) Income 0.00 0.00 (0.00) (0.00) Education 5.36 3.67 (0.33) (0.64) Adj. R2 0.69 0.51 0.72 0.83 Num. obs. 98 102 102 98 Note: OLS regressions with prestige as the response variable. Once you find a set of options that are common across your tables, make a function so you con’t need to retype them. my_reg_table &lt;- function(mods, ..., note = NULL) { htmlreg(mods, stars = NULL, custom.note = if (!is.null(note)) str_c(&quot;Note: &quot;, note) else NULL, caption.above = TRUE, # better for markdown doctype = FALSE, html.tag = FALSE, head.tag = FALSE) } my_reg_table(prestige_mods, custom.model.names = str_c(&quot;(&quot;, seq_along(prestige_mods), &quot;)&quot;), custom.coef.names = coefnames, note = note, # put intercept at the bottom reorder.coef = c(2, 3, 4, 5, 1), caption = &quot;Regressions of Occupational Prestige&quot;) Statistical models Model 1 Model 2 Model 3 Model 4 (Intercept) 35.53 27.14 -10.73 -0.62 (1.43) (2.27) (3.68) (5.23) typeprof 32.32 6.04 (2.23) (3.87) typewc 6.72 -2.74 (2.44) (2.51) income 0.00 0.00 (0.00) (0.00) education 5.36 3.67 (0.33) (0.64) R2 0.70 0.51 0.72 0.83 Adj. R2 0.69 0.51 0.72 0.83 Num. obs. 98 102 102 98 RMSE 9.50 12.09 9.10 7.09 Note: OLS regressions with prestige as the response variable. Note that I didn’t include every option in my_reg_table, only those arguments that will be common across tables. I use ... to pass arguments to htmlreg. Then when I call my_reg_table the only arguments are those specific to the content of the table, not the formatting, making it easier to understand what each table is saying. Of course, texreg also produces LaTeX output, with the function texreg. Almost all the options are the same as htmlreg. "],
["reproducible-research.html", "Chapter 20 Reproducible Research", " Chapter 20 Reproducible Research "],
["writing-resources.html", "Chapter 21 Writing Resources 21.1 Writing and Organizing Papers 21.2 Finding Research Ideas 21.3 Replications", " Chapter 21 Writing Resources 21.1 Writing and Organizing Papers Here are a few useful resources for writing papers: Chris Adolph. Writing Empirical Papers: 6 Rules &amp; 12 Recommendations Barry R. Weingast. 2015. Caltech Rules for Writing Papers: How to Structure Your Paper and Write an Introduction The Science of Scientific Writing American Scientist Deidre McCloskey. Economical Writing William Thompson. A Guide for the Young Economist. “Chapter 2: Writing Papers.” Stephen Van Evera. Guide to Methods for Students of Political Science. Appendix. Joseph M. Williams and Joseph Bizup. Style: Lessons in Clarity and Grace Strunk and White. The Elements of Style Chicago Manual of Style and APSA Style Manual for Political Science for editorial and style issues. How to construct a Nature summary paragraph. Though specifi to Nature is good advice for structuring abstracts or introductions. Ezra Klein. How researchers are terrible communications, and how they can do better. The advice in the AJPS Instructions for Submitting Authors is a concise description of how to write an abstract: The abstract should provide a very concise descriptive summary of the research stream to which the manuscript contributes, the specific research topic it addresses, the research strategy employed for the analysis, the results obtained from the analysis, and the implications of the findings. Concrete Advice for Writing Informative Abstracts and pHow to Carefully Choose Useless Titles for Academic Writing](http://www.socialsciencespace.com/2014/03/how-to-carefully-choose-useless-titles-for-academic-writing/) 21.2 Finding Research Ideas Paul Krugman How I Work Hal Varian. How to build an Economic Model in your spare time Greg Mankiw, My Rules of Thumb: links in Advice for Grad Students 21.3 Replications Gary King has advice on how to turn a replication into a publishable paper: Gary King How to Write a Publishable Paper as a Class Project Gary King. 2006. “Publication, Publication.” PS: Political Science and Politics. Political Science Should Not Stop Young Researchers from Replicating from the Political Science Replication blog. And see the examples of students replications from his Harvard course at https://politicalsciencereplication.wordpress.com/ Famous replications. David Broockman, Joahua Kalla, and Peter Aronow. 2015. Irregularities in LaCour (2014). Homas Herndon, Michael Ash &amp; Robert Pollin (2013). Does High Public Debt Consistently Stifle Economic Growth? A Critique of Reinhart and Rogoff. Working Paper Series 322. Political Economy Research Institute. [URL] However, although those replications are famous for finding fraud or obvious errors in the analysis, replications can lead to extensions and generate new ideas. This was the intent of Brookman, Kalla, and Aronow when starting the replication. "],
["multivariate-normal-distribution.html", "A Multivariate Normal Distribution", " A Multivariate Normal Distribution library(&quot;tidyverse&quot;) library(&quot;viridis&quot;) library(&quot;mvtnorm&quot;) The multivariate normal distribution is the generalization of the univariate normal distribution to more than one dimension.9 The random variable, \\(\\vec{x}\\), is a length \\(k\\) vector. The \\(k\\) length vector \\(\\vec{\\mu}\\) are the means of \\(\\vec{x}\\), and the \\(k \\times k\\) matrix, \\(\\mat{\\Sigma}\\), is the variance-covariance matrix, \\[ \\begin{aligned}[t] \\vec{x} &amp;\\sim \\dmvnorm{k}\\left(\\vec{\\mu}, \\mat{\\Sigma} \\right) \\\\ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_k \\end{bmatrix} &amp; \\sim \\dmvnorm{k} \\left( \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_k \\end{bmatrix}, \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{1,2} &amp; \\cdots &amp; \\sigma_{1, k} \\\\ \\sigma_{2,1} &amp; \\sigma_2^2 &amp; \\cdots &amp; \\sigma_{2, k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{k,1} &amp; \\sigma_{k,2} &amp; \\cdots &amp; \\sigma_{k, k} \\end{bmatrix} \\right) \\end{aligned} \\] The density function of the multivariate normal is, \\[ p(\\vec{x}; \\vec{\\mu}, \\mat{\\Sigma}) = (2 k)^{-\\frac{k}{2}} \\left| \\mat{\\Sigma} \\right|^{-\\frac{1}{2}} \\exp \\left( -\\frac{1}{2} (\\vec{x} - \\vec{\\mu})\\T \\mat{\\Sigma}^{-1} (\\vec{x} - \\vec{\\mu}) \\right) . \\] You can sample from and calculate the density for the multivariate normal distribution with the functions dmvnorm and rmvnorm from the package mvtnorm. Density plots of different bivariate normal distributions, See Multivariate normal distribution and references therein.↩ "],
["references-1.html", "References", " References "]
]
