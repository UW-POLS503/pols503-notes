[
["index.html", "Data Analysis Notes Chapter 1 Introduction", " Data Analysis Notes Jeffrey B. Arnold 2018-05-07 Chapter 1 Introduction Notes used when teaching “POLS/CS&amp;SS 501: Advanced Political Research Design and Analysis” and “POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology” at the University of Washington. \\[ \\] "],
["regression-anatomy.html", "Chapter 2 Regression Anatomy 2.1 Example 2.2 Variations 2.3 Questions", " Chapter 2 Regression Anatomy library(&quot;tidyverse&quot;) Summary: The coefficient of \\(x\\) in a multiple regression of \\(y\\) on \\(x\\) and \\(z\\) is equivalent to regressing \\(y\\) on the part of \\(x\\) not explained by \\(z\\) (the residuals of a regression of \\(x\\) on \\(z\\)) Regression anatomy is a term from Angrist and Pischke (2009) for the coefficient of variable \\(x\\) linear regression in a multiple regression is equivalent to the coefficient of a bivariate model using the residual from a regression of that variable regressed on all the other variables. Regression anatomy provides an answer to the how of control variables work. Consider a regression with two predictors, \\[ \\Vec{y} = \\beta_0 + \\beta_1 \\Vec{x}_1 + \\beta_1 \\Vec{x}_2 + u . \\] The OLS estimator of \\(\\beta\\) is \\[ (\\beta_1, \\beta_2) = \\Vec{\\beta} = (\\Mat{X}&#39;\\Mat{X})^{-1} \\Mat{X}&#39; \\Vec{y} \\] We can also recover the OLS estimate of \\(\\beta_1\\) through two bivariate regressions and the following procedure. \\[ \\beta_1 = \\frac{\\Cov(\\Vec{y}, \\tilde{\\Vec{u}})}{\\Var(\\tilde{\\Vec{u}})} . \\] where \\(\\tilde{\\Vec{u}}\\) is the vector of residuals from the regression of \\(\\Vec{x}_1\\) on \\(\\Vec{x}_2\\), \\[ \\begin{aligned}[t] \\Vec{x}_1 = \\gamma_0 + \\gamma_1 \\Vec{x}_2 + v . \\end{aligned} \\] This can be extended to more than two two variables by repeating the above steps as many times as necessary. This result is called the Frisch, Waugh, and Lovell (FWL) and discussed in Angrist and Pischke (2009). A complete proof can be found in advanced econometrics textbooks such as Davidson and MacKinnon (1993, p. 19–24) or Ruud (2000, p. 54–60). Note: This is a mechanical property of OLS It does depend an underlying Data Generating Process (DGP). Nevertheless, useful for understanding how OLS can be used for causal inference 2.1 Example For this example we will use the Duncan data from the car package. data(&quot;Duncan&quot;, package = &quot;carData&quot;) Duncan &lt;- rownames_to_column(Duncan, var = &quot;occupation&quot;) Regress prestige on education and income using lm. mod1 &lt;- lm(prestige ~ education + income, data = Duncan) mod1 ## ## Call: ## lm(formula = prestige ~ education + income, data = Duncan) ## ## Coefficients: ## (Intercept) education income ## -6.0647 0.5458 0.5987 Now, use the regression anatomy methods to find the regression of education by regression anatomy methods: Regress education on income: mod2a &lt;- lm(education ~ income, data = Duncan) mod2a ## ## Call: ## lm(formula = education ~ income, data = Duncan) ## ## Coefficients: ## (Intercept) income ## 15.6114 0.8824 Store the residuals from that regression. For convenience, use the augment function from the broom package. Duncan &lt;- modelr::add_residuals(Duncan, mod2a, var = &quot;resid_education&quot;) Regress prestige on the residuals of education regressed on ` mod2b &lt;- lm(prestige ~ resid_education, data = Duncan) mod2b ## ## Call: ## lm(formula = prestige ~ resid_education, data = Duncan) ## ## Coefficients: ## (Intercept) resid_education ## 47.6889 0.5458 The coefficients of these two regressions are approximately equal. They are different due to floating point rounding errors. coef(mod1)[&quot;education&quot;] - coef(mod2b)[&quot;resid_education&quot;] ## education ## 1.110223e-16 Q: Plot the regression lines Duncan %&gt;% select(prestige, education, resid_education) %&gt;% gather(variable, value, -prestige) %&gt;% mutate(variable = dplyr::recode(variable, resid_education = &quot;Residuals&quot;, education = &quot;education&quot;)) %&gt;% ggplot(aes(x = value, y = prestige, colour = variable)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) 2.2 Variations Let’s consider a couple of variations. First, consider the case in which we regress the residuals from \\(y\\) regressed on \\(x_2\\) on the residuals of \\(x_1\\) regressed on \\(x_2\\). Regress \\(y\\) on \\(x_2\\). Let \\(\\tilde{y}\\) be the residuals from that regression. Regress \\(x_1\\) on \\(x_2\\). Let \\(\\tilde{x}_1\\) be the residuals from that regression. Regress \\(\\tilde{y}\\) on \\(\\tilde{x}_1\\); do not include an intercept. Q: Will the coefficient of \\(\\tilde{x}_1\\) be the same as in the first case? Q: In the last stage why is there no intercept? Return to the previous example using the Duncan occupational prestige data. Regress prestige on income. mod3 &lt;- lm(prestige ~ income, data = Duncan) mod3 ## ## Call: ## lm(formula = prestige ~ income, data = Duncan) ## ## Coefficients: ## (Intercept) income ## 2.457 1.080 Add the residuals from this regression to Duncan dataset. Duncan &lt;- modelr::add_residuals(Duncan, mod3, var = &quot;resid_prestige&quot;) Regress the residuals of from the regression of prestige on income on the residuals from the regression of income on education. Do not include an intercept. lm(resid_prestige ~ 0 + resid_education, data = Duncan) ## ## Call: ## lm(formula = resid_prestige ~ 0 + resid_education, data = Duncan) ## ## Coefficients: ## resid_education ## 0.5458 What happens if we don’t include the intercept? lm(resid_prestige ~ resid_education, data = Duncan) ## ## Call: ## lm(formula = resid_prestige ~ resid_education, data = Duncan) ## ## Coefficients: ## (Intercept) resid_education ## -8.904e-16 5.458e-01 The intercept is estimate to be approximately zero - since the the regression line must go through \\((\\bar{y}, \\bar{x})\\) and residuals have mean 0. Plot the residuals and their regression line against the original values and their regression line. Subtract the mean from prestige and income so both linear regression lines go through the origin, which make it easier to compare them. bind_rows(mutate(select(Duncan, occupation, prestige, education), residuals = FALSE, prestige = prestige - mean(prestige), education = education - mean(education)), mutate(select(Duncan, occupation, prestige = resid_prestige, education = resid_education), residuals = TRUE)) %&gt;% ggplot(aes(x = education, y = prestige, colour = residuals)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) Second, consider the case in which we regress the residuals from \\(y\\) regressed on \\(x_2\\) on \\(x_1\\). Regress \\(y\\) on \\(x_2\\). Let \\(\\tilde{y}\\) be the residuals from that regression. Regress \\(\\tilde{y}\\) on \\(x_1\\). Regress the residuals of the regression of prestige on income on education. lm(resid_prestige ~ education, data = Duncan) ## ## Call: ## lm(formula = resid_prestige ~ education, data = Duncan) ## ## Coefficients: ## (Intercept) education ## -13.6285 0.2593 The coefficient on education is not the same as the multivariate OLS coefficient. In this case, we have not “controlled” for anything. The coefficient of education still includes the relationship between income and education. 2.3 Questions How does the variance of a variable compare to the variance of its residuals after regressing it on other control variables? What is the implication of that result for its standard error? If two variables are co-linear what is the residual after the first stage? How does that explain why OLS cannot estimate a linear regression with collinear variables. Suppose that two variables, \\(x_1\\) and \\(x_2\\) are highly correlated. In the first stage you regress \\(x_1\\) on \\(x_2\\). What do you expect the coefficient of \\(x_2\\) to be? What do you expect the residuals of this regression to be? How do you expect the coefficient of \\(x_1\\) on Consider two highly correlated variables. What are the implications for this in the first stage and second stage of regression anatomy? Confirm that regression anatomy also works for the coefficient of income in the previous regressions. Conduct regression anatomy for the coefficient of education in lm(prestige ~ education + type, data = Duncan) In this case we have a continuous variable (education) and a discrete control (type). How do would you interpret the residuals of the regression of education on type? Does the second stage of regression anatomy produce the correct standard errors? Informally, why do you think that is not the case. Retaining the regression anatomy procedure, generate correct standard errors using resampling. In causal inference, informally “selection on observables” attempts to estimate a causal effect by comparing “similar” observations. How does and doesn’t linear regression do this? Using the results of regression anatomy, how would you expect omitting control variables to effect the coefficients of a variable? Recall that the slope of a linear regression coefficient can be represented as a weighted average of the outcome variable, \\[ \\hat{\\beta} = \\sum_{i = 1}^n w_i y_i \\] where \\[ w_i = \\frac{(x_i - \\bar{x})}{\\sum_{i = 1} (x_i - \\bar{x})^2} . \\] Given regression anatomy, what are the weights of observations in multiple regression? References "],
["ols-in-matrix-form.html", "Chapter 3 OLS in Matrix Form Setup 3.1 Purpose 3.2 Matrix Algebra Review 3.3 Matrix Operations 3.4 Matrices as vectors 3.5 Special matrices 3.6 Multiple linear regression in matrix form 3.7 Residuals 3.8 Scalar inverses 3.9 Matrix Inverses 3.10 OLS Estimator 3.11 Implications of OLS 3.12 Covariance/variance interpretation of OLS", " Chapter 3 OLS in Matrix Form Setup This will use the Duncan data in a few examples. library(&quot;tidyverse&quot;) data(&quot;Duncan&quot;, package = &quot;carData&quot;) 3.1 Purpose We can write regression model as, \\[ y_i = \\beta_0 + x_{i1} \\beta_1 + x_{i2} \\beta_2 + \\cdots + x_{ik} \\beta_k + u_k . \\] It will be cleaner to write the linear regression as \\[ y_i = \\Vec{x}_{i} \\Vec{\\beta} + u_i \\] where \\(\\Vec{x}_i\\) is a \\(1 \\times (K + 1)\\) row vector and \\(\\Vec{\\beta}\\) is a \\((K + 1) \\times 1\\) column vector for a single observation \\(i\\). Or we can write it as, \\[ \\Vec{y} = \\Mat{X} \\Vec{\\beta} + \\Vec{u} \\] where \\(\\Vec{y}\\) is a \\(N \\times 1\\) row vector, \\(\\Mat{X}\\) is a \\(N \\times (K + 1)\\) matrix, and \\(\\Vec{\\beta}\\) is a \\((K + 1) \\times 1\\) column vector for all \\(N\\) observations. 3.2 Matrix Algebra Review 3.2.1 Vectors A vector is a list of numbers or random variables. A \\(1 \\times k\\) row vector is arranged \\[ \\Vec{b} = \\begin{bmatrix} b_1 &amp; b_2 &amp; b_3 &amp; \\dots &amp; b_k \\end{bmatrix} \\] A \\(1 \\times k\\) column vector is arranged \\[ \\Vec{a} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3\\\\ \\dots \\\\ b_k \\end{bmatrix} \\] Convention: assume vectors are column vectors Convention: use lower-case bold Latin letters, e.g. \\(\\Vec{x}\\). Vector Examples Vector of all covariates for a particular unit \\(i\\) as a row vector, \\[ \\Vec{x}_{i} = \\begin{bmatrix} x_{i1} &amp; x_{i2} &amp; \\dots &amp; x_{ik} \\end{bmatrix} \\] E.g. in the Duncan data, \\[ \\Vec{x}_{i} = \\begin{bmatrix} \\mathtt{education}_{i} &amp; \\mathtt{income}_{i} &amp; \\mathtt{type}_i \\end{bmatrix} \\] Vector of the values of covariate \\(k\\) for all observations, \\[ x_{.,k} = \\begin{bmatrix} 1 \\\\ x_{i1} \\\\ x_{i2} \\\\ \\dots \\\\ x_{ik} \\end{bmatrix} \\] E.g. For the education variable in the column vector. \\[ \\Vec{x}_{i} = \\begin{bmatrix} \\mathtt{education}_{1} \\\\ \\mathtt{education}_{2} \\\\ \\dots &amp; \\vdots &amp; \\mathtt{education}_N \\end{bmatrix} \\] 3.2.2 Matrices A matrix is a rectangular array of numbers A matrix is \\(n \\times k\\) (“\\(n\\) by \\(k\\)”) if it has \\(n\\) rows and \\(k\\) columns A matrix \\[ A = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\dots &amp; a_{1k} \\\\ a_{21} &amp; a_{22} &amp; \\dots &amp; a_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\dots &amp; a_{nk} \\end{bmatrix} \\] 3.2.2.1 Examples The design matrix is the matrix of predictors/covariates in a regression: \\[ X = \\begin{bmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; \\dots &amp; a_{1k} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; \\dots &amp; a_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; a_{n2} &amp; \\dots &amp; a_{nk} \\end{bmatrix} \\] The vector of ones is the constant. In the Duncan data, for the regression prestige ~ income + education + type, the design matrix is, \\[ X = \\begin{bmatrix} 1 &amp; \\mathtt{income}_{1} &amp; \\mathtt{education}_{1} &amp; \\mathtt{wc}_{1} &amp; \\mathtt{prof}_{1} \\\\ 1 &amp; \\mathtt{income}_{2} &amp; \\mathtt{education}_{2} &amp; \\mathtt{wc}_{2} &amp; \\mathtt{prof}_{2} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; \\mathtt{income}_{N} &amp; \\mathtt{education}_{N} &amp; \\mathtt{wc}_{N} &amp; \\mathtt{prof}_{N} \\\\ \\end{bmatrix} \\] Use R function model.matrix to create the design matrix from a formula and a data frame. model.matrix(prestige ~ income + education + type, data = Duncan) ## (Intercept) income education typeprof typewc ## accountant 1 62 86 1 0 ## pilot 1 72 76 1 0 ## architect 1 75 92 1 0 ## author 1 55 90 1 0 ## chemist 1 64 86 1 0 ## minister 1 21 84 1 0 ## professor 1 64 93 1 0 ## dentist 1 80 100 1 0 ## reporter 1 67 87 0 1 ## engineer 1 72 86 1 0 ## undertaker 1 42 74 1 0 ## lawyer 1 76 98 1 0 ## physician 1 76 97 1 0 ## welfare.worker 1 41 84 1 0 ## teacher 1 48 91 1 0 ## conductor 1 76 34 0 1 ## contractor 1 53 45 1 0 ## factory.owner 1 60 56 1 0 ## store.manager 1 42 44 1 0 ## banker 1 78 82 1 0 ## bookkeeper 1 29 72 0 1 ## mail.carrier 1 48 55 0 1 ## insurance.agent 1 55 71 0 1 ## store.clerk 1 29 50 0 1 ## carpenter 1 21 23 0 0 ## electrician 1 47 39 0 0 ## RR.engineer 1 81 28 0 0 ## machinist 1 36 32 0 0 ## auto.repairman 1 22 22 0 0 ## plumber 1 44 25 0 0 ## gas.stn.attendant 1 15 29 0 0 ## coal.miner 1 7 7 0 0 ## streetcar.motorman 1 42 26 0 0 ## taxi.driver 1 9 19 0 0 ## truck.driver 1 21 15 0 0 ## machine.operator 1 21 20 0 0 ## barber 1 16 26 0 0 ## bartender 1 16 28 0 0 ## shoe.shiner 1 9 17 0 0 ## cook 1 14 22 0 0 ## soda.clerk 1 12 30 0 0 ## watchman 1 17 25 0 0 ## janitor 1 7 20 0 0 ## policeman 1 34 47 0 0 ## waiter 1 8 32 0 0 ## attr(,&quot;assign&quot;) ## [1] 0 1 2 3 3 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$type ## [1] &quot;contr.treatment&quot; 3.3 Matrix Operations 3.3.1 Transpose The transpose of a matrix \\(A\\) flips the rows and columns. It is denoted \\(A&#39;\\) or \\(A^{T}\\). The transpose of a \\(3 \\times 2\\) matrix is a \\(2 \\times 3\\) matrix, \\[ A = \\begin{bmatrix} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\\\ a_{31} &amp; a_{32} \\end{bmatrix} = \\begin{bmatrix} a_{11} &amp; a_{21} &amp; a_{31} \\\\ a_{12} &amp; a_{22} &amp; a_{32} \\end{bmatrix} \\] Transposing turns a \\(1 \\times k\\) row vector into a \\(k \\times 1\\) column vector and vice-versa. \\[ \\begin{aligned}[t] x_i &amp;= \\begin{bmatrix} 1 \\\\ x_{i1} \\\\ x_{i2} \\\\ \\vdots \\\\ x_{ik} \\end{bmatrix} \\\\ x_i&#39; &amp;= \\begin{bmatrix} 1 &amp; x_{i1} &amp; x_{i2} &amp; \\dots &amp; x_{ik} \\end{bmatrix} \\end{aligned} \\] A &lt;- matrix(1:6, ncol = 3, nrow = 2) A ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 t(A) ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 ## [3,] 5 6 a &lt;- 1:6 t(a) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 2 3 4 5 6 3.4 Matrices as vectors A matrix is a collection of row (or column) vectors. Write the matrix as a collection of row vectors \\[ A = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\end{bmatrix} = \\begin{bmatrix} \\Vec{a}_1&#39; \\\\ \\Vec{a}_2&#39; \\end{bmatrix} \\] \\[ B = \\begin{bmatrix} b_{11} &amp; b_{12} \\\\ b_{21} &amp; b_{22} \\\\ b_{31} &amp; b_{32} \\end{bmatrix} = \\begin{bmatrix} \\Vec{b}_1 \\\\ \\Vec{b}_2 \\Vec{b}_3 \\end{bmatrix} \\] How does \\(X\\) relate to the model specification? See the model.matrix model.matrix(prestige ~ education * income + type, data = Duncan) %&gt;% head() ## (Intercept) education income typeprof typewc education:income ## accountant 1 86 62 1 0 5332 ## pilot 1 76 72 1 0 5472 ## architect 1 92 75 1 0 6900 ## author 1 90 55 1 0 4950 ## chemist 1 86 64 1 0 5504 ## minister 1 84 21 1 0 1764 The OLS estimator of coefficients is \\[ \\hat{\\beta} = \\underbrace{(X&#39; X)^{-1}}_{Var(X)} \\underbrace{X&#39; y}_{Cov(X, Y)} \\] 3.5 Special matrices A square matrix has equal numbers of rows and columns The identity matrix, \\(\\Mat{I}_K\\) is a \\(K \\times K\\) square matrix with 1s on the diagonal, and 0s everywhere else. \\[ \\Mat{I}_3 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] The identity matrix multiplied by any matrix returns the matrix, \\[ \\Mat{A} \\Mat{I}_{K} = \\Mat{A} = \\Mat{I}_{M} \\Mat{A} \\] where \\(\\Mat{A}\\) is an \\(M \\times K\\) matrix. In R, to get the diagonal of a matrix use diag(), b &lt;- diag(1:4, nrow = 2L, ncol = 2L) b &lt;- diag(b) The function diag() also creates identity matrices, diag(3L) ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 The zero matrix is a matrix of all zeros, \\[ \\Mat{0}_K = \\begin{bmatrix} 0 &amp; 0 &amp; \\dots 0 \\\\ 0 &amp; 0 &amp; \\dots 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 0 \\end{bmatrix} \\] The zero vector is a matrix of all zeros, \\[ \\Mat{0}_K = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\] The ones vector is a vector of all ones, \\[ \\Mat{0}_K = \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} \\] 3.6 Multiple linear regression in matrix form Let \\(\\widehat{\\Vec{\\beta}}\\) be the matrix of estimated regression coefficients, and \\(\\hat{\\Vec{y}}\\) be the vector of fitted values: \\[ \\begin{aligned}[t] \\widehat{\\Vec{\\beta}} &amp;= \\begin{bmatrix} \\widehat{\\beta}_0 \\\\ \\widehat{\\beta}_1 \\\\ \\vdots \\\\ \\widehat{\\beta}_K \\end{bmatrix} &amp; \\hat{\\Vec{y}} &amp;= \\Mat{X} \\widehat{\\Vec{\\beta}} \\end{aligned} \\] This could be expanded to, \\[ \\begin{aligned}[t] \\hat{\\Vec{y}} &amp;= \\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_N \\end{bmatrix} &amp;= \\Mat{X} \\widehat{\\Vec{\\beta}} &amp;= \\begin{aligned} 1\\widehat{\\beta}_0 + x_{11} \\widehat{\\beta}_1 + x_{12} \\widehat{\\beta}_2 + \\dots + x_{1K} \\widehat{\\beta}_K \\\\ 1\\widehat{\\beta}_0 + x_{21} \\widehat{\\beta}_1 + x_{22} \\widehat{\\beta}_2 + \\dots + x_{2K} \\widehat{\\beta}_K \\\\ \\vdots \\\\ 1\\widehat{\\beta}_0 + x_{N1} \\widehat{\\beta}_1 + x_{N2} \\widehat{\\beta}_2 + \\dots + x_{NK} \\widehat{\\beta}_K \\\\ \\end{aligned} \\end{aligned} \\] 3.7 Residuals The residuals of a regression are, \\[ \\Vec{u} = \\Vec{y} - \\Mat{X} \\widehat{\\beta} \\] In two dimensions the Euclidian distance is, \\[ d(a, b) = \\sqrt{a^2 + b^2} \\] Think the hypotenuse of a triangle. The norm or length of a vector generalizes the Euclidian distance to multiple dimensions1 For a \\(K \\times 1\\) vector \\(\\Vec{a}\\), \\[ | \\Vec{a} | = \\sqrt{a_1^2 + a_2^2 + \\dots + a_K^2} \\] The norm can be written as the inner product, \\[ {| \\Vec{a} |}^2 = \\Vec{a}\\T \\Vec{a} \\] Note that when the mean of a vector is 0, the norm is equal to \\(N\\) times the sample variance (using the \\(N\\) denominator) \\[ \\begin{aligned} \\Var{\\Vec{u}} &amp;= \\frac{1}{N} \\sum_{i = 1}^N (u_i - \\var{u})^2 \\\\ &amp;= \\frac{1}{N} \\sum_{i = 1}^N u_i^2 \\\\ &amp;= \\frac{1}{N} \\Vec{u}\\T \\Vec{u} \\\\ &amp;= \\frac{1}{N} {| \\Vec{u} |}^2 \\\\ \\end{aligned} \\] 3.8 Scalar inverses What is division? You may think of it as the inverse of multiplication (which it is), but it means that for number \\(a\\) there exists another number (the inverse of \\(a\\)) denoted \\(a^{-1}\\) or \\(1 / a\\) such that \\(a \\times a^{-1} = 1\\). This inverse does not always exist. There is no inverse for 0: \\(0 \\times ? = 1\\) has no solution. If the inverse exists, we can solve algebraic expressions like \\(ax = b\\) for \\(x\\), \\[ \\begin{aligned} ax &amp;= b \\\\ \\frac{1}{a} ax &amp;= \\frac{1}{a} b &amp; \\text{multiply both sides by the inverse of \\[a\\]} \\\\ x = \\frac{b}{a} \\end{aligned} \\] We’ll see in matrix algebra, the intuition is similar. The inverse is a matrix such that when it multiplies a number it results in 1 (or the equivalent) The inverse doesn’t always exist The inverse can be used to solve 3.9 Matrix Inverses If it exists (it does not always), the inverse of square matrix \\(\\Mat{A}\\), denoted \\(\\Mat{A}^{-1}\\), is the matrix such that \\[ \\Mat{A}^{-1} \\Mat{A} = \\Mat{A} \\Mat{A}^{-1} = \\Mat{I} \\] The inverse can be used to solve systems of equations (like OLS) \\[ \\begin{aligned}[t] \\Mat{A} \\Vec{x} &amp;= \\Vec{b} \\\\ \\Mat{A}^{-1} \\Mat{A} \\Vec{x} &amp;= \\Mat{A}^{-1} \\Vec{b} \\\\ I \\Vec{x} &amp;= \\Mat{A}^{-1} \\Vec{b} \\\\ \\Vec{x} &amp;= \\Mat{A}^{-1} \\Vec{b} \\end{aligned} \\] If the inverse exists, then \\(\\Mat{A}\\) is called invertible or nonsingular. 3.10 OLS Estimator OLS minimizes the sum of squared residuals \\[ \\arg \\min \\] 3.11 Implications of OLS Independent variables are orthogonal to the residuals \\[ \\Mat{X}\\T \\hat{\\Vec{u}} = \\Mat{X}\\T(\\Vec{y} - \\Mat{X} \\widehat{\\Vec{\\beta}}) = 0 \\] Fitted values are orthogonal to the residuals \\[ \\Vec{y}\\T \\hat{\\Vec{u}} =(\\Mat{X} \\widehat{\\Vec{\\beta}})\\T \\hat{\\Vec{u}} = \\widehat{\\Vec{\\beta}}\\T \\Mat{X}\\T \\hat{\\Vec{u}} = 0 \\] 3.11.1 OLS in Matrix Form \\[ Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_k x_{ik} + \\epsilon_i \\] We can write this as \\[ \\begin{aligned}[t] Y_i &amp;= \\begin{bmatrix} 1 &amp; x_{i1} &amp; x_{i2} &amp; \\dots &amp; x_{ik} \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k \\end{bmatrix} + \\epsilon_i \\\\ &amp;= \\underbrace{{x_i&#39;}}_{1 \\times k + 1} \\underbrace{\\beta}_{k + 1 \\times 1} + \\epsilon_i \\end{aligned} \\] \\[ Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_k x_{ik} + \\epsilon_i \\] We can write it as \\[ \\begin{aligned}[t] \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} &amp;= \\begin{bmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; \\dots &amp; x_{1k} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; \\dots &amp; x_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; \\dots &amp; x_{nk} \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix} \\\\ \\underbrace{y}_{(n \\times 1)} &amp;= \\underbrace{{x_i&#39;}}_{(n \\times k + 1)} \\underbrace{\\beta}_{(k + 1 \\times 1)} + \\underbrace{\\epsilon}_{(n \\times 1)} \\end{aligned} \\] The regression standard error of the regression is \\[ \\hat{\\sigma}_y^2 = \\frac{\\sum_i^n \\epsilon_i^2}{n - k - 1} \\] Write this using matrix notation. Note that \\[ E(X_i)^2 = \\frac{\\sum X_i^2}{n} \\] In matrix notation this is, \\[ \\begin{bmatrix} x_1 &amp; x_2 &amp; \\dots &amp; x_n \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\dots \\\\ x_n \\end{bmatrix} = x&#39; x \\] If \\(\\bar{X} = 0\\), then \\[ \\frac{X&#39; X}{N} = Var(X) \\] What is the vcov matrix of \\(\\beta\\)? When would it be diagonal? What is on the off-diagonal? What is on the diagonal? Extract the standard errors from it. OLS Standard errors \\[ \\hat{\\beta}_{OLS} = (X&#39; X)^{-1} X&#39; y \\] \\[ V(\\hat{\\beta}) = \\begin{bmatrix} V(\\hat{\\beta}_0) &amp; Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) &amp; \\dots &amp; Cov(\\hat{\\beta}_0, \\hat{\\beta}_k) \\\\ Cov(\\hat{\\beta}_1, \\hat{\\beta}_0) &amp; V(\\hat{\\beta}_1) &amp; \\dots &amp; Cov(\\hat{\\beta}_1, \\hat{\\beta}_k) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ Cov(\\hat{\\beta}_k, \\hat{\\beta}_0) &amp; Cov(\\hat{\\beta}_k, \\hat{\\beta}_1) &amp; \\dots &amp; V(\\hat{\\beta}_k) \\\\ \\end{bmatrix} \\] Which of these matrices are Homoskedastic Heteroskedastic Clustered standard errors Serially correlated Show how \\((X&#39; X)^{-1} X&#39; y\\) is equivalent to the bivariate estimator. Write out \\(\\beta\\) and plug in for the true \\(Y\\) in terms of \\(X\\) and \\(\\epsilon\\) Take the variance of \\(\\hat{\\beta} - \\beta\\) \\[ \\hat{\\beta} = \\beta + (X&#39; X)^{-1} X&#39; \\epsilon \\\\ \\Var(\\hat{\\beta} - \\beta) = var((X&#39; X)^{-1} X&#39; \\epsilon) \\\\ \\] We know that \\((X&#39; X)^{-1} X&#39; \\epsilon\\) has mean zero since \\(E(X&#39; \\epsilon) = 0\\). \\(var(z) = E(Z^2) - 0\\) In matrix form \\(Z Z&#39;\\) to get full matrix form \\[ V((X&#39; X)^{-1} X&#39; \\epsilon) = (X&#39; X)^{-1} X&#39; \\epsilon \\epsilon&#39; X (X&#39; X)^{-1} = (X&#39; X)^{-1} X&#39; \\Sigma X (X&#39; X)^{-1} \\] We need a way to estimate \\(\\hat{\\Sigma}\\). But it has \\(n (n + 1) / 2\\) elements … and we have only \\(n\\) observations, and \\(n - k - 1\\) degrees of freedom left after estimating the coefficients. If homoskedasticity, \\(\\Sigma = \\sigma^2 I\\). \\[ V((X&#39; X)^{-1} X&#39; \\epsilon) = \\sigma^2 (X&#39; X)^{-1} \\] Panel of countries. Correlation within each year that is always the same 3.12 Covariance/variance interpretation of OLS \\[ \\Mat{X}\\T \\Vec{y} = \\sum_{i = 1}^N \\begin{bmatrix} y_i \\\\ y_i x_{i1} \\\\ y_i x_{i2}\\\\ \\vdots \\\\ y_i x_{iK} \\end{bmatrix} \\approx \\begin{bmatrix} n\\bar{y} \\\\ \\widehat{\\Cov}[y_i, x_{i1}] \\\\ \\widehat{\\Cov}[y_i, x_{i2}] \\\\ \\vdots \\\\ \\widehat{\\Cov}[y_i, x_{iK}] \\end{bmatrix} \\] \\[ \\begin{aligned} \\Mat{X}\\T \\Mat{X} &amp;= \\sum_{i = 1}^N \\begin{bmatrix} 1 &amp; x_{i1} &amp; x_{i2}&amp; \\cdots &amp; x_{ik} \\\\ x_{i1} &amp; x_{i1}^2 &amp; x_{i2} x_{i1} &amp; \\cdots &amp; x_{i1} x_{iK} \\\\ x_{i2} &amp; x_{i1} x_{i2} &amp; x_{i2}^2 &amp; \\cdots &amp; x_{i2} x_{iK} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{iK} &amp; x_{i1} x_{iK} &amp; x_{i2} x_{iK} &amp; \\cdots &amp; x_{ik} x_{iK} \\end{bmatrix} \\\\ &amp;\\approx \\begin{bmatrix} n &amp; n \\bar{x}_1 &amp; n \\bar{x}_2 &amp; \\cdots &amp; n \\bar{x}_K \\\\ n \\bar{x}_3 &amp; \\widehat{\\Var}[x_{i1}] &amp; \\widehat{\\Cov}[x_{i1}, x_{i2}] &amp; \\cdots &amp; n \\widehat{Cov}[x_{i1}, x_{iK}] \\\\ n \\bar{x}_3 &amp; \\widehat{\\Cov}[x_{i1}, x_{i2}] &amp; \\widehat{\\Var}[x_{i2}] &amp; \\cdots &amp; n \\widehat{\\Cov}[x_{i2}, x_{iK}] \\\\ \\vdots \\\\ n \\bar{x}_K &amp; \\widehat{\\Cov}[x_{iK}, x_{i1}] &amp; \\widehat{\\Cov}[x_{iK}, x_{i2}] &amp; \\cdots &amp; n \\widehat{\\Var}[x_{iK}] \\end{bmatrix} \\end{aligned} \\] This is technically the 2-norm, as there are other norms.↩ "],
["collinearity-and-multicollinearity.html", "Chapter 4 Collinearity and Multicollinearity 4.1 (Perfect) collinearity 4.2 What to do about it? 4.3 Multicollinearity 4.4 What do do about it?", " Chapter 4 Collinearity and Multicollinearity library(&quot;tidyverse&quot;) library(&quot;carData&quot;) 4.1 (Perfect) collinearity In order to estimate unique \\(\\hat{\\beta}\\) OLS requires the that the columns of the design matrix \\(\\Vec{X}\\) are linearly independent. Common examples of groups of variables that are not linearly independent: Categorical variables in which there is no excluded category. You can also include all categories of a categorical variable if you exclude the intercept. Note that although they are not (often) used in political science, there are other methods of transforming categorical variables to ensure the columns in the design matrix are independent. A constant variable. This can happen in practice with dichotomous variables of rare events; if you drop some observations for whatever reason, you may end up dropping all the 1’s in the data. So although the variable is not constant in the population, in your sample it is constant and cannot be included in the regression. A variable that is a multiple of another variable. E.g. you cannot include \\(\\log(\\text{GDP in millions USD})\\) and \\(\\log({GDP in USD})\\) since \\(\\log(\\text{GDP in millions USD}) = \\log({GDP in USD}) / 1,000,000\\). A variable that is the sum of two other variables. E.g. you cannot include \\(\\log(population)\\), \\(\\log(GDP)\\), \\(\\log(GDP per capita)\\) in a regression since \\[\\log(\\text{GDP per capita}) = \\log(\\text{GDP} / \\text{population}) = \\log(\\text{GDP}) - \\log(\\text{population})\\]. 4.2 What to do about it? R and most statistical programs will run regressions with collinear variables, but will drop variables until only linearly independent columns in \\(\\Mat{X}\\) remain. For example, consider the following code. The variable type is a categorical variable with categories “bc”, “wc”, and “prof”. data(Duncan, package = &quot;carData&quot;) # Create dummy variables for each category Duncan &lt;- mutate(Duncan, bc = type == &quot;bc&quot;, wc = type == &quot;wc&quot;, prof = type == &quot;prof&quot;) lm(prestige ~ bc + wc + prof, data = Duncan) ## ## Call: ## lm(formula = prestige ~ bc + wc + prof, data = Duncan) ## ## Coefficients: ## (Intercept) bcTRUE wcTRUE profTRUE ## 80.44 -57.68 -43.78 NA R runs the regression, but coefficient and standard errors for prof are set to NA. You should not rely on the software to fix this for you; once you (or the software) notices the problem check the reasons it occurred. The rewrite your regression to remove whatever was creating linearly dependent variables in \\(\\Mat{X}\\). 4.3 Multicollinearity Multicollinearity is the (poor) name for less-than-perfect collinearity. Even though there is enough variation in \\(\\Mat{X}\\) to estimate OLS coefficients, if some set of variables in \\(\\Mat{X}\\) is highly correlated it will result in large, but unbiased, standard errors on the estimates. What happens if variables are not linearly dependent, but nevertheless highly correlated? If \\(\\Cor(\\Vec{x}_1, vec{x}_2) = 1\\), then they are linearly dependent and the regression cannot be estimated (see above). But if \\(\\Cor(\\Vec{x}_1, vec{x}_2) = 0.99\\), the OLS can estimate unique values of of \\(\\hat\\beta\\). However, it everything was fine with OLS estimates until, suddenly, when there is linearly independence everything breaks. The answer is yes, and no. As \\(|\\Cor(\\Vec{x}_1, \\Vec{x}_2)| \\to 1\\) the standard errors on the coefficients of these variables increase, but OLS as an estimator works correctly; \\(\\hat\\beta\\) and \\(\\se{\\hat\\beta}\\) are unbiased. With multicollinearity, OLS gives you the “right” answer, but it cannot say much with certainty. For a bivariate regression, the distribution of the slope coefficient has variance, \\[ \\Var(\\hat{\\beta}_1) = \\frac{\\sigma_u^2}{\\sum_{i = 1} (x_i - \\bar{x})^2} . \\] What affects the standard error of \\(\\hat{\\beta}\\)? The error variance (\\(\\sigma_u^2\\)). The higher the variance of the residuals, the higher the variance of the coefficients. The variance of \\(\\Vec{x}\\). The lower variation in \\(\\Mat{x}\\), the bigger the standard errors of the slope. Now consider a multiple regression, \\[ \\Vec{y} = \\beta_0 + \\beta_1 \\Vec{x}_1 + \\beta_2 \\Vec{x}_2 + u \\] this becomes, \\[ \\Var(\\hat{\\beta}_1) = \\frac{\\sigma_u^2}{(1 - R^2_1) \\sum_{i = 1}^n (x_i - \\bar{x})^2} \\] where \\(R^2_1\\) is the \\(R^2\\) from the regression of \\(\\Vec{x}_1\\) on \\(\\Vec{x}_2\\), \\[ \\Vec{x} = \\hat{\\delta}_0 + \\hat{\\delta}_1 \\Vec{x}_2 . \\] The factors affecting standard errors are Error variance: higher residuals leads to higher standard errors. Variance of \\(\\Vec{x}_1\\): lower variation in \\(\\Vec{x}_2\\) leads to higher standard errors. The strength of the relationship between \\(x_1\\) and \\(x_2\\). Stronger relationship between \\(x_1\\) and \\(x_2\\) (higher \\(R^2\\) of the regression of \\(x_1\\) on \\(x_2\\)) leads to higher standard errors. These arguments generalize to more than two predictors. 4.4 What do do about it? Multicollinearity is not an “error” in the model. All you can do is: Get more data Find more conditional variation in the predictor of interest What it means depends on what you are doing. Prediction: then you are interested in \\(\\hat{\\Vec{y}}\\) and not \\(\\hat{\\beta}}\\) (or its standard errors). In this case, multicollinearity is irrelevant. Causal inference: in this case you are interested in \\(\\hat{\\Vec{\\beta}}\\). Multicollinearity does not bias \\(\\hat{\\beta}\\). You should include all regressors to achieve balance, and include all relevant pre-treatment variables and not include post-treatment variables. Multicollinearity is not directly relevant in this choice. All multicollinearity means is that the variation in the treatment after accounting for selection effects is very low, making it hard to say anything about the treatment effect with that observational data. More sophisticated methods may trade off some bias for a lower variance (e.g. shrinkage methods), but that must be done systematically, and not ad-hoc dropping relevant pre-treatment variables that simply correlate highly with your treatment variable. "],
["bootstrapping.html", "Chapter 5 Bootstrapping 5.1 Non-parametric bootstrap 5.2 Standard Errors 5.3 Confidence Intervals 5.4 Alternative methods 5.5 Bagging 5.6 Hypothesis Testing 5.7 How many samples? 5.8 References", " Chapter 5 Bootstrapping The central analogy of bootstrapping is The population is to the sample as the sample is to the bootstrap samples (Fox 2008, 590) To calculate standard errors to use in confidence intervals we need to know sampling distribution of the statistic of interest. In the case of a mean, we can appeal to the central limit theorem if the sample size is large enough. Bootstrapping takes a different approach. We use the sample as an estimator of the sampling distribution. E.g. bootstrap claims \\[ \\text{sample distribution} \\approx \\text{population distribution} \\] and then proceeds to plug-in the sample distribution for the population distribution, and then draw new samples to generate a sampling distribution. The bootstrap relies upon the plug-in principle. The plug-in principle is that when something is unknown, use an estimate of it. An example is the use of the sample standard deviation in place of the population standard deviation, when calculating the standard error of the mean, \\[ \\SE(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{\\hat{\\sigma}}{\\sqrt{n}} \\] Bootstrap is the plug-in principal on ’roids. It uses the empirical distribution as a plug-in for the unknown population distribution. See Figures 4 and 5 of Hesterberg (2015). Bootstrap principles The substitution of the empirical distribution for the population works. Sample with replacement. The bootstrap is for inference not better estimates. It can estimate uncertainty, not improve \\(\\bar{x}\\). It is not generating new data out of nowhere. However, see the section on bagging for how bootstrap aggregation can be used. 5.1 Non-parametric bootstrap The non-parametric bootstrap resamples the data with replacement \\(B\\) times and calculates the statistic on each resample. 5.2 Standard Errors The bootstrap is primarily a means to calculate standard errors. The bootstrap standard error is Suppose there are \\(r\\) bootstrap replicates. Let \\(\\hat{\\theta}^{*}_1, \\dots, \\hat{\\theta}^{*}_r\\) be statistics calculated on each bootstrap samples. \\[ \\SE^{*}\\left(\\hat{\\theta}^{*}\\right) = \\sqrt{\\frac{\\sum_{b = 1}^r {(\\hat{\\theta}^{*}_b - \\bar{\\theta}^{*})}^2}{r - 1}} \\] where \\(\\bar{\\theta}^{*}\\) is the mean of bootstrap statistics, \\[ \\bar{\\theta}^{*} = \\frac{\\sum_{b = 1}^r}{r} . \\] 5.3 Confidence Intervals There are multiple ways to calculate confidence intervals from bootstrap. Normal-Theory Intervals Percentile Intervals ABC Intervals 5.4 Alternative methods 5.4.1 Parametric Bootstrap The parametric bootstrap draws samples from the estimated model. For example, in linear regression, we can start from the model, \\[ y_i = \\Vec{x}_i \\Vec{\\beta} + \\epsilon_i \\] Estimate the regression model to get \\(\\hat{\\beta}\\) and \\(\\hat{\\sigma}\\) For \\(1, \\dots, r\\) bootstrap replicates: Generate bootstrap sample \\((\\Vec{y}^{*}, \\Mat{X})\\), where \\(\\Mat{X}\\) are those from the original sample, and the values of \\(\\Vec{y}^{*}\\) are generated by sampling from the residual distribution, \\[ y_i^{*}_b = \\Vec{x}_i \\Vec{\\hat{\\beta}} + \\epsilon^{*}_{i,b} \\] where \\(\\epsilon^{*}_{i,b} \\sim \\mathrm{Normal}(0, \\hat{\\sigma})\\). Re-estimate a regression on \\((\\Vec{y}^{*}, \\Mat{X})\\) to estimate \\(\\hat{\\beta}^{*}\\). Calculate any statistics of the regression results. Alternatively, we could have drawn the values of \\(\\Vec{\\epsilon}^*_b\\) from the empirical distribution of residuals or the Wild Bootstrap. See the the discussion in the boot::boot() function, for sim = &quot;parametric&quot;. 5.4.2 Clustered bootstrap We can incorporate complex sampling methods into the bootstrap (Fox 2008, Sec 21.5). In particular, by resampling clusters instead of individual observations, we get the clustered bootstrap.(Esarey and Menger 2017) 5.4.3 Time series bootstrap Since data are not independent in time-series, variations of the bootstrap have to be used. See the references in the documentation for boot::tsboot. 5.4.4 How to sample? Draw the bootstrap sample in the same way it was drawn from the population (if possible) (Hesterberg 2015, 19) The are a few exceptions: Condition on the observed information. We should fix known quantities, e.g. observed sample sizes of sub-samples (Hesterberg 2015) For hypothesis testing, the sampling distribution needs to be modified to represent the null distribution (Hesterberg 2015) 5.4.5 Caveats Bootstrapping does not work well for the median or other quantities that depend on the small number of observations out of larger sample.(Hesterberg 2015) Uncertainty in the bootstrap estimator is due to both (1) Monte Carlo sampling (taking a finite number of samples), and (2) the sample itself. The former can be decreased by increasing the number of bootstrap samples. The latter is irreducible without a new sample. The bootstrap distribution will reflect the data. If the sample was “unusual”, then the bootstrap distribution will also be so.(Hesterberg 2015) In small samples there is a narrowness bias. (Hesterberg 2015, 24). As always, small samples is problematic. 5.4.6 Why use bootstrapping? The common practice of relying on asymmetric results may understate variability by ignoring dependencies or heteroskedasticity. These can be incorporated into bootstrapping.(Fox 2008, 602) it is general purpose algorithm that can generate standard errors and confidence intervals in cases where an analytic solution does not exist. however, it may require programming to implement and computational power to execute 5.5 Bagging Note that in all the previous discussion, the original point estimate is used. Bootstrapping is only used to generate (1) standard errors and confidence intervals (2). Bootstrap aggregating or bagging is a meta-algorithm that constructs a point estimate by averaging the point-estimates from bootstrap samples. Bagging can reduce the variance of some estimators, so can be thought of as a sort of regularization method. 5.6 Hypothesis Testing Hypothesis testing with bootstrap is more complicated. 5.7 How many samples? There is no fixed rule of thumb (it will depend on the statistic you are calculating and the population distribution), but if you want a single number, 1,000 is good lower bound. Higher levels of confidence require more samples Note that the results of the percentile method will be more variable than the normal-approximation method. The ABC confidence intervals will be even better. One ad-hoc recipe suggested here is: Choose a \\(B\\) Run the bootstrap Run the bootstrap again (ensure there is a different random number seed) If results differ, increase the size. Davidson and MacKinnon (2000) suggest the following: 5%: 399 1%: 1499 Though it also suggests a pre-test method. Hesterberg (2015) suggests far a larger bootstrap sample size: 10,000 for routine use. It notes that for a t-test, 15,000 samples for the a 95% probability that the one-sided levels fall within 10% of the true values, for 95% intervals and 5% tests. 5.8 References See Fox (2008 Ch. 21). Hesterberg (2015) is for “teachers of statistics” but is a great overview of bootstrapping. I found it more useful than the treatment of bootstrapping in many textbooks. For some Monte Carlo results on the accuracy of the bootstrap see Hesterberg (2015), p. 21. R packages. For general purpose bootstrapping and cross-validation I suggest the rsample package, which works well with the tidyverse and seems to be useful going forward. The boot package included in the recommended R packages is a classic package that implements many bootstrapping and resampling methods. Most of them are parallelized. However, its interface is not as nice as rsample. https://www.statmethods.net/advstats/bootstrapping.html http://avesbiodiv.mncn.csic.es/estadistica/boot1.pdf See this spreadsheet for some Monte Carlo simulations on Bootstrap vs. t-statistic. References "],
["prediction.html", "Chapter 6 Prediction Prerequisites 6.1 Prediction Questions vs. Causal Questions 6.2 Why is prediction important? 6.3 Many problems are prediction problems 6.4 Prediction vs. Explanation 6.5 Bias-Variance Tradeoff 6.6 Prediction policy problems", " Chapter 6 Prediction Prerequisites library(&quot;tidyverse&quot;) library(&quot;broom&quot;) library(&quot;jrnoldmisc&quot;) You will need to install jrnoldmisc with devtools::install_github(&quot;jrnold/jrnoldmisc&quot;) 6.1 Prediction Questions vs. Causal Questions Prediction vs. Causal questions can be reduced to: Do you care about \\(\\hat{y}\\) or \\(\\hat{beta}\\)? Take a standard regression model, \\[ y = X \\beta + \\epsilon . \\] We can use regression for prediction or causal inference. The difference is what we care about. In a prediction prediction problem we are interested in \\(\\hat{y} = X \\hat{\\beta}\\). The values of \\(\\hat{\\beta}\\) are not interesting in and of themselves. In a causal-inference problem we are are interested in getting the best estimate of \\(\\beta\\), or more generally \\(\\partial y / \\partial x\\) (the change in the response due to a change in x). If we had a complete model of the world, then we could use the same model for both these tasks. However, we don’t and never will. So there are different methods for each of these questions that are tailored to improving our estimates of those. 6.2 Why is prediction important? Much of the emphasis in social science is on “causal” questions, and “prediction” is often discussed pejoratively. Apart from the fact that this belief is often due to a deep ignorance of statistics and the philosophy of science and a lack of introspection into their own research, there are a few reasons why understanding prediction questions. 6.3 Many problems are prediction problems Causal inferential methods are best for estimating the effect of a policy intervention. Many problems in the political science are discussed as if they are causal, but any plausible research question is predictive since there is no plausible intervention to estimate. I would place many questions in international relations and comparative politics in this realm. 6.3.1 Counterfactuals The fundamental problem of causal inference is a prediction problem. We do not observe the counterfactuals, so we must predict what would have happened if a different treatment were applied. The currently developed causal inference methods are adapting methods and insights from machine learning into these causal inference models. 6.3.2 Controls The bias-variance trade-off is useful for helping to think about and choose control variables. 6.3.3 What does overfitting mean The term overfitting is often informally used. It has no meaning outside of prediction. 6.4 Prediction vs. Explanation Consider this regression model, \\[ y = \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon \\] where \\(y\\) is a \\(n \\times 1\\) vector and \\(\\epsilon\\) is a \\(n \\times 1\\) vector, \\[ \\epsilon_i \\sim \\mathrm{Normal}(0, \\sigma^2). \\] We will estimate two models on this data and compare their predictive performance: The true model, \\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon \\] and the underspecified model, \\[ y = \\beta_0 + \\beta_1 x_1 + \\epsilon \\] We will evaluate their performance by repeatedly sampling from the true distribution and comparing their out of sample performance. Write a function to simulate from the population. We will include the sample size, regression standard deviation, correlation between the covariates, and the coefficients as arguments. size: sample size sigma: the standard deviation of the population errors rho: the correlation between \\(x_1\\) and \\(x_2\\) beta: the coefficients (\\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\)) sim_data &lt;- function(size = 100, beta = c(0, 1, 1), rho = 0, sigma = 1) { # Create a matrix of size 1 dat &lt;- jrnoldmisc::rmvtnorm_df(size, loc = rep(0, 2), R = equicorr(2, rho)) # calc mean dat$fx &lt;- model.matrix(~ X1 + X2, data = dat) %*% beta %&gt;% as.numeric() dat$y &lt;- dat$fx + rnorm(size, 0, sigma ^ 2L) dat$y_test &lt;- dat$fx + rnorm(size, 0, sigma ^ 2L) dat } The output of sim_data is a data frame with size rows and columns X1, X2: The values of \\(x_1\\) and \\(x_2\\) fx: The mean function \\(f(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\) y: The values of \\(y\\) in the sample that will be used to train the model. y_test: Another draw of \\(y\\) from the population which will be used to evaluate the trained model. head(sim_data(100)) ## # A tibble: 6 x 5 ## X1 X2 fx y y_test ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.616 0.721 0.105 -2.19 -0.0293 ## 2 -0.728 -1.12 -1.85 -1.84 -2.07 ## 3 1.05 0.793 1.85 0.809 -0.250 ## 4 0.501 -0.931 -0.429 -0.203 -0.947 ## 5 0.944 -0.152 0.792 1.46 1.39 ## 6 -0.509 -0.290 -0.799 0.362 -0.664 For each training and test samples we draw we want to fit the true model using y evaluate the prediction accuracy of the true model on y_test fit the underspecified model using y evaluate the prediction accuracy of the underspecified model on y_test The function sim_predict does this sim_predict &lt;- function(f, data) { # run regression mod &lt;- lm(f, data = data) # predict the y_test values augdat &lt;- augment(mod, data = data) %&gt;% # evaluate and return MSE mutate(err_out = (.fitted - y_test) ^ 2, err_in = (.fitted - y) ^ 2) tibble(r_squared = glance(mod)$r.squared, mse_in = mean(augdat$err_in), mse_out = mean(augdat$err_out)) } So each simulation is: data &lt;- sim_data(100, rho = 0.9, sigma = 3) mod_under &lt;- sim_predict(y ~ X1, data = data) mod_under ## # A tibble: 1 x 3 ## r_squared mse_in mse_out ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0446 85.0 65.2 mod_true &lt;- sim_predict(y ~ X1 + X2, data = data) mod_true ## # A tibble: 1 x 3 ## r_squared mse_in mse_out ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0490 84.6 64.6 We are estimating the expected error of new data. Without an analytical solution, we need to simulate this. The run_sim function simulates new test and training samples of y and y_test, runs both the true and underspecified models on them, and returns the results as a data frame with two rows the columns r_squared: In-sample \\(R^2\\) mse_in: In-sample mean-squared-error. mse_out: Out-of-sample mean-squared-error. model: Either “true” or “underspecified” to indicate the model. .iter: An iteration number, used only for bookkeeping. run_sim &lt;- function() { data &lt;- sim_data(100, rho = 0.9, sigma = 3) mod_under &lt;- sim_predict(y ~ X1, data = data) %&gt;% mutate(model = &quot;underspecified&quot;) mod_true &lt;- sim_predict(y ~ X1 + X2, data = data) %&gt;% mutate(model = &quot;true&quot;) bind_rows(mod_under, mod_true) } Run the simulation n_sims times and then calculate the mean \\(R^2\\), in-sample MSE, and out-of-sample MSE: n_sims &lt;- 512 rerun(n_sims, run_sim()) %&gt;% bind_rows() %&gt;% group_by(model) %&gt;% summarise_all(funs(mean)) ## # A tibble: 2 x 4 ## model r_squared mse_in mse_out ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 true 0.0637 78.4 83.7 ## 2 underspecified 0.0511 79.4 82.9 Generally, the underspecified model can yield more accurate predictions when (Shmueli 2010): data are very noisy (large \\(\\sigma\\)). In these cases, increasing the complexity of the model will increase variance with little decrease in the variance since most of the variation in the sample is simply noise. magnitude of omitted variables are small. In this case, those omitted variables don’t predict the response well, but could increase the overfitting in samples. predictors are highly correlated. In this case, the information contained in the omitted variables is largely contained in the original variables. sample size is small or the range of left out variables is small. See Shmueli (2010) for more. Exercise Try different parameter values for the simulation to confirm this. The take-away. Prediction doesn’t necessarily select the “true model”, and knowing the “true model” may not help prediction. Note that this entire exercise operated in an environment in which we knew the true model and thus does not resemble any realistic situation. Since “all models are wrong” the question is not whether it is useful to use the “true” model. What this simulation reveals is our models of the world are contingent on the size and quality of the data. If the data are noisy or few, then we need to use simpler models. If the covariates are highly correlated, it may not matter which one one we use in our theory. 6.5 Bias-Variance Tradeoff Consider the general regression setup, \\[ Y = f(\\Vec{X}) + \\epsilon, \\] where \\[ \\begin{aligned}[t] \\E[\\epsilon] &amp;= 0 &amp; \\Var[\\epsilon] &amp;= \\sigma^2 . \\end{aligned} \\] When given a random pair \\((X, Y)\\), we would like to “predict” \\(Y\\) with some function of \\(X\\), say, \\(f(X)\\). However, in general we do not know \\(f(X)\\). So given some data consisting of realizations of pairs of \\(X\\) and \\(Y\\), \\(\\mathcal{D} = (x_i, y_i)\\), the goal of regression is to estimate function \\(\\hat{f}\\) that is a good approximation of the true function \\(f\\). What is a good \\(\\hat{f}\\) function? A good \\(\\hat{f}\\) will have low expected prediction error (EPE), which is the error for predicting a new observation. \\[ \\begin{aligned}[t] EPE(Y, \\hat{f}(x)) &amp;= \\mathbb{E}\\left[(y - \\hat{f}(x))^2\\right] \\\\ &amp;= \\underbrace{\\left(\\mathbb{E}(\\hat{f}(x)) - f(x)\\right)^{2}}_{\\text{bias}} + \\underbrace{\\mathbb{E}\\left[\\hat{f}(x) - \\mathbb{E}(\\hat{f}(x))\\right]^2}_{\\text{variance}} + \\underbrace{\\mathbb{E}\\left[y - f(x)\\right]^{2}}_{\\text{irreducible error}} \\\\ &amp;= \\underbrace{\\mathrm{Bias}^2 + \\mathbb{V}[\\hat{f}(x)]}_{\\text{reducible error}} + \\sigma^2 \\end{aligned} \\] In general, there is a bias-variance tradeoff. The following three plots are three stylized examples of bias variance tradeoffs: when the variance influence the prediction error more than bias, when neither is dominant, and when the bias is more important. As model complexity increases, bias decreases, while variance increases. There is some some sweet spot in model complexity that minimizes the expected prediction error. By understanding the tradeoff between bias and variance, we can find a model complexity to predict unseen observations well. 6.5.1 Example Consider the function, \\[ y = x^2 + \\epsilon \\] where \\(\\epsilon \\sim \\mathrm{Normal}(0, 1)\\) Here is an example of some data generated from this model. We will write a function to calculate \\(f(x)\\). regfunc &lt;- function(x) { x ^ 2 } Write a function that draws a single sample from the model. sim_data &lt;- function(x) { sigma &lt;- 1 # number of rows n &lt;- length(x) # proportion of observations in the test set p_test &lt;- 0.3 tibble(x = x, fx = regfunc(x), y = fx + rnorm(n, 0, sd = sigma), test = sample(c(TRUE, FALSE), size = n, replace = TRUE)) } Calculate this function n &lt;- seq(0, 1, length.out = 30) sim_data(n) %&gt;% ggplot(aes(x = x)) + geom_point(aes(y = y, colour = test)) + geom_line(aes(y = fx)) For fit the data we will estimate polynomial models of increasing complexity, from only an intercept to a polynomial of degree 4. \\(y_i = \\beta_0\\) \\(y_i = \\beta_0 + \\beta_1 x\\) \\(y_i = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\) \\(y_i = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3x^3\\) \\(y_i = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_3 x^4\\) We will write a function to estimate these models. As input it takes the degree of the polynomial, the data to use to estimate it, and (optionally) an .iter variable that can be used to keep track of which iteration it is from. est_poly &lt;- function(degree, data, .iter = NULL) { if (degree == 0) { mod &lt;- lm(y ~ 1, data = filter(data, !test)) } else { mod &lt;- lm(y ~ poly(x, degree), data = filter(data, !test)) } out &lt;- augment(mod, newdata = filter(data, test)) %&gt;% mutate(degree = degree) %&gt;% select(-.se.fit) out[[&quot;.iter&quot;]] &lt;- .iter out } For example, we will use a fixed \\(x\\) in each model. We will use an evenly spaced grid between 0 and 1. x &lt;- seq(0, 1, length.out = 100) data &lt;- sim_data(x) est_poly(2, data) ## x fx y test .fitted degree ## 1 0.00000000 0.0000000000 0.89041816 TRUE -0.225623535 2 ## 2 0.01010101 0.0001020304 0.29205861 TRUE -0.215626796 2 ## 3 0.02020202 0.0004081216 0.44925488 TRUE -0.205634048 2 ## 4 0.06060606 0.0036730946 -0.03221284 TRUE -0.165702965 2 ## 5 0.07070707 0.0049994898 0.15205437 TRUE -0.155730172 2 ## 6 0.08080808 0.0065299459 0.40707455 TRUE -0.145761370 2 ## 7 0.09090909 0.0082644628 -0.30919290 TRUE -0.135796558 2 ## 8 0.10101010 0.0102030405 -0.65388315 TRUE -0.125835737 2 ## 9 0.11111111 0.0123456790 1.93214526 TRUE -0.115878908 2 ## 10 0.14141414 0.0199979594 -0.43103603 TRUE -0.086032364 2 ## 11 0.15151515 0.0229568411 -0.85837114 TRUE -0.076091499 2 ## 12 0.18181818 0.0330578512 -0.42607043 TRUE -0.046292847 2 ## 13 0.23232323 0.0539740843 2.12900106 TRUE 0.003291755 2 ## 14 0.26262626 0.0689725538 0.83605063 TRUE 0.032994624 2 ## 15 0.29292929 0.0858075707 -0.54569890 TRUE 0.062661575 2 ## 16 0.30303030 0.0918273646 1.90519280 TRUE 0.072542577 2 ## 17 0.31313131 0.0980512193 -1.15360371 TRUE 0.082419588 2 ## 18 0.33333333 0.1111111111 -0.50180729 TRUE 0.102161637 2 ## 19 0.34343434 0.1179471483 0.38956259 TRUE 0.112026675 2 ## 20 0.37373737 0.1396796245 -1.42193217 TRUE 0.141597844 2 ## 21 0.42424242 0.1799816345 0.38510327 TRUE 0.190803306 2 ## 22 0.49494949 0.2449750026 0.42090271 TRUE 0.259523334 2 ## 23 0.53535354 0.2866034078 0.36153111 TRUE 0.298704120 2 ## 24 0.55555556 0.3086419753 1.56795728 TRUE 0.318270568 2 ## 25 0.63636364 0.4049586777 1.32440427 TRUE 0.396376722 2 ## 26 0.64646465 0.4179165391 1.08913606 TRUE 0.406122031 2 ## 27 0.67676768 0.4580144883 2.19561530 TRUE 0.435334016 2 ## 28 0.68686869 0.4717885930 0.36778411 TRUE 0.445063362 2 ## 29 0.71717172 0.5143352719 -0.31989160 TRUE 0.474227455 2 ## 30 0.72727273 0.5289256198 -0.37991742 TRUE 0.483940837 2 ## 31 0.75757576 0.5739210285 1.14267823 TRUE 0.513057039 2 ## 32 0.76767677 0.5893276196 -0.24018247 TRUE 0.522754458 2 ## 33 0.78787879 0.6207529844 1.04576276 TRUE 0.542137322 2 ## 34 0.79797980 0.6367717580 1.64778212 TRUE 0.551822768 2 ## 35 0.80808081 0.6529945924 2.00215625 TRUE 0.561504223 2 ## 36 0.81818182 0.6694214876 1.18357836 TRUE 0.571181687 2 ## 37 0.82828283 0.6860524436 1.23946467 TRUE 0.580855160 2 ## 38 0.83838384 0.7028874605 2.63472582 TRUE 0.590524642 2 ## 39 0.84848485 0.7199265381 1.23562945 TRUE 0.600190134 2 ## 40 0.86868687 0.7546168758 0.02923681 TRUE 0.619509143 2 ## 41 0.87878788 0.7722681359 0.68094578 TRUE 0.629162662 2 ## 42 0.88888889 0.7901234568 -0.05673413 TRUE 0.638812189 2 ## 43 0.90909091 0.8264462810 -0.46999024 TRUE 0.658099271 2 ## 44 0.94949495 0.9015406591 0.09575092 TRUE 0.696625544 2 ## 45 0.95959596 0.9208244057 -0.64757995 TRUE 0.706247135 2 ## 46 0.96969697 0.9403122130 -0.55067372 TRUE 0.715864735 2 ## 47 0.98989899 0.9799000102 0.01762342 TRUE 0.735087962 2 Draw one sample from the data and run all model run_sim &lt;- function(.iter) { # degrees of models to evaluate degrees &lt;- 0:5 # the grid of data to sample x &lt;- seq(0, 1, length.out = 64) data &lt;- sim_data(x) # run all models map_df(degrees, est_poly, data = data, .iter = .iter) } Run the full simulation, drawing n_sims samples, running all the different models estimates. n_sims &lt;- 2 ^ 12 all_sims &lt;- map_df(seq_len(n_sims), ~ run_sim(.x)) For each model plot the expected regression line at the values of \\(x\\), which we’ll define as the average prediction of the model at each point.2 \\[ \\hat{f}(X = x) = \\frac{1}{S} \\sum_{s = 1}^S \\hat{E}(y | X = x) \\] ggplot() + geom_line(data = filter(all_sims, .iter &lt; 10), mapping = aes(x = x, y = .fitted, group = .iter)) + geom_line(data = filter(all_sims, .iter == 1), mapping = aes(x = x, y = fx), colour = &quot;red&quot;) + facet_wrap(~ degree) Now calculate the bias and variance of these models at each \\(x\\). poly_estimators &lt;- all_sims %&gt;% group_by(degree, x) %&gt;% summarise(estimate = mean(.fitted), variance = var(.fitted), mse_in = mean((.fitted - fx)[!test]), mse_out = mean((.fitted - fx)[test], na.rm = TRUE), fx = mean(fx)) Plot the values of \\(\\hat{f}(x)\\) for all models against the true model. On average squared model fits the true function well, and higher order polynomials cannot improve it. ggplot(poly_estimators, aes(x = x, y = estimate, colour = factor(degree))) + geom_line() poly_estimators %&gt;% mutate(bias = estimate - fx) %&gt;% group_by(degree) %&gt;% summarise(bias2 = mean(bias ^ 2), variance = mean(variance, na.rm = TRUE), mse_in = mean(mse_in, na.rm = TRUE), mse_out = mean(mse_out, na.rm = TRUE)) %&gt;% gather(variable, value, -degree) %&gt;% ggplot(aes(x = degree, y = value, colour = variable)) + geom_line() Since \\(\\hat{f}\\) varies sample to sample, there is variance in \\(\\hat{f}\\). However, OLS requires zero bias in sample, and thus means that there is no trade-off. 6.5.2 Overview low bias, high variance (overfit) more complex (flexible functions) estimated function closer to the true function estimated function varies more, sample to sample overfit high bias, low variance (underfit) simple function simpler estimated function estimated function varies less, sample to sample underfit What to do? low bias, high variance: simplify model high bias, low variance: make model more complex high bias, high variance: more data low bias, low variance: your good The general rule. more training data reduces both bias and variance regularization and model selection methods can choose an optimal bias/variance trade-off 6.6 Prediction policy problems Kleinberg et al. (2015) distinguish two types of policy questions. Consider two questions related to rain. In 2011, Governor Rick Perry of Texas designated days for prayer for rain in order to end the Texas drought. It is cloudy out. Do you bring an umbrella (or rain coat) when leaving the house? How does the pray-for-rain problem differ from the umbrella problem? Prayer problems are causal questions, because the payoff depends on the causal question as to whether a prayer-day can cause rain. Umbrella questions are prediction problems, because an umbrella does not cause rain. However, the utility of bringing an umbrella depends on the probability of rain. Many policy problems are a mix of prediction and causation. The policymaker needs to know whether the intervention has a causal effect, and also the predicted value of some other value which will determine how useful the intervention is. More formally, let \\(y\\) be an outcome variable which depends on the values of \\(x\\) (\\(x\\) may cause \\(y\\)). Let \\(u(x, y)\\) be the policymaker’s payoff function. The change in utility with response to a new policy (\\(\\partial u(x, y) / \\partial x)\\) can be decomposed into two terms, \\[ \\frac{\\partial u(x, y)}{\\partial x} = \\frac{\\partial u}{\\partial x} \\times \\underbrace{y}_{\\text{prediction}} + \\frac{\\partial u}{\\partial y} \\times \\underbrace{\\frac{\\partial y}{\\partial x}}_{\\text{causation}} . \\] Understanding the payoff of a policy requires understanding the two unknown terms \\(\\frac{\\partial u}{\\partial x}\\): how does \\(x\\) affect the utility. This needs to evaluated at the value of \\(y\\), which needs to be predicted. The utility of carrying an umbrella depends on whether it rains or no. This is predictive. \\(\\frac{\\partial y}{\\partial x}\\): how does \\(y\\) change with changes in \\(x\\)? This is causal. 6.6.1 References Parts of the bias-variance section are derived from R for Statistical Learning, Bias-Variance Tradeoff Also see: Understanding the Bias-Variance Tradeoff References "],
["cross-validation.html", "Chapter 7 Cross-Validation Prerequisites 7.1 Example: Predicting Bordeaux Wine 7.2 Cross Validation 7.3 Out-of-Sample Error 7.4 Approximations 7.5 References", " Chapter 7 Cross-Validation Prerequisites library(&quot;tidyverse&quot;) library(&quot;broom&quot;) library(&quot;modelr&quot;) library(&quot;purrr&quot;) library(&quot;stringr&quot;) 7.1 Example: Predicting Bordeaux Wine The bordeaux dataset contains the prices of vintages of Bordeaux wines sold at auction in New York, as well as the the weather and rainfall for the years of the wine. AshenfelterAshemoreLalonde1995a uses this data was used to show that the quality of a wine vintage, as measured by its price, can largely be predicted by a small number of variables. At the time, these prediction were not taken kindly by wine connoisseurs.[^wine] bordeaux &lt;- datums::bordeaux %&gt;% mutate(lprice = log(price / 100)) %&gt;% filter(!is.na(price)) The data contains 27 prices of Bordeaux wines sold at auction in New York in 1993 for vintages from 1952 to 1980; the years 1952 and 1954 were excluded because they were rarely sold. Prices are measured as an index where 100 is the price of the 1961. The dataset also includes three predictors of the price of each vintage: time_sv: Time since the vintage, where 0 is 1983. wrain: Winter (October–March) rain hrain: Harvest (August–September) rain degrees: Average temperature in degrees centigrade from April to September in the vintage year. The first variable to consider is the age of the vintage and the price: ggplot(filter(bordeaux, !is.na(price), !is.na(vint)), aes(y = log(price), x = vint)) + geom_point() + geom_rug() + geom_smooth(method = &quot;lm&quot;) Ashenfelter, Ashmore, and Lalonde (1995) run two models. All models were estimated using OLS with log-price as the outcome variable. The predictors in the models were: vintage age vintage age, winter rain, harvest rain We will start by considering these models. Since we are running several models, we’ll define the model formulas in a list mods_f &lt;- list(lprice ~ time_sv, lprice ~ time_sv + wrain + hrain + degrees) Run each model and store the results in a data frame as list column of lm objects: mods_res &lt;- tibble( model = seq_along(mods_f), formula = map_chr(mods_f, deparse), mod = map(mods_f, ~ lm(.x, data = bordeaux)) ) mods_res ## # A tibble: 2 x 3 ## model formula mod ## &lt;int&gt; &lt;chr&gt; &lt;list&gt; ## 1 1 lprice ~ time_sv &lt;S3: lm&gt; ## 2 2 lprice ~ time_sv + wrain + hrain + degrees &lt;S3: lm&gt; Now that we have these models, extract the coefficients into a data frame with the broom function tidy: mods_coefs &lt;- mods_res %&gt;% # Add column with the results of tidy for each model # conf.int = TRUE adds confidence intervals to the data mutate(tidy = map(mod, tidy, conf.int = TRUE)) %&gt;% # use unnest() to expand the data frame to one row for each row in the tidy # elements unnest(tidy, .drop = TRUE) glimpse(mods_coefs) ## Observations: 7 ## Variables: 9 ## $ model &lt;int&gt; 1, 1, 2, 2, 2, 2, 2 ## $ formula &lt;chr&gt; &quot;lprice ~ time_sv&quot;, &quot;lprice ~ time_sv&quot;, &quot;lprice ~ ti... ## $ term &lt;chr&gt; &quot;(Intercept)&quot;, &quot;time_sv&quot;, &quot;(Intercept)&quot;, &quot;time_sv&quot;, ... ## $ estimate &lt;dbl&gt; -2.025199170, 0.035429560, -12.145333577, 0.02384741... ## $ std.error &lt;dbl&gt; 0.2472286519, 0.0136624941, 1.6881026571, 0.00716671... ## $ statistic &lt;dbl&gt; -8.191604, 2.593199, -7.194665, 3.327521, 2.420525, ... ## $ p.value &lt;dbl&gt; 1.522111e-08, 1.566635e-02, 3.278791e-07, 3.055739e-... ## $ conf.low &lt;dbl&gt; -2.534376e+00, 7.291126e-03, -1.564624e+01, 8.984546... ## $ conf.high &lt;dbl&gt; -1.516022230, 0.063567993, -8.644422940, 0.038710280... walk(mods_res$mod, ~ print(summary(.x))) ## ## Call: ## lm(formula = .x, data = bordeaux) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.8545 -0.4788 -0.0718 0.4562 1.2457 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.02520 0.24723 -8.192 1.52e-08 *** ## time_sv 0.03543 0.01366 2.593 0.0157 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5745 on 25 degrees of freedom ## Multiple R-squared: 0.212, Adjusted R-squared: 0.1804 ## F-statistic: 6.725 on 1 and 25 DF, p-value: 0.01567 ## ## ## Call: ## lm(formula = .x, data = bordeaux) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.46027 -0.23864 0.01347 0.18600 0.53446 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.215e+01 1.688e+00 -7.195 3.28e-07 *** ## time_sv 2.385e-02 7.167e-03 3.328 0.00306 ** ## wrain 1.167e-03 4.820e-04 2.421 0.02420 * ## hrain -3.861e-03 8.075e-04 -4.781 8.97e-05 *** ## degrees 6.164e-01 9.518e-02 6.476 1.63e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2865 on 22 degrees of freedom ## Multiple R-squared: 0.8275, Adjusted R-squared: 0.7962 ## F-statistic: 26.39 on 4 and 22 DF, p-value: 4.058e-08 Likewise, extract model statistics such as, \\(R^2\\), adjusted \\(R^2\\), and \\(\\hat{\\sigma}\\): mods_glance &lt;- mutate(mods_res, .x = map(mod, glance)) %&gt;% unnest(.x, .drop = TRUE) mods_glance %&gt;% select(formula, r.squared, adj.r.squared, sigma) ## # A tibble: 2 x 4 ## formula r.squared adj.r.squared sigma ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lprice ~ time_sv 0.212 0.180 0.574 ## 2 lprice ~ time_sv + wrain + hrain + degrees 0.828 0.796 0.287 7.2 Cross Validation k &lt;- 20 f &lt;- map(seq_len(k), ~ as.formula(str_c(&quot;lprice ~ poly(time_sv, &quot;, .x, &quot;)&quot;))) names(f) &lt;- seq_len(k) mods_overfit &lt;- map(f, ~ lm(.x, data = bordeaux)) fits &lt;- map_df(mods_overfit, glance, .id = &quot;.id&quot;) fits %&gt;% select(.id, r.squared, adj.r.squared, df.residual) %&gt;% gather(stat, value, -.id) %&gt;% mutate(.id = as.integer(.id)) %&gt;% ggplot(aes(x = .id, y = value)) + geom_point() + geom_line() + facet_wrap(~ stat, ncol = 2, scales = &quot;free&quot;) The larger the polynomial, the more wiggly the line. library(&quot;modelr&quot;) invoke(gather_predictions, .x = c(list(data = bordeaux), mods_overfit)) %&gt;% ggplot(aes(x = vint)) + geom_point(aes(y = lprice)) + geom_line(aes(y = pred, group = model, colour = as.numeric(model))) Intuitively it seems that as we increase the flexibility of the model by increasing the number of variables the model is overfitting the data, but what does it actually mean to overfit? If we use \\(R^2\\) as the “measure of fit”, more variables always leads to better fit. Adjusted \\(R^2\\) does not increase, because the decrease in errors is offset by the increase in the degrees of freedom. However, there is little justification for the specific formula of \\(R^2\\). The problem with over-fitting is that the model starts to fit peculiarities of the sample (errors) rather than the true model. Since we never know the true model, all we can check is if the model predicts new data. 7.3 Out-of-Sample Error To compare predictive models, we want to compare how well they predicts (duh). This means estimating how well it will work on new data. The problem with this is that new data is just that, …, new. The trick is to reuse the sample data to get an estimate of how well the model will work on new data. This is done by fitting the model on a subset of the data, and predicting another subset of the data which was not used to fit the model; often this is done repeatedly. There are a variety of ways to do this, depending on the nature of the data and the predictive task. However, they all implicitly assume that the sample of data that was used to fit the model is representative of future data. many_mods &lt;- list( lprice ~ time_sv, lprice ~ wrain, lprice ~ hrain, lprice ~ degrees, lprice ~ wrain + hrain + degrees, lprice ~ time_sv + wrain + hrain + degrees, lprice ~ time_sv + wrain * hrain * degrees, lprice ~ time_sv * (wrain + hrain + degrees), lprice ~ time_sv * wrain * hrain * degrees ) 7.3.1 Held-out data A common rule of thumb is to use 70% of the data for training, and 30% of the data for testing. In this case, let’s partition the data to use the first 70% of the observations as training data, and the remaining 30% of the data as testing. n_test &lt;- round(0.3 * nrow(bordeaux)) n_train &lt;- nrow(bordeaux) - n_test mod_train &lt;- lm(lprice ~ time_sv + wrain + hrain + degrees, data = head(bordeaux, n_train)) mod_train ## ## Call: ## lm(formula = lprice ~ time_sv + wrain + hrain + degrees, data = head(bordeaux, ## n_train)) ## ## Coefficients: ## (Intercept) time_sv wrain hrain degrees ## -1.080e+01 1.999e-02 9.712e-04 -4.461e-03 5.533e-01 # in-sample RMSE sqrt(mean(mod_train$residuals ^ 2)) ## [1] 0.2280059 The out-of-sample RMSE is higher than the in-sample RMSE. outsample &lt;- augment(mod_train, newdata = tail(bordeaux, n_test)) sqrt(mean( (outsample$lprice - outsample$.fitted) ^ 2)) ## [1] 0.351573 This is common, but not necessarily the case. But note that this value is highly dependent on the subset of data used for testing. In some sense, we may choose as model that “overfits” the testing data. 7.3.2 \\(k\\)-fold Cross-validation A more robust approach is to repeat this training/testing split multiple times. The most common approach is to to partition the data into k-folds, and use each fold once as the testing subset, where the model is fit on the other \\(k - 1\\) folds. A common rule of thumb is to use 5 to 10 folds. cv &lt;- modelr::crossv_kfold(bordeaux, k = 10) cv_rmse &lt;- function(f, cv) { fits &lt;- map(cv$train, ~ lm(f, data = .x, model = FALSE)) } 7.3.3 Leave-one-Out Cross-Validation Leave-one-out cross validation estimates is a \\(k\\)-fold cross-validation with folds equal to the number of observations. The model is fit \\(n\\) times, leaving training the model on \\(n - 1\\) observations and predicted the remaining observation. cv &lt;- modelr::crossv_kfold(bordeaux, k = nrow(bordeaux)) 7.4 Approximations For some models, notably linear regression, analytical approximations to the expected out of sample error can be made. Each of these approximations will make some slightly different assumptions to plug in some unknown values. In linear regression, the LOO-CV MSE can be calculated analytically, and without simulation. It is (James et al. 2013, 180): \\[ \\text{LOO-CV} = \\frac{1}{n} \\sum_{i = 1}^n {\\left(\\frac{y_i - \\hat{y}_i}{1 - h_i} \\right)}^2 = \\frac{1}{n} \\sum_{i = 1}^n {\\left(\\frac{\\hat{\\epsilon}_i}{1 - h_i} \\right)}^2 = \\frac{1}{n} \\times \\text{PRESS} \\] where PRESS is the predictive residual sum of squares, and \\(h_i\\) is the hat-value of observation \\(i\\) (Fox 2016, 270, 289), \\[ h_i = \\Mat{X}(\\Mat{X}&#39; \\Mat{X})^{-1} \\Mat{X}&#39; \\] loocv &lt;- function(x) { mean( (residuals(x) / (1 - hatvalues(x))) ^ 2) } An alternative approximation of the expected out-of-sample error is the generalized cross-validation criterion (GCV) is (Fox 2016), \\[ \\text{GCV} = \\frac{n \\times RSS}{df_{res}^2} = \\frac{n \\sum \\hat{\\epsilon}^2}{(n - k - 1)^2} \\] gcv &lt;- function(x) { err &lt;- residuals(x) rss &lt;- sum(err ^ 2) length(err) * rss / (x[[&quot;df.residual&quot;]] ^ 2) } Other measures that are also equivalent to some form of an estimate of the out-of-sample error are the AIC and BIC. 7.5 References https://tomhopper.me/2014/05/16/can-we-do-better-than-r-squared/ http://andrewgelman.com/2007/08/29/rsquared_useful/ References "],
["regularization.html", "Chapter 8 Regularization 8.1 Ridge Regression 8.2 Regularization for Causal Inference 8.3 References", " Chapter 8 Regularization library(&quot;glmnet&quot;) ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## expand ## Loading required package: foreach ## ## Attaching package: &#39;foreach&#39; ## The following objects are masked from &#39;package:purrr&#39;: ## ## accumulate, when ## Loaded glmnet 2.0-16 library(&quot;tidyverse&quot;) library(&quot;broom&quot;) UScrime &lt;- MASS::UScrime %&gt;% mutate_at(vars(y, M, Ed, Po1, Po2, LF, M.F, Pop, NW, U1, U2, GDP, Ineq, Prob, Time), funs(log)) varlist &lt;- c(&quot;M&quot;, &quot;Ed&quot;, &quot;Po1&quot;, &quot;Po2&quot;, &quot;LF&quot;, &quot;M.F&quot;, &quot;Pop&quot;, &quot;NW&quot;, &quot;U1&quot;, &quot;U2&quot;, &quot;GDP&quot;, &quot;Ineq&quot;, &quot;Prob&quot;, &quot;Time&quot;) By default, glmnet will return and entire range of coefficients. mod_lasso &lt;- glmnet(as.matrix(UScrime[, varlist]), UScrime[[&quot;y&quot;]]) mod_ridge &lt;- glmnet(as.matrix(UScrime[, varlist]), UScrime[[&quot;y&quot;]], alpha = 0) bind_rows( mutate(tidy(mod_lasso), model = &quot;Lasso&quot;), mutate(tidy(mod_ridge), model = &quot;Ridge&quot;) ) %&gt;% filter(term != &quot;(Intercept)&quot;) %&gt;% ggplot(aes(x = dev.ratio, y = estimate, colour = term)) + geom_line() + facet_wrap(~ model, ncol = 1) Alternatively, the lasso and ridge regression models are the solutions to the problems \\[ \\hat{\\beta}_{\\text{lasso}} = \\arg \\min_\\beta \\left\\{ \\sum_{i =1}^n \\left(y_i - \\beta_0 - \\sum_{j = 1}^p \\beta_j x_{ij} \\right)^{2} \\right\\} \\text{s.t.} \\sum_{j = 1}^p \\beta_j^2 \\leq c, \\] and \\[ \\begin{aligned}[t] \\hat{\\beta}_{\\text{lasso}} &amp;= \\arg \\min_\\beta \\left\\{ \\sum_{i =1}^n \\left(y_i - \\beta_0 - \\sum_{j = 1}^p \\beta_j x_{ij} \\right)^{2} \\right\\} \\\\ \\text{s.t.}&amp; \\sum_{j = 1}^p |\\beta_j| \\leq c \\end{aligned} \\] In other words, these methods try to find the \\(\\Vec{\\beta}\\) with the smallest sum of squared errors that has a \\(\\Vec{\\beta}\\) with a norm less than \\(c\\). The value of \\(c\\) corresponds to some value of \\(\\lambda\\) in the previous methods. Think of \\(c\\) as a fixed budget. The lasso and ridge regressions try to find the variables that explain \\(y\\) the best without going over the budget (James et al. 2013, 221): Consider the case with only coefficients: \\(\\beta_1\\) and \\(\\beta_2\\). In the lasso, we want to find the values of \\(\\beta_1\\) and \\(\\beta_2\\) \\[ |\\beta_1| + |\\beta_2| \\leq c \\] never trust OLS with more than five regressors — Zvi Grilliches Regularization theory was one of the first signs of the existence of intelligent inference — Zapnik Rather than choose the best fit, there is some penalty to avoid over-fitting. This is to choose the optimal optimal point on the expected predicted value. There are two questions method of regularization amount of regularization There are several choices of the former, chosen for different reasons. The latter is almost always chosen by cross-validation. While OLS is okay for estimating \\(\\beta\\) (best linear unbiased property). However, with \\(K \\geq 3\\) regressors, OLS is poor. The approaches to regularization in regression are Shrink estimates to zero (Ridge) Sparsity, limit number of non-zero estimates (Lasso) Combination of the two (Bridge) 8.1 Ridge Regression \\[ \\hat{\\beta}_{\\text{OLS}} = \\arg \\min_{\\beta} \\sum_{i=1}^{n} (y_i - \\Vec{x}_{i} \\Vec{\\beta})^{2} \\] Regularized regression adds a penalty that is a function of \\(\\beta\\). This encourages \\(\\beta\\) to be close to zero. \\[ \\hat{\\beta}_{\\text{regularized}} = \\arg \\min_{\\beta} \\sum_{i=1}^{n} (y_i - \\Vec{x}_{i} \\Vec{\\beta})^{2} + \\lambda f(\\beta) \\] Where \\(\\lambda\\) is a penalty parameter, and \\(f(\\beta)\\) is a function that increases in the total magnitudes of the coefficients. \\(\\lambda \\to \\infty\\): all coefficients are zero \\(\\lambda \\to 0\\): same as OLS How do we choose the value of \\(\\lambda\\)? Currently: cross-validation Historically: there were some default plug-in estimators, especially for ridge regression. Ridge regression penalizes the \\(\\Vec{\\beta}\\) vector by the \\[ \\hat{\\beta}_{\\text{ridge}} = \\arg \\min_{\\beta} \\sum_{i=1}^{n} (y_i - \\Vec{x}_{i} \\Vec{\\beta})^{2} + \\sum_{k = 1}^{p} \\beta_k^2 \\] Lasso penalizes the coefficients by an the \\(L1\\) norm. Suppose we want to find the best subset of \\(\\leq k\\) covariates . \\[ \\hat{\\beta}_{\\text{lasso}} = \\arg \\min_{\\beta} \\sum_{i=1}^{n} (y_i - \\Vec{x}_{i} \\Vec{\\beta})^{2} + \\lambda \\sum_{k = 1}^p |\\beta_k| \\] If true distribution of coefficients is a few big ones and many small ones, LASSO will do better. If many small/modest sized effects, ridge may do better. LASSO does not work well with highly correlated coefficients. Ridge: \\(\\hat{\\beta}_{1} + \\hat{\\beta}_{2} \\approx (\\beta_1 + \\beta_2)/ 2\\). LASSO: Indifferent between \\(\\hat{\\beta}_1 = 0\\), \\(\\hat{\\beta}_2 = \\beta_1 + \\beta_2\\), \\(\\hat{\\beta}_1 = \\beta_1 + \\beta_2\\), and \\(\\hat{\\beta}_2 = 0\\). Approximate best-subset selection. Suppose that we would really like to select the best subset of \\(q &lt; k\\) coefficients and set the rest to zero (this is the variable selection problem). That is a hard problem since there are \\(\\binom{k}{q}\\). Lasso can be viewed as an approximation of the problem. Oracle property. If the true model is sufficiently sparse, we can ignore the selection stage and use OLS standard errors of the non-zero variables for inference. Bridge regression penalizes the \\(\\Vec{\\beta}\\) vector by the \\[ \\hat{\\beta}_{\\text{bridge}} = \\arg \\min_{\\beta} \\sum_{i=1}^{n} (y_i - \\Vec{x}_{i} \\Vec{\\beta})^{2} + \\lambda_1 \\sum_{k = 1}^{p} |\\beta_k| + \\lambda_2 \\sum_{k = 1}^{p} \\beta_k^2 \\] Bridge regression has some of the properties of both ridge and Lasso. It will select correlated regressors, yet also shrink coefficients to zero for a sparse solution. The R package glmnet is the most commonly used package to estimate Lasso, ridge, and bridge regression for linear and generalized linear models. However, these methods are common enough that all machine learning frameworks will have some implementation of them. See other packages for variations on the lasso that take into account other dependencies in the data. How to find the value of \\(\\lambda\\)? Cross validation. The function cv.glmet() uses cross-validation to select the penalty parameter. 8.2 Regularization for Causal Inference Belloni, Chernozhukov, and Hansen (2014) propose a simple method for using Lasso for causal effects. What’s the problem with regularized regression for causal inference? Suppose we estimate a model with the aim to recover \\(\\beta_1\\). \\[ \\Vec{y} = \\alpha + \\beta x + \\gamma_1 z_1 + \\cdots + \\gamma_k z_{k-1} + \\epsilon \\] If we estimate it with a regularized model, like lasso, then \\(\\beta_1\\) will be shrunk in addition to the controls. If we instead do not shrink \\(\\beta_1\\) but we shrink the controls enough. It will be closer to not controlling for the other variables since any part of of the treatment prediction of the outcome explained by the controls will be shrunk since those coefficients are penalized, but the treatment coefficient is not. Run Lasso with the outcome \\(y\\) on all controls, \\(z_1, \\dots, \\z_k\\). Keep all non-zero coefficients. Run Lasso with the treatment \\(z\\) on all controls, \\(z_1, \\dots, z_k\\). Keep all non-zero coefficients. Run OLS with the outcome \\(y\\) on the treatment, \\(x\\), and all variables with a non-zero coefficient in either step 1 or 2. If the true model is sparse (and asymptotics), then by the Oracle property, we can treat the standard errors of the OLS coefficients in the last step as if the selection stage did not occur. See https://arxiv.org/pdf/1603.01700.pdf and the hdm which implements this method, and extensions to work with high dimensional data in R. 8.3 References It is a few years old, but the 2015 NBER Summer course has a good introduction to machine learning that is targeted at social scientists. References "],
["regression.html", "Chapter 9 Regression 9.1 Observational Studies 9.2 Regression and Causality 9.3 Treatment Effects 9.4 OLS 9.5 Heterogeneous Effects and Regression 9.6 Balance and Imbalance 9.7 Key Things 9.8 Limited Dependent Variables 9.9 Assessing Selection on Observables 9.10 Omitted Variable Tests 9.11 My Advice", " Chapter 9 Regression Two views of regression: Linear regression is a model of the data generating process: \\[ \\begin{aligned} Y &amp;= X \\beta + \\epsilon \\\\ \\epsilon &amp;\\sim \\mathsf{Normal}(0, \\sigma^2) \\] Gauss-Markov assumptions mean BLUE \\(\\hat{beta}\\) unbiased \\(\\epsilon\\) uncorrelated with \\(y\\), \\(\\beta\\) are unbiased even if assumptions about errors are wrong. If errors uncorrelated and homoskedastic, if sample size large, the classic standard errors are correct even if errors aren’t normal. Conditional expectation function OLS is also a model of \\(E(y| X)\\), which is what we are interested Even if the functional form is wrong (not-linear), OLS produces the best linear approximation to the conditional expectation function (CEF). Agnostic justifications for When is a CEF Linear? Saturated model Outcome and covariates are multivariate normal OLS produces the best linear predictor (mean squared error) of \\(y_i\\) Asymptotic OLS Inference \\[ \\hat{\\beta}_{OLS} = (X&#39;X)X&#39;y \\approx E(Y | X) \\] Heteroskedasticity When will it occur? CEF is linear, but \\(\\sigma^(x) = \\Var(Y_i | X = x)\\) is not linear \\(E(Y_i | X_i)\\) is not linear, but use a linear regression to approximate it. How to estimate the variance covariance matrix of \\(\\beta\\)? 9.1 Observational Studies Identification assumptions Positivity: All observations can receive the treatment \\[ 0 &lt; \\Pr(D_i = 1|X, Y(1), Y(0)) &lt; 1 \\] No unmeasured confounding \\[ \\Pr(D_i = 1 | X, Y(1), Y(0)) = \\Pr(D_i = 1 | X) \\] or \\[ D_i \\perp (Y_i(0), Y_i(1) | X_i \\] This can be called unconfoundedness, ignorability, selection on observables, no omitted variables, exogenous, conditional exchangeable. 9.2 Regression and Causality What does it mean for the \\(\\hat{\\beta}\\) to be biased? OVB Doesn’t make sense without a causal question? Regression is causal if the CEF is causal When is the CEF is causal? 9.3 Treatment Effects Potential outcomes \\(Y_i(1)\\) and \\(Y_i(0)\\). ATE (Average Treatment Effect) \\[ \\tau = E(Y_i(1) - Y_i(0)) \\] ATT (Average Treatment Effect on the Treated) \\[ \\tau_{ATT} = E(Y_i(1) - Y_i(0) | D_i = 1) \\] 9.4 OLS Suppose there is a constant treatment effect, \\(\\tau\\): \\[ \\begin{aligned} Y_i(0) &amp;= \\alpha + X&#39;_i \\beta + u_i \\\\ Y_i(1) &amp;= \\alpha + \\tau + X&#39;_i \\beta + u_i \\end{aligned} \\] This means that \\[ Y_i(1) - Y_i(0) = \\tau \\] for all observations. The usual regression formula is, \\[ \\begin{aligned}[t] Y_i &amp;= \\underbrace{Y_i(1) D_i}_{\\text{received treatment} } + \\underbrace{Y_i(0) (1 - D_i)}_{\\text{didn&#39;t receive treatment}} \\\\ &amp;= \\color{orange}{Y_i(0)} + \\color{blue}{(Y_i(1) - Y_i(0)) D_i} \\\\ &amp;= \\color{orange}{\\alpha} + \\color{blue}{\\tau D_i} + \\color{orange}{x&#39;_i \\beta} + \\color{orange}{u_i} \\end{aligned} \\] Remember regression anatomy? Estimate residuals of regression of the treatment and outcome on the covariates \\[ \\begin{aligned} \\tilde{Y}_i = Y_i - E(Y_i | X_i) \\\\ \\tilde{D}_i = D_i - E(D_i | X_i) \\end{aligned} \\] Then running a regression of \\(\\tilde{Y}_i\\) on \\(\\tilde{D}_i\\) is equivalent to controlling for \\(X_i\\). \\[ \\begin{aligned} \\tilde{Y}_i &amp;= \\alpha + \\tau D_i + x&#39;_i \\beta + u_i \\\\ \\tilde{Y}_i &amp;= \\alpha + \\tau \\tilde{D}_i + \\tilde{u}_i \\end{aligned} \\] What does OLS estimate? \\[ \\hat{\\tau}_{OLS} = \\tau + \\frac{\\Cov(\\tilde{D}_i, \\tilde{u}_i)}{\\Var(\\tilde{D}_i)} \\] The key identification assumption is \\[ \\Cov(\\tilde{D}_i, \\tilde{u}_i) = 0 \\] I.e. conditional on \\(X\\), there is no relationship between \\(D_i\\) and \\(u_i\\). So \\(u_i\\) is a function of \\(X_i\\) and \\(Y_i(d)\\): \\(u_i = Y_i(0) - \\alpha - X&#39;_i \\beta\\) when \\(D_i = 0\\) \\(u_i = Y_i(1) - \\alpha - \\tau - X&#39;_i \\beta\\) when \\(D_i = 1\\) Given \\(X_i\\), only variation in \\(u_i\\) comes from \\(Y_i(d)\\) No unmeasured confounding implies this: \\[ D_i \\perp (Y_i(1), Y_i(0)) | X_i \\to D_i \\perp u_i | X_i \\to \\Cov(\\tilde{D}_i, \\tilde{u}_i) = 0 \\] What if it is violated? Suppose there is an omitted variable \\(Z_i\\) (residualized from \\(X_i\\)), \\[ \\tilde{u}_i = \\gamma \\tilde{Z}_i + \\omega_i \\] \\[ \\hat{\\tau}_{OLS} = \\tau + \\gamma \\frac{\\Cov(\\tilde{D}_i, \\tilde{Z}_i)}{\\Var(\\tilde{D}_i)} \\] What is the bias in OLS? Coefficient of \\(Z\\) on \\(y\\) (\\(\\gamma\\)) Correlation between \\(\\tilde{D}_i\\) and \\(\\tilde{Z}_i\\) 9.5 Heterogeneous Effects and Regression When does \\(\\tau = \\tau_R\\)? Constant treatment effects: \\(\\tau(x) = \\tau = \\tau_{R}\\) Constant probability of treatment: \\(e(x) = P(D_i = 1|X_i = x) = e\\) Incorrect model (linearity) in \\(X\\) leads to more bias 9.6 Balance and Imbalance See Gelman and Hill (p. 202) Overlap: the extent to which the range of data is the same across treatment groups Balance: the extent to which the distribution of data is the same across treatment groups Imbalance and lack of overlap mean more reliance on the model. How to evaluate overlap … see below. But plotting the averages of groups for a binary variable (Gelman and Hill, p. 202), or the standardized coefficients of regressions for continuous variables is one way. Convex hull King and Zheng: The bias comes from four sources: \\[ \\text{Bias} = \\hat{\\tau}_{OLS} - \\tau = \\Delta_o + \\Delta_p + \\Delta_{e} + \\Delta_{i} \\] where \\(\\Delta_o\\): Omitted variable bias \\(\\Delta_p\\): Post-treatment bias \\(\\Delta_e\\): Extrapolation bias—wrong functional form outside the available data \\(\\Delta_i\\): Interpolation bias—wrong functional form inside the available data One way to address this: The problem: lack of overlap and balance can lead to higher \\(\\Delta_i\\) and \\(\\Delta_e\\). What can we do about it? check for balance and overlap use matching instead of regression matching before regression. Sometimes called doubly robust. 9.7 Key Things Agnostic view of regression: CEF and robust standard errors How to evaluate OVB? Model dependence Pre-treatment vs. post-treatment variables Regression vs. matching 9.8 Limited Dependent Variables Usual advice: use logit/probit for binary, Poisson for counts OLS (with robust SE) is correct when: binary treatment and no covariates (difference in means) binary treatment, discrete covariates, saturated models (stratified diff-in-means) In unsaturated models, OLS is not bad Want estimate of the Average Marginal Effect (\\(\\int_X p(X) E(y | X) dX\\)). OLS “line” is not a good estimator of any individual marginal effect, but may be fine for overall AME. Logit/probit/Poisson: try to get model of CEF then calculate AME from estimated CEF Logit/probit/Poisson impose additional distributional assumptions to get CEF. Can help if good. Can hurt if wrong. May not be necessary if only care about AME. More common in econ than political science. Many marginally methodological political scientists will look at using OLS for LDVs weirdly. Do whatever the reviewer wants even if they are wrong on this front. The entire point is that OLS vs. these models probably won’t be that different for AME (but if they are … you probably have to think very carefully about your model). 9.9 Assessing Selection on Observables It’s not testable, because it places no restrictions on the observed data. It requires that the distribution of \\(Y_i(0)\\) is the same for the treatment and non-treatment groups, \\[ \\underbrace{p( Y_i(0) | D_i = 1, X_i )}_{\\text{unobserved}} = \\underbrace{p(Y_i(0) | D_i = 0, X_i)}_{\\text{observed}} \\] But that requires observing the counterfactual, which isn’t known. Can use DAGs to use find variables to control for via the backdoor criterion, but the DAG must be correct. What can we do? Placebo tests: Test a different \\(D_i\\) (placebo) that should have no effect. Balancing tests: Test that the observed covariate distributions are the same for the treated and untreated groups. Sensitivity/Coefficient stability tests: Test that the treatment effect is insensitive to the addition of new control variables. Control for as much pre-treatment variables as possible and use principled variable selection methods to optimally trade-off bias and variance. But the general problem is that this assumption is fundamentally untestable. This is why there is a preference for experiments and methods that try to rely on plausibly exogenous variation in the assignment of \\(D_i\\): instrumental variables (randomization + exclusion restriction) over-time-variation (diff-in-diff, fixed effects) arbitrary thresholds for treatment assignment (RDD) 9.10 Omitted Variable Tests The long equation, where we are interested in the estimate of \\(\\beta\\): \\[ y_i = \\alpha + \\beta x_i + \\gamma z_i + \\epsilon \\] If we estimate \\[ y_i = \\alpha^{(s)} + \\beta^{(s)} x_i + \\epsilon^{(s)} \\] the omitted variable bias formula is, \\[ \\beta^{(s)} - \\beta = \\gamma \\delta \\] where \\(\\delta\\) is the coefficient of the balancing equation (regression of \\(x_i\\) on \\(z_i\\)): \\[ z_i = \\delta_0 + \\delta x_i + u_i . \\] How to assess omitted variable bias? Balancing tests: Test the null hypothesis that \\(x\\) and \\(z\\) are uncorrelated. \\[ H_0: \\delta = 0 \\] Coefficient comparison (sensitivity) test, \\[ H_0: \\beta^{(s)} - \\beta = 0 \\] Note that the coefficient comparison test is equivalent to \\[ H_0: \\beta^{(s)} - \\beta = 0 \\leftrightarrow \\delta = 0 \\text{ or } \\gamma = 0 \\] How to implement these? More difficult for multiple controls. Balancing test: Regress \\(z\\) on \\(x\\). Hard with multiple controls. See Sec 5.4. Covariate comparison: Basically the assumption/intuition in these tests is that going from no-controls to observed controls gives the researcher some insight into how problematic unobserved controls could be. Informally compare coefficients. Add variables and see if \\(\\hat{\\beta}\\) changes substantively. Hausmann test and Chow tests proposed in Hausmann. Pischke and Pei. Altonji, Elder, and Taber (2005) propose and Nunn and Wantchekon (2011) use a slightly different method assess potential bias. Let \\(\\hat{\\beta}\\) be the OLS estimate of \\(\\beta\\) from the full regression (all covariates), and \\(\\hat{\\beta}^{(s)}\\) be the OLS estimate from a regression with no (or few) controls. Calculate \\[ B = \\frac{\\hat{\\beta}}{\\hat{\\beta}^{(s)} - \\hat{\\beta}} . \\] The smaller \\(\\hat{\\beta}^{(s)} - \\hat{\\beta}\\), the less the regression is selected by selection on observables. This should be greater than one. The interpretation of \\(B\\) is that it is the multiplier that unobserved covariates would have to have in order to make the estimated effect equal to zero. This coefficient stability argument makes some sense. If we think of observed covariates as a sample from the population of possible covariates, they provide information about the distribution of other unobserved covariates. But this sample isn’t random, and that can have perverse incentives: If the researcher does a good job controlling for confounders, then the effect of future confounders may appear large. If the researcher does a poor job controlling for confounders, then the effect of future confounders will appear minimal. This depends on a pre-existing “good” model, but there’s no way to assess that. Oster (2016) adjusts this method to account for the \\(R^2\\) of the regression. If it explains a lot, there is less for omitted variables to do. However, the solution is complicated. 9.11 My Advice Focus on one variable at a time (no all causes regression) Control for as many pre-treatment covariates as you can Check that you aren’t controlling for post-treatment variables If you are only interested in the AME of the treatment, you can include it as a linear term. It is more important to be flexible as the controls (while trading off bias and variance). You shouldn’t worry about discretizing continuous controls. It isn’t wrong because they are continuous. It may or may not be useful for improving balance. Formally or informally check the balance and overlap of your regression. Use regression anatomy to understand how many effective number of observations you have … which observations have the most variation in the treatment after conditioning on the controls. These are the cases which are providing all the information about your inferences. Use either a principled model selection method with regularization, and/or the OVB tests Use a placebo test if possible Use heteroskedasticity consistent (robust) standard errors; possible with autocorrelation or clustering adjustment. Take the OLS model “seriously but not literally”. It is an approximation of the CEF, and under some assumptions it is a type of weighted average treatment effect. It does not “assume” or “require” that the effect is linear or homogeneous (though it works better in those situations). "],
["panel-data-fixed-effects-and-difference-in-difference.html", "Chapter 10 Panel Data: Fixed Effects and Difference-in-Difference 10.1 Panel (Longitudinal) Data 10.2 Terminology 10.3 Fixed Effects 10.4 Difference-in-Difference 10.5 Lagged Dependent Variables 10.6 Random Effects", " Chapter 10 Panel Data: Fixed Effects and Difference-in-Difference Another source of variation is repeated measures of the same unit over time. This can allow for identification with different identifying assumptions. There are two identification approaches we will focus on Fixed Effects Different-in-Difference See Angrist and Pischke (2014 Ch. 5) and Angrist and Pischke (2009 Ch 5) on fixed effects and difference-in-difference approaches. 10.1 Panel (Longitudinal) Data In these methods there are repeated measurements of the same unit over time. This requires different methods and also has implications for causal inference. While simply having panel data does not identify an effect, it allows the researcher to claim identification using different assumptions than simply selection on observables (as in the cross-sectional case). 10.2 Terminology There are several closely related concepts and terminology to cover. Panel (longitudinal) data small \\(T\\), large \\(N\\). Examples: longitudinal surveys with a few rounds. Time series cross-section data large \\(T\\), medium \\(N\\). Examples: most country-year panels in CPE/IPE with several decades of data. For the purposes of causal inference, identification relies on the same assumptions. However, different estimators work differently under different data types. Some estimators work well as \\(N \\to \\infty\\), some as \\(T \\to \\infty\\), and usually these are not the same. Additionally, longer time series may require and/or have enough data for the researcher to estimate serial correlation in the errors. There are some additional related concepts that should also be mentioned at this time, hopefully to spare the reader future confusion (and not to add to it): Hierarchical Models units nested within groups. E.g. children in schools, districts within states Time-series Models large \\(T\\), usually \\(N = 1\\), or the different units modeled separately. Terminology can be confusing and varies across fields and literatures. In particular, fixed effects and random effects are used differently and often estimated differently in statistics and econometrics. This is easily seen by comparing the lme4 and plm packages in R which both estimate fixed and random effects models. Hierarchical models will often used fixed and random effects even though there is no time component, and thus they are not longitudinal models. The reason that I bring up this terminology is that if you search for fixed and random effects you can quickly be confused when it seems that people are talking about seemingly different concepts; they more of less may be. By panel data we will mean repeated measures for a unit, \\(i \\in 1, \\dots, N\\), over time, \\(t \\in 1, \\dots, T\\). same individuals in multiple surveys over time countries or districts over years individuals over time There are many different terms for repeated measurement data, including longitudinal, panel, and time-series cross-sectional data. Generally, The issues of causality are mostly the same, for these two types of data. However, the estimation methods are different. Estimation methods often rely on asymptotic assumptions about observations going to infinity. In repeated measurements there are two dimensions: number of units, and number of periods. Different estimators will work better for small \\(T\\) vs. large \\(T\\), and small \\(N\\) vs. large \\(N\\). 10.3 Fixed Effects Suppose that there are \\(i \\in 1, \\dots n\\) unit, and \\(t \\in 1, \\dots, T\\) time periods. The key assumption is that the treatment is independent of time, observed covariates, *and the identity of the observation. \\[ \\E[Y_{it}(0) | U_i, X_{it}, t, D_{it}] = \\E[Y_{it}(0) | U_i, X_{it}, t] \\] This is effectively a control for an unobserved factors for a unit. We need to make a few assumptions. Assume that time and linear effects are constant, \\[ \\E[Y_{it}(0) | U_i, X_{it}, t] = \\alpha + U_i&#39; \\gamma + X&#39;_{it} \\beta . \\] Assume that the causal effect is constant, and has a linear functional form: \\[ \\E[Y_{it}(1) | U_i, X_{it}, t] = \\E[Y_{it}(0) | U_i, X_{it}, t] + \\tau \\] Together this implies \\[ \\E[Y_{it} | U_i, X_{it}, t, D_{it}] = \\alpha + \\tau D_{it} + \\delta_{t} + U_i&#39; \\gamma + X&#39; \\beta . \\] This implies, \\[ Y_{it} = \\alpha_i + \\delta_t + \\tau D_{it} + X&#39;_{it} \\beta + \\epsilon_{it} \\] where \\[ \\epsilon_{it} = Y_{it}(0) - \\E[Y_{it}(0) | U_i, X_{it}, t] . \\] This implies that the fixed effects regression will be a CEF if \\(\\epsilon_{it}\\) has an expected value of 0. Fixed effects allows us to identify causal effects within units, and it is constant within the unit. You can think of this as a special kind of control. This requires some more stringent functional forms assumptions than regression, but it also can handle a specific form of unobserved confounders. 10.3.1 Estimators Given this model, there are several different estimators that are used. 10.3.1.1 Within Estimator First calculate the unit-level averages, \\[ \\bar{Y}_i = \\alpha_i + \\bar{\\delta} + \\tau \\bar{D}_{i} + \\bar{X}_{it}&#39; \\beta + \\bar{\\epsilon}_{i} . \\] The within estimator subtracts these unit-level means from the response, treatment, and all the controls: \\[ Y_{it} - \\bar{Y}_i = (\\delta_t - \\bar{\\delta}) + (X_{it} - \\bar{X}_i)&#39; \\beta + \\tau (D_{it} - \\bar{D}_i) + (\\epsilon_{it} - \\bar{\\epsilon}_{i}) \\] Also note that since \\(\\bar{Y}_i\\) are unit averages, and the unobserved effect is constant over time, subtracting off the mean also subtracts that unobserved effect. Implications are: Cannot use fixed effects to estimate causal effects of treatments that are constant within a unit. We do not need to include any time-constant controls. Only removes time-constant unobserved effects in a unit. See difference-in-difference for a method to remove some types of time-varying unobserved values. 10.3.1.2 Least Squares Dummy Variable (LSDV) Dummy variable regression is an alternative way to estimate fixed effects models. Called the least squared dummy variable (LSDV) estimator. Include a matrix of indicator variables (\\(W_i\\)) for each observation. \\[ Y_{it} = \\tau D_{it} + w_i&#39; \\gamma + x&#39;_{it} \\beta + \\epsilon_{it} \\] Within vs. LSDV are equivalent algebraically. LSDV is more computationally demanding. With \\(p\\) covariates and \\(G\\) groups, within estimator’s design matrix has only \\(p\\) columns, whereas the LSDV design matrix has \\(p + G\\) columns. If the within estimator is manually estimated by demeaning variables and then using OLS, the standard errors will be incorrect. They need to account for the degrees of freedom due to calculating the group means. In LSDV, the fixed effects themselves are not consistent if \\(T\\) fixed and \\(N \\to \\infty\\). However, the other coefficients are consistent, and those are the ones we care about. (Angrist and Pischke 2009, 224) 10.3.1.3 First-differences estimation Given that \\(U_i\\) is constant over time, first difference model is an alternative to mean-differences. The model is, \\[ \\begin{aligned}[t] Y_{it} - Y_{i,t-1} &amp;= (x&#39;_{it} - x&#39;_{i,t-1}) \\beta + \\tau (D_{it} - D_{i,t-1}) + (\\epsilon_{it} - \\epsilon_{i,t-1}) \\\\ \\Delta Y_{it} &amp;= \\Delta x&#39;_{it} \\beta + \\tau \\Delta D_{it} + \\Delta \\epsilon_{it} \\end{aligned} \\] If \\(U_i\\) are time-fixed, then first-differences are an alternative to mean-differences If the difference in errors, \\(\\Delta \\epsilon_{it}\\) are homoskedastic, OLS standard errors work fine. But implies that original errors must have had serial correlation: \\(\\epsilon_{it} = \\epsilon_{i,t-1} + \\Delta \\epsilon_{it}\\). If serial correlation, then more efficient than FE. Robust/sandwich SEs can be used. See Angrist and Pischke (2009 Ch 5.3) for a discussion of the difference between lagged dependent variables and fixed effects. LDV and FE estimators bound the causal effect of interest (Angrist and Pischke 2009, 246). If lagged dependent variable and fixed effect are both included then there is bias. Though this bias is not too bad and declines with the amount of data (CITE?). Instrumental variable approaches can be used, which are unbiased but very high variance, and thus OLS is often as good (same CITE) 10.3.2 Issues with Fixed Effects The only use within unit variation. This does not address changes over time. Susceptible to measurement error. There may be little variation within each unit. This will make it harder to estimate effects, which is okay, because the lack of within-unit variation tells us that this is a poor identification strategy. IV could be used (Angrist and Pischke 2014, 226) Fixed effects only identifies contemporaneous effects. See Blackwell (2013) for an approach to dynamic panel data . Since a fixed effect approach can usually be turned into a difference-in-difference approach by including period level dummies, there is often little reason not to do a DiD. 10.4 Difference-in-Difference The difference-in-difference estimator is similar to the fixed effect model, but relies on different assumptions. 10.4.1 Basic differences-in-differences model Treatment Control Pre \\(E(Y_{i1}(0) | D_i = 1)\\) \\(E(Y_{i1}(0) | D_i = 0)\\) Post \\(E(Y_{i2}(1) | D_i = 1)\\) \\(E(Y_{i2}(0) | D_i = 0)\\) The difference-in-difference (DiD) estimator is \\[ DiD = \\underbrace{(\\E(Y_{i2}(1) | D_i = 1) - \\E(Y_{i1}(0) | D_i = 1))}_{\\text{Treatment Difference}} - \\underbrace{(\\E(Y_{i2}(0) | D_i = 0) - E(Y_{i1}(0) | D_i = 0))}_{\\text{Control treatment}} . \\] For the general case of multiple time periods \\(t = 1, \\dots, T\\), for observations \\(i = 1, \\dots, N\\) in \\(g = 1, \\dots, G\\) groups, the DiD regression is, \\[ y_{itg} = \\alpha + \\underbrace{\\sum_{t = 1}^T \\beta_{t} I(t = \\tau)}_{\\text{time dummies}} + \\underbrace{\\sum_{k = 1}^G \\gamma_{g} I(k = g)}_{\\text{group dummies}} + \\underbrace{\\delta D_{tg}}_{\\text{treatment}} + \\epsilon_{it} \\] Identification comes from inter-temporal variation between groups. 10.4.2 Potential Outcomes Approach to DiD 10.4.2.1 Constant Effects Linear DiD Model Causal effects are constant across individuals and time, \\[ \\E[Y_{it}(1) - Y_{it}(0)] = \\tau . \\] The effects of time \\(\\delta_t\\) and individuals \\(\\alpha_g\\) are linearly separable, \\[ \\E[Y_{it}(0)] = \\delta_t + \\alpha_{g} . \\] Then the model is, \\[ Y_{igt} = \\delta_{t} \\] 10.4.3 Threats to identification Treatment independent of idiosyncratic shocks, so variation in outcome is the same for treated and control groups. Example: Ashenfelter’s Dip is an empirical phenomena in which people who enroll in job training programs see their earnings decline . It may be possible to condition on covariates (control) in order to make treatment and shocks independent. 10.4.4 Robustness Checks Lags and Leads If \\(D_{igt}\\) causes \\(Y_{igt}\\), then current and lagged values should have an effect on \\(Y_{igt}\\), but future values of \\(D_{igt}\\) should not. Placebo permutation test. Randomly assign the intervention(s) to create the sampling distribution of the null hypothesis. Use different comparison groups. Different groups should have the same affect. Use an outcome variable that you know is not affected by the intervention. If DiD estimates not zero, then there is some other difference between groups. Time Trends If more than two time periods, add unit specific linear trends to regression DiD model. \\[ Y_{igt} = \\delta_{t} + \\tau G_{i} + \\alpha_{0g} + \\alpha_{1g} \\times t + \\epsilon_{igt} , \\] where \\(\\alpha_{0g}\\) are group fixed effects, \\(\\delta_t\\) is the overall (not necessarily linear) time trend, and \\(\\alpha_{1g}\\) is the group linear time trend. Helps detect if varying trends when estimated from pre-treatment data. 10.4.5 Extensions The general DiD model relies on linear-separability and constant treatment effects. The parallel trends assumption is the important assumption: \\[ \\E[ Y_{i1}(0) - Y_{i0} | X_i, G_i = 1] = \\E[ Y_{i1}(0) - Y_{i0} | X_i, G_i = 0]. \\] It says that the potential trend under control is the same for the control and treated groups, conditional on covariates. With the parallel trends assumption unconditional ATT is, \\[ \\E[Y_{i1}(1) - Y_{i1}(0) | G_i = 1] = \\E_{X}[\\E[Y_{i1}(1) - Y_{i1}(0) | G_i = 1]] = , 𝐺𝑖 = 1]]. \\] What we need is an estimator of each CEF. This doesn’t need to be linear or parametric. However, cannot estimate ATE because \\(\\E(Y_{i1}(1) | X_i, G_i = 0)\\) could be anything. With covariates we can estimate conditional DiD in several ways. Regression DiD Match on \\(X_i\\) and then use regular DiD Weighting approaches Abadie (2005) Regression DiD includes \\(X_i\\) in a linear, additive manner, \\[ Y_{it} = \\mu + x&#39;_i \\beta_t + \\delta I(t = 1) + \\tau(I(t = 1) \\times G_i) + \\epsilon_{it} \\] If there are repeated observations, take difference between \\(t = 0\\) and \\(t = 1\\), \\[ Y_{i1} - Y_{i0} = \\delta + x&#39;_i \\beta + \\tau G_i + (\\epsilon_{i1} - \\epsilon_{i0}) \\] Have \\(\\beta = \\beta_1 - \\beta_0\\). Because everyone is untreated in first period, \\(D_{i1} - D_{i0} = D_{i1}\\). For panel data, regress changes on treatment. Depends on constant effects and linearity in \\(X_i\\). Matching could reduce model dependence. 10.4.6 Standard Error Issues 10.4.6.1 Serial Correlation A major issue with errors in difference-in-difference models is serial correlation (Bertrand, Duflo, and Mullainathan 2004). Consider the DiD model, \\[ Y_{igt} = \\mu_g + \\delta_t + \\tau (I_{it} \\times G_i) + \\nu_{gt} + \\epsilon_{igt} . \\] The problem is that \\(\\nu_{gt}\\) can be serially correlated \\[ Cor(\\nu_{gt}, \\nu_{gs}) \\neq 0 \\text{ for } s \\neq t . \\] An example called \\(AR(1)\\) serial correlation is when each \\(\\nu_t\\) is a function of its lag, \\[ \\nu_t = \\rho \\nu_{t - 1} + \\eta_t \\text{ where } \\rho \\in (0, 1). \\] Since errors are usually positively correlated, the outcomes are correlated over time and effectively there are fewer independent observations in the sample; it’s almost as if the same observation was simply copy and pasted over time with a little error added. This will mean that the standard errors will likely be too optimistic (too narrow). See Bertrand, Duflo, and Mullainathan (2004) for a longer discussion of this. here are a couple of solutions: Clustered standard errors at the group level Clustered bootstrap (re-sample groups, not individual observations) Aggregated to \\(g\\) units with two time periods each: pre- and post-intervention. Since correlation makes the panel data closer to simply a two-period DiD, this takes that all the way. All these solutions depend on larger numbers of groups. Do not use the off-the-shelf clustered standard errors unless the number of groups is large. See Esarey and Menger (2018) for an extensive discussion of this. Also see the associated clusterSE package and clubSandwich for implementations of cluster robust standard errors that work with smaller numbers of clusters. 10.4.7 Other DiD Approaches Changes-in-changes (Athey and Imbens 2006) generalizes DiD to allow for different changes in the distribution of \\(Y_{it}\\), not just the mean. This allows for estimating ATT or any changes in distribution (quantiles, variance, etc.). Unfortunately requires more data than estimating the mean. Synthetic controls is used when there is one treated group, but many controls. (See Abadie and Gardeazabel and the paper on the Seattle minimum wage). The basic idea is to compare the time series of the outcome in the treated group to a control. But what if there are many control group? What if they aren’t comparable to the treated? Synthetic control uses a weighted average of different controls. 10.5 Lagged Dependent Variables This is a different thing … See Dafoe (2018) for advice on when to use LDV. A different model is to assume a lagged dependent variable, \\[ Y_{i,t} = \\rho Y&#39;_{i,t-1} + X&#39;_{i,t} \\beta + \\varepsilon_{i,t} \\] This captures some of the unit-specific aspects that the fixed effects capture. However, the LDV model is making a different assumption than fixed effects. The FE model assumes that each unit has a separate effect that is constant over time, while the LDV model assumes that anything specific about a unit is captured through the value of the dependent variable in the previous period. Beck and Katz recommendation of LDV with PCSE. The LDV and Fixed Effects models make different assumptions, and they are not nested. So why not combine them into a single model? \\[ Y_{i,t} = \\rho Y&#39;_{i,t-1} + X&#39;_{i,t} \\beta + \\alpha_i + \\varepsilon_{i,t} . \\] There is a problem with this approach. OLS is biased. The fixed effect estimator includes demeaned values of the outcome variable and covariates. S the FE model with a LDV will use \\(Y_{i,t - 1} - \\bar{Y}_{i,t-1}\\). This average includes \\(Y_{i,t}\\) and \\(Y_{i,t} = ... + \\varepsilon_{i,t}\\). T us by construction, \\(Y_{i,t} - \\bar{Y}_{i,t-1}\\) is correlated with the errors. So what can we do about this? There are two options. Ignore it. The bias is proportional to \\(1/T\\). In panels with 20 or more periods, the bias may be small. Moreover, the bias is generally largest in the coefficient of the lagged dependent variable itself, which may not be of primary interest. Accept the bias. Use both LDV and FE models. The LDV and FE methods can bound the effects of the coefficient of interest. See Angrist and Pischke (2009). Use IV methods to instrument the lagged dependent variable. See Arrellano-Bond methods. This is a case where the difference between panel and TSCS is important. In many TSCS settings with larger \\(T\\) it is probably fine to estimate fixed effects with LDV. However, if you have panel data model with few \\(T\\), then you should use either method 2 or 3. 10.6 Random Effects Consider the panel data model, \\[ Y_{i,t} = \\alpha + X&#39;_{i,t} \\beta + u_i + \\varepsilon_{i,t} \\] In fixed effect, the errors are assumed to be uncorrelated with both the unit effects and the covariates, \\[ \\E(\\varepsilon_{i,t} | X_{i}, u_i) = 0 . \\] With random effects we make an additional assumption, the unit effects are uncorrelated with the covariates, \\[ \\E(u_i | X_i) = \\E(u_i) = 0 . \\] What this means that under the assumptions of random effects, omitting \\(u_i\\) would not bias \\(\\beta\\) since they are assumed to be uncorrelated with \\(X\\). Thus, there’s no omitted variable bias. So why use random effects? To fix standard errors. \\[ Y_{i,t} = X&#39;_{i,t} \\beta + \\nu_i \\] where \\(\\nu_i = u_i + \\varepsilon_{i,t}\\). However, this means that \\[ \\Cov(Y_{i,1}, Y_{i,2} | X_{i,t}) = \\sigma^2_u . \\] This violates the OLS assumption of non-autocorrelation. Using random effects gets consistent standard errors. 10.6.1 How to estimate random effects? There are a variety of methods, but the econometric method is to use quasi-demeaning or partial pooling, \\[ (Y_{i,t} - \\theta \\bar{Y}_i) = (X_{i,t} - \\bar{X}_i)&#39; \\beta + (\\nu_{i,t} - \\theta \\Var{\\nu}_i) \\] where \\(\\theta \\in [0, 1]\\) where \\(\\theta = 0\\) is OLS, and \\(\\theta = 1\\) is fixed effects. Some math (TM) shows, \\[ \\theta = 1 - \\left( \\sigma_u^2 / (\\sigma^2_u + T \\sigma^2_epsilon) \\right)^{1/2} . \\] The random effects estimator runs pooled OLS on this model, but replaces \\(\\theta\\) with the estimate \\(\\hat{\\theta}\\). See the R package plm. The R package lme4 and Bayesian methods, e.g. Gelman and Hill, take a different approach to estimating random effects. References "],
["regression-discontinuity.html", "Chapter 11 Regression Discontinuity 11.1 Examples 11.2 Example: Close Elections 11.3 Software 11.4 References", " Chapter 11 Regression Discontinuity Summary: If there are thresholds whereby some observations receive the treatment above it, other those below it do not, and those immediately above or below that threshold are similar, we can use the difference of the outcome between those just above and those just below the threshold to estimate the causal effect of the treatment. Suppose there is a running variable \\(x\\) such that any person receives the treatment, \\(d\\) if \\(x \\geq a\\) and does not if \\(x \\leq a\\), \\[ d = \\begin{cases} 1 &amp; x \\geq a \\\\ 0 &amp; x &lt; a \\end{cases} \\] A simple regression discontinuity model is, \\[ \\begin{aligned}[t] y_i = \\alpha + \\beta x_i + \\tau d_i + \\gamma x_i d_i + \\epsilon_i \\end{aligned} \\] The local causal effect of the treatment at the discontinuity is \\(\\tau\\). Figure 11.1: Fake Example of a Regression Discontinuity. The difference at the threshold (50) is the effect of the treatment. However, there are several choices Functional form of the trends before and after the discontinuity The size of the window of observations before and after the trend which to compare. How to choose? parametric: chooses specific functional forms non-parametric: uses flexible forms, and chooses a bandwidth (Imbens and Kalyanaraman 2011) Sharp vs. Fuzzy Discontinuity? Sharp: the assignment of the treatment occurs with certainty at the threshold. Fuzzy: the assignment of the treatment occurs only probabilistically at the threshold. Suppose that the causal effect of treatment \\(T \\in \\{0, 1\\}\\) on unit \\(i\\) is \\(\\tau_i = Y_i(1) - Y_i(0)\\) where \\(Y_i(1)\\) is the potential outcome of \\(i\\) under the treatment and \\(Y_i(0)\\) is the potential outcome of \\(i\\) under the control. If potential outcomes are distributed smoothly at the cut-point \\(c\\), then the average causal effect of the treatment at the cut-point, \\(Z_i = c\\): \\[ \\tau_{RD} = \\E[Y_{i}(1) - Y_i(0)| Z_i = c] = \\lim_{Z_i \\downarrow c}\\E[Y_{i}(1) | Z_i = c] - \\lim_{Z_i \\uparrow c}\\E[Y_i(0)| Z_i = c] \\] An advantage of RD designs is that unlike selection on observables or IV, its identifying assumptions are more observable and testable. There are two basic tests (Lee and Lemieux (2010)): Continuity of pre-treatment covariates. E.g. density test of McCrary (2008). Whether the ratio of treated to control units departs from chance. A difficulty is that balance only holds in the limit, and covariance balance may still be present in finite samples. Irrelevance of covariates to the treatment-outcome relationship. There should be no systematic association between covariates and treatment, so controlling for them shouldn’t affect the estimates. 11.1 Examples Thistlethwaite and Campbell (1960) was the first example of RD. Outcome: Career choices in teaching Running variable: PSAS scores Cutoff: receiving National Merit Finalist Discussed: Angrist and Pischke (2014 Ch 4) Carpenter and Dobkin (2011), Carpenter and Dobkin (2009) Running variable: age Cutoff: ability to drink alcohol legally Outcome: Death, accidents Discussed: Angrist and Pischke (2014 Ch 4) Abdulkadiroğlu, Angrist, and Pathak (2014) Running variable: exam score Cutoff: above threshold receive an offer from a school. This is fuzzy since not all those who receive the offer attend. Outcome: Educational outcomes Discussed: Angrist and Pischke (2014 Ch 4) Eggers and Hainmueller (2009) units: UK MPs outcome: personal wealth treatment: winning an election (holding office) running variable: vote share Litschig and Morrison (2013) units: Brazilian municipalities outcome: education, literacy, poverty rate treatment: receiving a cash transfer from the central government (there are population cutoffs) running variable: population Gelman and Hill (2007, 213–17) units: US Congressional members outcome: ideology of representative treatment: winning election running variable: vote share Gelman and Katz (2007), Gelman and Hill (2007, 232) units: patients outcome: length of hospital stay treatment: new surgery method cutoff: not performed on those over 80 running variable: age Also see derived examples in Bailey (2016 Ex. 6.3). See Button (2015) for a replication. units: congressional districts outcome: ideology of nominees treatment: election running variable: vote share Jacob and Lefgren (2004) units: students outcome: education achievement treatment: summer school, retention running variable: standardized test 11.2 Example: Close Elections A common use of RD in political science and econ is election outcomes. In this case the “treatment” is winning the election; it is applied to the candidate whose vote exceeds the threshold of 50%, but not to candidates arbitrarily below that threshold. Thus “close” elections are a common use of RD designs. This design was formalized in Lee (2008). Several papers question whether close elections satisfy the assumptions of RD: Caughey and Sekhon (2011) look at US House elections (1942-2008). They find that close elections are more imbalanced. They attribute this to national partisan waves. Grimmer et al. (2011) look at all US House elections 1880-2008. They find that structurally advantaged candidates (strong party, incumbents) are more likely to win close elections. The ways in which close elections can be non-random are lawsuit challenges and fraud. Eggers et al. (2014) addresses these concerns with a systematic review of 40,000 close elections: “U.S. House in other time periods, statewide, state legislative, and mayoral races in the U.S. and national or local elections in nine other countries” Only the US House appears to have these issues. 11.3 Software See the R packages rddtools: a new and fairly complete package of regression discontinuity from primary data viz to other tests. rdd rdrobust: Tools for data-driven graphical and analytical statistical inference in RD. rdpower: Calculate power for RD designs. rdmulti: Analyze designs with multiple cutoffs. See entries in the Econometrics task view. 11.4 References Textbooks and Reviews: Angrist and Pischke (2014 Ch. 4) Gelman and Hill (2007 Sec. 10.4) Bailey (2016 Ch. 11) Linden, Adams, and Roberts (2006) for applications to medicine Hahn, Todd, and Klaauw (2001) An early review of RD in economics Methods: Imbens and Kalyanaraman (2011) propose an optimal bandwidth selection method References "],
["formatting-tables.html", "Chapter 12 Formatting Tables 12.1 Overview of Packages 12.2 Summary Statistic Table Example 12.3 Regression Table Example", " Chapter 12 Formatting Tables 12.1 Overview of Packages R has multiple packages and functions for directly producing formatted tables for LaTeX, HTML, and other output formats. Given the See the Reproducible Research Task View for an overview of various options. xtable is a general purpose package for creating LaTeX, HTML, or plain text tables in R. texreg is more specifically geared to regression tables. It also outputs results in LaTeX (texreg), HTML (texreg), and plain text. The packages stargazer and apsrtable are other popular packages for formatting regression output. However, they are less-well maintained and have less functionality than texreg. For example, apsrtable hasn’t been updated since 2012, stargazer since 2015. The texreg vignette is a good introduction to texreg, and also discusses the These blog posts by Will Lowe cover many of the options. Additionally, for simple tables, knitr, the package which provides the heavy lifting for R markdown, has a function knitr. knitr also has the ability to customize how R objects are printed with the knit_print function. Other notable packages are: pander creates output in markdown for export to other formats. tables uses a formula syntax to define tables ReportR has the most complete support for creating Word documents, but is likely too much. For a political science perspective on why automating the research process is important see: Nicholas Eubank Embrace Your Fallibility: Thoughts on Code Integrity, based on this article Matthew Gentzkow Jesse M. Shapiro.Code and Data for the Social Sciences: A Practitioner’s Guide. March 10, 2014. Political Methodologist issue on Workflow Management 12.2 Summary Statistic Table Example The xtable package has methods to convert many types of R objects to tables. library(&quot;gapminder&quot;) gapminder_summary &lt;- gapminder %&gt;% # Keep numeric variables select_if(is.numeric) %&gt;% # gather variables gather(variable, value) %&gt;% # Summarize by variable group_by(variable) %&gt;% # summarise all columns summarise(n = sum(!is.na(value)), `Mean` = mean(value), `Std. Dev.` = sd(value), `Median` = median(value), `Min.` = min(value), `Max.` = max(value)) gapminder_summary ## # A tibble: 4 x 7 ## variable n Mean `Std. Dev.` Median Min. Max. ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 gdpPercap 1704 7215. 9857. 3532. 241. 113523. ## 2 lifeExp 1704 59.5 12.9 60.7 23.6 82.6 ## 3 pop 1704 29601212. 106157897. 7023596. 60011. 1318683096. ## 4 year 1704 1980. 17.3 1980. 1952. 2007. Now that we have a data frame with the table we want, use xtable to create it: library(&quot;xtable&quot;) foo &lt;- xtable(gapminder_summary, digits = 0) %&gt;% print(type = &quot;html&quot;, html.table.attributes = &quot;&quot;, include.rownames = FALSE, format.args = list(big.mark = &quot;,&quot;)) variable n Mean Std. Dev. Median Min. Max. gdpPercap 1,704 7,215 9,857 3,532 241 113,523 lifeExp 1,704 59 13 61 24 83 pop 1,704 29,601,212 106,157,897 7,023,596 60,011 1,318,683,096 year 1,704 1,980 17 1,980 1,952 2,007 Note that there we two functions to get HTML. The function xtable creates an xtable R object, and the function xtable (called as print()), which prints the xtable object as HTML (or LaTeX). The default HTML does not look nice, and would need to be formatted with CSS. If you are copy and pasting it into Word, you would do some post-processing cleanup anyways. Another alternative is the knitr function in the knitr package, which outputs R markdown tables. knitr::kable(gapminder_summary) variable n Mean Std. Dev. Median Min. Max. gdpPercap 1704 7.215327e+03 9.857455e+03 3531.8470 241.1659 1.135231e+05 lifeExp 1704 5.947444e+01 1.291711e+01 60.7125 23.5990 8.260300e+01 pop 1704 2.960121e+07 1.061579e+08 7023595.5000 60011.0000 1.318683e+09 year 1704 1.979500e+03 1.726533e+01 1979.5000 1952.0000 2.007000e+03 This is useful for producing quick tables. Finally, htmlTables package unsurprisingly produces HTML tables. library(&quot;htmlTable&quot;) htmlTable(txtRound(gapminder_summary, 0), align = &quot;lrrrr&quot;) variable n Mean Std. Dev. Median Min. Max. 1 gdpPercap 1704 7 10 3532 241 1 2 lifeExp 1704 6 1 61 24 8 3 pop 1704 3 1 7023596 60011 1 4 year 1704 2 2 1980 1952 2 It has more features for producing HTML tables than xtable, but does not output LaTeX. 12.3 Regression Table Example library(&quot;tidyverse&quot;) library(&quot;texreg&quot;) We will run several regression models with the Duncan data data(&quot;Duncan&quot;, package = &quot;carData&quot;) Since I’m running several regressions, I will save them to a list. If you know that you will be creating multiple objects, and programming with them, always put them in a list. First, create a list of the regression formulas, formulae &lt;- list( prestige ~ type, prestige ~ income, prestige ~ education, prestige ~ type + education + income ) Write a function to run a single model, Now use map to run a regression with each of these formulae, and save them to a list, prestige_mods &lt;- map(formulae, ~ lm(.x, data = Duncan, model = FALSE)) This is a list of lm objects, map(prestige_mods, class) ## [[1]] ## [1] &quot;lm&quot; ## ## [[2]] ## [1] &quot;lm&quot; ## ## [[3]] ## [1] &quot;lm&quot; ## ## [[4]] ## [1] &quot;lm&quot; We can look at the first model, prestige_mods[[1]] ## ## Call: ## lm(formula = .x, data = Duncan, model = FALSE) ## ## Coefficients: ## (Intercept) typeprof typewc ## 22.76 57.68 13.90 Now we can format the regression table in HTML using htmlreg. The first argument of htmlreg is a list of models: htmlreg(prestige_mods) &lt;!DOCTYPE HTML PUBLIC “-//W3C//DTD HTML 4.01 Transitional//EN” “http://www.w3.org/TR/html4/loose.dtd”&gt; Statistical models Model 1 Model 2 Model 3 Model 4 (Intercept) 22.76*** 2.46 0.28 -0.19 (3.47) (5.19) (5.09) (3.71) typeprof 57.68*** 16.66* (5.10) (6.99) typewc 13.90 -14.66* (7.35) (6.11) income 1.08*** 0.60*** (0.11) (0.09) education 0.90*** 0.35** (0.08) (0.11) R2 0.76 0.70 0.73 0.91 Adj. R2 0.75 0.69 0.72 0.90 Num. obs. 45 45 45 45 RMSE 15.88 17.40 16.69 9.74 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 By default, htmlreg() prints out HTML, which is exactly what I want in an R markdown document. To save the output to a file, specify a non-null file argument. For example, to save the table to the file prestige.html, htmlreg(prestige_mods, file = &quot;prestige.html&quot;) Since this function outputs HTML directly to the console, it can be hard to tell what’s going on. If you want to preview the table in RStudio while working on it, this snippet of code uses htmltools package to do so: library(&quot;htmltools&quot;) htmlreg(prestige_mods) %&gt;% HTML() %&gt;% browsable() The htmlreg function has many options to adjust the table formatting. Below, I clean up the table. I remove stars using stars = NULL. It is a growing convention to avoid the use of stars indicating significance in regression tables (see AJPS and Political Analysis guidelines). The arguments doctype, html.tag, head.tag, body.tag control what sort of HTML is created. Generally all these functions (whether LaTeX or HTML output) have some arguments that determine whether it is creating a standalone, complete document, or a fragment that will be copied into another document. The arguments include.rsquared, include.adjrs, and include.nobs are passed to the function extract() which determines what information the texreg package extracts from a model to put into the table. I get rid of \\(R^2\\), but keep adjusted \\(R^2\\), and the number of observations. library(&quot;stringr&quot;) coefnames &lt;- c(&quot;Professional&quot;, &quot;Working Class&quot;, &quot;Income&quot;, &quot;Education&quot;) note &lt;- &quot;OLS regressions with prestige as the response variable.&quot; htmlreg(prestige_mods, stars = NULL, custom.model.names = str_c(&quot;(&quot;, seq_along(prestige_mods), &quot;)&quot;), omit.coef = &quot;\\\\(Intercept\\\\)&quot;, custom.coef.names = coefnames, custom.note = str_c(&quot;Note: &quot;, note), caption.above = TRUE, caption = &quot;Regressions of Occupational Prestige&quot;, # better for markdown doctype = FALSE, html.tag = FALSE, head.tag = FALSE, body.tag = FALSE, # passed to extract() method for &quot;lm&quot; include.adjr = TRUE, include.rsquared = FALSE, include.rmse = FALSE, include.nobs = TRUE) Regressions of Occupational Prestige (1) (2) (3) (4) Professional 57.68 16.66 (5.10) (6.99) Working Class 13.90 -14.66 (7.35) (6.11) Income 1.08 0.60 (0.11) (0.09) Education 0.90 0.35 (0.08) (0.11) Adj. R2 0.75 0.69 0.72 0.90 Num. obs. 45 45 45 45 Note: OLS regressions with prestige as the response variable. Once you find a set of options that are common across your tables, make a function so you do not need to retype them. my_reg_table &lt;- function(mods, ..., note = NULL) { htmlreg(mods, stars = NULL, custom.note = if (!is.null(note)) str_c(&quot;Note: &quot;, note) else NULL, caption.above = TRUE, # better for markdown doctype = FALSE, html.tag = FALSE, head.tag = FALSE) } my_reg_table(prestige_mods, custom.model.names = str_c(&quot;(&quot;, seq_along(prestige_mods), &quot;)&quot;), custom.coef.names = coefnames, note = note, # put intercept at the bottom reorder.coef = c(2, 3, 4, 5, 1), caption = &quot;Regressions of Occupational Prestige&quot;) Statistical models Model 1 Model 2 Model 3 Model 4 (Intercept) 22.76 2.46 0.28 -0.19 (3.47) (5.19) (5.09) (3.71) typeprof 57.68 16.66 (5.10) (6.99) typewc 13.90 -14.66 (7.35) (6.11) income 1.08 0.60 (0.11) (0.09) education 0.90 0.35 (0.08) (0.11) R2 0.76 0.70 0.73 0.91 Adj. R2 0.75 0.69 0.72 0.90 Num. obs. 45 45 45 45 RMSE 15.88 17.40 16.69 9.74 Note: OLS regressions with prestige as the response variable. Note that I didn’t include every option in my_reg_table, only those arguments that will be common across tables. I use ... to pass arguments to htmlreg. Then when I call my_reg_table the only arguments are those specific to the content of the table, not the formatting, making it easier to understand what each table is saying. Of course, texreg also produces LaTeX output, with the function texreg. Almost all the options are the same as htmlreg. "],
["reproducible-research.html", "Chapter 13 Reproducible Research", " Chapter 13 Reproducible Research "],
["typesetting-and-word-processing-programs.html", "Chapter 14 Typesetting and Word Processing Programs 14.1 LaTeX 14.2 Word", " Chapter 14 Typesetting and Word Processing Programs 14.1 LaTeX LaTeX is a document markup language (think something like HTML) that is widely used in academia.3 Its primary advantages over Word (and word processors) are the separation of content and presentation and its formatting of mathematical equations. In addition to papers, it is often used for academic slides; many talk slides are prepared with beamer. 14.1.1 Learning LaTeX Here are some links to get started learning LaTeX: Overleaf Free &amp; Interactive Online Introduction to LaTeX LaTeX Tutorial has interactive lessons ShareLaTeX Documentation Overleaf Example Templates has many different examples of LaTeX documents. LaTeX Wikibook Not So Short Introduction to LaTeX is a classic, but not as as new-user friendly as the others. 14.1.2 Using LaTeX Use an online service such as Overleaf or ShareLaTeX. These are great for collaboration, but become inflexible when you want to customize your workflow. Write it with a specialized editor such as TeXmaker, TeXStudio, or TeXshop. These generally have built ways to insert text, and also live preview. I would stay away from editors such as LyX that are WYSIWYG. Write it with an general purpose editor such as Atom or Sublime Text.4 Most editors have a plugin to make writing LaTeX easier. For Atom there is LaTeXTools, and for Sublime Text, LaTeXTools 14.1.3 LaTeX with R This is pretty easy. Rnw, also called Sweave, documents allow you to mix R chunks with LaTeX. This is similar to R markdown, but with LaTeX instead of markdown.5 Many packages, such as xtable, stargazer, or texreg produce formatted output in LaTeX. When you use these programs, do not copy and paste the output. Instead, save it to a file, and use \\input{} to include the contents in your document. 14.2 Word While I use LaTeX in my own work, Microsoft Word is powerful piece of software, and many of the complaints against Word come down to not being aware of its features. There are many tools you can use to build your research paper; whatever tool you use, learn how to use it proficiently. 14.2.1 General Advice This guide on using Microsoft Word for Dissertations covers everything and more that I would have. Also see this separate presentation and content using styles Automatically number figures and tables Use a reference manager like Mendeley, Zotero, colwiz, or Papers. They have plugins for citations in Word. When exporting figures for Word, if you must use a raster graphic use PNG files (not JPEG). For publication, use a high DPI (600) with PNG graphics. Learn to use Fields. You can insert figures from files that you can update using Insert &gt; Field &gt; Links and References &gt; IncludePicture. This is useful for programmatically generating figures to insert into your document. Likewise, you can insert text from files that you can update using Insert &gt; Field &gt; Links and References &gt; IncludeText. 14.2.2 Using R with Word For a dynamic reports you can use R Markdown and export to a word document. When doing this, use a reference document to set the the styles that you will use. See Happy collaboration with Rmd to docx for more advice on using R Markdown with Word. When using functions from packages such as xtable, stargazer, or texreg output HTML, which can be copy and pasted into word. Finally, the ReporteR package is an alternative method to generate Word Documents from R. TeX is pronounced as “teck” because the X is a Greek chi. The pronunciation of of LaTeX is thus lah-teck or lay-teck. It is not pronounced like the rubber compound. See this StackExchange question on the pronunciation of LaTeX.↩ And of course Vim or Emacs.↩ And Sweave files preceded R markdown and knitr by many years.↩ "],
["writing-resources.html", "Chapter 15 Writing Resources 15.1 Writing and Organizing Papers 15.2 Finding Research Ideas 15.3 Replications", " Chapter 15 Writing Resources 15.1 Writing and Organizing Papers Chris Adolph. Writing Empirical Papers: 6 Rules &amp; 12 Recommendations Barry R. Weingast. 2015. CalTech Rules for Writing Papers: How to Structure Your Paper and Write an Introduction The Science of Scientific Writing American Scientist Deidre McCloskey. Economical Writing William Thompson. A Guide for the Young Economist. “Chapter 2: Writing Papers.” Stephen Van Evera. Guide to Methods for Students of Political Science. Appendix. Joseph M. Williams and Joseph Bizup. Style: Lessons in Clarity and Grace Strunk and White. The Elements of Style Chicago Manual of Style and APSA Style Manual for Political Science for editorial and style issues. How to construct a Nature summary paragraph. Though specific to Nature, it provides good advice for structuring abstracts or introductions. Ezra Klein. How researchers are terrible communications, and how they can do better. The advice in the AJPS Instructions for Submitting Authors is a concise description of how to write an abstract: The abstract should provide a very concise descriptive summary of the research stream to which the manuscript contributes, the specific research topic it addresses, the research strategy employed for the analysis, the results obtained from the analysis, and the implications of the findings. Concrete Advice for Writing Informative Abstracts and How to Carefully Choose Useless Titles for Academic Writing 15.2 Finding Research Ideas Paul Krugman How I Work Hal Varian. How to build an Economic Model in your spare time Greg Mankiw, My Rules of Thumb: The links in Advice for Grad Students 15.3 Replications Gary King has advice on how to turn a replication into a publishable paper: Gary King How to Write a Publishable Paper as a Class Project Gary King. 2006. “Publication, Publication.” PS: Political Science and Politics. Political Science Should Not Stop Young Researchers from Replicating from the Political Science Replication blog. And see the examples of students replications from his Harvard course at https://politicalsciencereplication.wordpress.com/. Famous replications. &quot;Irregularities in LaCour (2014) (Broockman, Kalla, and Aronow 2015) “Does High Public Debt Consistently Stifle Economic Growth? A Critique of Reinhart and Rogoff.” (Herndon, Ash, and Pollin 2013) However, although those replications are famous for finding fraud or obvious errors in the analysis, replications can lead to extensions and generate new ideas. This was the intent of Broockman, Kalla, and Aronow (2015) when starting the replication. References "],
["references-5.html", "References", " References "]
]
