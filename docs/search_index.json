[
["index.html", "Data Analysis Notes Chapter 1 Introduction", " Data Analysis Notes Jeffrey B. Arnold 2018-04-15 Chapter 1 Introduction Notes used when teaching “POLS/CS&amp;SS 501: Advanced Political Research Design and Analysis” and “POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology” at the University of Washington. \\[ \\] "],
["regression-anatomy.html", "Chapter 2 Regression Anatomy 2.1 Example 2.2 Variations 2.3 Questions", " Chapter 2 Regression Anatomy library(&quot;tidyverse&quot;) Summary: The coefficient of \\(x\\) in a multiple regression of \\(y\\) on \\(x\\) and \\(z\\) is equivalent to regressing \\(y\\) on the part of \\(x\\) not explained by \\(z\\) (the residuals of a regression of \\(x\\) on \\(z\\)) Regression anatomy is a term from (???) for the coefficient of variable \\(x\\) linear regression in a multiple regression is equivalent to the coefficient of a bivariate model using the residual from a regression of that variable regressed on all the other variables. Regression anatomy provides an answer to the how of control variables work. Consider a regression with two predictors, \\[ \\Vec{y} = \\beta_0 + \\beta_1 \\Vec{x}_1 + \\beta_1 \\Vec{x}_2 + u . \\] The OLS estimator of \\(\\beta\\) is \\[ (\\beta_1, \\beta_2) = \\Vec{\\beta} = (\\Mat{X}&#39;\\Mat{X})^{-1} \\Mat{X}&#39; \\Vec{y} \\] We can also recover the OLS estimate of \\(\\beta_1\\) through two bivariate regressions and the following procedure. \\[ \\beta_1 = \\frac{\\Cov(\\Vec{y}, \\tilde{\\Vec{u}})}{\\Var(\\tilde{\\Vec{u}})} . \\] where \\(\\tilde{\\Vec{u}}\\) is the vector of residuals from the regression of \\(\\Vec{x}_1\\) on \\(\\Vec{x}_2\\), \\[ \\begin{aligned}[t] \\Vec{x}_1 = \\gamma_0 + \\gamma_1 \\Vec{x}_2 + v . \\end{aligned} \\] This can be extended to more than two two variables by repeating the above steps as many times as necessary. This result is called the Frisch, Waugh, and Lovell (FWL) and discussed in Angrist and Pischke (2009). A complete proof can be found in advanced econometrics textbooks such as Davidson and MacKinnon (1993, p. 19–24) or Ruud (2000, p. 54–60). Note: This is a mechanical property of OLS It does depend an underlying Data Generating Process (DGP). Nevertheless, useful for understanding how OLS can be used for causal inference 2.1 Example For this example we will use the Duncan data from the car package. data(&quot;Duncan&quot;, package = &quot;carData&quot;) Duncan &lt;- rownames_to_column(Duncan, var = &quot;occupation&quot;) Regress prestige on education and income using lm. mod1 &lt;- lm(prestige ~ education + income, data = Duncan) mod1 ## ## Call: ## lm(formula = prestige ~ education + income, data = Duncan) ## ## Coefficients: ## (Intercept) education income ## -6.0647 0.5458 0.5987 Now, use the regression anatomy methods to find the regression of education by regression anatomy methods: Regress education on income: mod2a &lt;- lm(education ~ income, data = Duncan) mod2a ## ## Call: ## lm(formula = education ~ income, data = Duncan) ## ## Coefficients: ## (Intercept) income ## 15.6114 0.8824 Store the residuals from that regression. For convenience, use the augment function from the broom package. Duncan &lt;- modelr::add_residuals(Duncan, mod2a, var = &quot;resid_education&quot;) Regress prestige on the residuals of education regressed on ` mod2b &lt;- lm(prestige ~ resid_education, data = Duncan) mod2b ## ## Call: ## lm(formula = prestige ~ resid_education, data = Duncan) ## ## Coefficients: ## (Intercept) resid_education ## 47.6889 0.5458 The coefficients of these two regressions are approximately equal. They are different due to floating point rounding errors. coef(mod1)[&quot;education&quot;] - coef(mod2b)[&quot;resid_education&quot;] ## education ## 1.110223e-16 Q: Plot the regression lines Duncan %&gt;% select(prestige, education, resid_education) %&gt;% gather(variable, value, -prestige) %&gt;% mutate(variable = dplyr::recode(variable, resid_education = &quot;Residuals&quot;, education = &quot;education&quot;)) %&gt;% ggplot(aes(x = value, y = prestige, colour = variable)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) 2.2 Variations Let’s consider a couple of variations. First, consider the case in which we regress the residuals from \\(y\\) regressed on \\(x_2\\) on the residuals of \\(x_1\\) regressed on \\(x_2\\). Regress \\(y\\) on \\(x_2\\). Let \\(\\tilde{y}\\) be the residuals from that regression. Regress \\(x_1\\) on \\(x_2\\). Let \\(\\tilde{x}_1\\) be the residuals from that regression. Regress \\(\\tilde{y}\\) on \\(\\tilde{x}_1\\); do not include an intercept. Q: Will the coefficient of \\(\\tilde{x}_1\\) be the same as in the first case? Q: In the last stage why is there no intercept? Return to the previous example using the Duncan occupational prestige data. Regress prestige on income. mod3 &lt;- lm(prestige ~ income, data = Duncan) mod3 ## ## Call: ## lm(formula = prestige ~ income, data = Duncan) ## ## Coefficients: ## (Intercept) income ## 2.457 1.080 Add the residuals from this regression to Duncan dataset. Duncan &lt;- modelr::add_residuals(Duncan, mod3, var = &quot;resid_prestige&quot;) Regress the residuals of from the regression of prestige on income on the residuals from the regression of income on education. Do not include an intercept. lm(resid_prestige ~ 0 + resid_education, data = Duncan) ## ## Call: ## lm(formula = resid_prestige ~ 0 + resid_education, data = Duncan) ## ## Coefficients: ## resid_education ## 0.5458 What happens if we don’t include the intercept? lm(resid_prestige ~ resid_education, data = Duncan) ## ## Call: ## lm(formula = resid_prestige ~ resid_education, data = Duncan) ## ## Coefficients: ## (Intercept) resid_education ## -8.904e-16 5.458e-01 The intercept is estimate to be approximately zero - since the the regression line must go through \\((\\bar{y}, \\bar{x})\\) and residuals have mean 0. Plot the residuals and their regression line against the original values and their regression line. Subtract the mean from prestige and income so both linear regression lines go through the origin, which make it easier to compare them. bind_rows(mutate(select(Duncan, occupation, prestige, education), residuals = FALSE, prestige = prestige - mean(prestige), education = education - mean(education)), mutate(select(Duncan, occupation, prestige = resid_prestige, education = resid_education), residuals = TRUE)) %&gt;% ggplot(aes(x = education, y = prestige, colour = residuals)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) Second, consider the case in which we regress the residuals from \\(y\\) regressed on \\(x_2\\) on \\(x_1\\). Regress \\(y\\) on \\(x_2\\). Let \\(\\tilde{y}\\) be the residuals from that regression. Regress \\(\\tilde{y}\\) on \\(x_1\\). Regress the residuals of the regression of prestige on income on education. lm(resid_prestige ~ education, data = Duncan) ## ## Call: ## lm(formula = resid_prestige ~ education, data = Duncan) ## ## Coefficients: ## (Intercept) education ## -13.6285 0.2593 The coefficient on education is not the same as the multivariate OLS coefficient. In this case, we have not “controlled” for anything. The coefficient of education still includes the relationship between income and education. 2.3 Questions How does the variance of a variable compare to the variance of its residuals after regressing it on other control variables? What is the implication of that result for its standard error? If two variables are co-linear what is the residual after the first stage? How does that explain why OLS cannot estimate a linear regression with collinear variables. Suppose that two variables, \\(x_1\\) and \\(x_2\\) are highly correlated. In the first stage you regress \\(x_1\\) on \\(x_2\\). What do you expect the coefficient of \\(x_2\\) to be? What do you expect the residuals of this regression to be? How do you expect the coefficient of \\(x_1\\) on Consider two highly correlated variables. What are the implications for this in the first stage and second stage of regression anatomy? Confirm that regression anatomy also works for the coefficient of income in the previous regressions. Conduct regression anatomy for the coefficient of education in lm(prestige ~ education + type, data = Duncan) In this case we have a continuous variable (education) and a discrete control (type). How do would you interpret the residuals of the regression of education on type? Does the second stage of regression anatomy produce the correct standard errors? Informally, why do you think that is not the case. Retaining the regression anatomy procedure, generate correct standard errors using resampling. In causal inference, informally “selection on observables” attempts to estimate a causal effect by comparing “similar” observations. How does and doesn’t linear regression do this? Using the results of regression anatomy, how would you expect omitting control variables to effect the coefficients of a variable? Recall that the slope of a linear regression coefficient can be represented as a weighted average of the outcome variable, \\[ \\hat{\\beta} = \\sum_{i = 1}^n w_i y_i \\] where \\[ w_i = \\frac{(x_i - \\bar{x})}{\\sum_{i = 1} (x_i - \\bar{x})^2} . \\] Given regression anatomy, what are the weights of observations in multiple regression? "],
["ols-in-matrix-form.html", "Chapter 3 OLS in Matrix Form Setup 3.1 Purpose 3.2 Matrix Algebra Review 3.3 Matrix Operations 3.4 Matrices as vectors 3.5 Special matrices 3.6 Multiple linear regression in matrix form 3.7 Residuals 3.8 Scalar inverses 3.9 Matrix Inverses 3.10 OLS Estimator 3.11 Implications of OLS 3.12 Covariance/variance interpretation of OLS", " Chapter 3 OLS in Matrix Form Setup This will use the Duncan data in a few examples. library(&quot;tidyverse&quot;) data(&quot;Duncan&quot;, package = &quot;carData&quot;) 3.1 Purpose We can write regression model as, \\[ y_i = \\beta_0 + x_{i1} \\beta_1 + x_{i2} \\beta_2 + \\cdots + x_{ik} \\beta_k + u_k . \\] It will be cleaner to write the linear regression as \\[ y_i = \\Vec{x}_{i} \\Vec{\\beta} + u_i \\] where \\(\\Vec{x}_i\\) is a \\(1 \\times (K + 1)\\) row vector and \\(\\Vec{\\beta}\\) is a \\((K + 1) \\times 1\\) column vector for a single observation \\(i\\). Or we can write it as, \\[ \\Vec{y} = \\Mat{X} \\Vec{\\beta} + \\Vec{u} \\] where \\(\\Vec{y}\\) is a \\(N \\times 1\\) row vector, \\(\\Mat{X}\\) is a \\(N \\times (K + 1)\\) matrix, and \\(\\Vec{\\beta}\\) is a \\((K + 1) \\times 1\\) column vector for all \\(N\\) observations. 3.2 Matrix Algebra Review 3.2.1 Vectors A vector is a list of numbers or random variables. A \\(1 \\times k\\) row vector is arranged \\[ \\Vec{b} = \\begin{bmatrix} b_1 &amp; b_2 &amp; b_3 &amp; \\dots &amp; b_k \\end{bmatrix} \\] A \\(1 \\times k\\) column vector is arranged \\[ \\Vec{a} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3\\\\ \\dots \\\\ b_k \\end{bmatrix} \\] Convention: assume vectors are column vectors Convention: use lower-case bold Latin letters, e.g. \\(\\Vec{x}\\). Vector Examples Vector of all covariates for a particular unit \\(i\\) as a row vector, \\[ \\Vec{x}_{i} = \\begin{bmatrix} x_{i1} &amp; x_{i2} &amp; \\dots &amp; x_{ik} \\end{bmatrix} \\] E.g. in the Duncan data, \\[ \\Vec{x}_{i} = \\begin{bmatrix} \\mathtt{education}_{i} &amp; \\mathtt{income}_{i} &amp; \\mathtt{type}_i \\end{bmatrix} \\] Vector of the values of covariate \\(k\\) for all observations, \\[ x_{.,k} = \\begin{bmatrix} 1 \\\\ x_{i1} \\\\ x_{i2} \\\\ \\dots \\\\ x_{ik} \\end{bmatrix} \\] E.g. For the education variable in the column vector. \\[ \\Vec{x}_{i} = \\begin{bmatrix} \\mathtt{education}_{1} \\\\ \\mathtt{education}_{2} \\\\ \\dots &amp; \\vdots &amp; \\mathtt{education}_N \\end{bmatrix} \\] 3.2.2 Matrices A matrix is a rectangular array of numbers A matrix is \\(n \\times k\\) (“\\(n\\) by \\(k\\)”) if it has \\(n\\) rows and \\(k\\) columns A matrix \\[ A = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\dots &amp; a_{1k} \\\\ a_{21} &amp; a_{22} &amp; \\dots &amp; a_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\dots &amp; a_{nk} \\end{bmatrix} \\] 3.2.2.1 Examples The design matrix is the matrix of predictors/covariates in a regression: \\[ X = \\begin{bmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; \\dots &amp; a_{1k} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; \\dots &amp; a_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; a_{n2} &amp; \\dots &amp; a_{nk} \\end{bmatrix} \\] The vector of ones is the constant. In the Duncan data, for the regression prestige ~ income + education + type, the design matrix is, \\[ X = \\begin{bmatrix} 1 &amp; \\mathtt{income}_{1} &amp; \\mathtt{education}_{1} &amp; \\mathtt{wc}_{1} &amp; \\mathtt{prof}_{1} \\\\ 1 &amp; \\mathtt{income}_{2} &amp; \\mathtt{education}_{2} &amp; \\mathtt{wc}_{2} &amp; \\mathtt{prof}_{2} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; \\mathtt{income}_{N} &amp; \\mathtt{education}_{N} &amp; \\mathtt{wc}_{N} &amp; \\mathtt{prof}_{N} \\\\ \\end{bmatrix} \\] Use R function model.matrix to create the design matrix from a formula and a data frame. model.matrix(prestige ~ income + education + type, data = Duncan) ## (Intercept) income education typeprof typewc ## accountant 1 62 86 1 0 ## pilot 1 72 76 1 0 ## architect 1 75 92 1 0 ## author 1 55 90 1 0 ## chemist 1 64 86 1 0 ## minister 1 21 84 1 0 ## professor 1 64 93 1 0 ## dentist 1 80 100 1 0 ## reporter 1 67 87 0 1 ## engineer 1 72 86 1 0 ## undertaker 1 42 74 1 0 ## lawyer 1 76 98 1 0 ## physician 1 76 97 1 0 ## welfare.worker 1 41 84 1 0 ## teacher 1 48 91 1 0 ## conductor 1 76 34 0 1 ## contractor 1 53 45 1 0 ## factory.owner 1 60 56 1 0 ## store.manager 1 42 44 1 0 ## banker 1 78 82 1 0 ## bookkeeper 1 29 72 0 1 ## mail.carrier 1 48 55 0 1 ## insurance.agent 1 55 71 0 1 ## store.clerk 1 29 50 0 1 ## carpenter 1 21 23 0 0 ## electrician 1 47 39 0 0 ## RR.engineer 1 81 28 0 0 ## machinist 1 36 32 0 0 ## auto.repairman 1 22 22 0 0 ## plumber 1 44 25 0 0 ## gas.stn.attendant 1 15 29 0 0 ## coal.miner 1 7 7 0 0 ## streetcar.motorman 1 42 26 0 0 ## taxi.driver 1 9 19 0 0 ## truck.driver 1 21 15 0 0 ## machine.operator 1 21 20 0 0 ## barber 1 16 26 0 0 ## bartender 1 16 28 0 0 ## shoe.shiner 1 9 17 0 0 ## cook 1 14 22 0 0 ## soda.clerk 1 12 30 0 0 ## watchman 1 17 25 0 0 ## janitor 1 7 20 0 0 ## policeman 1 34 47 0 0 ## waiter 1 8 32 0 0 ## attr(,&quot;assign&quot;) ## [1] 0 1 2 3 3 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$type ## [1] &quot;contr.treatment&quot; 3.3 Matrix Operations 3.3.1 Transpose The transpose of a matrix \\(A\\) flips the rows and columns. It is denoted \\(A&#39;\\) or \\(A^{T}\\). The transpose of a \\(3 \\times 2\\) matrix is a \\(2 \\times 3\\) matrix, \\[ A = \\begin{bmatrix} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\\\ a_{31} &amp; a_{32} \\end{bmatrix} = \\begin{bmatrix} a_{11} &amp; a_{21} &amp; a_{31} \\\\ a_{12} &amp; a_{22} &amp; a_{32} \\end{bmatrix} \\] Transposing turns a \\(1 \\times k\\) row vector into a \\(k \\times 1\\) column vector and vice-versa. \\[ \\begin{aligned}[t] x_i &amp;= \\begin{bmatrix} 1 \\\\ x_{i1} \\\\ x_{i2} \\\\ \\vdots \\\\ x_{ik} \\end{bmatrix} \\\\ x_i&#39; &amp;= \\begin{bmatrix} 1 &amp; x_{i1} &amp; x_{i2} &amp; \\dots &amp; x_{ik} \\end{bmatrix} \\end{aligned} \\] A &lt;- matrix(1:6, ncol = 3, nrow = 2) A ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 t(A) ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 ## [3,] 5 6 a &lt;- 1:6 t(a) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 2 3 4 5 6 3.4 Matrices as vectors A matrix is a collection of row (or column) vectors. Write the matrix as a collection of row vectors \\[ A = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\end{bmatrix} = \\begin{bmatrix} \\Vec{a}_1&#39; \\\\ \\Vec{a}_2&#39; \\end{bmatrix} \\] \\[ B = \\begin{bmatrix} b_{11} &amp; b_{12} \\\\ b_{21} &amp; b_{22} \\\\ b_{31} &amp; b_{32} \\end{bmatrix} = \\begin{bmatrix} \\Vec{b}_1 \\\\ \\Vec{b}_2 \\Vec{b}_3 \\end{bmatrix} \\] How does \\(X\\) relate to the model specification? See the model.matrix model.matrix(prestige ~ education * income + type, data = Duncan) %&gt;% head() ## (Intercept) education income typeprof typewc education:income ## accountant 1 86 62 1 0 5332 ## pilot 1 76 72 1 0 5472 ## architect 1 92 75 1 0 6900 ## author 1 90 55 1 0 4950 ## chemist 1 86 64 1 0 5504 ## minister 1 84 21 1 0 1764 The OLS estimator of coefficients is \\[ \\hat{\\beta} = \\underbrace{(X&#39; X)^{-1}}_{Var(X)} \\underbrace{X&#39; y}_{Cov(X, Y)} \\] 3.5 Special matrices A square matrix has equal numbers of rows and columns The identity matrix, \\(\\Mat{I}_K\\) is a \\(K \\times K\\) square matrix with 1s on the diagonal, and 0s everywhere else. \\[ \\Mat{I}_3 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] The identity matrix multiplied by any matrix returns the matrix, \\[ \\Mat{A} \\Mat{I}_{K} = \\Mat{A} = \\Mat{I}_{M} \\Mat{A} \\] where \\(\\Mat{A}\\) is an \\(M \\times K\\) matrix. In R, to get the diagonal of a matrix use diag(), b &lt;- diag(1:4, nrow = 2L, ncol = 2L) b &lt;- diag(b) The function diag() also creates identity matrices, diag(3L) ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 The zero matrix is a matrix of all zeros, \\[ \\Mat{0}_K = \\begin{bmatrix} 0 &amp; 0 &amp; \\dots 0 \\\\ 0 &amp; 0 &amp; \\dots 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 0 \\end{bmatrix} \\] The zero vector is a matrix of all zeros, \\[ \\Mat{0}_K = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\] The ones vector is a vector of all ones, \\[ \\Mat{0}_K = \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} \\] 3.6 Multiple linear regression in matrix form Let \\(\\widehat{\\Vec{\\beta}}\\) be the matrix of estimated regression coefficients, and \\(\\hat{\\Vec{y}}\\) be the vector of fitted values: \\[ \\begin{aligned}[t] \\widehat{\\Vec{\\beta}} &amp;= \\begin{bmatrix} \\widehat{\\beta}_0 \\\\ \\widehat{\\beta}_1 \\\\ \\vdots \\\\ \\widehat{\\beta}_K \\end{bmatrix} &amp; \\hat{\\Vec{y}} &amp;= \\Mat{X} \\widehat{\\Vec{\\beta}} \\end{aligned} \\] This could be expanded to, \\[ \\begin{aligned}[t] \\hat{\\Vec{y}} &amp;= \\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_N \\end{bmatrix} &amp;= \\Mat{X} \\widehat{\\Vec{\\beta}} &amp;= \\begin{aligned} 1\\widehat{\\beta}_0 + x_{11} \\widehat{\\beta}_1 + x_{12} \\widehat{\\beta}_2 + \\dots + x_{1K} \\widehat{\\beta}_K \\\\ 1\\widehat{\\beta}_0 + x_{21} \\widehat{\\beta}_1 + x_{22} \\widehat{\\beta}_2 + \\dots + x_{2K} \\widehat{\\beta}_K \\\\ \\vdots \\\\ 1\\widehat{\\beta}_0 + x_{N1} \\widehat{\\beta}_1 + x_{N2} \\widehat{\\beta}_2 + \\dots + x_{NK} \\widehat{\\beta}_K \\\\ \\end{aligned} \\end{aligned} \\] 3.7 Residuals The residuals of a regression are, \\[ \\Vec{u} = \\Vec{y} - \\Mat{X} \\widehat{\\beta} \\] In two dimensions the Euclidian distance is, \\[ d(a, b) = \\sqrt{a^2 + b^2} \\] Think the hypotenuse of a triangle. The norm or length of a vector generalizes the Euclidian distance to multiple dimensions1 For a \\(K \\times 1\\) vector \\(\\Vec{a}\\), \\[ | \\Vec{a} | = \\sqrt{a_1^2 + a_2^2 + \\dots + a_K^2} \\] The norm can be written as the inner product, \\[ {| \\Vec{a} |}^2 = \\Vec{a}\\T \\Vec{a} \\] Note that when the mean of a vector is 0, the norm is equal to \\(N\\) times the sample variance (using the \\(N\\) denominator) \\[ \\begin{aligned} \\Var{\\Vec{u}} &amp;= \\frac{1}{N} \\sum_{i = 1}^N (u_i - \\var{u})^2 \\\\ &amp;= \\frac{1}{N} \\sum_{i = 1}^N u_i^2 \\\\ &amp;= \\frac{1}{N} \\Vec{u}\\T \\Vec{u} \\\\ &amp;= \\frac{1}{N} {| \\Vec{u} |}^2 \\\\ \\end{aligned} \\] 3.8 Scalar inverses What is division? You may think of it as the inverse of multiplication (which it is), but it means that for number \\(a\\) there exists another number (the inverse of \\(a\\)) denoted \\(a^{-1}\\) or \\(1 / a\\) such that \\(a \\times a^{-1} = 1\\). This inverse does not always exist. There is no inverse for 0: \\(0 \\times ? = 1\\) has no solution. If the inverse exists, we can solve algebraic expressions like \\(ax = b\\) for \\(x\\), \\[ \\begin{aligned} ax &amp;= b \\\\ \\frac{1}{a} ax &amp;= \\frac{1}{a} b &amp; \\text{multiply both sides by the inverse of \\[a\\]} \\\\ x = \\frac{b}{a} \\end{aligned} \\] We’ll see in matrix algebra, the intuition is similar. The inverse is a matrix such that when it multiplies a number it results in 1 (or the equivalent) The inverse doesn’t always exist The inverse can be used to solve 3.9 Matrix Inverses If it exists (it does not always), the inverse of square matrix \\(\\Mat{A}\\), denoted \\(\\Mat{A}^{-1}\\), is the matrix such that \\[ \\Mat{A}^{-1} \\Mat{A} = \\Mat{A} \\Mat{A}^{-1} = \\Mat{I} \\] The inverse can be used to solve systems of equations (like OLS) \\[ \\begin{aligned}[t] \\Mat{A} \\Vec{x} &amp;= \\Vec{b} \\\\ \\Mat{A}^{-1} \\Mat{A} \\Vec{x} &amp;= \\Mat{A}^{-1} \\Vec{b} \\\\ I \\Vec{x} &amp;= \\Mat{A}^{-1} \\Vec{b} \\\\ \\Vec{x} &amp;= \\Mat{A}^{-1} \\Vec{b} \\end{aligned} \\] If the inverse exists, then \\(\\Mat{A}\\) is called invertible or nonsingular. 3.10 OLS Estimator OLS minimizes the sum of squared residuals \\[ \\arg \\min \\] 3.11 Implications of OLS Independent variables are orthogonal to the residuals \\[ \\Mat{X}\\T \\hat{\\Vec{u}} = \\Mat{X}\\T(\\Vec{y} - \\Mat{X} \\widehat{\\Vec{\\beta}}) = 0 \\] Fitted values are orthogonal to the residuals \\[ \\Vec{y}\\T \\hat{\\Vec{u}} =(\\Mat{X} \\widehat{\\Vec{\\beta}})\\T \\hat{\\Vec{u}} = \\widehat{\\Vec{\\beta}}\\T \\Mat{X}\\T \\hat{\\Vec{u}} = 0 \\] 3.11.1 OLS in Matrix Form \\[ Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_k x_{ik} + \\epsilon_i \\] We can write this as \\[ \\begin{aligned}[t] Y_i &amp;= \\begin{bmatrix} 1 &amp; x_{i1} &amp; x_{i2} &amp; \\dots &amp; x_{ik} \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k \\end{bmatrix} + \\epsilon_i \\\\ &amp;= \\underbrace{{x_i&#39;}}_{1 \\times k + 1} \\underbrace{\\beta}_{k + 1 \\times 1} + \\epsilon_i \\end{aligned} \\] \\[ Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_k x_{ik} + \\epsilon_i \\] We can write it as \\[ \\begin{aligned}[t] \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} &amp;= \\begin{bmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; \\dots &amp; x_{1k} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; \\dots &amp; x_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; \\dots &amp; x_{nk} \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix} \\\\ \\underbrace{y}_{(n \\times 1)} &amp;= \\underbrace{{x_i&#39;}}_{(n \\times k + 1)} \\underbrace{\\beta}_{(k + 1 \\times 1)} + \\underbrace{\\epsilon}_{(n \\times 1)} \\end{aligned} \\] The regression standard error of the regression is \\[ \\hat{\\sigma}_y^2 = \\frac{\\sum_i^n \\epsilon_i^2}{n - k - 1} \\] Write this using matrix notation. Note that \\[ E(X_i)^2 = \\frac{\\sum X_i^2}{n} \\] In matrix notation this is, \\[ \\begin{bmatrix} x_1 &amp; x_2 &amp; \\dots &amp; x_n \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\dots \\\\ x_n \\end{bmatrix} = x&#39; x \\] If \\(\\bar{X} = 0\\), then \\[ \\frac{X&#39; X}{N} = Var(X) \\] What is the vcov matrix of \\(\\beta\\)? When would it be diagonal? What is on the off-diagonal? What is on the diagonal? Extract the standard errors from it. OLS Standard errors \\[ \\hat{\\beta}_{OLS} = (X&#39; X)^{-1} X&#39; y \\] \\[ V(\\hat{\\beta}) = \\begin{bmatrix} V(\\hat{\\beta}_0) &amp; Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) &amp; \\dots &amp; Cov(\\hat{\\beta}_0, \\hat{\\beta}_k) \\\\ Cov(\\hat{\\beta}_1, \\hat{\\beta}_0) &amp; V(\\hat{\\beta}_1) &amp; \\dots &amp; Cov(\\hat{\\beta}_1, \\hat{\\beta}_k) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ Cov(\\hat{\\beta}_k, \\hat{\\beta}_0) &amp; Cov(\\hat{\\beta}_k, \\hat{\\beta}_1) &amp; \\dots &amp; V(\\hat{\\beta}_k) \\\\ \\end{bmatrix} \\] Which of these matrices are Homoskedastic Heteroskedastic Clustered standard errors Serially correlated Show how \\((X&#39; X)^{-1} X&#39; y\\) is equivalent to the bivariate estimator. Write out \\(\\beta\\) and plug in for the true \\(Y\\) in terms of \\(X\\) and \\(\\epsilon\\) Take the variance of \\(\\hat{\\beta} - \\beta\\) \\[ \\hat{\\beta} = \\beta + (X&#39; X)^{-1} X&#39; \\epsilon \\\\ \\Var(\\hat{\\beta} - \\beta) = var((X&#39; X)^{-1} X&#39; \\epsilon) \\\\ \\] We know that \\((X&#39; X)^{-1} X&#39; \\epsilon\\) has mean zero since \\(E(X&#39; \\epsilon) = 0\\). \\(var(z) = E(Z^2) - 0\\) In matrix form \\(Z Z&#39;\\) to get full matrix form \\[ V((X&#39; X)^{-1} X&#39; \\epsilon) = (X&#39; X)^{-1} X&#39; \\epsilon \\epsilon&#39; X (X&#39; X)^{-1} = (X&#39; X)^{-1} X&#39; \\Sigma X (X&#39; X)^{-1} \\] We need a way to estimate \\(\\hat{\\Sigma}\\). But it has \\(n (n + 1) / 2\\) elements … and we have only \\(n\\) observations, and \\(n - k - 1\\) degrees of freedom left after estimating the coefficients. If homoskedasticity, \\(\\Sigma = \\sigma^2 I\\). \\[ V((X&#39; X)^{-1} X&#39; \\epsilon) = \\sigma^2 (X&#39; X)^{-1} \\] Panel of countries. Correlation within each year that is always the same 3.12 Covariance/variance interpretation of OLS \\[ \\Mat{X}\\T \\Vec{y} = \\sum_{i = 1}^N \\begin{bmatrix} y_i \\\\ y_i x_{i1} \\\\ y_i x_{i2}\\\\ \\vdots \\\\ y_i x_{iK} \\end{bmatrix} \\approx \\begin{bmatrix} n\\bar{y} \\\\ \\widehat{\\Cov}[y_i, x_{i1}] \\\\ \\widehat{\\Cov}[y_i, x_{i2}] \\\\ \\vdots \\\\ \\widehat{\\Cov}[y_i, x_{iK}] \\end{bmatrix} \\] \\[ \\begin{aligned} \\Mat{X}\\T \\Mat{X} &amp;= \\sum_{i = 1}^N \\begin{bmatrix} 1 &amp; x_{i1} &amp; x_{i2}&amp; \\cdots &amp; x_{ik} \\\\ x_{i1} &amp; x_{i1}^2 &amp; x_{i2} x_{i1} &amp; \\cdots &amp; x_{i1} x_{iK} \\\\ x_{i2} &amp; x_{i1} x_{i2} &amp; x_{i2}^2 &amp; \\cdots &amp; x_{i2} x_{iK} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{iK} &amp; x_{i1} x_{iK} &amp; x_{i2} x_{iK} &amp; \\cdots &amp; x_{ik} x_{iK} \\end{bmatrix} \\\\ &amp;\\approx \\begin{bmatrix} n &amp; n \\bar{x}_1 &amp; n \\bar{x}_2 &amp; \\cdots &amp; n \\bar{x}_K \\\\ n \\bar{x}_3 &amp; \\widehat{\\Var}[x_{i1}] &amp; \\widehat{\\Cov}[x_{i1}, x_{i2}] &amp; \\cdots &amp; n \\widehat{Cov}[x_{i1}, x_{iK}] \\\\ n \\bar{x}_3 &amp; \\widehat{\\Cov}[x_{i1}, x_{i2}] &amp; \\widehat{\\Var}[x_{i2}] &amp; \\cdots &amp; n \\widehat{\\Cov}[x_{i2}, x_{iK}] \\\\ \\vdots \\\\ n \\bar{x}_K &amp; \\widehat{\\Cov}[x_{iK}, x_{i1}] &amp; \\widehat{\\Cov}[x_{iK}, x_{i2}] &amp; \\cdots &amp; n \\widehat{\\Var}[x_{iK}] \\end{bmatrix} \\end{aligned} \\] This is technically the 2-norm, as there are other norms.↩ "],
["collinearity-and-multicollinearity.html", "Chapter 4 collinearity and Multicollinearity 4.1 (Perfect) collinearity 4.2 What to do about it? 4.3 Multicollinearity 4.4 What do do about it?", " Chapter 4 collinearity and Multicollinearity 4.1 (Perfect) collinearity In order to estimate unique \\(\\hat{\\beta}\\) OLS requires the that the columns of the design matrix \\(\\Vec{X}\\) are linearly independent. Common examples of groups of variables that are not linearly independent: Categorical variables in which there is no excluded category. You can also include all categories of a categorical variable if you exclude the intercept. Note that although they are not (often) used in political science, there are other methods of transforming categorical variables to ensure the columns in the design matrix are independent. A constant variable. This can happen in practice with dichotomous variables of rare events; if you drop some observations for whatever reason, you may end up dropping all the 1’s in the data. So although the variable is not constant in the population, in your sample it is constant and cannot be included in the regression. A variable that is a multiple of another variable. E.g. you cannot include \\(\\log(\\text{GDP in millions USD})\\) and \\(\\log({GDP in USD})\\) since \\(\\log(\\text{GDP in millions USD}) = \\log({GDP in USD}) / 1,000,000\\). A variable that is the sum of two other variables. E.g. you cannot include \\(\\log(population)\\), \\(\\log(GDP)\\), \\(\\log(GDP per capita)\\) in a regression since \\[\\log(\\text{GDP per capita}) = \\log(\\text{GDP} / \\text{population}) = \\log(\\text{GDP}) - \\log(\\text{population})\\]. 4.2 What to do about it? R and most statistical programs will run regressions with collinear variables, but will drop variables until only linearly independent columns in \\(\\Mat{X}\\) remain. For example, consider the following code. The variable type is a categorical variable with categories “bc”, “wc”, and “prof”. data(Duncan, package = &quot;car&quot;) # Create dummy variables for each category Duncan &lt;- mutate(Duncan, bc = type == &quot;bc&quot;, wc = type == &quot;wc&quot;, prof = type == &quot;prof&quot;) lm(prestige ~ bc + wc + prof, data = Duncan) ## ## Call: ## lm(formula = prestige ~ bc + wc + prof, data = Duncan) ## ## Coefficients: ## (Intercept) bcTRUE wcTRUE profTRUE ## 80.44 -57.68 -43.78 NA R runs the regression, but coefficient and standard errors for prof are set to NA. You should not rely on the software to fix this for you; once you (or the software) notices the problem check the reasons it occurred. The rewrite your regression to remove whatever was creating linearly dependent variables in \\(\\Mat{X}\\). 4.3 Multicollinearity Multicollinearity is the (poor) name for less-than-perfect collinearity. Even though there is enough variation in \\(\\Mat{X}\\) to estimate OLS coefficients, if some set of variables in \\(\\Mat{X}\\) is highly correlated it will result in large, but unbiased, standard errors on the estimates. What happens if variables are not linearly dependent, but nevertheless highly correlated? If \\(\\Cor(\\Vec{x}_1, vec{x}_2) = 1\\), then they are linearly dependent and the regression cannot be estimated (see above). But if \\(\\Cor(\\Vec{x}_1, vec{x}_2) = 0.99\\), the OLS can estimate unique values of of \\(\\hat\\beta\\). However, it everything was fine with OLS estimates until, suddenly, when there is linearly independence everything breaks. The answer is yes, and no. As \\(|\\Cor(\\Vec{x}_1, \\Vec{x}_2)| \\to 1\\) the standard errors on the coefficients of these variables increase, but OLS as an estimator works correctly; \\(\\hat\\beta\\) and \\(\\se{\\hat\\beta}\\) are unbiased. With multicollinearity, OLS gives you the “right” answer, but it cannot say much with certainty. For a bivariate regression, the distribution of the slope coefficient has variance, \\[ \\Var(\\hat{\\beta}_1) = \\frac{\\sigma_u^2}{\\sum_{i = 1} (x_i - \\bar{x})^2} . \\] What affects the standard error of \\(\\hat{\\beta}\\)? The error variance (\\(\\sigma_u^2\\)). The higher the variance of the residuals, the higher the variance of the coefficients. The variance of \\(\\Vec{x}\\). The lower variation in \\(\\Mat{x}\\), the bigger the standard errors of the slope. Now consider a multiple regression, \\[ \\Vec{y} = \\beta_0 + \\beta_1 \\Vec{x}_1 + \\beta_2 \\Vec{x}_2 + u \\] this becomes, \\[ \\Var(\\hat{\\beta}_1) = \\frac{\\sigma_u^2}{(1 - R^2_1) \\sum_{i = 1}^n (x_i - \\bar{x})^2} \\] where \\(R^2_1\\) is the \\(R^2\\) from the regression of \\(\\Vec{x}_1\\) on \\(\\Vec{x}_2\\), \\[ \\Vec{x} = \\hat{\\delta}_0 + \\hat{\\delta}_1 \\Vec{x}_2 . \\] The factors affecting standard errors are Error variance: higher residuals leads to higher standard errors. Variance of \\(\\Vec{x}_1\\): lower variation in \\(\\Vec{x}_2\\) leads to higher standard errors. The strength of the relationship between \\(x_1\\) and \\(x_2\\). Stronger relationship between \\(x_1\\) and \\(x_2\\) (higher \\(R^2\\) of the regression of \\(x_1\\) on \\(x_2\\)) leads to higher standard errors. These arguments generalize to more than two predictors. 4.4 What do do about it? Multicollinearity is not an “error” in the model. All you can do is: Get more data Find more conditional variation in the predictor of interest What it means depends on what you are doing. Prediction: then you are interested in \\(\\hat{\\Vec{y}}\\) and not \\(\\hat{\\beta}}\\) (or its standard errors). In this case, multicollinearity is irrelevant. Causal inference: in this case you are interested in \\(\\hat{\\Vec{\\beta}}\\). Multicollinearity does not bias \\(\\hat{\\beta}\\). You should include all regressors to achieve balance, and include all relevant pre-treatment variables and not include post-treatment variables. Multicollinearity is not directly relevant in this choice. All multicollinearity means is that the variation in the treatment after accounting for selection effects is very low, making it hard to say anything about the treatment effect with that observational data. More sophisticated methods may trade off some bias for a lower variance (e.g. shrinkage methods), but that must be done systematically, and not ad-hoc dropping relevant pre-treatment variables that simply correlate highly with your treatment variable. "],
["formatting-tables.html", "Chapter 5 Formatting Tables 5.1 Overview of Packages 5.2 Summary Statistic Table Example 5.3 Regression Table Example", " Chapter 5 Formatting Tables 5.1 Overview of Packages R has multiple packages and functions for directly producing formatted tables for LaTeX, HTML, and other output formats. Given the See the Reproducible Research Task View for an overview of various options. xtable is a general purpose package for creating LaTeX, HTML, or plain text tables in R. texreg is more specifically geared to regression tables. It also outputs results in LaTeX (texreg), HTML (texreg), and plain text. The packages stargazer and apsrtable are other popular packages for formatting regression output. However, they are less-well maintained and have less functionality than texreg. For example, apsrtable hasn’t been updated since 2012, stargazer since 2015. The texreg vignette is a good introduction to texreg, and also discusses the These blog posts by Will Lowe cover many of the options. Additionally, for simple tables, knitr, the package which provides the heavy lifting for R markdown, has a function knitr. knitr also has the ability to customize how R objects are printed with the knit_print function. Other notable packages are: pander creates output in markdown for export to other formats. tables uses a formula syntax to define tables ReportR has the most complete support for creating Word documents, but is likely too much. For a political science perspective on why automating the research process is important see: Nicholas Eubank Embrace Your Fallibility: Thoughts on Code Integrity, based on this article Matthew Gentzkow Jesse M. Shapiro.Code and Data for the Social Sciences: A Practitioner’s Guide. March 10, 2014. Political Methodologist issue on Workflow Management 5.2 Summary Statistic Table Example The xtable package has methods to convert many types of R objects to tables. library(&quot;gapminder&quot;) gapminder_summary &lt;- gapminder %&gt;% # Keep numeric variables select_if(is.numeric) %&gt;% # gather variables gather(variable, value) %&gt;% # Summarize by variable group_by(variable) %&gt;% # summarise all columns summarise(n = sum(!is.na(value)), `Mean` = mean(value), `Std. Dev.` = sd(value), `Median` = median(value), `Min.` = min(value), `Max.` = max(value)) gapminder_summary ## # A tibble: 4 x 7 ## variable n Mean `Std. Dev.` Median Min. Max. ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 gdpPercap 1704 7215. 9857. 3532. 241. 113523. ## 2 lifeExp 1704 59.5 12.9 60.7 23.6 82.6 ## 3 pop 1704 29601212. 106157897. 7023596. 60011. 1318683096. ## 4 year 1704 1980. 17.3 1980. 1952. 2007. Now that we have a data frame with the table we want, use xtable to create it: library(&quot;xtable&quot;) foo &lt;- xtable(gapminder_summary, digits = 0) %&gt;% print(type = &quot;html&quot;, html.table.attributes = &quot;&quot;, include.rownames = FALSE, format.args = list(big.mark = &quot;,&quot;)) variable n Mean Std. Dev. Median Min. Max. gdpPercap 1,704 7,215 9,857 3,532 241 113,523 lifeExp 1,704 59 13 61 24 83 pop 1,704 29,601,212 106,157,897 7,023,596 60,011 1,318,683,096 year 1,704 1,980 17 1,980 1,952 2,007 Note that there we two functions to get HTML. The function xtable creates an xtable R object, and the function xtable (called as print()), which prints the xtable object as HTML (or LaTeX). The default HTML does not look nice, and would need to be formatted with CSS. If you are copy and pasting it into Word, you would do some post-processing cleanup anyways. Another alternative is the knitr function in the knitr package, which outputs R markdown tables. knitr::kable(gapminder_summary) variable n Mean Std. Dev. Median Min. Max. gdpPercap 1704 7.215327e+03 9.857455e+03 3531.8470 241.1659 1.135231e+05 lifeExp 1704 5.947444e+01 1.291711e+01 60.7125 23.5990 8.260300e+01 pop 1704 2.960121e+07 1.061579e+08 7023595.5000 60011.0000 1.318683e+09 year 1704 1.979500e+03 1.726533e+01 1979.5000 1952.0000 2.007000e+03 This is useful for producing quick tables. Finally, htmlTables package unsurprisingly produces HTML tables. library(&quot;htmlTable&quot;) htmlTable(txtRound(gapminder_summary, 0), align = &quot;lrrrr&quot;) variable n Mean Std. Dev. Median Min. Max. 1 gdpPercap 1704 7 10 3532 241 1 2 lifeExp 1704 6 1 61 24 8 3 pop 1704 3 1 7023596 60011 1 4 year 1704 2 2 1980 1952 2 It has more features for producing HTML tables than xtable, but does not output LaTeX. 5.3 Regression Table Example library(&quot;tidyverse&quot;) library(&quot;texreg&quot;) We will run several regression models with the Duncan data Prestige &lt;- car::Prestige Since I’m running several regressions, I will save them to a list. If you know that you will be creating multiple objects, and programming with them, always put them in a list. First, create a list of the regression formulas, formulae &lt;- list( prestige ~ type, prestige ~ income, prestige ~ education, prestige ~ type + education + income ) Write a function to run a single model, Now use map to run a regression with each of these formulae, and save them to a list, prestige_mods &lt;- map(formulae, ~ lm(.x, data = Prestige, model = FALSE)) This is a list of lm objects, map(prestige_mods, class) ## [[1]] ## [1] &quot;lm&quot; ## ## [[2]] ## [1] &quot;lm&quot; ## ## [[3]] ## [1] &quot;lm&quot; ## ## [[4]] ## [1] &quot;lm&quot; We can look at the first model, prestige_mods[[1]] ## ## Call: ## lm(formula = .x, data = Prestige, model = FALSE) ## ## Coefficients: ## (Intercept) typeprof typewc ## 35.527 32.321 6.716 Now we can format the regression table in HTML using htmlreg. The first argument of htmlreg is a list of models: htmlreg(prestige_mods) &lt;!DOCTYPE HTML PUBLIC “-//W3C//DTD HTML 4.01 Transitional//EN” “http://www.w3.org/TR/html4/loose.dtd”&gt; Statistical models Model 1 Model 2 Model 3 Model 4 (Intercept) 35.53*** 27.14*** -10.73** -0.62 (1.43) (2.27) (3.68) (5.23) typeprof 32.32*** 6.04 (2.23) (3.87) typewc 6.72** -2.74 (2.44) (2.51) income 0.00*** 0.00*** (0.00) (0.00) education 5.36*** 3.67*** (0.33) (0.64) R2 0.70 0.51 0.72 0.83 Adj. R2 0.69 0.51 0.72 0.83 Num. obs. 98 102 102 98 RMSE 9.50 12.09 9.10 7.09 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 By default, htmlreg() prints out HTML, which is exactly what I want in an R markdown document. To save the output to a file, specify a non-null file argument. For example, to save the table to the file prestige.html, htmlreg(prestige_mods, file = &quot;prestige.html&quot;) Since this function outputs HTML directly to the console, it can be hard to tell what’s going on. If you want to preview the table in RStudio while working on it, this snippet of code uses htmltools package to do so: library(&quot;htmltools&quot;) htmlreg(prestige_mods) %&gt;% HTML() %&gt;% browsable() The htmlreg function has many options to adjust the table formatting. Below, I clean up the table. I remove stars using stars = NULL. It is a growing convention to avoid the use of stars indicating significance in regression tables (see AJPS and Political Analysis guidelines). The arguments doctype, html.tag, head.tag, body.tag control what sort of HTML is created. Generally all these functions (whether LaTeX or HTML output) have some arguments that determine whether it is creating a standalone, complete document, or a fragment that will be copied into another document. The arguments include.rsquared, include.adjrs, and include.nobs are passed to the function extract() which determines what information the texreg package extracts from a model to put into the table. I get rid of \\(R^2\\), but keep adjusted \\(R^2\\), and the number of observations. library(&quot;stringr&quot;) coefnames &lt;- c(&quot;Professional&quot;, &quot;Working Class&quot;, &quot;Income&quot;, &quot;Education&quot;) note &lt;- &quot;OLS regressions with prestige as the response variable.&quot; htmlreg(prestige_mods, stars = NULL, custom.model.names = str_c(&quot;(&quot;, seq_along(prestige_mods), &quot;)&quot;), omit.coef = &quot;\\\\(Intercept\\\\)&quot;, custom.coef.names = coefnames, custom.note = str_c(&quot;Note: &quot;, note), caption.above = TRUE, caption = &quot;Regressions of Occupational Prestige&quot;, # better for markdown doctype = FALSE, html.tag = FALSE, head.tag = FALSE, body.tag = FALSE, # passed to extract() method for &quot;lm&quot; include.adjr = TRUE, include.rsquared = FALSE, include.rmse = FALSE, include.nobs = TRUE) Regressions of Occupational Prestige (1) (2) (3) (4) Professional 32.32 6.04 (2.23) (3.87) Working Class 6.72 -2.74 (2.44) (2.51) Income 0.00 0.00 (0.00) (0.00) Education 5.36 3.67 (0.33) (0.64) Adj. R2 0.69 0.51 0.72 0.83 Num. obs. 98 102 102 98 Note: OLS regressions with prestige as the response variable. Once you find a set of options that are common across your tables, make a function so you do not need to retype them. my_reg_table &lt;- function(mods, ..., note = NULL) { htmlreg(mods, stars = NULL, custom.note = if (!is.null(note)) str_c(&quot;Note: &quot;, note) else NULL, caption.above = TRUE, # better for markdown doctype = FALSE, html.tag = FALSE, head.tag = FALSE) } my_reg_table(prestige_mods, custom.model.names = str_c(&quot;(&quot;, seq_along(prestige_mods), &quot;)&quot;), custom.coef.names = coefnames, note = note, # put intercept at the bottom reorder.coef = c(2, 3, 4, 5, 1), caption = &quot;Regressions of Occupational Prestige&quot;) Statistical models Model 1 Model 2 Model 3 Model 4 (Intercept) 35.53 27.14 -10.73 -0.62 (1.43) (2.27) (3.68) (5.23) typeprof 32.32 6.04 (2.23) (3.87) typewc 6.72 -2.74 (2.44) (2.51) income 0.00 0.00 (0.00) (0.00) education 5.36 3.67 (0.33) (0.64) R2 0.70 0.51 0.72 0.83 Adj. R2 0.69 0.51 0.72 0.83 Num. obs. 98 102 102 98 RMSE 9.50 12.09 9.10 7.09 Note: OLS regressions with prestige as the response variable. Note that I didn’t include every option in my_reg_table, only those arguments that will be common across tables. I use ... to pass arguments to htmlreg. Then when I call my_reg_table the only arguments are those specific to the content of the table, not the formatting, making it easier to understand what each table is saying. Of course, texreg also produces LaTeX output, with the function texreg. Almost all the options are the same as htmlreg. "],
["reproducible-research.html", "Chapter 6 Reproducible Research", " Chapter 6 Reproducible Research "],
["typesetting-and-word-processing-programs.html", "Chapter 7 Typesetting and Word Processing Programs 7.1 LaTeX 7.2 Word", " Chapter 7 Typesetting and Word Processing Programs 7.1 LaTeX LaTeX is a document markup language (think something like HTML) that is widely used in academia.2 Its primary advantages over Word (and word processors) are the separation of content and presentation and its formatting of mathematical equations. In addition to papers, it is often used for academic slides; many talk slides are prepared with beamer. 7.1.1 Learning LaTeX Here are some links to get started learning LaTeX: Overleaf Free &amp; Interactive Online Introduction to LaTeX LaTeX Tutorial has interactive lessons ShareLaTeX Documentation Overleaf Example Templates has many different examples of LaTeX documents. LaTeX Wikibook Not So Short Introduction to LaTeX is a classic, but not as as new-user friendly as the others. 7.1.2 Using LaTeX Use an online service such as Overleaf or ShareLaTeX. These are great for collaboration, but become inflexible when you want to customize your workflow. Write it with a specialized editor such as TeXmaker, TeXStudio, or TeXshop. These generally have built ways to insert text, and also live preview. I would stay away from editors such as LyX that are WYSIWYG. Write it with an general purpose editor such as Atom or Sublime Text.3 Most editors have a plugin to make writing LaTeX easier. For Atom there is LaTeXTools, and for Sublime Text, LaTeXTools 7.1.3 LaTeX with R This is pretty easy. Rnw, also called Sweave, documents allow you to mix R chunks with LaTeX. This is similar to R markdown, but with LaTeX instead of markdown.4 Many packages, such as xtable, stargazer, or texreg produce formatted output in LaTeX. When you use these programs, do not copy and paste the output. Instead, save it to a file, and use \\input{} to include the contents in your document. 7.2 Word While I use LaTeX in my own work, Microsoft Word is powerful piece of software, and many of the complaints against Word come down to not being aware of its features. There are many tools you can use to build your research paper; whatever tool you use, learn how to use it proficiently. 7.2.1 General Advice This guide on using Microsoft Word for Dissertations covers everything and more that I would have. Also see this separate presentation and content using styles Automatically number figures and tables Use a reference manager like Mendeley, Zotero, colwiz, or Papers. They have plugins for citations in Word. When exporting figures for Word, if you must use a raster graphic use PNG files (not JPEG). For publication, use a high DPI (600) with PNG graphics. Learn to use Fields. You can insert figures from files that you can update using Insert &gt; Field &gt; Links and References &gt; IncludePicture. This is useful for programmatically generating figures to insert into your document. Likewise, you can insert text from files that you can update using Insert &gt; Field &gt; Links and References &gt; IncludeText. 7.2.2 Using R with Word For a dynamic reports you can use R Markdown and export to a word document. When doing this, use a reference document to set the the styles that you will use. See Happy collaboration with Rmd to docx for more advice on using R Markdown with Word. When using functions from packages such as xtable, stargazer, or texreg output HTML, which can be copy and pasted into word. Finally, the ReporteR package is an alternative method to generate Word Documents from R. TeX is pronounced as “teck” because the X is a Greek chi. The pronunciation of of LaTeX is thus lah-teck or lay-teck. It is not pronounced like the rubber compound. See this StackExchange question on the pronunciation of LaTeX.↩ And of course Vim or Emacs.↩ And Sweave files preceded R markdown and knitr by many years.↩ "],
["writing-resources.html", "Chapter 8 Writing Resources 8.1 Writing and Organizing Papers 8.2 Finding Research Ideas 8.3 Replications", " Chapter 8 Writing Resources 8.1 Writing and Organizing Papers Chris Adolph. Writing Empirical Papers: 6 Rules &amp; 12 Recommendations Barry R. Weingast. 2015. CalTech Rules for Writing Papers: How to Structure Your Paper and Write an Introduction The Science of Scientific Writing American Scientist Deidre McCloskey. Economical Writing William Thompson. A Guide for the Young Economist. “Chapter 2: Writing Papers.” Stephen Van Evera. Guide to Methods for Students of Political Science. Appendix. Joseph M. Williams and Joseph Bizup. Style: Lessons in Clarity and Grace Strunk and White. The Elements of Style Chicago Manual of Style and APSA Style Manual for Political Science for editorial and style issues. How to construct a Nature summary paragraph. Though specific to Nature, it provides good advice for structuring abstracts or introductions. Ezra Klein. How researchers are terrible communications, and how they can do better. The advice in the AJPS Instructions for Submitting Authors is a concise description of how to write an abstract: The abstract should provide a very concise descriptive summary of the research stream to which the manuscript contributes, the specific research topic it addresses, the research strategy employed for the analysis, the results obtained from the analysis, and the implications of the findings. Concrete Advice for Writing Informative Abstracts and How to Carefully Choose Useless Titles for Academic Writing 8.2 Finding Research Ideas Paul Krugman How I Work Hal Varian. How to build an Economic Model in your spare time Greg Mankiw, My Rules of Thumb: The links in Advice for Grad Students 8.3 Replications Gary King has advice on how to turn a replication into a publishable paper: Gary King How to Write a Publishable Paper as a Class Project Gary King. 2006. “Publication, Publication.” PS: Political Science and Politics. Political Science Should Not Stop Young Researchers from Replicating from the Political Science Replication blog. And see the examples of students replications from his Harvard course at https://politicalsciencereplication.wordpress.com/. Famous replications. “Irregularities in LaCour (2014) [(???)”015a] “Does High Public Debt Consistently Stifle Economic Growth? A Critique of Reinhart and Rogoff.” (Herndon, Ash, and Pollin 2013) However, although those replications are famous for finding fraud or obvious errors in the analysis, replications can lead to extensions and generate new ideas. This was the intent of Broockman, Kalla, and Aronow (2015) when starting the replication. References "],
["references.html", "References", " References "]
]
