[
["index.html", "Data Analysis Notes Chapter 1 Introduction", " Data Analysis Notes Jeffrey B. Arnold 2017-04-14 Chapter 1 Introduction Notes used when teaching “POLS/CS&amp;SS 501: Advanced Political Research Design and Analysis” and “POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology” at the University of Washington. \\[ \\] "],
["regression.html", "Chapter 2 Regression 2.1 Joint vs. Conditional models 2.2 Conditional expectation function", " Chapter 2 Regression library(&quot;tidyverse&quot;) library(&quot;modelr&quot;) library(&quot;stringr&quot;) library(&quot;HistData&quot;) 2.1 Joint vs. Conditional models In most problems, the researcher is concerned with relationships between multiple variables. For example, suppose that we want to model the relationship between two variables, \\(Y\\) and \\(X\\). There are two main approaches to modeling this relationship. joint model: Jointly model \\(Y\\) and \\(X\\) as \\(f(Y, X)\\). For example, we can model \\(Y\\) and \\(X\\) as coming from a bivariate normal distribution.1 conditional model: Model \\(Y\\) as a conditional function of \\(X\\). This means we calculate a function of \\(Y\\) for each value of \\(X\\).2 Most often we focus on modeling a conditional statistic of \\(Y\\), and linear regression will focus on modeling the conditional mean of \\(Y\\), \\(\\E(Y | X)\\). regression (conditional) model \\(p(y | x_1, \\dots, x_k) = f(x_1, \\dots, x_k)\\) and \\(x_1, \\dots, x_k\\) are given. joint model \\(p(y, x_1, \\dots, x_k) = f(y, x_1, \\dots, x_k)\\) If we knew the joint model we could calculate the conditional model, \\[ (y | x_1, \\dots, x_k) = \\frac{p(y, x_1, \\dots, x_k)}{p(x_1, \\dots, x_k)} . \\] However, especially when there are specific outcome variables of interest, the conditional model, i.e. regression, is easier because the analyst can focus on modeling how \\(Y\\) varies with respect to \\(X\\), without necessarily having to model the process by which \\(X\\) is generated. However, that very convenience of not modeling the process which generates \\(X\\) will be the problem when regression is used for causal inference on observational data. At its most general, “regression analysis, broadly construed, traces the distribution of a response variable (denoted by \\(Y\\))—or some characteristic of this distribution (such as its mean)—as a function of one of more explanatory variables (Fox 2016, 15).” is a procedure that is used to summarize conditional relationships. That is, the average value of an outcome variable conditional on different values of one or more explanatory variables. While this section will generally cover linear regression, it is not the only form of regression. So let’s start with a definition of regression. Most generally, a regression represents a function of a variable, \\(Y\\) as a function of another variable or variables, \\(X\\), and and error. \\[ g(Y_i) = f(X_i) + \\text{error}_i \\] The conditional expectation function (CEF) or regression function of \\(Y\\) given \\(X\\) is denoted, \\[ \\mu(x) = \\E\\left[Y | X = x\\right] \\] But if regression represents \\(Y\\) as a function of \\(X\\), what’s the alternative? Instead of modeling \\(Y\\) as a function of \\(X\\), we could jointly model both \\(Y\\) and \\(X\\). A regression model of \\(Y\\) and \\(X\\) would be multivariate function, \\(f(Y, X)\\). In machine learning these approaches are sometimes called descriminative (regression) and generative (joint models) models. 2.2 Conditional expectation function 2.2.1 Discrete Covariates Before turning to considering continuous variable, it is useful to consider the conditional expectation function for a discrete \\(Y\\) and \\(X\\). Consider the datasets dataset included in the recommended R package datasets. It is a cross-tabulation of the survival of the 2,201 passengers in the sinking of the Titanic in 1912, as well as characteristics of those passengers: passenger class, gender, and age. Titanic &lt;- as_tibble(datasets::Titanic) %&gt;% mutate(Survived = (Survived == &quot;Yes&quot;)) The proportion of passengers who survived was summarise(Titanic, prop_survived = sum(n * Survived) / sum(n)) ## # A tibble: 1 × 1 ## prop_survived ## &lt;dbl&gt; ## 1 0.323035 Since Survived is a A conditional expectation function is a function that calculates the mean of Y for different values of X. For example, the conditional expectation function for Calculate the CEF for Survived conditional on Age, Titanic %&gt;% group_by(Age) %&gt;% summarise(prop_survived = sum(n * Survived) / sum(n)) ## # A tibble: 2 × 2 ## Age prop_survived ## &lt;chr&gt; &lt;dbl&gt; ## 1 Adult 0.3126195 ## 2 Child 0.5229358 conditional on Sex, Titanic %&gt;% group_by(Sex) %&gt;% summarise(prop_survived = sum(n * Survived) / sum(n)) ## # A tibble: 2 × 2 ## Sex prop_survived ## &lt;chr&gt; &lt;dbl&gt; ## 1 Female 0.7319149 ## 2 Male 0.2120162 conditional on Class, Titanic %&gt;% group_by(Class) %&gt;% summarise(prop_survived = sum(n * Survived) / sum(n)) ## # A tibble: 4 × 2 ## Class prop_survived ## &lt;chr&gt; &lt;dbl&gt; ## 1 1st 0.6246154 ## 2 2nd 0.4140351 ## 3 3rd 0.2521246 ## 4 Crew 0.2395480 finally, conditional on all combinations of the other variables (Age, Sex, Class), titanic_cef_3 &lt;- Titanic %&gt;% group_by(Class, Age, Sex) %&gt;% summarise(prop_survived = sum(n * Survived) / sum(n)) titanic_cef_3 ## Source: local data frame [16 x 4] ## Groups: Class, Age [?] ## ## Class Age Sex prop_survived ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1st Adult Female 0.97222222 ## 2 1st Adult Male 0.32571429 ## 3 1st Child Female 1.00000000 ## 4 1st Child Male 1.00000000 ## 5 2nd Adult Female 0.86021505 ## 6 2nd Adult Male 0.08333333 ## 7 2nd Child Female 1.00000000 ## 8 2nd Child Male 1.00000000 ## 9 3rd Adult Female 0.46060606 ## 10 3rd Adult Male 0.16233766 ## 11 3rd Child Female 0.45161290 ## 12 3rd Child Male 0.27083333 ## 13 Crew Adult Female 0.86956522 ## 14 Crew Adult Male 0.22273782 ## 15 Crew Child Female NaN ## 16 Crew Child Male NaN The CEF can be used to predict outcome variables given \\(X\\) variables. What is the predicted probability of survival for each of these characters from the movie Titanic? Rose (Kate Winslet): 1st class, adult female (survived) Jack (Leonardo DiCaprio): 3rd class, adult male (did not survive) Cal (Billy Zane) : 1st class, adult male (survived) titanic_chars &lt;- tribble( ~ name, ~ Class, ~ Age, ~ Sex, ~ Survived, &quot;Rose&quot;, &quot;1st&quot;, &quot;Adult&quot;, &quot;Female&quot;, TRUE, &quot;Jack&quot;, &quot;3rd&quot;, &quot;Adult&quot;, &quot;Male&quot;, FALSE, &quot;Cal&quot;, &quot;1st&quot;, &quot;Adult&quot;, &quot;Male&quot;, TRUE ) left_join(titanic_chars, titanic_cef_3, by = c(&quot;Class&quot;, &quot;Age&quot;, &quot;Sex&quot;)) ## # A tibble: 3 × 6 ## name Class Age Sex Survived prop_survived ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 Rose 1st Adult Female TRUE 0.9722222 ## 2 Jack 3rd Adult Male FALSE 0.1623377 ## 3 Cal 1st Adult Male TRUE 0.3257143 Rose was predicted to survive 97% of 1st class adult females survived, and she did. Jack was not predicted to survive (only 16% of 3rd class adult males survived, and he did not.3 Cal was not predicted to survive (33% of 1st class adult males survived), but he did, though through less than honorable means in the movie. Note that we haven’t made any assumptions about distributions of the variables. In this case, the outcome variable in the CEF was a binary variable, and we calculated a proportion. However, the proportion is the expected value (mean) of a binary variable, so the calculation of the CEF wouldn’t change. If we continued to condition on more discrete variables, the number of observed cell sizes would get smaller and smaller (and possibly zero), with larger standard errors. 2.2.2 Continuous Covariates But what happens if the conditioning variables are continuous? Galton (1886) examined the joint distribution of the heights of parents and their children. He was estimating the average height of children conditional upon the height of their parents. He found that this relationship was approximately linear with a slope of 2/3. This means that on average taller parents had taller children, but the children of taller parents were on average shorter than they were, and the children of shorter parents were on average taller than they were. In other words, the children’s height was more average than parent’s height. This phenomenon was called regression to the mean, and the term regression is now used to describe conditional relationships (Hansen 2010). His key insight was that if the marginal distributions of two variables are the same, then the linear slope will be less than one. He also found that when the variables are standardized, the slope of the regression of \\(y\\) on \\(x\\) and \\(x\\) on \\(y\\) are the same. They are both the correlation between \\(x\\) and \\(y\\), and they both show regression to the mean. Galton &lt;- as_tibble(Galton) Galton ## # A tibble: 928 × 2 ## parent child ## &lt;dbl&gt; &lt;dbl&gt; ## 1 70.5 61.7 ## 2 68.5 61.7 ## 3 65.5 61.7 ## 4 64.5 61.7 ## 5 64.0 61.7 ## 6 67.5 62.2 ## 7 67.5 62.2 ## 8 67.5 62.2 ## 9 66.5 62.2 ## 10 66.5 62.2 ## # ... with 918 more rows Calculate the regression of children’s heights on parents. Interpret the regression. child_reg &lt;- lm(child ~ parent, data=Galton) child_reg ## ## Call: ## lm(formula = child ~ parent, data = Galton) ## ## Coefficients: ## (Intercept) parent ## 23.9415 0.6463 Calculate the regression of parent’s heights on children’s heights. Interpret the regression. parent_reg &lt;- lm(parent ~ child, data=Galton) parent_reg ## ## Call: ## lm(formula = parent ~ child, data = Galton) ## ## Coefficients: ## (Intercept) child ## 46.1353 0.3256 Regression calculates the conditional expectation function, \\(f(Y, X) = \\E(Y | X) + \\epsilon\\), but we could instead jointly model \\(Y\\) and \\(X\\). This is a topic for multivariate statistics (principal components, factor analysis, clustering). In this case, an alternative would be to model the heights of fathers and sons as a bivariate normal distribution. ggplot(Galton, aes(y = child, x = parent)) + geom_jitter() + geom_density2d() # covariance matrix Galton_mean &lt;- c(mean(Galton$parent), mean(Galton$child)) # variance covariance matrix Galton_cov &lt;- cov(Galton) Galton_cov ## parent child ## parent 3.194561 2.064614 ## child 2.064614 6.340029 var(Galton$parent) ## [1] 3.194561 var(Galton$child) ## [1] 6.340029 cov(Galton$parent, Galton$child) ## [1] 2.064614 Calculate density for a multivariate normal distribution library(&quot;mvtnorm&quot;) Galton_mvnorm &lt;- function(parent, child) { # mu and Sigma will use the values calculated earlier dmvnorm(cbind(parent, child), mean = Galton_mean, sigma = Galton_cov) } Galton_mvnorm(Galton$parent[1], Galton$child[1]) ## [1] 4.272599e-05 Galton_dist &lt;- Galton %&gt;% modelr::data_grid(parent = seq_range(parent, 50), child = seq_range(child, 50)) %&gt;% mutate(dens = map2_dbl(parent, child, Galton_mvnorm)) Why don’t I calculate the mean and density using the data grid? library(&quot;viridis&quot;) ggplot(Galton_dist, aes(x = parent, y = child)) + geom_raster(mapping = aes(fill = dens)) + #geom_contour(mapping = aes(z = dens), colour = &quot;white&quot;, alpha = 0.3) + #geom_jitter(data = Galton, colour = &quot;white&quot;, alpha = 0.2) + scale_fill_viridis() + theme_minimal() + theme(panel.grid = element_blank()) + labs(y = &quot;Parent height (in)&quot;, x = &quot;Child height (in)&quot;) Using the plotly library we can make an interactive 3D plot: x &lt;- unique(Galton_dist$parent) y &lt;- unique(Galton_dist$child) z &lt;- Galton_dist %&gt;% arrange(child, parent) %&gt;% spread(parent, dens) %&gt;% select(-child) %&gt;% as.matrix() plotly::plot_ly(z = z, type = &quot;surface&quot;) But with regression we are calculating only one margin. Galton_means &lt;- Galton %&gt;% group_by(parent) %&gt;% summarise(child = mean(child)) ggplot(Galton, aes(x = factor(parent), y = child)) + geom_jitter(width = 0) + geom_point(data = Galton_means, colour = &quot;red&quot;) Note that in this example, it doesn’t really matter since a bivariate normal distribution happens to describe the data very well. This is not true in general, and we are simplifying our analysis by calculating the CEF rather than jointly modeling both. References "],
["interpreting-coefficients.html", "Chapter 3 Interpreting Coefficients 3.1 Interactions and Polynomials 3.2 Average Marginal Effects 3.3 Standardized Coefficients", " Chapter 3 Interpreting Coefficients That the coefficients are the marginal effects of each predictor makes linear regression particularly easy to interpret. However, this interpretation of predictors becomes more complicated once a variable is included in multiple terms through interactions or nonlinear functions, such as polynomials. Consider the regression, \\[ Y_i = \\beta_0 + \\beta_1 X + \\beta_2 Z + \\varepsilon \\] The regression coefficient \\(\\beta_1\\) it the change in the expected value of \\(Y\\) associated with a one-unit change in \\(X\\) holding \\(Z\\) constant, \\[ \\begin{aligned}[t] E(Y | X = x, Z = z) - E(Y | X = x + 1, Z = z) &amp;= (\\beta_0 + \\beta_1 X + \\beta_2 z) - (\\beta_0 + \\beta_1 (x + 1) + \\beta_2 z) \\\\ &amp;= \\beta_1 x - \\beta_1 (x + 1) \\\\ &amp;= \\beta_1 (x - x + 1) \\\\ &amp;= \\beta_1 \\end{aligned} \\] More formally, the coefficient \\(\\beta_k\\) is the partial derivative of \\(E(Y | X)\\) with respect to \\(X_k\\), \\[ \\begin{aligned}[t] \\frac{\\partial\\,E(Y | X)}{\\partial\\,X_k} = \\frac{\\partial}{\\partial\\,X_k} \\left( \\beta_0 + \\sum_{k = 1}^K \\beta_k X_k) \\right) = \\beta_k \\end{aligned} \\] Implications: If \\(X\\) is multiplied by a constant scalar \\(a\\), \\[ E(Y | X) = \\tilde{\\beta}_0 + \\tilde{\\beta}_1 a X = \\beta_0 + (a \\beta_1) X . \\] If \\(X_k\\) has a scalar \\(a\\) added to it, \\[ E(Y | X) = \\tilde{\\beta}_0 + \\tilde{\\beta}_1 (X + a) = (\\beta_0 + \\tilde{\\beta}_1 a) + \\tilde{\\beta}_1 X \\] Thus, \\(\\tilde{\\beta}_0 = (\\beta_0 + \\beta_1 a)\\) and \\(\\tilde{\\beta}_1 = \\beta_1\\). Consider the regression model \\[ \\vec{Y} = \\vec{\\beta} \\mat{X} + \\vec{\\epsilon} \\] Rather than staring by asking what do the regression coefficients, \\(\\vec{\\beta}\\), mean, we should start by asking what we want to estimate (i.e. the estimand) and then figure out how to extract that from the regression model. Let’s start with what it that we want to calculate. We want to calculate the “marginal effect” of changing the \\(j\\)th predictors while holding other predictors constant. In particular, one common estimand is the predicted change in the expected value of \\(Y\\) from a change in the \\(j\\)th predictor variable while holding the other predictors constant. The regression model is a model of the expected value of \\(Y\\) as a function of \\(\\mat{X}\\), \\[ \\hat{\\vec{y}} = \\E(Y) = \\hat{\\vec{beta}} \\mat{X} \\] For a continuous variable, \\(\\vec{x}_j\\), this is called the “marginal effect” and it is the partial derivative of the regression line with respect to \\(\\vec{x}_j\\), \\[ ME_{i,j} = \\frac{\\partial \\E( Y_i | x_{i,j}, x_{i,-j})}{\\partial x_{i,j}} \\] For a discrete change in \\(x_j\\), this is called the “partial effect” or “first difference”, and is simply the difference of predicted values, \\[ ME_{i,j} = \\E(Y_i | x_{i,j}, x_{i,-j}) - \\E(Y_i | x_{i,j} + \\Delta x_{i,j}, x_{i,-j}) \\] Now consider the linear regression with two predictors for a change in \\(x_1\\), \\[ \\begin{aligned}[t] ME_j &amp;= E(y | x_1, \\tilde{x}_2) - E(y | x_1 + \\Delta x_1, \\tilde{x}_2) \\end{aligned} \\] Since the linear regression equation is \\(E(y | x)\\), this simplifies to \\[ \\begin{aligned}[t] ME_j &amp;= (\\beta_0 + \\beta_1 x_1 + \\tilde{x}_2) - (\\beta_0 + \\beta_1 (x_1 + \\Delta x_1) \\tilde{x}_2) \\\\ &amp;= \\beta_1 \\Delta x_1 \\end{aligned} \\] or as \\(\\Delta x_1 \\to 0\\), this simplifies to the coefficient itself. \\[ \\begin{aligned}[t] ME_j &amp;= \\frac{\\partial E(y | x_1, x_2)}{\\partial x_1} \\\\ \\frac{\\partial (\\beta_0 + \\beta_1 x_1 + \\tilde{x}_2)}{\\partial x_1} &amp;= \\beta_1 \\end{aligned} \\] All of the previous equations were at the population level. The sample estimate for the marginal effect is \\[ \\widehat{ME}_j = \\hat{\\beta}_1 \\] So, for a linear regression, the marginal effect of \\(x_j\\), defined as the change in the expected value of \\(y\\) for a small a unit of \\(j\\) The equation presented above is not causal, it is simply a function derived from the population or estimated equation. If population equation is not the as the linear regression, \\(\\hat{\\beta_j}\\) can still be viewed as an estimator of \\(ME_j\\). In OLS, the \\(ME_j\\) is weighted by observations with the most variation in \\(x_j\\), after accounting for the parts of \\(x_j\\) and \\(y\\) predicted by the other predictors. See the discussion Angrist and Pischke (2009) and Aronow and Samii (2015). For regressions other than OLS, the coefficients are not the \\(ME_j\\). It is a luxury that the coefficients happen to have a nice interpretation in OLS. In most other regressions, the coefficients are not directly useful. This is yet another reason to avoid the mindless presentation of tables and star-worshiping. The researcher should focus on inference about the research quantity of interest, whether or not that happens to be conveniently provided as a parameter of the model that was estimated. 3.1 Interactions and Polynomials Even for OLS, if \\(x_j\\) is included as part of a function, e.g. a polynomial or an interaction, then its coefficient cannot be interpreted as the marginal effect. Suppose that the regression equation is \\[ \\vec{y} = \\vec{\\beta}_0 + \\vec{\\beta}_1 x_1 + \\vec{\\beta}_2 x_1^2 + \\vec{\\beta}_3 x_2, \\] then the marginal effect of \\(x_1\\) is, \\[ \\begin{aligned}[t] ME_j &amp;= \\frac{\\partial E(y | x_1, x_2)}{\\partial x_1} \\\\ &amp;= \\frac{\\partial (\\beta_0 + \\beta_1 x_1 + \\beta_1 x_1^2 + \\beta_3 \\tilde{x}_2)}{\\partial x_1} \\\\ &amp;= \\beta_1 + 2 \\beta_2 x_1 \\end{aligned} \\] Note that the marginal effect of \\(x_1\\) is not, \\(\\beta_1\\). That would require a change in \\(x_1\\) while holding \\(x_1 ^ 2\\) constant, which is a logical impossibility. Instead, the marginal effect of \\(x_1\\) depends on the value of \\(x_2\\) at which it is evaluated, and, thus, observations will have different marginal effects. Similarly, if there is an interaction between \\(x_1\\) and \\(x_2\\), the coefficient of a predictor is not its marginal effect. For example, in \\[ y = \\vec{\\beta}_0 + \\vec{\\beta}_1 x_1 + \\vec{\\beta}_2 x_1 + \\vec{\\beta}_3 x_1 x_2 \\] the marginal effect of \\(x_1\\) is \\[ \\begin{aligned}[t] ME_j &amp;= \\frac{\\partial E(y | x_1, x_2)}{\\partial x_1} \\\\ &amp;= \\frac{\\partial (\\beta_0 + \\beta_1 x_1 + \\beta_1 x_1^2 + \\beta_2 \\tilde{x}_2)}{\\partial x_1} \\\\ &amp;= \\beta_1 + \\beta_3 x_2 \\end{aligned} \\] Now the marginal effect of \\(x_1\\) is a function of another variable \\(x_2\\). 3.2 Average Marginal Effects For marginal effects that are functions of the data, there are multiple ways to calculate them. They include, AME: Average Marginal Effect. Average the marginal effects at each observed \\(x\\). MEM: Marginal Effect at the mean. Calculate the marginal effect with all observations at their means or other central values. MER: Marginal Effect at a representative value. Similar to MEM but with another meaningful value. Of these, the AME is the preferred one; marginal effects should be calculated for all observations, and then averaged (Hanmer and Kalkan 2012). When it is discrete change in \\(x\\), it is called a partial effect (APE) or a first difference. The difference in the expected value of y, given a change in \\(x_j\\) from \\(x^*\\) to \\(x^* + \\Delta\\) is \\(\\beta_j \\Delta\\), and the standard error can be calculated analytically by the Delta method, \\[ \\se(\\hat{\\beta}_j \\Delta x_j) = \\sqrt{\\Var\\hat{\\beta}_j (\\Delta x_j)^2} = \\se\\hat{\\beta}_j \\Delta x_j. \\] The Delta method can be used to analytically derive approximations of the standard errors for other nonlinear functions and interaction in regression, but it scales poorly, and it is often easier to use bootstrapping or software than calculate it by hand. See the margins package. 3.3 Standardized Coefficients A standardized coefficient is the coefficient on \\(X\\), when \\(X\\) is standardized so that \\(\\mean(X) = 0\\) and \\(\\Var(X) = 1\\). In that case, \\(\\beta_1\\) is the change in \\(\\E(Y)\\) associated with a one standard deviation change in \\(X\\). Additionally, if all predictors are set so that \\(\\mean(X) = 0\\), \\(\\beta_0\\) is the expected value of \\(Y\\) when all \\(X\\) are at their means. However, if any variables appear in multiple terms, then the standardized coefficients are not particularly useful. Standardized coefficients are generally not used in political science. (King How Not to Lie with Statistics, p. 669) More often, the effects of variables are compared by the first difference between the value of the variable at the mean, and a one standard deviation change. While, this is equivalent to the standardized coefficient Note, that standardizing variables can help computationally in some cases. In OLS, there is a closed-form solution, so iterative optimization algorithms are not needed in to find the best parameters. However, in more complicated models which require iterative optimization, standardizing variables can often improve the performance of the optimization. Thus standardizing variables before analysis is common in machine learning. However, the purpose is for ease of computation, not for ease of interpretation. References "],
["omitted-variable-bias.html", "Chapter 4 Omitted Variable Bias 4.1 Prerequisites 4.2 Simpson’s Paradox 4.3 Omitted Variable Bias 4.4 Multicollinearity 4.5 Measurement Error", " Chapter 4 Omitted Variable Bias 4.1 Prerequisites This chapter uses the car dataset in the car package. library(&quot;car&quot;) library(&quot;tidyverse&quot;) 4.2 Simpson’s Paradox UC Berkeley Graduate Admissions Bickel, Hammel, and O’Connell (1975) analyze a claim of sex bias in graduate admissions against UC-Berkeley in the 1970s. In 1973, 8,442 men and 4,321 women applied for admission to Berkeley graduate programs. In aggregate, UC Berkeley admitted 44% of men and 35% of women applicants, seemingly supporting that claim. However, when the admissions rates were disaggregated by graduate department, the acceptance rates by department were not, on average, different. What is going on? On average, more women applied to more selective (higher rejection rate) departments than men. The dataset datasets in the datasets package contains data for the largest 6 programs. data(&quot;UCBAdmissions&quot;, package = &quot;datasets&quot;) admissions &lt;- as_tibble(UCBAdmissions) %&gt;% spread(Admit, n) %&gt;% mutate(applicants = Admitted + Rejected, accepted = Admitted / applicants) ggplot(admissions, aes(x = Dept, y = accepted, size = applicants, colour = Gender)) + geom_point() select(admissions, Dept, Gender, applicants, accepted) %&gt;% arrange(Dept, Gender) ## # A tibble: 12 × 4 ## Dept Gender applicants accepted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A Female 108 0.82407407 ## 2 A Male 825 0.62060606 ## 3 B Female 25 0.68000000 ## 4 B Male 560 0.63035714 ## 5 C Female 593 0.34064081 ## 6 C Male 325 0.36923077 ## 7 D Female 375 0.34933333 ## 8 D Male 417 0.33093525 ## 9 E Female 393 0.23918575 ## 10 E Male 191 0.27748691 ## 11 F Female 341 0.07038123 ## 12 F Male 373 0.05898123 An interactive visualization is this UC Berkeley VUD Lab visualization: impson’s Paradox. Stop. Go to that link. Explore that visualization, and build your intution Simpson’s Paradox is an example of a more general phenomena in regression: omitted variable bias due to confounding variables.4 In the UC-Berkeley case, the confounding variable was the department. For some other examples see Moore (2005), Wagner (1982), Wikipedia, Julious and Mullee (1994) (Kidney stone treatment). 4.3 Omitted Variable Bias Suppose that the population model is, \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 Z_i + \\epsilon_i , \\] but given a sample, we run a regression with only \\(\\vec{x}\\) and not \\(\\vec{z}\\). \\[ y_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i + \\hat{\\epsilon}_i . \\] What is the relationship between \\(\\beta_1\\) and \\(\\hat{\\beta}_1\\)? Is \\(\\hat{\\beta}_1\\) an unbiased estimator of \\(\\beta_1\\) ? \\[ \\text{omitted variable bias} = (\\text{effect of $Z_i$ on $Y_i$}) \\times (\\text{effect of $X_i$ on $Z_i$}) \\] What does the omitted variable bias An irrelevant variable is one that is uncorrelated with \\(Y_i\\), meaning that its population coefficient is 0. Suppose \\(Z_i\\) is an irrelevant variable, \\[ Y_i = \\beta_0 + \\beta_1 X_i + 0 \\times Z_i = \\epsilon_i \\] In this case OLS is unbiased … \\[ \\begin{aligned}[t] \\E(\\hat\\beta_0) &amp;= \\beta_0 \\\\ \\E(\\hat\\beta_1) &amp;= \\beta_1 \\\\ \\E(\\hat\\beta_2) &amp;= 0 \\end{aligned} \\] However, including an irrelevant variable will increase the standard errors for \\(\\hat{\\beta}_1\\). Why? 4.4 Multicollinearity 4.4.1 Sampling Variance for simple linear regression In bivariate linear regression, the sampling distribution of the slope coefficient is \\[ \\Var(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum_{i = 1}^n (X_i - \\bar{X}_i)^ 2}, \\] and the standard error is \\(\\se(\\hat{\\beta}_1} =\\sqrt{\\Var{\\hat{\\beta}_1}}\\). What factors affect the standard errors and how? The error variance: \\(\\uparrow \\sigma^2 \\to \\uparrow \\se(\\hat{\\beta})_1\\) The variance of \\(X\\): \\(\\uparrow \\Var(X) \\to \\downarrow \\se(\\hat{\\beta})_1\\) Consider the linear regression model, \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i. \\] What if we included \\(X_i\\) twice? \\[ Y_i = \\tilde\\beta_0 + \\tilde\\beta_1 X_i + \\tilde\\beta_2 X_i + \\epsilon_i. \\] Clearly, any combination of \\(\\tilde\\beta_1\\) and \\(\\tilde\\beta_2\\) where \\[ \\tilde\\beta_1 + \\tilde\\beta_2 =\\beta_1 \\] will fit the model as well as any other. Consider cases of bivariate OLS with “effective” number of observations continuous OLS \\(\\Cov(X_1, X_2)\\) \\(\\Cov(X_2, Y) &gt; 0\\) \\(\\Cov(X_2, Y) = 0\\) \\(\\Cov(X_2, Y) &lt; 0\\) \\(&gt; 0\\) \\(0\\) \\(&lt; 0\\) + 0 - 0 0 0 - 0 + Summary: OVB is intrinsic to observational methods relying on selection on observables—not just regression. Control for all plausible “pre-treatment” variables Reason about possible biases due to OVB Sensitivity of coefficients to inclusion of control variables is an indication of the plausibility of OVB. Altonji, Elder, and Taber (2005) formalize this. In practice, this is the primary problem of many papers and papers. That is because it biases the coefficient of interest. Reviewers and discussants will often ask about whether you have considered “controlling” for insert variable here. Although these may be legitimate concerns, not all commenters understand the purpose of controls variables so some of these may not be legitimate, and in fact harmful. There two arguments to consider when controlling for a variable. The omitted variable has to plausibly be correlated with both the variable of interest and the outcome variable, and the burden is on the commenter to provide at a confounding variable and plausible relationships. Simply stating that there could be an unobservable variable is trivially true, uninteresting, and not a fatal critique. That said, the plausibility of a causal claim would be higher if with methods less susceptible to unobserved confounders, such as experiments, instrumental variables, regression discontinuity, and difference-in-differences. The omitted variable should be not be “post treatment” variable. If the omitted variable should not be one of the causal pathways by which \\(X\\) affects \\(Y\\), it should not be controlled for. If \\(Z\\) affects the values of \\(X\\) and also affects \\(Y\\), then it needs to be controlled for. How to assess There are two common ways of assessing plausibility. Informal method. This is the methods that you see in many empirical papers. They estimate the model including different control variables. The less sensitive the coefficient(s) of the variables of interest are to the inclusion of control variables, the more plausible it is that the variable of interest also not sensitive to unobserved confounders (Angrist and Pischke 2014). Oster (2016) states A common heuristic for evaluating the robustness of a result to omitted variable bias concerns is to look at the sensitivity of the treatment effect to inclusion of observed controls. In three top general interest economics journals in 2012, 75% of non-experimental empirical papers included such sensitivity analysis. The intuitive appeal of this approach lies in the idea that the bias arising from the observed controls is informative about the bias that arises from the unobserved ones. Note that what is important is that the magnitude of the coefficient is stable to the inclusion of controls, not that the coefficient remains statistically significant. Formal methods: There are several attempts to Altonji, Elder, and Taber (2005), Bellows and Miguel (2009), and Oster (2016), formalize the intuition behind the heuristic of coefficient stability to assess the sensitivity of the treatment to OVB. Bellows and Miguel (2009) propose the following simple variable: \\[ \\delta = \\frac{\\hat{\\beta}_F}{\\hat{\\beta}_R - \\hat{\\beta}_C}, \\] The statistic \\(\\delta\\) is interpreted as the magnitude of covariance between the unobserved part of the controls and the treatment variable necessary to explain away the entire treatment effect of \\(X\\) on \\(Y\\). A larger ratio suggests it is implausible that omitted variable bias could explain away the entire observed effect.[^ovb1] [ovb1]: See Bellows and Miguel (2009 Appendix A.) for the derivation. Bellows and Miguel (2009) generalizes Altonji, Elder, and Taber (2005) from binary to continuous treatment variables. Oster (2016) further generalizes that estimator. OVB is a intrinsic problem in observational research, and there is nothing you can do to ever ensure that you have controlled for all relevant variables. However, all inference is uncertain, all models are wrong and misspecified, and so really people should learn to deal with uncertainty. Note that methods such as matching, propensity scores, or inverse weighting still depend on assumptions about selection on observables. The differ from regression in the estimand or their sensitivity to model misspecification. Preferences for “design based” inference is mostly driven by a desire to find situations (designs) where other assumptions can substitute for “selection on observables”. Apart from experiments, these include instrumental variables, regression discontinuity, and difference-in-differences. Pei, Pischke, and Schwandt (2017) suggest two tests: Coefficient Comparison Test: Add confounder and test that coefficient of interest does not change. Balancing Test: Regress confounder on treatment. \\[ Z = \\alpha + \\beta X + \\epsilon \\] Test that \\(\\beta\\) is 0. Regrssion without the confounder: \\[ y_i = \\tilde\\alpha + \\tilde\\beta x_i + \\epsilon_i \\] Regression with the confounder: \\[ y_i = \\alpha + \\beta x_i + \\gamma z_i + \\epsilon_i \\] The balancing test regression is the regression of \\(x\\) on \\(z\\): \\[ z_i = \\delta_0 + \\delta x_i + \\eta_i \\] The change in coefficient from adding \\(x_i\\) to the regression comes from the OVB formula, \\[ \\beta - \\tilde\\beta = \\gamma \\delta \\] The balancing test is testing: \\[ H_0: \\delta = 0 \\] The Coefficient Comparison Test is testing: \\[ H_0: \\tilde\\beta - \\beta = 0 \\] which is true if either \\(\\gamma = 0\\) or \\(\\delta = 0\\). The balancing test if more powerful if \\(z_i\\) is measured with error. Adding covariates sequentially, to see which confounders influence the coefficient of interest, is not appropriate. The order is arbitrary, but important for interpretation. See Ge 4.5 Measurement Error 4.5.1 What’s the problem? It biases coefficients: Variable with measurement error: biases \\(\\beta\\) towards zero (attenuation bias) Other variables: Biases \\(\\beta\\) similarly to omitted variable bias. In other words, when a variable has measurement error it is an imperfect control. You can think of omitted variables as the limit of the effect of measurement error as it increases. 4.5.2 What to do about it? There’s no easy fix within the OLS framework. If the measurement error is in the variable of interest, then the variable will be biased towards zero, and your estimate is too large. Find better measures with lower measurement errors. If the variable is the variable of interest, then perhaps combine multiple variables into a single index. If the measurement error is in the control variables, then include several measures. That these measure correlate closely increases their standard errors, but the control variables are not the object of the inferential analysis. More complicated methods: errors in variable models, structural equation models, instrumental variable (IV) models, and Bayesian methods. References "],
["multiple-regression.html", "Chapter 5 Multiple Regression 5.1 Omitted Variable 5.2 What to do about Omitted Variable Bias 5.3 Regression Anatomy 5.4 More Information", " Chapter 5 Multiple Regression 5.1 Omitted Variable \\[ Y_i = \\beta_0 + \\beta_x X + \\beta_z Z \\] Suppose that \\[ y_i = x_i \\beta + z_i \\delta + \\epsilon_i \\] Use least squares calculation, \\[ \\hat{\\beta} = (\\mat{X}\\T \\mat{X})^{-1} \\mat{X}\\T \\vec{y} \\] Sustitute \\(y\\), \\[ \\begin{aligned}[t] \\hat{\\beta} &amp;= (\\mat{X}\\T \\mat{X})^{-1} \\mat{X}\\T (\\mat{X} \\vec{\\beta} + Z\\delta + \\epsilon) \\\\ &amp;= \\mat{X}\\T \\mat{X})^{-1} \\mat{X}\\T \\mat{X} \\vec{\\beta} + \\mat{X}\\T \\mat{X})^{-1} \\mat{X}\\T Z \\delta + + \\mat{X}\\T \\mat{X})^{-1} \\mat{X}\\T \\epsilon \\end{aligned} \\] Taking the expectation and noting that \\(\\E(X&#39; \\epsilon) = 0\\), \\[ \\begin{aligned}[t] \\E(\\hat{\\beta} | X) &amp;= \\beta + \\mat{X}\\T \\mat{X})^{-1} \\mat{X}\\T Z \\delta \\\\ &amp;= \\beta + \\text{bias} \\end{aligned} \\] There is no bias if either of these is true \\(\\delta\\) is 0 (Z has no effect on \\(Y\\)) \\(Z\\) is uncorrelated with \\(X\\) 5.2 What to do about Omitted Variable Bias There are two old (but simple) preliminary tests Regress \\(y\\) on \\(\\hat{y}\\) and \\(\\hat{y}^2\\) (Stata command linktest) - tests that the dependent variable has the correct functional form Ramsey RESET test (stata command ovtest) Coefficient Inclusing - will it reduce selection bias Oster and similar tests - e.g. Coefficient Comparison Tests Balancing Tests - regress x on the treatment. Adding covariates sequentially doesn’t make sense (but see Gelbach) 5.3 Regression Anatomy \\[ \\hat{\\beta}_k = \\frac{\\cov{y_i, \\tilde{x}_{ki}}}{\\var(\\tilde{x}_{ki})} \\] See https://mpra.ub.uni-muenchen.de/23245/1/reganat.pdf for a proof. 5.4 More Information 5.4.1 Simpson’s Paradox See Samuels (1993) for more discussion of Simpson’s Paradox Moore (2005) collects and succinctly describes several examples of Simpson’s Paradox An interactive visualization of the Simpson’s Paradox Horton. 2015. Fun with Simpson’s Paradox: Simulating Confounders Horton. 2012. Example 9.20: visualizing Simpson’s paradox See the Wikipedia Page US Median Wage by Education Level. Overall wages have risen, but within every group, the wage has fallen. Nielsen. Reinventing Explanation has a visual explanation of the Simpson’s paradox Gelman. Understanding Simpson’s Praradox Using a Graph. April 8, 2014. Discusses the Nielsen post, provides other visualizations, and notes how aggregation problems arise even in non-causal cases. Armstrong and Wattenberg (2014) introduce the Comet Chart for visualizing Simpson’s Paradoxes. See this page for code and examples, including an R implementation. reganatomy &lt;- function(model, variable) { variable &lt;- if (is.character(variable) &amp; 1 == length(variable)) { variable } else { deparse(substitute(variable)) } mod.mat &lt;- model.matrix(model) var.names &lt;- colnames(mod.mat) var &lt;- which(variable == var.names) if (0 == length(var)) { stop(paste(variable, &quot;is not a column of the model matrix.&quot;)) } response &lt;- model.response(model.frame(model)) if (is.null(weights(model))) { wt &lt;- rep(1, length(response)) } else { wt &lt;- weights(model) } res0 &lt;- as_tibble(lm(cbind(mod.mat[, var], response) ~ 1, weights = wt)$residual) names(res0) &lt;- c(&quot;y&quot;, &quot;x&quot;) res &lt;- as_tibble(lsfit(mod.mat[, -var], cbind(mod.mat[, var], response), wt = wt, intercept = FALSE)$residuals) names(res) &lt;- c(&quot;y_partial&quot;, &quot;x_partial&quot;) bind_cols(res0, res) } data(&quot;Bfox&quot;, package = &quot;car&quot;) m1 &lt;- lm(partic ~ tfr + menwage + womwage + debt + parttime, data = Bfox) an &lt;- reganatomy(m1, &quot;womwage&quot;) ggplot(an, aes(x = x, y = y)) + geom_point() ggplot(an, aes(x = x_partial, y = y_partial)) + geom_point() betawt &lt;- function(model, variables = NULL) { X &lt;- model.matrix(model) var_names &lt;- colnames(X) variables &lt;- variables %||% setdiff(colnames(X), &quot;(Intercept)&quot;) badvars &lt;- setdiff(variables, var_names) if (length(badvars)) { stop(&quot;Variables not found in model matrix: &quot;, paste0(badvars, collapse = &quot;, &quot;)) } wt &lt;- model$weights offset &lt;- model$offset f &lt;- function(i) { ii &lt;- which(i == var_names) mod2 &lt;- if (!is.null(wt)) { lm.wfit(X[ , -ii], X[, ii], wt, offset = offset) } else { lm.fit(X[ , -ii], X[, ii], offset = offset) } weighted.residuals(mod2) ^ 2 } as_tibble(set_names(map(variables, f), variables)) } data(&quot;Bfox&quot;, package = &quot;car&quot;) m1 &lt;- lm(partic ~ tfr + menwage + womwage + debt + parttime, data = Bfox) an &lt;- betawt(m1, c(&quot;womwage&quot;, &quot;tfr&quot;, &quot;debt&quot;, &quot;parttime&quot;)) Comments on Aronow and Samii https://tompepinsky.com/2012/02/27/regression-estimates-the-conditional-variance-weighted-zzzzzzzzzzz/ https://tompepinsky.com/2013/08/08/regression-representativeness-and-external-validity/ https://tompepinsky.com/2012/02/08/identification-is-neither-necessary-nor-sufficient-for-policy-relevance/ https://afinetheorem.wordpress.com/2016/02/26/does-regression-produce-representative-estimates-of-causal-effects-p-aronow-c-samii-2016/ ytilde &lt;- lm(prestige ~ type, data = Duncan)$residuals xtilde &lt;- lm(income ~ type, data = Duncan)$residuals lm(ytilde ~ xtilde - 1) ## ## Call: ## lm(formula = ytilde ~ xtilde - 1) ## ## Coefficients: ## xtilde ## 0.6758 lm(prestige ~ income + type, data = Duncan) ## ## Call: ## lm(formula = prestige ~ income + type, data = Duncan) ## ## Coefficients: ## (Intercept) income typeprof typewc ## 6.7039 0.6758 33.1557 -4.2772 Duncan &lt;- Duncan %&gt;% group_by(type) %&gt;% mutate(prestige_res = prestige - mean(prestige), income_res = income - mean(income)) lm(prestige_res ~ income_res - 1, data = Duncan) ## ## Call: ## lm(formula = prestige_res ~ income_res - 1, data = Duncan) ## ## Coefficients: ## income_res ## 0.6758 lm(prestige ~ income_res, data = Duncan) ## ## Call: ## lm(formula = prestige ~ income_res, data = Duncan) ## ## Coefficients: ## (Intercept) income_res ## 47.6889 0.6758 References "],
["non-constant-and-correlated-errors.html", "Chapter 6 Non-Constant and Correlated Errors 6.1 Prerequisites 6.2 Heteroskedasticity 6.3 Robust Standard Errors 6.4 Bootstrapping", " Chapter 6 Non-Constant and Correlated Errors 6.1 Prerequisites In addition to tidyverse pacakges, this chaper uses the sandwich and lmtest packages which provide robust standard errors and tests that use robust standard errors. library(&quot;sandwich&quot;) library(&quot;lmtest&quot;) library(&quot;tidyverse&quot;) library(&quot;broom&quot;) library(&quot;modelr&quot;) 6.2 Heteroskedasticity \\[ \\hat{\\beta} = (\\mat{X}\\T \\mat{X})^{-1} \\mat{X}\\T \\vec{y} \\] and \\[ \\Var(\\vec{\\epsilon}) = \\mat{\\Sigma} \\] is the variance-covariance matrix of the errors. Assumptions 1-4 give the expression for the sampling variance, \\[ \\Var(\\hat{\\beta}) = (\\mat{X}&#39;\\mat{X})^{-1} \\mat{X}\\T \\mat{\\Sigma} \\mat{X} (\\mat{X}\\T \\mat{X})^{-1} \\] under homoskedasticity, \\[ \\mat{\\Sigma} = \\sigma^2 \\mat{I}, \\] so the the variance-covariance matrix simplifies to \\[ \\Var(\\hat{\\beta} | X) = \\sigma^2 (\\mat{X}\\T \\mat{X})^{-1} \\] Homoskedastic: \\[ \\Var(\\vec{\\epsilon} | \\mat{X}) = \\sigma^2 I = \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma^2 &amp; 0 &amp; 0 &amp; \\cdots &amp; \\sigma^2 \\end{bmatrix} \\] Heteroskedastic \\[ \\Var(\\vec{\\epsilon} | \\mat{X}) = \\sigma^2 I = \\begin{bmatrix} \\sigma_1^2 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma_2^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma^2 &amp; 0 &amp; 0 &amp; \\cdots &amp; \\sigma_n^2 \\end{bmatrix} \\] - independent, since the only non-zero values are on the diagonal, meaning that there are no correlated errors between observations - non-identical, since the values on the diagonal are not equal, e.g. \\(\\sigma_1^2 \\neq \\sigma_2^2\\). - \\(\\Cov(\\epsilon_i, \\epsilon_j | \\mat{X}) = 0\\) - \\(\\Var(\\epsilon_i | \\vec{x}_i) = \\sigma^2_i\\) tibble( x = runif(100, 0, 3), `Homoskedastic` = rnorm(length(x), mean = 0, sd = 1), `Heteroskedasticity` = rnorm(length(x), mean = 0, sd = x) ) %&gt;% gather(type, `error`, -x) %&gt;% ggplot(aes(x = x, y = error)) + geom_hline(yintercept = 0, colour = &quot;white&quot;, size = 2) + geom_point() + facet_wrap(~ type, nrow = 1) Consequences \\(\\hat{\\vec{\\beta}}\\) are still unbiased and consistent estimators of \\(\\vec{\\beta}\\) Standard error estimates are biased, likely downward, meaning that the estimated standard errors will be smaller than the true standard errors (too optimistic). Test statstics won’t be distributed \\(t\\) or \\(F\\) \\(\\alpha\\)-level tests will have Type I errors \\(\\neq \\alpha\\) Coverage of confidence intervals will not be correct. OLS is not BLUE Visual diagnostics Plot residuals vs. fitted values Spread-location plot. y: square root of absolute value of residuals x: fitted values loess trend curve Dealing with NCV Transform the dependent variable Model the heteroskedasticity using WLS Use an estimator of \\(\\Var(\\hat{\\beta} | \\mat{X})\\) that is robust to heteroskedasticity Admit we are using the wrong model and use a different model 6.3 Robust Standard Errors The standard way to “fix” robust heteroskedasticity is to use so-called “robust” standard errors, more formally called Heteroskedasticity Consistent (HC), and heteroskedasticity and Autocorrelation Consistent standard errors. HC and HAC errors are implemented in the R package sandwich. See Zeileis (2006) and Zeileis2004a for succint discussion of the estimators themselves and examples of their usage. With robust standard errors, the coefficients of the model are estimated using lm(). Then a HC or HAC variance-covariance matrix is computed which corrects for heteroskedasticity (and autocorrelation). 6.3.1 Example: Duncan’s Occupation Data mod &lt;- lm(prestige ~ income + education + type, data = car::Duncan) The classic OLS variance covariance matrix is, vcov(mod) ## (Intercept) income education typeprof typewc ## (Intercept) 13.7920916 -0.115636760 -0.257485549 14.0946963 7.9021988 ## income -0.1156368 0.007984369 -0.002924489 -0.1260105 -0.1090485 ## education -0.2574855 -0.002924489 0.012906986 -0.6166508 -0.3881200 ## typeprof 14.0946963 -0.126010517 -0.616650831 48.9021401 30.2138627 ## typewc 7.9021988 -0.109048528 -0.388119979 30.2138627 37.3171167 and the standard errors are the diagonal of this matrix sqrt(diag(vcov(mod))) ## (Intercept) income education typeprof typewc ## 3.7137705 0.0893553 0.1136089 6.9930065 6.1087737 Now, use vcovHC to estimate the “robust” variance covariance matrix vcovHC(mod) ## (Intercept) income education typeprof typewc ## (Intercept) 15.2419440 -0.233347755 -0.255838779 25.6093353 12.4984902 ## income -0.2333478 0.023224098 -0.009806392 -0.6101496 -0.4039528 ## education -0.2558388 -0.009806392 0.019805541 -0.7730126 -0.4128297 ## typeprof 25.6093353 -0.610149584 -0.773012579 90.8056216 52.2164675 ## typewc 12.4984902 -0.403952792 -0.412829731 52.2164675 42.2001856 and the robust standard errors are the diagonal of the matrix sqrt(diag(vcovHC(mod))) ## (Intercept) income education typeprof typewc ## 3.9040932 0.1523945 0.1407322 9.5291984 6.4961670 Note that the robust standard errors are larger than the classic standard errors; this is almost always the case. If you need to use the robust standard errors to calculate t-statistics or p-values. coeftest(mod, vcovHC(mod)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.18503 3.90409 -0.0474 0.962436 ## income 0.59755 0.15239 3.9210 0.000337 *** ## education 0.34532 0.14073 2.4537 0.018589 * ## typeprof 16.65751 9.52920 1.7480 0.088128 . ## typewc -14.66113 6.49617 -2.2569 0.029547 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 TODO An example that uses vcovHAC() to calculate heteroskedasticity and autocorrelation consistent standard errors. 6.3.1.1 WLS vs. White’s esimator WLS: different estimator for \\(\\beta\\): \\(\\hat{\\beta}_{WLS} \\neq \\hat{\\beta}_{OLS}\\) With known weights: efficient \\(\\hat{\\se}(\\hat{\\beta}_{WLS})\\) are consistent If weights aren’t known … then biased for both \\(\\hat{\\beta}\\) and standard errors. White’s esimator (heteroskedasticity consistent standard errors): uses OLS estimator for \\(\\beta\\) consistent for \\(\\Var(\\hat{\\beta})\\) for any form of heteroskedasticity relies on consistency and large samples, and for small samples the performance may be poor. 6.3.2 Notes An additional use of robust standard errors is to diagnose potential model fit problems. The OLS line is still the minimum squared error of the population regression, but large differences may suggest that it is a poor approximation. King and Roberts (2015) suggest a formal test for this using the variance-covariance matrix. Note that there are other functions that have options to input variance-covariance matrices along with the lm object in order to use robust standard errors with that test or routine. Heteroskedastic consistent standard errors can be used with MLE models (White 1982). However, this is More generally, robust standard errors can be controversial: King and Roberts (2015) suggest using them to diagnose model fit problems. 6.4 Bootstrapping Non-parametric bootstrapping estimates standard errors and confidence intervals by resampling the observations in the data. The modelr function in modelr implements simple non-parametric bootstrapping.5 It generates n bootstrap replicates. library(&quot;modelr&quot;) bsdata &lt;- modelr::bootstrap(car::Duncan, n = 1024) glimpse(bsdata) ## Observations: 1,024 ## Variables: 2 ## $ strap &lt;list&gt; [&lt;2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2,... ## $ .id &lt;chr&gt; &quot;0001&quot;, &quot;0002&quot;, &quot;0003&quot;, &quot;0004&quot;, &quot;0005&quot;, &quot;0006&quot;, &quot;0007&quot;, ... It returns a data frame with two columns an id, and list column, strap containing modelr objects. The resample objects consist of two elements: data, the data frame; idx, the indexes of the data in the sample. bsdata[[&quot;strap&quot;]][[1]] ## &lt;resample [45 x 4]&gt; 2, 13, 26, 6, 22, 13, 4, 15, 20, 22, ... Since the data object hasn’t changed it doesn’t take up any additional memory until subsets are created, allowing for the creation of lazy subsamples of a dataset. A resample object can be turned into a data frame with as.data.frame: as.data.frame(bsdata[[&quot;strap&quot;]][[1]]) ## type income education prestige ## pilot prof 72 76 83 ## physician prof 76 97 97 ## electrician bc 47 39 53 ## minister prof 21 84 87 ## mail.carrier wc 48 55 34 ## physician.1 prof 76 97 97 ## author prof 55 90 76 ## teacher prof 48 91 73 ## banker prof 78 82 92 ## mail.carrier.1 wc 48 55 34 ## store.clerk wc 29 50 16 ## store.clerk.1 wc 29 50 16 ## watchman bc 17 25 11 ## undertaker prof 42 74 57 ## lawyer prof 76 98 89 ## undertaker.1 prof 42 74 57 ## banker.1 prof 78 82 92 ## mail.carrier.2 wc 48 55 34 ## janitor bc 7 20 8 ## RR.engineer bc 81 28 67 ## RR.engineer.1 bc 81 28 67 ## mail.carrier.3 wc 48 55 34 ## undertaker.2 prof 42 74 57 ## janitor.1 bc 7 20 8 ## gas.stn.attendant bc 15 29 10 ## cook bc 14 22 16 ## auto.repairman bc 22 22 26 ## RR.engineer.2 bc 81 28 67 ## reporter wc 67 87 52 ## factory.owner prof 60 56 81 ## waiter bc 8 32 10 ## dentist prof 80 100 90 ## gas.stn.attendant.1 bc 15 29 10 ## professor prof 64 93 93 ## truck.driver bc 21 15 13 ## welfare.worker prof 41 84 59 ## waiter.1 bc 8 32 10 ## coal.miner bc 7 7 15 ## machine.operator bc 21 20 24 ## engineer prof 72 86 88 ## physician.2 prof 76 97 97 ## coal.miner.1 bc 7 7 15 ## undertaker.3 prof 42 74 57 ## chemist prof 64 86 90 ## physician.3 prof 76 97 97 To generate standard errors for a statistic, estimate it on each bootstrap replicate. Suppose, we’d like to calculate robust standard errors for the regression coefficients in this regresion: lm(prestige ~ type + income + education, data = car::Duncan) ## ## Call: ## lm(formula = prestige ~ type + income + education, data = car::Duncan) ## ## Coefficients: ## (Intercept) typeprof typewc income education ## -0.1850 16.6575 -14.6611 0.5975 0.3453 Since we are interested in the coefficients, we need to re-run the regression with lm, extract the coefficients to a data frame using tidy, and return it all as a large data frame. For one bootstrap replicate this looks like, lm(prestige ~ type + income + education, data = as.data.frame(bsdata$strap[[1]])) %&gt;% tidy() %&gt;% select(term, estimate) ## term estimate ## 1 (Intercept) 1.0038364 ## 2 typeprof 18.9921030 ## 3 typewc -14.0803920 ## 4 income 0.7198451 ## 5 education 0.2047788 Note that the coefficients on this regression are slightly different than those in the original regression. bs_coef &lt;- map_df(bsdata$strap, function(dat) { lm(prestige ~ type + income + education, data = dat) %&gt;% tidy() %&gt;% select(term, estimate) }) There are multiple methods to estimate standard errors and confidence intervals using the bootstrap replicate estimates. Two simple ones are are Use the standard deviation of the boostrap estimates as \\(\\hat{se}(\\hat{\\beta})\\) instead of those produces by OLS. The confidence intervals are generated using the OLS coefficient estimate and the bootstrap standard errors, \\(\\hat{\\beta}_{OLS} \\pm t_{df,\\alpha/2}^* \\hat{se}_{boot}(\\hat{\\beta})\\) Use the quantiles of the bootstrap estimates as the endpoints of the confidence interval. E.g. the 95% confidence interval uses the 2.5th and 97.5th quantiles of the bootstrap estimates. The first (standard error) method requires less bootstrap replicates. The quantile method allows for asymmetric confidence intervals, but is noisier (the 5th and 95th quantiles vary more by samples) and requires more bootstrap replicates to get an accurate estimate. The bootstrap standard error confidence intervals: alpha &lt;- 0.95 tstar &lt;- qt(1 - (1 - alpha / 2), df = mod$df.residual) bs_est_ci1 &lt;- bs_coef %&gt;% group_by(term) %&gt;% summarise(std.error = sd(estimate)) %&gt;% left_join(select(tidy(mod), term, estimate, std.error_ols = std.error), by = &quot;term&quot;) %&gt;% mutate( conf.low = estimate - tstar * std.error, conf.high = estimate + tstar * std.error ) select(bs_est_ci1, term, conf.low, estimate, conf.high) ## # A tibble: 5 × 4 ## term conf.low estimate conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.03071912 -0.1850278 -0.4007748 ## 2 education 0.35309998 0.3453193 0.3375387 ## 3 income 0.60628354 0.5975465 0.5888094 ## 4 typeprof 17.19124717 16.6575134 16.1237796 ## 5 typewc -14.26095890 -14.6611334 -15.0613078 select(bs_est_ci1, term, std.error, std.error_ols) ## # A tibble: 5 × 3 ## term std.error std.error_ols ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 3.4190490 3.7137705 ## 2 education 0.1233038 0.1136089 ## 3 income 0.1384604 0.0893553 ## 4 typeprof 8.4583441 6.9930065 ## 5 typewc 6.3417634 6.1087737 The quantile confidence intervals: alpha &lt;- 0.95 bs_coef %&gt;% group_by(term) %&gt;% summarise( conf.low = quantile(estimate, (1 - alpha) / 2), conf.high = quantile(estimate, 1 - (1 - alpha) / 2) ) %&gt;% left_join(select(tidy(mod), term, estimate), by = &quot;term&quot;) %&gt;% select(term, estimate) ## # A tibble: 5 × 2 ## term estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) -0.1850278 ## 2 education 0.3453193 ## 3 income 0.5975465 ## 4 typeprof 16.6575134 ## 5 typewc -14.6611334 See the boot package (and other cites TODO) for more sophisticated methods of generating standard errors and quantiles. The package resamplr includes more methods using resampler objects. The package boot implements many more bootstrap methods (Canty 2002). References "],
["multiple-testing.html", "Chapter 7 Multiple Testing 7.1 Setup 7.2 Multiple Testing 7.3 Data snooping", " Chapter 7 Multiple Testing 7.1 Setup library(&quot;tidyverse&quot;) library(&quot;broom&quot;) library(&quot;stringr&quot;) library(&quot;magrittr&quot;) 7.2 Multiple Testing What happens if we run multiple regressions? What do p-values mean in that context? Simulate data where \\(Y\\) and \\(X\\) are all simulated from i.i.d. standard normal distributions, \\(Y_i \\sim N(0, 1)\\) and \\(X_{i,j} \\sim N(0, 1)\\). This means that \\(Y\\) and \\(X\\) are not associated. sim_reg_nothing &lt;- function(n, k, sigma = 1, .id = NULL) { .data &lt;- rnorm(n * k, mean = 0, sd = 1) %&gt;% matrix(nrow = n, ncol = k) %&gt;% set_colnames(str_c(&quot;X&quot;, seq_len(k))) %&gt;% as_tibble() .data$y &lt;- rnorm(n, mean = 0, sd = 1) # Run first regression .formula1 &lt;- as.formula(str_c(&quot;y&quot;, &quot;~&quot;, str_c(&quot;X&quot;, seq_len(k), collapse = &quot;+&quot;))) mod &lt;- lm(.formula1, data = .data, model = FALSE) df &lt;- tidy(mod) df[[&quot;.id&quot;]] &lt;- .id df } Here is an example with of running one regression: n &lt;- 1000 k &lt;- 19 results_sim &lt;- sim_reg_nothing(n, k) How many coefficients are significant at the 5% level? alpha &lt;- 0.05 arrange(results_sim, p.value) %&gt;% select(term, estimate, statistic, p.value) %&gt;% head(n = 20) ## term estimate statistic p.value ## 1 (Intercept) -0.057671023 -1.76761794 0.07743594 ## 2 X11 -0.048028515 -1.50903711 0.13161162 ## 3 X14 0.048992567 1.48746690 0.13721321 ## 4 X16 0.046164679 1.41286413 0.15801318 ## 5 X10 -0.044591056 -1.39512286 0.16329487 ## 6 X6 -0.034591848 -1.10784039 0.26820258 ## 7 X17 0.033580690 1.06105001 0.28892859 ## 8 X3 0.027898979 0.85598092 0.39221757 ## 9 X7 0.026048269 0.79321403 0.42784513 ## 10 X1 -0.021855838 -0.68917119 0.49087868 ## 11 X2 -0.021550954 -0.64034712 0.52209664 ## 12 X18 -0.020022613 -0.63555967 0.52521184 ## 13 X13 0.016805579 0.52445681 0.60007946 ## 14 X8 -0.015532376 -0.48393610 0.62853934 ## 15 X4 0.013266040 0.41097637 0.68117971 ## 16 X19 0.010020257 0.31736246 0.75103619 ## 17 X12 -0.010034560 -0.31199476 0.75511087 ## 18 X9 -0.009682185 -0.29624723 0.76710405 ## 19 X15 0.009669889 0.29186436 0.77045210 ## 20 X5 -0.002942765 -0.09249518 0.92632353 Is this surprising? No. Since the null hypothesis is true for all coefficients (\\(\\beta_j = 0\\)), a \\(p\\)-value of 5% means that 5% of the tests will be false positives (Type I error). Let’s confirm that with a larger number of simulations and also use it to calculate some other values. Run 1,024 simulations and save the results to a data frame. number_sims &lt;- 1024 sims &lt;- map_df(seq_len(number_sims), function(i) { sim_reg_nothing(n, k, .id = i) }) Calculate the number significant at the 5% level in each regression. n_sig &lt;- sims %&gt;% group_by(.id) %&gt;% summarise(num_sig = sum(p.value &lt; alpha)) %&gt;% count(num_sig) %&gt;% ungroup() %&gt;% mutate(p = n / sum(n)) Overall, we expect 5% to be significant at the 5 percent level. sims %&gt;% summarise(num_sig = sum(p.value &lt; alpha), n = n()) %&gt;% ungroup() %&gt;% mutate(p = num_sig / n) ## num_sig n p ## 1 1015 20480 0.04956055 What about the distribution of statistically significant coefficients in each regression? ggplot(n_sig, aes(x = num_sig, y = p)) + geom_bar(stat = &quot;identity&quot;) + scale_x_continuous(&quot;Number of significant coefs&quot;, breaks = unique(n_sig$num_sig)) + labs(y = &quot;Pr(reg has k signif coef)&quot;) What’s the probability that a regression will have no significant coefficients, \\(1 - (1 - \\alpha) ^ {k - 1}\\), (1 - (1 - alpha) ^ (k + 1)) ## [1] 0.6415141 What’s the take-away? Don’t be too impressed by statistical significance when many tests are run. Note that multiple hypothesis tests occur both within papers … and within literatures. 7.3 Data snooping A not-uncommon practice is to run a regresion, filter out variables with “insignificant” coefficients, and then run and report a regression with only the smaller number of “significant” variables. Most explicitly, this occurs with stepwise regression, the problems of which are well known (when used for inference). However, this can even occur in cases where the hypotheses are not specified in advance and there is no explicit stepwise function used. To see the issues with this method, let’s consider the worst case scenario, when there is no relationship between \\(Y\\) and \\(X\\). Suppose \\(Y_i\\) is sampled from a i.i.d. standard normal distributions, \\(Y_i \\sim N(0, 1)\\). Suppose that the design matrix, \\(\\mat{X}\\), consists of 50 variables, each sampled from i.i.d. standard normal distributions, \\(X_{i,k} \\sim N(0, 1)\\) for \\(i \\in 1:100\\), \\(k \\in 1:50\\). Given this, the \\(R^2\\) for these regressions should be approximately 0.50. As shown in the previous section, it will not be uncommon to have several “statistically” significant coefficients at the 5 percent level. The sim_datasnoop function simulates data, and runs two regressions: Regress \\(Y\\) on \\(X\\) Keep all variables in \\(X\\) with \\(p &lt; .25\\). Regress \\(Y\\) on the subset of \\(X\\), keeping only those variables that were significant in step 2. sim_datasnoop &lt;- function(n = 100, k = 50, p = 0.10) { .data &lt;- rnorm(n * k, mean = 0, sd = 1) %&gt;% matrix(nrow = n, ncol = k) %&gt;% set_colnames(str_c(&quot;X&quot;, seq_len(k))) %&gt;% as_tibble() .data$y &lt;- rnorm(n, mean = 0, sd = 1) # Run first regression .formula1 &lt;- as.formula(str_c(&quot;y&quot;, &quot;~&quot;, str_c(&quot;X&quot;, seq_len(k), collapse = &quot;+&quot;))) mod1 &lt;- lm(.formula1, data = .data, model = FALSE) # Select model with only significant values (ignoring intercept) signif_x &lt;- tidy(mod1) %&gt;% filter(p.value &lt; p, term != &quot;(Intercept)&quot;) %&gt;% `[[`(&quot;term&quot;) if (length(signif_x &gt; 0)) { .formula2 &lt;- str_c(str_c(&quot;y&quot;, &quot;~&quot;, str_c(signif_x, collapse = &quot;+&quot;))) mod2 &lt;- lm(.formula2, data = .data, model = FALSE) } else { mod2 &lt;- NULL } tibble(mod1 = list(mod1), mod2 = list(mod2)) } Now repeat this simulation 1,024 times, calculate the \\(R^2\\) and number of statistically signifcant coefficients at \\(\\alpha = .05\\). n_sims &lt;- 1024 alpha &lt;- 0.05 sims &lt;- rerun(n_sims, sim_datasnoop()) %&gt;% bind_rows() %&gt;% mutate( r2_1 = map_dbl(mod1, ~ glance(.x)$r.squared), r2_2 = map_dbl(mod2, function(x) if (is.null(x)) NA_real_ else glance(x)$r.squared), pvalue_1 = map_dbl(mod1, ~ glance(.x)$p.value), pvalue_2 = map_dbl(mod2, function(x) if (is.null(x)) NA_real_ else glance(x)$p.value), sig_1 = map_dbl(mod1, ~ nrow(filter(tidy(.x), term != &quot;(Intercept)&quot;, p.value &lt; alpha))), sig_2 = map_dbl(mod2, function(x) { if (is.null(x)) NA_real_ else nrow(filter(tidy(x), term != &quot;(Intercept)&quot;, p.value &lt; alpha)) }) ) select(sims, r2_1, r2_2, pvalue_1, pvalue_2, sig_1, sig_2) %&gt;% summarise_all(funs(mean(., na.rm = TRUE))) ## # A tibble: 1 × 6 ## r2_1 r2_2 pvalue_1 pvalue_2 sig_1 sig_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.5036307 0.1622974 0.5070183 0.03709078 2.523438 2.51002 While the average \\(R\\) squared of the second stage regressions are less, the average \\(p\\)-values of the F-test that all coefficients are zero are much less. The number of statistically significant coefficients in the first and second regressions are approximately the same, which the second regression being slightly What happens if the number of obs, number of variables, and filtering significance level are adjusted? So why are the significance levels of the overall \\(F\\) test incorrect? For a p-value to be correct, it has to have the correct sampling distribution of the observed data. Even though in this simulation we are sampling the data in the first stage from a model that satisfies the assumptions of the F-test, the second stage does not account for the original filtering. This example is known as Freedman’s Paradox (Freedman 1983). References "],
["weighted-regression.html", "Chapter 8 Weighted Regression 8.1 Weighted Least Squares (WLS) 8.2 When should you use WLS? 8.3 Correcting for Known Heteroskedasticity 8.4 Sampling Weights 8.5 References", " Chapter 8 Weighted Regression 8.1 Weighted Least Squares (WLS) Ordinary least squares estimates coefficients by finding the coefficients that minimize the sum of squared errors, \\[ \\hat{\\vec\\beta}_{OLS} = \\argmin_{\\vec{b}} \\sum_{i = 1}^N (y_i - \\vec{x}\\T \\vec{b})^2 . \\] Note that the objective function treats all observations equally; an error in one is as good as any other. However, there are several situations where we care more about minimizing some errors more than others. The next situation will discuss the reasons to use WLS, but Weighted least squares (WLS) requires only a small change to the OLS objective function. Each observation is given a weight, \\(w_i\\), and the weighted sum of squared errors is minimized, \\[ \\begin{aligned}[t] \\hat{\\vec\\beta}_{WLS} = \\argmin_{\\vec{b}} \\sum_{i = 1}^N w_i (y_i - \\vec{x}\\T \\vec{b})^2 \\end{aligned} \\hat{\\beta}_{WLS} = \\argmin_{\\vec{b}} \\sum_{i = 1}^N w_i (y_i - \\vec{x}\\T \\vec{b})^2 . \\] The weights \\(w_i\\) are provided by the analyst and are not estimated. Note that OLS is a special case of WLS where \\(w_i = 1\\) for all the observations. In order to minimize the errors, WLS will have to fit the line closer to observations with higher weights You can estimate WLS by using the weights argument to rdoc(&quot;stats&quot;, &quot;lm&quot;). 8.2 When should you use WLS? The previous section showed what WLS is, but when should you use weighted regression? It depends on the purpose of your analysis: If you are estimating population descriptive statistics, then weighting is needed to ensure that the sample is representative of the population. If you are concerned with causal inference, then weighting is more nuanced. You may or may not need to weight, and it will often be unclear which is better. There are three reasons for weighting in causal inference (Solon, Haider, and Wooldridge 2015): To correct standard errors for heteroskedasticity Get consistent estimates by correcting for endogenous sampling Identify average partial effects when there is unmodeled heterogeneity in the effects. Heteroskedasticity: Estimate OLS and WLS. If the model is misspecified or there is endogenous selection, then OLS and WLS have different probability limits. The contrast between OLS and WLS estimates is a diagnostic for model misspecification or endogenous sampling. Always use robust standard errors. Endogenous sampling: If the sample weights vary exogenously instead of endogenously, then weighting may be harmful for precision. The OLS still specifies the conditional mean. Sampling is exogenous if the sampling probabilities are independent of the error - e.g. if they are only functions of the explanatory variables. If the probabilities are a function of the dependent variable, then they are endogenous. if sampling rate is endogenous, weight by inverse selection. use robust standard errors. if the sampling rate is exogenous, then OLS and WLS are consistent. Use OLS and WLS as test of model misspecification. Heterogeneous effects: Identifying average partial effects. WLS estimates the linear regression of the population, but this is not the same as the average partial effects. But that is because OLS does not estimate the average partial effect, but weights according to the variance in X. Angrist and Pischke (2009, 92) suggest weighting when “they make it more likely that the regression you are estimaing is close to the population target you are trying to estimate”. sampling weights: yes grouped data (sums, averages): yes heteroskedasticity: no (just use robust standard errors) WLS can be more efficient than OLS if variance model is correct If \\(E(e_i | X)\\) is a poor approximation or measurements are noisy, WLS has bad finite sample properties If the CEF is not linear, then OLS and WLS are both wrong, but OLS still interpretable as the minimum mean squared error approximation of the CEF. The WLS is also an approx of CEF, but approx is a function of the weights. Advice of (Cameron and Trivedi 2010, 113). There are two approaches to using weights. Census parameter: Reweight regression to try to get the population regression estimates. Control function approach: Assuming that \\(\\E(\\epsilon_i | \\vec{x}_i) = 0\\), then weights are not needed. WLS is consistent for any weights, and OLS is more efficient. This means if we control for all covariates relevant to sampling probabilities, there is no need to weight. This works as long as sampling probabilities are a function of \\(x\\) and not of \\(y\\). It seems that Cameron and Trivedi (2010) “census parameter” approach is what Angrist and Pischke (2009) interprets it as, but it supports the model based approach. Weights should be used for predictions and computing average MEs. (Cameron and Trivedi 2010, 114–15). Fox (2016, 461): inverse probability weights are different than weights in heteroskedasticity, and WLS cannot be used. It will give the wrong SEs but correct point estimates. Seems to suggest using bootstrapping to get standard errors instead (Fox 2016, 661, 666). 8.3 Correcting for Known Heteroskedasticity Most generally, heteroskedasticity is “unknown” and robust standard errors should be used. However, there are some cases where heteroskedasticity is “known”. For example: The outcome variable consists of measurements with a given measurement error - perhaps they are estimates themselves. The error of the output depends on input variables in known ways. For example, the sampling error of polls. Examples: \\(\\E(\\epsilon)_i^2 \\propto z_i^2\\) where \\(a\\) is some observated variable. Then \\(w_i = z_i\\). \\(\\E(\\epsilon)_^2\\) is an average of values. Then \\(\\sigma^2_i = \\omega^2 / n_i\\). In WLS, \\(w_i = 1 / \\sqrt{n_i}\\). \\(\\E(\\epsilon)_^2\\) is the sum of values. Then $^2_i = n_i ^2 $. In WLS, \\(w_i = \\sqrt{n_i}\\). If \\(p_i^{-1}\\) is the inverse-sampling probability weight, then weight by \\(w_i\\) Suppose that the heteroskedasticity is known up to a multiplicative constant, \\[ \\Var(\\varepsilon_i | \\mat{X}) = a_i \\sigma^2 , \\] where \\(a_i = a_i \\vec{x}_i\\T\\) is a positive and known function of \\(\\vec{x}_i\\). Define the weighting matrix, \\[ \\mat{W} = \\begin{bmatrix} 1 / \\sqrt{a_1} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 / \\sqrt{a_2} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; 0 \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 / \\sqrt{a_N} \\end{bmatrix}, \\] and run the regression, \\[ \\begin{aligned}[t] \\mat{W} y &amp;= \\mat{W} \\mat{X} \\vec{\\beta} + \\mat{W} \\vec\\varepsilon \\\\ \\vec{y}^* &amp;= \\mat{X}^* \\vec{\\beta} + \\vec{\\varepsilon}^* . \\end{aligned} \\] Run the regression of \\(\\vec{y}^*\\) on \\(\\mat{X}^*\\), and the Gauss-Markov assumptions are satisfied. Then using the usual OLS formula, \\[ \\hat{\\vec\\beta}_{WLS} = ((\\mat{X}^*)&#39; \\mat{X}^*) (\\mat{X}^*)&#39; \\vec{y}^* = (\\mat{X}&#39; \\mat{W}&#39; \\mat{W} \\mat{X})^{-1} \\mat{X}&#39; \\mat{W}&#39; \\mat{W} \\vec{y} . \\] 8.4 Sampling Weights Sampling weights are the inverse probabilities of selection that are used to weight a sample to be representative of population (as if were a random draw from the population). In this situation, whether to use sampling weights depends on whether you are calculating If you are calculating a descriptive statistic from the sample as an estimator of a population parameter, you need to use weights if sample weights are a function of \\(X\\) only, estimates are unbiased and more efficient without weighting if the sample weights are a function of \\(Y | X\\), then use the weights With fixed \\(X\\), regression does not require random sampling, so the sampling weights of the \\(X\\) are irrelevant. If the original unweighted data are homoskedastic, then sampling weights induces heteroskedasticity. Suppose the true model is, \\[ Y_i = \\vec{x}\\T \\vec{\\beta} + \\varepsilon_i \\] where \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\). Then the weighted model is, \\[ \\sqrt{w_i} Y_i = \\sqrt{w_i} \\vec{x}\\T \\vec{\\beta} + \\sqrt{w_i} \\varepsilon_i \\] and now \\(\\sqrt{w_i} \\varepsilon_i \\sim N(0, w_i \\sigma^2)\\). If the sampling weights are only a function of the \\(X\\), then controlling for \\(X\\) is sufficient. In fact, OLS is preferred to WLS, and will produce unbiased and efficient estimates. The choice between OLS and WLS is a choice between different distributions of \\(\\mat{X}\\). However, if the model is specified correctly the coefficients should be the same, regardless of the distribution of \\(\\mat{X}\\). Thus, if the estimates of OLS and WLS differ, then it is evidence that the model is misspecified. Winship and Radbill (1994) suggest using the method of Dumouchel and Duncan (1983) to test whether the OLS and WLS are difference. Estimate \\(E(Y) = \\mat{X} \\beta\\) Estimate \\(E(Y) = \\mat{X} \\beta + \\delta \\vec{w} + \\vec{\\gamma} \\vec{w} \\mat{X}\\), where all \\(X\\) Test regression 1 vs. regression 2 using an F test. If the F-test is significant, then the weights are not simply a function of \\(X\\). Either try to respecify the model or use WLS with robust standard errors. If the F-test is insignificant, then the weights are simply a function of \\(X\\). Use OLS. Modern survey often use complex multi-stage sampling designs. Like clustering generally, this will affect the standard errors of these regressions. Clustering by primary sampling units is a good approximation of the standard errors from multistage sampling. 8.5 References The WLS derivation can be found in Fox (2016, 304–6, 335–36, 461). Other textbook discussions: Angrist and Pischke (2009, 91–94), Angrist and Pischke (2014), [p. 202-203], Davidson and MacKinnon (2004, 261–62), Wooldridge (2012, 409–13). Solon, Haider, and Wooldridge (2015) is a good (and recent) overview with practical advice of when to weight and when not-to weight linear regressions. Also see the advice from the World Bank blog. See also Deaton (1997), Dumouchel and Duncan (1983), and Wissoker (1999). Gelman (2007b), in the context of post-stratification, proposes controlling for variables related to selection into the sample instead of using survey weights; also see the responses (Bell and Cohen 2007; Breidt and Opsomer 2007; Little 2007; Pfeffermann 2007), and rejoinder (Gelman 2007a) and blog post. Gelman’s approach is similar to that earlier suggested by Winship and Radbill (1994). For survey weighting, see the R package survey. References "],
["discrete-outcome-variables.html", "Chapter 9 Discrete Outcome Variables 9.1 Linear Probability Model 9.2 Logit Model", " Chapter 9 Discrete Outcome Variables The linear regression model is: \\[ Y_i = \\beta X_i + \\epsilon_i \\] But what if the outcome, \\(Y_i\\), is binary \\(Y_i \\in \\{0, 1}\\)? When OLS is used to estimate a binary outcome variable it is called the linear probability model). This chapter considers the properties of OLS estimation with a binary outcome variable, and introduces logit, a generalized linear model (GLM), for binary outcome variables. 9.1 Linear Probability Model 9.2 Logit Model The logit model is a model of the probability that \\(y = 1\\), \\[ \\Pr(y_i = 1) = \\logit^{-1}(\\vec{x}_i\\T \\vec{\\beta}) , \\] with the assumption that \\(y_i\\) are independent given these probabilities. The value \\(\\vec{x}_i\\T \\vec{\\beta}\\) is called the linear predictor. The logit function transforms values between 0 and 1 to be between \\(-\\infty\\) and \\(\\+infty\\), \\[ \\logit(p) = \\log\\left( \\frac{p}{1 - p} \\right) = \\log(p) - \\log(1 - p) . \\] The logistic function or inverse logit function transforms continuous values \\((-\\infty, +\\infty)\\) to be between 0 and 1, \\[ \\logit^{-1}(x) = \\frac{e^x}{1 + e^x} = \\frac{1}{1 + e^{-x}} . \\] The logistic function is the inverse of the logit function, so, \\[ \\logit^{-1}(\\logit(p)) = p , \\] and \\[ \\logit(\\logit^{-1}(x)) = x . \\] TODO Plots of logit and logistic functions. Thus, in logit functions To estimate the logit model, minimize the following objective function, \\[ \\begin{aligned}[t] \\hat{\\vec{beta}} &amp;= \\argmax_{\\vec{b}} \\sum_i (y_i \\log \\Pr(y_i = 1) + (1 - y_i) \\Pr(y_i = 0) \\\\ &amp;= \\argmax_{\\vec{b}} \\sum_i y_i \\log(logit^{-1}(\\vec{x}_i\\T \\vec{b})) + (1 - y_i) \\log(1 - logit^{-1}(\\vec{x}_i\\T \\vec{b})) . \\end{aligned} \\] While OLS minimizes squared errors, the logit model minimizes a different objective function, which minimizes the log difference between the obeserved value (\\(y_i\\)) and the predicted probability of that value, \\(\\Pr(y_i = 1)\\) or \\(\\Pr(y_i = 0)\\). There are several methods fo find the values of \\(\\hat{\\beta}\\), including iterated least squares (cite), but these details are handled by the software. While the coefficients of OLS are (usually) directly interpretable as the marginal effect of \\(x\\) on \\(E(y)\\), those of logit regression models are not. The marginal effect of a logit model is, \\[ \\frac{\\partial\\,\\Pr(y = 1)}{\\partial x_k} = \\frac{1}{1 + e^{- \\vec{x}\\T \\vec{\\beta}}} = \\beta_k \\frac{e^{-\\vec{\\beta} \\vec{x}}}{(1 + e^{- \\vec{x} \\vec{\\beta}})^2}\\T = \\beta_k \\Pr(y = 1) \\Pr(y = 0) . \\] In an OLS model, the marginal effect of \\(x\\) does not depend on the the specific values of \\(x_i\\). In a logit model it does. Interpretation short-cuts Calculate the Average Marginal Effects or the Marginal Effects at the Mean (or other Representative Values). CITES The divide by four rule Divide \\(\\beta\\) by four to get an upper bound on the marginal effect (in probability) of \\(x\\). The slope of the logistic curve is steepest at its center where \\(\\logit^{-1}(\\alpha + \\beta x) = 0.5\\) and \\(\\alpha + \\beta x = 0\\). The slope of this curve at this point is \\(\\beta e^0 / (1 + e^0)^2 = \\beta / 4\\) (Gelman and Hill 2007, 82). See [this blog post] (http://www.r-bloggers.com/divide-by-4-rule-for-marginal-effects/) for some examples. log-odds While the marginal effects of the logit model are non-linear on the probability scale, they are linear on the log-odds scale. A one unit change in \\(x\\) is associated with a \\(\\beta\\) change in the log-odds \\(\\log(\\Pr(y = 1) / Pr(y = 0))\\). TODO Inference: standard errors, confidence intervals, statistical significance. Predictions: For each observation, there is a predictive probability, \\[ \\tilde{p}_i = Pr(\\tilde{y}_i = 1) = \\logit^{-1}(\\tilde{\\vec{x}}_i\\T \\hat{\\vec{\\beta}}) . \\]n TODO Diagnostics: leverage, outliers, what isn’t relevant from OLS. References "],
["robust-regression.html", "Chapter 10 Robust Regression 10.1 Prerequites 10.2 Examples 10.3 Notes", " Chapter 10 Robust Regression Consider a more general formulation of linear regression estimation, \\[ \\hat{\\vec{\\beta}} = \\argmin_{b} \\sum_i f(y_i - \\vec{x}_i\\T \\vec{b}) = \\argmin_{b} \\sum_i f(\\hat{\\epsilon}_i) \\] where \\(f\\) is some function of the errors to be minimized. OLS minimizes the squared errors, \\[ f(\\hat{\\epsilon}_i) = \\hat{\\epsilon}_i^2 \\] However, other objective functions could be used. Some of these are less influenced by outliers, and these are called robust or resistent methods (these are slightly different properties of the estimator). One example is least median squares, which minimizes the absolute \\[ f(\\hat{\\epsilon}_i) = |\\hat{\\epsilon}| \\] 10.1 Prerequites Many forms of robust regression are available through the **MASS* library: library(&quot;MASS&quot;) library(&quot;tidyverse&quot;) library(&quot;modelr&quot;) 10.2 Examples Methods of dealing with outliers include robust and resistant regression methods. These methods will weigh observations far from the regression line less, which makes them less influential on the regression line. The functions lqs (least quantile squares) and rls (robust least squares) in MASS implement several roubst and resistant methods. These include least median squares: models &lt;- list() models[[&quot;lms&quot;]] &lt;- lqs(prestige ~ type + income + education, data = car::Duncan, method = &quot;lms&quot;) models[[&quot;lms&quot;]] ## Call: ## lqs.formula(formula = prestige ~ type + income + education, data = car::Duncan, ## method = &quot;lms&quot;) ## ## Coefficients: ## (Intercept) typeprof typewc income education ## -3.8565 -3.2120 -27.2139 0.7298 0.4953 ## ## Scale estimates 4.678 6.263 least trimmed squares, models[[&quot;lqs&quot;]] &lt;- lqs(prestige ~ type + income + education, data = car::Duncan, method = &quot;lts&quot;) models[[&quot;lqs&quot;]] ## Call: ## lqs.formula(formula = prestige ~ type + income + education, data = car::Duncan, ## method = &quot;lts&quot;) ## ## Coefficients: ## (Intercept) typeprof typewc income education ## -8.4158 -0.2083 -22.8542 0.7292 0.5000 ## ## Scale estimates 6.20 6.78 M-method with Huber weighting, models[[&quot;m_huber&quot;]] &lt;- rlm(prestige ~ type + income + education, data = car::Duncan, method = &quot;M&quot;, scale.est = &quot;Huber&quot;) models[[&quot;m_huber&quot;]] ## Call: ## rlm(formula = prestige ~ type + income + education, data = car::Duncan, ## scale.est = &quot;Huber&quot;, method = &quot;M&quot;) ## Converged in 8 iterations ## ## Coefficients: ## (Intercept) typeprof typewc income education ## -1.2114545 15.8670992 -14.7744856 0.6663791 0.3024461 ## ## Degrees of freedom: 45 total; 40 residual ## Scale estimate: 9.03 MM-methods with Huber weighting, models[[&quot;mm_huber&quot;]] &lt;- rlm(prestige ~ type + income + education, data = car::Duncan, method = &quot;MM&quot;, scale.est = &quot;Huber&quot;) models[[&quot;mm_huber&quot;]] ## Call: ## rlm(formula = prestige ~ type + income + education, data = car::Duncan, ## scale.est = &quot;Huber&quot;, method = &quot;MM&quot;) ## Converged in 7 iterations ## ## Coefficients: ## (Intercept) typeprof typewc income education ## -2.0191196 14.3261207 -15.6664693 0.7196171 0.2803928 ## ## Degrees of freedom: 45 total; 40 residual ## Scale estimate: 8.21 10.3 Notes See the Fox (2016) chapter on Robust Regression See Western (1995) for discussion of robust regression in the context of political science See the MASS package for implementations of many of these methods. References "],
["prediction-and-model-comparison.html", "Chapter 11 Prediction and Model Comparison 11.1 Prerequisites 11.2 Measures of Prediction 11.3 Model Comparison 11.4 Example: Predicting the Price of Wine 11.5 Cross-Validation 11.6 Out of Sample Error 11.7 Analytic Covariance Methods 11.8 Further Resources 11.9 Measurement Error", " Chapter 11 Prediction and Model Comparison 11.1 Prerequisites library(&quot;tidyverse&quot;) library(&quot;broom&quot;) library(&quot;modelr&quot;) ## ## Attaching package: &#39;modelr&#39; ## The following object is masked from &#39;package:broom&#39;: ## ## bootstrap library(&quot;stringr&quot;) 11.2 Measures of Prediction Ideally the prediction measure should be derived from the problem at hand; There is no uniformly correct measure of accuracy, so absent other costs the the analysis should include the costs of outcomes to the [analyst(https://en.wikipedia.org/wiki/Decision_theory). Categorical variables Accuracy: (True Positive) + (True Negative) / (all observations) Precision: (True Positives) / (Classifier Positives) Sensitivity (recall): (True Positive) / (All positives) Specificity: (True negative) / (Classifier negatives) F1 score which balances precision (TP / (TP + FP)) and recall (TP / (TP + FN)) as (precision * recall) / (precision + recall) Continuous Variables Root Mean Squared Error (RMSE): \\(\\frac{1}{m} \\sum_i (\\hat{y}_i - y_i)^2\\). This is weights large errors heavily since it uses “quadratic errors”. Mean Absolute Devision (MAD): \\(\\frac{1}{m} \\sum_i \\|\\hat{y}_i - y_i \\|\\). This is robust to large errors, but sensitive to the scale of the forecasts. Mean Absolute Percentage Error (MAPE): \\(\\frac{1}{m} \\sum_i \\| (\\hat{y}_i - y_i) / y_i \\|\\). 11.3 Model Comparison For comparing models in terms of prediction we want to compare them on their expected error on future data, not their in-sample error. It is easy to minimize in-sample error, use the every-observation-is-special model—have a predictor for each observation. However, that model will have no ability to predict future observations. The fundamental problem to estimating the expected error of the model is that we don’t have the future data to evaluate it. Even if we acquire new data that did not exist at the time of fitting the model, all that can be done is to retrospectively evaluate the model performance, perhaps giving a better estimate of the expected error of the model in the future. Yet, any errors of the model with respect to any future data will still be unknown. For example the errors of model based forecasts of the popular vote share, electoral votes, or winner of the U.S. presidential election of 2016 can be transparently evaluated since they can be made prior to the data, and short of access to a time machine, the models cannot overfit or peak on future data. After the election the models can be evaluated. However, at that point it is their expected error in the next election in 2020 which is of interest, and that is still unknown. There are two main approaches to estimating the prediction error cross validation: Split the data into test and training subsets. The model is fit on the training data, and predictions are made on the test data. This is often done repeatedly. covariance estimates: These are analytic estimates of the expected error, usually restricted to either linear models and/or only asymptotically valid. But since they do not require resampling, they are fast. So when do these approaches work? When do measures based on in-sample data extrapolate to non-sample errors? Like pretty much every statistical method, they work when the sample used to fit the model resembles the data on which which inference is being made. 11.4 Example: Predicting the Price of Wine The bordeaux dataset contains the prices of vintages of Bordeaux wines sold at auction in New York, as well as the the weather and rainfall for the years of the wine. This data was used by economist, Orley Ashenfelter, to show that the quality of a wine vintage can be as measured by its price, can largely be predicted by its age, and the weather (temperature and rainfall) of its vintage year. At the time, these prediction were not taken kindly by the wine conoisseurs.[^wine] # devtools::install_github(&quot;jrnold/datums&quot;) bordeaux &lt;- datums::bordeaux %&gt;% mutate(lprice = log(price / 100)) %&gt;% dplyr::filter(!is.na(price)) The data contains 27 prices of Bordeaux wines sold at auction in New York in 1993 for vintages from 1952 to 1980; the years 1952 and 1954 were excluded because they were rarely sold. Prices are measured as an index where 100 is the price of the 1961. Since prices are 7 The dataset also includes three predictors of the price of each vintage: time_sv: Time since the vintage, where 0 is 1983. wrain: Winter (October–March) rain hrain: Harvest (August–September) rain degrees: Average temperature in degrees centigrade from April to September in the vintage year. The first variable to consider is the age of the vintage and the price: ggplot(filter(bordeaux, !is.na(price), !is.na(vint)), aes(y = log(price), x = vint)) + geom_point() + geom_rug() + geom_smooth(method = &quot;lm&quot;) Ashenfleter, Ashmore, and Lalonde (1995) run two models. All models were estimated using OLS with log-price as the outcome variable. The predictors in the models were: vintage age vintage age, winter rain, harvest rain We’ll start by considering these models. Since we are running several models, we’ll define the model formulae in a list mods_f &lt;- list(lprice ~ time_sv, lprice ~ time_sv + wrain + hrain + degrees) and, run each model and store the results in a data frame as list column of lm objects: mods_res &lt;- tibble( model = seq_along(mods_f), formula = map_chr(mods_f, deparse), mod = map(mods_f, ~ lm(.x, data = bordeaux)) ) mods_res ## # A tibble: 2 × 3 ## model formula mod ## &lt;int&gt; &lt;chr&gt; &lt;list&gt; ## 1 1 lprice ~ time_sv &lt;S3: lm&gt; ## 2 2 lprice ~ time_sv + wrain + hrain + degrees &lt;S3: lm&gt; Now that we have these models, extract the coefficients into a data frame with the broom function tidy: mods_coefs &lt;- mods_res %&gt;% # Add column with the results of tidy for each model # conf.int = TRUE adds confidence intervals to the data mutate(tidy = map(mod, tidy, conf.int = TRUE)) %&gt;% # use unnest() to expand the data frame to one row for each row in the tidy # elements unnest(tidy, .drop = TRUE) glimpse(mods_coefs) ## Observations: 7 ## Variables: 9 ## $ model &lt;int&gt; 1, 1, 2, 2, 2, 2, 2 ## $ formula &lt;chr&gt; &quot;lprice ~ time_sv&quot;, &quot;lprice ~ time_sv&quot;, &quot;lprice ~ ti... ## $ term &lt;chr&gt; &quot;(Intercept)&quot;, &quot;time_sv&quot;, &quot;(Intercept)&quot;, &quot;time_sv&quot;, ... ## $ estimate &lt;dbl&gt; -2.025199170, 0.035429560, -12.145333577, 0.02384741... ## $ std.error &lt;dbl&gt; 0.2472286519, 0.0136624941, 1.6881026571, 0.00716671... ## $ statistic &lt;dbl&gt; -8.191604, 2.593199, -7.194665, 3.327521, 2.420525, ... ## $ p.value &lt;dbl&gt; 1.522111e-08, 1.566635e-02, 3.278791e-07, 3.055739e-... ## $ conf.low &lt;dbl&gt; -2.534376e+00, 7.291126e-03, -1.564624e+01, 8.984546... ## $ conf.high &lt;dbl&gt; -1.516022230, 0.063567993, -8.644422940, 0.038710280... walk(mods_res$mod, ~ print(summary(.x))) ## ## Call: ## lm(formula = .x, data = bordeaux) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.8545 -0.4788 -0.0718 0.4562 1.2457 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.02520 0.24723 -8.192 1.52e-08 *** ## time_sv 0.03543 0.01366 2.593 0.0157 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5745 on 25 degrees of freedom ## Multiple R-squared: 0.212, Adjusted R-squared: 0.1804 ## F-statistic: 6.725 on 1 and 25 DF, p-value: 0.01567 ## ## ## Call: ## lm(formula = .x, data = bordeaux) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.46027 -0.23864 0.01347 0.18600 0.53446 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.215e+01 1.688e+00 -7.195 3.28e-07 *** ## time_sv 2.385e-02 7.167e-03 3.328 0.00306 ** ## wrain 1.167e-03 4.820e-04 2.421 0.02420 * ## hrain -3.861e-03 8.075e-04 -4.781 8.97e-05 *** ## degrees 6.164e-01 9.518e-02 6.476 1.63e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2865 on 22 degrees of freedom ## Multiple R-squared: 0.8275, Adjusted R-squared: 0.7962 ## F-statistic: 26.39 on 4 and 22 DF, p-value: 4.058e-08 Likewise, extract model statistics such as, \\(R^2\\), adjusted \\(R^2\\), and \\(\\hat{\\sigma}\\): mods_glance &lt;- mutate(mods_res, .x = map(mod, glance)) %&gt;% unnest(.x, .drop = TRUE) mods_glance %&gt;% select(formula, r.squared, adj.r.squared, sigma) ## # A tibble: 2 × 4 ## formula r.squared adj.r.squared ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lprice ~ time_sv 0.2119700 0.1804488 ## 2 lprice ~ time_sv + wrain + hrain + degrees 0.8275278 0.7961692 ## # ... with 1 more variables: sigma &lt;dbl&gt; 11.5 Cross-Validation o compare predictive models, we want to compare how well it predicts (duh), which means estimating how well it will work on new data. The problem with this is that new data is just that, …, new. The trick is to resuse the sample data to get an estimate of how well the model will work on new data. This is done by fitting the model on a subset of the data, and predicting another subset of the data which was not used to fit the model; often this is done repeatedly. There are a variety of ways to do this, depending on the nature of the data and the predictive task. However, they all implictly assume that the sample of data that was used to fit the model is representative of future data. … model validation is a good, simple, broadly aplicable procedure that is rarely used in social research (Fox, p. 630) The simple idea of splitting a sample into two and then developing the hypothesis on the basis of one part and testing it on the remainder may perhaps be said to be one of the most seriously neglected ideas in statistics, if we measure the degree of neglect by the ratio of the number of cases in where a method could give help to the number where it was actually used. (Barnard 1974, p. 133, quoted in Fox, p. 630) 11.6 Out of Sample Error k &lt;- 20 f &lt;- map(seq_len(k), ~ as.formula(str_c(&quot;lprice ~ poly(time_sv, &quot;, .x, &quot;)&quot;))) names(f) &lt;- seq_len(k) mods_overfit &lt;- map(f, ~ lm(.x, data = bordeaux)) fits &lt;- map_df(mods_overfit, glance, .id = &quot;.id&quot;) fits %&gt;% select(.id, r.squared, adj.r.squared, df.residual) %&gt;% gather(stat, value, -.id) %&gt;% mutate(.id = as.integer(.id)) %&gt;% ggplot(aes(x = .id, y = value)) + geom_point() + geom_line() + facet_wrap(~ stat, ncol = 2, scales = &quot;free&quot;) The larger the polynomial, the more wiggly the line. library(&quot;modelr&quot;) invoke(gather_predictions, .x = c(list(data = bordeaux), mods_overfit)) %&gt;% ggplot(aes(x = vint)) + geom_point(aes(y = lprice)) + geom_line(aes(y = pred, group = model, colour = as.numeric(model))) Intuitively it seems that as we increase the flexibility of the model by increasing the number of variables the model is overfitting the data, but what does it actually mean to overfit? If we use \\(R^2\\) as the “measure of fit”, more variables always leads to better fit. Adjusted \\(R^2\\) does not increase, because the decrease in errors is offset by the increase in the degrees of freedom. However, there is little justification for the specific formula of \\(R^2\\). The problem with over-fitting is that the model starts to fit pecularities of the sample (errors) rather than the underlying model. We’ll never know the underlying model, but what we can see is if the model predicts new data. wine_mods_f &lt;- list( lprice ~ time_sv, lprice ~ poly(time_sv, 2), lprice ~ wrain, lprice ~ hrain, lprice ~ degrees, lprice ~ wrain + hrain + degrees, lprice ~ time_sv + wrain + hrain + degrees, lprice ~ time_sv + wrain * hrain * degrees, lprice ~ time_sv * (wrain + hrain + degrees), lprice ~ time_sv * wrain * hrain * degrees, lprice ~ time_sv * wrain * hrain * degrees + I(time_sv ^ 2) ) 11.6.1 Held-out data A common rule of thumb is to use 70% of the data for training, and 30% of the data for testing. In this case, let’s partition the data to use the first 70% of the observations as training data, and the remaining 30% of the data as testing. n_test &lt;- round(0.3 * nrow(bordeaux)) n_train &lt;- nrow(bordeaux) - n_test mod_train &lt;- lm(lprice ~ time_sv + wrain + hrain + degrees, data = head(bordeaux, n_train)) mod_train ## ## Call: ## lm(formula = lprice ~ time_sv + wrain + hrain + degrees, data = head(bordeaux, ## n_train)) ## ## Coefficients: ## (Intercept) time_sv wrain hrain degrees ## -1.080e+01 1.999e-02 9.712e-04 -4.461e-03 5.533e-01 # in-sample RMSE sqrt(mean(mod_train$residuals ^ 2)) ## [1] 0.2280059 The out-of-sample RMSE is higher than the in-sample RMSE. outsample &lt;- augment(mod_train, newdata = tail(bordeaux, n_test)) sqrt(mean((outsample$lprice - outsample$.fitted) ^ 2)) ## [1] 0.351573 This is common, but not necessarily the case. But note that this value is highly dependent on the subset of data used for testing. In some sense, we may choose as model that “overfits” the testing data. 11.6.2 Leave-One-Out Cross-Validation For each \\(i \\in 1, \\dots, n\\) Estimate the model using all observations but \\(i\\) Predict \\(\\hat{y}_i\\) using that model Calculate some measure(s) of model fit Let’s create a function to fit the model on a dataset dropping a single observation. i &lt;- 1 f &lt;- lprice ~ time_sv + wrain + hrain + degrees mod &lt;- lm(f, data = bordeaux) mod ## ## Call: ## lm(formula = f, data = bordeaux) ## ## Coefficients: ## (Intercept) time_sv wrain hrain degrees ## -12.145334 0.023847 0.001167 -0.003861 0.616392 mod_loo1 &lt;- lm(f, data = bordeaux[-i, ]) mod_loo1 ## ## Call: ## lm(formula = f, data = bordeaux[-i, ]) ## ## Coefficients: ## (Intercept) time_sv wrain hrain degrees ## -12.336504 0.025916 0.001188 -0.003832 0.625560 Unsurprisingly the fits of models fit with and without the first observation are similar, since they were fit using \\(n - 1\\) observations. Now use the model fit without the first observation to yhat_loo1 &lt;- predict(mod_loo1, newdata = bordeaux[1, ]) yhat_loo1 ## 1 ## -0.7262297 bordeaux$lprice[1] - yhat_loo1 ## 1 ## -0.2724503 Now we want to repeat this for all observations, fit_loo &lt;- function(i, formula, data) { # fit without i m &lt;- lm(formula, data = data[-i, ]) # predict i yhat &lt;- predict(m, newdata = data[i, ]) tibble(i = i, pred = yhat, resid = yhat - data[[&quot;lprice&quot;]][[i]]) } cv_loo &lt;- map_df(seq_len(nrow(bordeaux)), fit_loo, formula = f, data = bordeaux) sqrt(mean(cv_loo$resid ^ 2)) ## [1] 0.3230043 11.6.3 k-fold Cross-validation The most common approach is to to partition the data into k-folds, and use each fold once as the testing subset, where the model is fit on the other \\(k - 1\\) folds. [Cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics) is a non-parametric method that splits the data into training and test subsets. The data is fit on the training set and then used the predict the test set. The most common form of cross validation is 5- or 10-fold cross validation? Why this number of folds? See ISLR 5.1.4 “Bias-Variance Trade-Off for k-Fold Cross Validation” Larger number of folds requires more computation: a \\(k\\)-fold cross validation requires running the model \\(k\\) times A large number of folds has low bias because the result of \\((n - 1)\\) observations is approximately the same as the result of \\(n\\) observations But larger folds results in higher variance. LOOCV is averaging \\(n\\) models, but all those models will be similar, because they share almost all the same observations. But with fewer folds the models are fit with fewer overlapping observations and thus will have less correlated results. cv_fold5 &lt;- modelr::crossv_kfold(bordeaux, k = 5) glimpse(cv_fold5) ## Observations: 5 ## Variables: 3 ## $ train &lt;list&gt; [&lt;1952.00000, 1953.00000, 1955.00000, 1957.00000, 1958.... ## $ test &lt;list&gt; [&lt;1952.00000, 1953.00000, 1955.00000, 1957.00000, 1958.... ## $ .id &lt;chr&gt; &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot; cv_fold5$train[[1]] ## &lt;resample [21 x 7]&gt; 1, 2, 6, 7, 8, 9, 10, 11, 12, 13, ... cv_fold5$test[[1]] ## &lt;resample [6 x 7]&gt; 3, 4, 5, 15, 22, 26 as.data.frame(cv_fold5$train[[1]]) ## # A tibble: 21 × 7 ## vint price wrain degrees hrain time_sv lprice ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1952 36.83654 600 17.1167 160 31 -0.99868 ## 2 1953 63.48288 690 16.7333 80 30 -0.45440 ## 3 1959 65.83622 485 17.4833 187 24 -0.41800 ## 4 1960 13.87738 763 16.4167 290 23 -1.97491 ## 5 1961 100.00000 830 17.3333 38 22 0.00000 ## 6 1962 33.09725 697 16.3000 52 21 -1.10572 ## 7 1963 16.84730 608 15.7167 155 20 -1.78098 ## 8 1964 30.59450 402 17.2667 96 19 -1.18435 ## 9 1965 10.62522 602 15.3667 267 18 -2.24194 ## 10 1966 47.26359 819 16.5333 86 17 -0.74943 ## # ... with 11 more rows as.data.frame(cv_fold5$test[[1]]) ## # A tibble: 6 × 7 ## vint price wrain degrees hrain time_sv lprice ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1955 44.57665 502 17.1500 130 28 -0.80796 ## 2 1957 22.10735 420 16.1333 110 26 -1.50926 ## 3 1958 17.96850 582 16.4167 187 25 -1.71655 ## 4 1968 10.53803 610 16.2000 292 15 -2.25018 ## 5 1975 30.06886 572 16.9500 171 8 -1.20168 ## 6 1979 21.44669 717 16.1667 122 4 -1.53960 In k-fold cross-validation, each observation appears in the test-set in one fold, and is in the the training set in the remaining \\(k - 1\\) folds. The following plot shows this, cv_fold5_obs &lt;- cv_fold5 %&gt;% rowwise() %&gt;% do(tibble(vint = c(as.data.frame(.$train)$vint, as.data.frame(.$test)$vint), fold = .$.id, set = c(rep(&quot;train&quot;, dim(.$train)[1]), rep(&quot;test&quot;, dim(.$test)[1])))) ggplot(cv_fold5_obs, aes(y = factor(vint), x = fold, fill = set)) + geom_raster() + scale_fill_manual(values = c(&quot;train&quot; = &quot;black&quot;, &quot;test&quot; = &quot;gray&quot;)) + theme_minimal() + theme(axis.ticks = element_blank(), legend.position = &quot;bottom&quot;) + labs(x = &quot;vint&quot;, y = &quot;CV fold&quot;, fill = &quot;&quot;) Example with one fold fit_train &lt;- lm(f, data = as.data.frame(cv_fold5$train[[1]])) fit_train ## ## Call: ## lm(formula = f, data = as.data.frame(cv_fold5$train[[1]])) ## ## Coefficients: ## (Intercept) time_sv wrain hrain degrees ## -1.131e+01 3.114e-02 9.724e-04 -3.965e-03 5.662e-01 fit_test &lt;- augment(fit_train, newdata = as.data.frame(cv_fold5$test[[1]])) %&gt;% select(vint, lprice, .fitted) %&gt;% mutate(.resid = .fitted - lprice) fit_test ## vint lprice .fitted .resid ## 1 1955 -0.80796 -0.7542891 0.05367086 ## 2 1957 -1.50926 -1.3926673 0.11659270 ## 3 1958 -1.71655 -1.4111139 0.30543610 ## 4 1968 -2.25018 -2.2342730 0.01590699 ## 5 1975 -1.20168 -1.5847434 -0.38306339 ## 6 1979 -1.53960 -1.8175094 -0.27790943 # could also use modelr::rmse() mod_rmse &lt;- sqrt(mean(sum(fit_test$.resid ^ 2))) mod_rmse ## [1] 0.5779186 Let’s apply that to each row using map. That way we keep the results together in the same data frame. fit_train &lt;- lm(f, data = as.data.frame(cv_fold5$train[[1]])) fit_train ## ## Call: ## lm(formula = f, data = as.data.frame(cv_fold5$train[[1]])) ## ## Coefficients: ## (Intercept) time_sv wrain hrain degrees ## -1.131e+01 3.114e-02 9.724e-04 -3.965e-03 5.662e-01 fit_test &lt;- augment(fit_train, newdata = as.data.frame(cv_fold5$test[[1]])) %&gt;% select(vint, lprice, .fitted) %&gt;% mutate(.resid = .fitted - lprice) fit_test ## vint lprice .fitted .resid ## 1 1955 -0.80796 -0.7542891 0.05367086 ## 2 1957 -1.50926 -1.3926673 0.11659270 ## 3 1958 -1.71655 -1.4111139 0.30543610 ## 4 1968 -2.25018 -2.2342730 0.01590699 ## 5 1975 -1.20168 -1.5847434 -0.38306339 ## 6 1979 -1.53960 -1.8175094 -0.27790943 # could also use modelr::rmse() mod_rmse &lt;- sqrt(mean(sum(fit_test$.resid ^ 2))) mod_rmse ## [1] 0.5779186 fit_cv_fold5 &lt;- cv_fold5 %&gt;% mutate( # fit each row using train as the data fit_train = map(train, ~ lm(f, data = as.data.frame(.x))), # predicted values predict_test = map(train, ~ predict(fit_train, newdata = as.data.frame(.x))), # calculate out-of-sample RMSE rmse = map2_dbl(train, predict_test, ~ sqrt(mean((as.data.frame(.x)$lprice - .y) ^ 2)))) Let’s apply this to all the models. In the previous steps we kept a lot of extra information in order to understand how cross-validation worked. But really, all we care about is the average RMSE. mod_rmse_fold &lt;- function(f, train, test) { fit &lt;- lm(f, data = as.data.frame(train)) # sqrt(mean(residuals(fit, newdata = as.data.frame(test)) ^ 2)) test_data &lt;- as.data.frame(test) err &lt;- test_data$lprice - predict(fit, newdata = test_data) sqrt(mean(err ^ 2)) } mod_rmse_fold(f, cv_fold5$train[[1]], cv_fold5$test[[1]]) ## [1] 0.2359343 Now calculate this for a single model formula, averaging over all folds: mod_rmse &lt;- function(f, data) { map2_dbl(data$train, data$test, function(train, test) { mod_rmse_fold(f, train, test) }) %&gt;% mean() } Now we can apply this to all the models: mod_comparison &lt;- tibble(formula = wine_mods_f, .name = map_chr(formula, deparse), .id = seq_along(wine_mods_f), rmse = map_dbl(wine_mods_f, mod_rmse, data = cv_fold5)) mod_comparison %&gt;% select(.name, rmse) ## # A tibble: 11 × 2 ## .name rmse ## &lt;chr&gt; &lt;dbl&gt; ## 1 lprice ~ time_sv 0.6086064 ## 2 lprice ~ poly(time_sv, 2) 0.6100899 ## 3 lprice ~ wrain 0.6918896 ## 4 lprice ~ hrain 0.5925636 ## 5 lprice ~ degrees 0.5025091 ## 6 lprice ~ wrain + hrain + degrees 0.3733184 ## 7 lprice ~ time_sv + wrain + hrain + degrees 0.3250124 ## 8 lprice ~ time_sv + wrain * hrain * degrees 0.4522841 ## 9 lprice ~ time_sv * (wrain + hrain + degrees) 0.3926909 ## 10 lprice ~ time_sv * wrain * hrain * degrees 1.0599623 ## 11 lprice ~ time_sv * wrain * hrain * degrees + I(time_sv^2) 1.4167142 11.7 Analytic Covariance Methods For some models, notably linear regression, analytical approximations to the expected out of sample error can be made. Each of these approximations will make some slightly different assumptions to plug in some unknown values. Adjusted \\(R^2\\) is most often seen in statistical software and in papers (though often never interpreted). It intuitively penalizes a regression for a higher number of predictors; however apart from that intuitive appeal, and unlike the other measures presented here, there is no deeper justification for it (Fox, p. 609): \\[ \\mathrm{adj}\\,R^2 = 1 - \\frac{\\hat{\\sigma}^2}{\\Var{(Y)}} = 1 - \\frac{n - 1}{n - k - 1} \\times \\frac{\\sum (y_i - \\hat{y}_i^2)}{\\sum (y_i - \\bar{y})^2} \\] In linear regression, the LOO-CV MSE can be calculated analytically, and without simulation. It is (ISLR, p. 180): \\[ \\text{LOO-CV} = \\frac{1}{n} \\sum_{i = 1}^n {\\left(\\frac{y_i - \\hat{y}_i}{1 - h_i} \\right)}^2 = \\frac{1}{n} \\sum_{i = 1}^n {\\left(\\frac{\\hat{\\epsilon}_i}{1 - h_i} \\right)}^2 = \\frac{1}{n} \\times \\text{PRESS} \\] where PRESS is the predictive residual sum of squares, and \\(h_i\\) is the hat-value of observation \\(i\\) (Fox, p. 270, 289) \\[ h_i = \\mat{X}(\\mat{X}&#39; \\mat{X})^{-1} \\mat{X}&#39; \\] loocv &lt;- function(x) { mean((residuals(x) / (1 - hatvalues(x))) ^ 2) } An alternative approximation of the expected out-of-sample error is the generalized cross-validation criterion (GCV) is (Fox, 673) \\[ GCV = \\frac{n}{df_{res}^2} \\times RSS = \\frac{n}{(n - k - 1)^2} \\times \\sum \\hat{\\epsilon}^2_i \\] gcv &lt;- function(x) { err2 &lt;- residuals(x) ^ 2 n &lt;- length(err2) (n / x[[&quot;df.residual&quot;]] ^ 2) * sum(err2) } Since we generated the LOO data manually using a loop, create a LOO cross validation data frame using crossv_kfold: cv_loo &lt;- crossv_kfold(bordeaux, nrow(bordeaux)) mod_comparison &lt;- tibble(formula = wine_mods_f, .name = map_chr(formula, deparse), .id = seq_along(wine_mods_f), rmse_5fold = map_dbl(wine_mods_f, mod_rmse, data = cv_fold5), rmse_loo = map_dbl(wine_mods_f, mod_rmse, data = cv_loo), mod = map(formula, lm, data = bordeaux), gcv = sqrt(map_dbl(mod, gcv)), loocv = sqrt(map_dbl(mod, loocv)) ) mod_comparison %&gt;% select(.name, rmse_loo, rmse_5fold, gcv, loocv) ## # A tibble: 11 × 5 ## .name rmse_loo ## &lt;chr&gt; &lt;dbl&gt; ## 1 lprice ~ time_sv 0.5194518 ## 2 lprice ~ poly(time_sv, 2) 0.5312953 ## 3 lprice ~ wrain 0.5703790 ## 4 lprice ~ hrain 0.4635317 ## 5 lprice ~ degrees 0.4031840 ## 6 lprice ~ wrain + hrain + degrees 0.3092453 ## 7 lprice ~ time_sv + wrain + hrain + degrees 0.2767282 ## 8 lprice ~ time_sv + wrain * hrain * degrees 0.3199831 ## 9 lprice ~ time_sv * (wrain + hrain + degrees) 0.3128323 ## 10 lprice ~ time_sv * wrain * hrain * degrees 0.7493031 ## 11 lprice ~ time_sv * wrain * hrain * degrees + I(time_sv^2) 0.7287324 ## # ... with 3 more variables: rmse_5fold &lt;dbl&gt;, gcv &lt;dbl&gt;, loocv &lt;dbl&gt; Other measures that are also equivalent to some form of an estimate of the out-of-sample error are the AIC and BIC. 11.8 Further Resources Fox (2016) Chapter 22: “Model Selection, Averaging, and Validation”, p. 669. James et al. (2013), Ch. 5. “Resampling Methods” Hastie, Tibshirani, and Friedman (2009), Ch. 7. “Model Assessment and Selection” Rob Hyndman’s blog posts on cross validation, time series cross validation, and leave-one-out CV in linear models. 11.9 Measurement Error 11.9.1 What’s the problem? It biases coefficients. The way in which it biases coefficients depends on which variables have measurement error. Variable with measurement error: biases \\(\\beta\\) towards zero (attenuation bias) Other variables: Biases \\(\\beta\\) similarly to omitted variable bias. In other words, when a variable has measurement error it is an imperfect control. You can think of omitted variables as the limit of the effect of measurement error as it increases. 11.9.2 What to do about it? There’s no easy fix within the OLS framework. If the measurement error is in the variable of interest, then the variable will be biased towards zero, and your estimate is too large. Find better measures with lower measurement errors. If the variable is the variable of interest, then perhaps combine multiple variables into a single index. If the measurement error is in the control variables, then include several measures. That these measure correlate closely increases their standard errors, but the control variables are not the object of the inferential analysis. More complicated methods: errors in variable models, structural equation models, instrumental variable (IV) models, and Bayesian methods. References "],
["rs-forumula-syntax.html", "Chapter 12 R’s Forumula Syntax 12.1 Setup 12.2 Introduction to Formula Objects 12.3 Programming with Formulas", " Chapter 12 R’s Forumula Syntax These notes build off of the topics discussed in the chapter Many Models in R for Data Science. It uses the functionals (map() function) for iteration, string functions, and list columns in data frames. 12.1 Setup data(&quot;Duncan&quot;, package = &quot;car&quot;) library(&quot;tidyverse&quot;) library(&quot;stringr&quot;) library(&quot;broom&quot;) 12.2 Introduction to Formula Objects Many R functions, especially those for statistical models such as lm(), use a convenient syntax to compactly specify the outcome and predictor variables. This format is described in more detail in the stats. Formula objects in R are created with the tilde operator (~), and can be either one- or two-sided. prestige ~ type + income * education ~ prestige + type + income Formula are used in a variety of different contexts in R, e.g. ggplot2, but are commonly associated with statistical modeling functions as a compact way to specify the outcome and design matrix. lm(formula = prestige ~ type + income * education, data = Duncan) estimates this, \\[ \\mathtt{prestige} = \\beta_0 + \\beta_1 \\mathtt{income} + \\beta_2 \\mathtt{education} + \\beta_3 \\mathtt{income} \\times \\mathtt{education} . \\] Note that in a formula, the operators + and * do not refer to addition or multiplication. Instead adding + adds terms to the regression, and * creates interactions. Symbol Example Meaning + +x Include \\(x\\) - -x Exclude \\(x\\) : x : z Include \\(x z\\) as a predictor * x * z Include \\(x\\), \\(z\\), and their interaction \\(x z\\) as predictors ^ (x + w + z) ^ 3 Include variables and interactions up to three way: \\(x\\), \\(z\\), \\(w\\), \\(xz\\), \\(xw\\), \\(zw\\), \\(xzw\\). I I(x ^ 2) as is: include a new variable with \\(x ^ 2\\). 1 -1 Intercept; Use -1 to delete, and +1 to include Formula Equation y ~ x + y + z \\(y = \\beta_0 + beta_1 x + beta_2 y + beta_3 z\\) y ~ x + y - 1 \\(y = \\beta_1 x + \\beta_2 y\\) y ~ 1 \\(y = \\beta_0\\) y ~ x:z \\(y = \\beta_0 + \\beta_1 xz\\) y ~ x*z \\(y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 xz\\) y ~ x*z - x \\(y = \\beta_0 + \\beta_1 z + \\beta_2 xz\\) y ~ (x + z)^2 \\(y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3\\) y ~ (x + z + w)^2 \\(y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 w + \\beta_4 w + \\beta_5 xz + \\beta_6 xw + \\beta_7 zw\\) y ~ (x + z + w)^2 \\(y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 w + \\beta_4 w + \\beta_5 xz + \\beta_6 xw + \\beta_7 zw + \\beta_8 xzw\\) y ~ x + I(x ^ 2) \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\) y ~ poly(x, 2) \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\) y ~ I(x * z) \\(y = \\beta_0 + \\beta_1 xz\\) y ~ I(x + z) + I(x - z) \\(y = \\beta_0 + \\beta_1 (x + z) + \\beta_2 (x - z)\\) y ~ log(x) \\(y = \\beta_0 + \\beta_1 \\log(x)\\) y ~ x / z \\(y = \\beta_0 + \\beta_1 (x / z)\\) y ~ x + 0 \\(y = \\beta_1 x\\) Formula objects provide a convenient means of specifying the statistical model and also generating temporary variables that we only needed in the model. For example, if we want to include \\(\\log(x)\\), \\(z\\), their interaction, and the square of \\(z\\), we could write y ~ log(x) * y + I(y ^ 2) instead of of having to create new columns with the log, square, and interaction variables before running the regression. This becomes especially useful if you need to include large polynomials, splines, interaction, or similarly complicated functional forms. Warning: Often you will want to include polynomials in a regression. However, ^ already has a special meaning in R’s syntax, so you cannot use x ^ 2. There are two ways to specify polynomials. - Use the as is operator, I(). For example, I(x ^ 2) includes \\(x^2\\) in the regression. - Use the poly function to generate polynomials. For example, I(x ^ 2) will include \\(x\\) and \\(x^2\\) in the regression. This can be more convenient and clear than writing x + I(x ^ 2)$. Warning: R’s formula syntax is flexible and includes more features than what was covered in this section. What was described in this section were features that lm() uses. Other functions may have more features or the parts will have different meanings. However, they will be mostly be similar to what was described here. An example of a function that has slightly different syntax for its formula is lme4 which adds notation for random and fixed effects. Some functions like stats use a formula syntax even though they aren’t statistical modeling functions. The package Formula provides an even more powerful Formula syntax than that included with base R. If you have to write a function or package that uses a formula, use that package. Inspired by this handout from Richard Hahn and the handout form the NPS. 12.3 Programming with Formulas In these examples, we’ll use the car dataset in the car package. Prestige &lt;- car::Prestige Each observation is an occupation, and contains the prestige score of the occupation from a survey, and the average education, income, percentage of women, and type of occupation. glimpse(Prestige) ## Observations: 102 ## Variables: 6 ## $ education &lt;dbl&gt; 13.11, 12.26, 12.77, 11.42, 14.62, 15.64, 15.09, 15.... ## $ income &lt;int&gt; 12351, 25879, 9271, 8865, 8403, 11030, 8258, 14163, ... ## $ women &lt;dbl&gt; 11.16, 4.02, 15.70, 9.11, 11.68, 5.13, 25.65, 2.69, ... ## $ prestige &lt;dbl&gt; 68.8, 69.1, 63.4, 56.8, 73.5, 77.6, 72.6, 78.1, 73.1... ## $ census &lt;int&gt; 1113, 1130, 1171, 1175, 2111, 2113, 2133, 2141, 2143... ## $ type &lt;fctr&gt; prof, prof, prof, prof, prof, prof, prof, prof, pro... We will run several regressions with prestige as the outcome variable, and the over variables are explanatory variables. In R, the formulas are objects (of class &quot;formula&quot;). That means we can program on them, and importantly, perhaps avoid excessive copying and pasting if we run multiple models. A formula object is created with the ~ operator: f &lt;- prestige ~ type + education class(f) ## [1] &quot;formula&quot; f ## prestige ~ type + education A useful function for working with formulas is update. The update function allows you to easily update a formula object # the . is replaced by the original formula values update(f, . ~ income) ## prestige ~ income update(f, income ~ .) ## income ~ type + education update(f, . ~ . + type + women) ## prestige ~ type + education + women Also note that many types of models have update method which will rerun the model with a new formula. Sometimes this can help computational time if the model is able to reuse some previous results or data. You can also create formula objects from a character vector as.formula(&quot;prestige ~ income + education&quot;) ## prestige ~ income + education This means that you can create model formula objects programmatically which is useful if you are running many models, or simply to keep the logic of your code clear. xvars &lt;- c(&quot;type&quot;, &quot;income&quot;, &quot;education&quot;) as.formula(str_c(&quot;prestige&quot;, &quot;~&quot;, str_c(xvars, collapse = &quot; + &quot;))) ## prestige ~ type + income + education Often you will need to run multiple models. Since most often the only thing that changes between models is the formula (the outcome or response variables), storing the formula in a list, and then running the models by iterating through the list is a clean strategy for estimating your models. xvar_list &lt;- list(c(&quot;type&quot;), c(&quot;income&quot;), c(&quot;education&quot;), c(&quot;type&quot;, &quot;income&quot;), c(&quot;type&quot;, &quot;income&quot;, &quot;education&quot;)) formulae &lt;- vector(&quot;list&quot;, length(xvar_list)) for (i in seq_along(xvar_list)) { formulae[[i]] &lt;- str_c(&quot;prestige ~ &quot;, str_c(xvar_list[[i]], collapse = &quot; + &quot;)) } formulae ## [[1]] ## [1] &quot;prestige ~ type&quot; ## ## [[2]] ## [1] &quot;prestige ~ income&quot; ## ## [[3]] ## [1] &quot;prestige ~ education&quot; ## ## [[4]] ## [1] &quot;prestige ~ type + income&quot; ## ## [[5]] ## [1] &quot;prestige ~ type + income + education&quot; Alternatively, create this list of formula objects with a functional, make_mod_f &lt;- function(x) { str_c(&quot;prestige ~ &quot;, str_c(x, collapse = &quot; + &quot;)) } formulae &lt;- map(xvar_list, make_mod_f) Now that we have a list with formula objects for each model that we want to run, we can loop over the list and run each model. But first, we need to create a function that runs a single model that returns a data frame with a single row and a column named mod, which is a list column with an lm object containing the fitted model. In this function, I set model = FALSE because by default an lm model stores the data used to estimate. This is convenient, but if you are estimating many models, this can consume much memory. run_reg &lt;- function(f) { mod &lt;- lm(f, data = Prestige, model = FALSE) data_frame(mod = list(mod)) } ret &lt;- run_reg(formulae[[1]]) ret[[&quot;mod&quot;]][[1]] ## ## Call: ## lm(formula = f, data = Prestige, model = FALSE) ## ## Coefficients: ## (Intercept) typeprof typewc ## 35.527 32.321 6.716 Since each data frame has only one row, it is not particularly useful on its own, but it will be convenient to keep all the models in a data frame. Now, run run_reg for each formula in formulae using map_df to return the results as a data frame with a list column, mod, containing the lm objects. prestige_fits &lt;- map_df(formulae, run_reg, .id = &quot;.id&quot;) prestige_fits ## # A tibble: 5 × 2 ## .id mod ## &lt;chr&gt; &lt;list&gt; ## 1 1 &lt;S3: lm&gt; ## 2 2 &lt;S3: lm&gt; ## 3 3 &lt;S3: lm&gt; ## 4 4 &lt;S3: lm&gt; ## 5 5 &lt;S3: lm&gt; To extract the original formulas and add them to the data set, run formula() on each lm object using map, and then convert it to a character string using deparse: prestige_fits &lt;- prestige_fits %&gt;% mutate(formula = map_chr(mod, ~ deparse(formula(.x)))) prestige_fits$formula ## [1] &quot;prestige ~ type&quot; ## [2] &quot;prestige ~ income&quot; ## [3] &quot;prestige ~ education&quot; ## [4] &quot;prestige ~ type + income&quot; ## [5] &quot;prestige ~ type + income + education&quot; Get a data frame of the coefficients for all models using tidy and tidyr: mutate(prestige_fits, x = map(mod, tidy)) %&gt;% unnest(x) ## # A tibble: 16 × 7 ## .id formula term estimate ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 prestige ~ type (Intercept) 35.527272727 ## 2 1 prestige ~ type typeprof 32.321114370 ## 3 1 prestige ~ type typewc 6.716205534 ## 4 2 prestige ~ income (Intercept) 27.141176368 ## 5 2 prestige ~ income income 0.002896799 ## 6 3 prestige ~ education (Intercept) -10.731981968 ## 7 3 prestige ~ education education 5.360877731 ## 8 4 prestige ~ type + income (Intercept) 27.997056941 ## 9 4 prestige ~ type + income typeprof 25.055473883 ## 10 4 prestige ~ type + income typewc 7.167155112 ## 11 4 prestige ~ type + income income 0.001401196 ## 12 5 prestige ~ type + income + education (Intercept) -0.622929165 ## 13 5 prestige ~ type + income + education typeprof 6.038970651 ## 14 5 prestige ~ type + income + education typewc -2.737230718 ## 15 5 prestige ~ type + income + education income 0.001013193 ## 16 5 prestige ~ type + income + education education 3.673166052 ## # ... with 3 more variables: std.error &lt;dbl&gt;, statistic &lt;dbl&gt;, ## # p.value &lt;dbl&gt; Get a data frame of model summary statistics for all models using glance, mutate(prestige_fits, x = map(mod, glance)) %&gt;% unnest(x) ## # A tibble: 5 × 14 ## .id mod formula r.squared ## &lt;chr&gt; &lt;list&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 &lt;S3: lm&gt; prestige ~ type 0.6976287 ## 2 2 &lt;S3: lm&gt; prestige ~ income 0.5110901 ## 3 3 &lt;S3: lm&gt; prestige ~ education 0.7228007 ## 4 4 &lt;S3: lm&gt; prestige ~ type + income 0.7764569 ## 5 5 &lt;S3: lm&gt; prestige ~ type + income + education 0.8348574 ## # ... with 10 more variables: adj.r.squared &lt;dbl&gt;, sigma &lt;dbl&gt;, ## # statistic &lt;dbl&gt;, p.value &lt;dbl&gt;, df &lt;int&gt;, logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, ## # BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt; "],
["duncan-occupational-prestige.html", "Chapter 13 Duncan Occupational Prestige 13.1 Setup 13.2 Coefficients, Standard errors 13.3 Residuals, Fitted Values, 13.4 Broom 13.5 Plotting Fitted Regression Results", " Chapter 13 Duncan Occupational Prestige 13.1 Setup library(&quot;tidyverse&quot;) library(&quot;broom&quot;) This example makes use of the Duncan Occpuational prestige included in the car package. This is data from a classic sociology paper and contains data on the prestige and other characteristics of 45 U.S. occupations in 1950. data(&quot;Duncan&quot;, package = &quot;car&quot;) The dataset Duncan contains four variables: type, income, education, and prestige, glimpse(Duncan) ## Observations: 45 ## Variables: 4 ## $ type &lt;fctr&gt; prof, prof, prof, prof, prof, prof, prof, prof, wc,... ## $ income &lt;int&gt; 62, 72, 75, 55, 64, 21, 64, 80, 67, 72, 42, 76, 76, ... ## $ education &lt;int&gt; 86, 76, 92, 90, 86, 84, 93, 100, 87, 86, 74, 98, 97,... ## $ prestige &lt;int&gt; 82, 83, 90, 76, 90, 87, 93, 90, 52, 88, 57, 89, 97, ... You run a regression in R using the function lm. This runs a linear regression of occupational prestige on income, lm(prestige ~ income, data = Duncan) ## ## Call: ## lm(formula = prestige ~ income, data = Duncan) ## ## Coefficients: ## (Intercept) income ## 2.457 1.080 This estimates the linear regression \\[ \\mathtt{prestige} = \\beta_0 + \\beta_1 \\mathtt{income} \\] In R, \\(\\beta_0\\) is named (Intercept), and the other coefficients are named after the associated predictor. The function lm returns an lm object that can be used in future computations. Instead of printing the regression result to the screen, save it to the variable mod1, mod1 &lt;- lm(prestige ~ income, data = Duncan) We can print this object print(mod1) ## ## Call: ## lm(formula = prestige ~ income, data = Duncan) ## ## Coefficients: ## (Intercept) income ## 2.457 1.080 Somewhat counterintuitively, the summary function returns more information about a regression, summary(mod1) ## ## Call: ## lm(formula = prestige ~ income, data = Duncan) ## ## Residuals: ## Min 1Q Median 3Q Max ## -46.566 -9.421 0.257 9.167 61.855 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.4566 5.1901 0.473 0.638 ## income 1.0804 0.1074 10.062 7.14e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.4 on 43 degrees of freedom ## Multiple R-squared: 0.7019, Adjusted R-squared: 0.695 ## F-statistic: 101.3 on 1 and 43 DF, p-value: 7.144e-13 The summary function also returns an object that we can use later, summary_mod1 &lt;- summary(mod1) summary_mod1 ## ## Call: ## lm(formula = prestige ~ income, data = Duncan) ## ## Residuals: ## Min 1Q Median 3Q Max ## -46.566 -9.421 0.257 9.167 61.855 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.4566 5.1901 0.473 0.638 ## income 1.0804 0.1074 10.062 7.14e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.4 on 43 degrees of freedom ## Multiple R-squared: 0.7019, Adjusted R-squared: 0.695 ## F-statistic: 101.3 on 1 and 43 DF, p-value: 7.144e-13 Now lets estimate a multiple linear regression, mod2 &lt;- lm(prestige ~ income + education + type, data = Duncan) mod2 ## ## Call: ## lm(formula = prestige ~ income + education + type, data = Duncan) ## ## Coefficients: ## (Intercept) income education typeprof typewc ## -0.1850 0.5975 0.3453 16.6575 -14.6611 13.2 Coefficients, Standard errors Coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\): coef(mod2) ## (Intercept) income education typeprof typewc ## -0.1850278 0.5975465 0.3453193 16.6575134 -14.6611334 Variance-covariance matrix of the coefficients, \\(\\Var{\\hat{\\boldsymbol{\\beta}}}\\): vcov(mod2) ## (Intercept) income education typeprof typewc ## (Intercept) 13.7920916 -0.115636760 -0.257485549 14.0946963 7.9021988 ## income -0.1156368 0.007984369 -0.002924489 -0.1260105 -0.1090485 ## education -0.2574855 -0.002924489 0.012906986 -0.6166508 -0.3881200 ## typeprof 14.0946963 -0.126010517 -0.616650831 48.9021401 30.2138627 ## typewc 7.9021988 -0.109048528 -0.388119979 30.2138627 37.3171167 The standard errors of the coefficients, \\(\\se{\\hat{\\boldsymbol{\\beta}}}\\), are the square root diagonal of the vcov matrix, sqrt(diag(vcov(mod2))) ## (Intercept) income education typeprof typewc ## 3.7137705 0.0893553 0.1136089 6.9930065 6.1087737 This can be confirmed by comparing their values to those in the summary table, summary(mod2) ## ## Call: ## lm(formula = prestige ~ income + education + type, data = Duncan) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.890 -5.740 -1.754 5.442 28.972 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.18503 3.71377 -0.050 0.96051 ## income 0.59755 0.08936 6.687 5.12e-08 *** ## education 0.34532 0.11361 3.040 0.00416 ** ## typeprof 16.65751 6.99301 2.382 0.02206 * ## typewc -14.66113 6.10877 -2.400 0.02114 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.744 on 40 degrees of freedom ## Multiple R-squared: 0.9131, Adjusted R-squared: 0.9044 ## F-statistic: 105 on 4 and 40 DF, p-value: &lt; 2.2e-16 13.3 Residuals, Fitted Values, To get the fitted or predicted values (\\(\\hat{\\mathbf{y}} = \\mathbf{X} \\hat{\\boldsymbol\\beta}\\)) from a regression, mod1_fitted &lt;- fitted(mod1) head(mod1_fitted) ## accountant pilot architect author chemist minister ## 69.44073 80.24463 83.48580 61.87801 71.60151 25.14476 or mod1_predict &lt;- predict(mod1) head(mod1_predict) ## accountant pilot architect author chemist minister ## 69.44073 80.24463 83.48580 61.87801 71.60151 25.14476 The difference between predict and fitted is how they handle missing values in the data. Fitted values will not include predictions for missing values in the data, while predict will include values for Using predict, we can also predict values for new data. For example, create a data frame with each category of type, and in which income and education are set to their mean values. Duncan_at_means &lt;- data.frame(type = unique(Duncan$type), income = mean(Duncan$income), education = mean(Duncan$education)) Duncan_at_means ## type income education ## 1 prof 41.86667 52.55556 ## 2 wc 41.86667 52.55556 ## 3 bc 41.86667 52.55556 Now use this with the newdata argument, predict(mod2, newdata = Duncan_at_means) ## 1 2 3 ## 59.63821 28.31957 42.98070 To get the residuals (\\(\\hat{\\boldsymbol{\\epsilon}} = \\mathbf{y} - \\hat{\\mathbf{y}}\\)). mod1_resid &lt;- residuals(mod1) head(mod1_resid) ## accountant pilot architect author chemist minister ## 12.559266 2.755369 6.514200 14.121993 18.398486 61.855242 13.4 Broom The package broom has some functions that reformat the results of statistical modeling functions (t.test, lm, etc.) to data frames that work nicer with ggplot2, dplyr, and friends. The broom package has three main functions: glance: Information about the model. tidy: Information about the estimated parameters augment: The original data with estimates of the model. glance: Always return a one-row data.frame that is a summary of the model: e.g. R2, adjusted R2, etc. glance(mod2) ## r.squared adj.r.squared sigma statistic p.value df logLik ## 1 0.9130657 0.9043723 9.744171 105.0294 1.170871e-20 5 -163.6522 ## AIC BIC deviance df.residual ## 1 339.3045 350.1444 3797.955 40 tidy: Transforms into a ready-to-go data.frame the coefficients, SEs (and CIs if given), critical values, and p-values in statistical tests’ outputs tidy(mod2) ## term estimate std.error statistic p.value ## 1 (Intercept) -0.1850278 3.7137705 -0.0498221 9.605121e-01 ## 2 income 0.5975465 0.0893553 6.6873093 5.123720e-08 ## 3 education 0.3453193 0.1136089 3.0395443 4.164463e-03 ## 4 typeprof 16.6575134 6.9930065 2.3820246 2.206245e-02 ## 5 typewc -14.6611334 6.1087737 -2.4000125 2.114015e-02 augment: Add columns to the original data that was modeled. This includes predictions, estandard error of the predictions, residuals, and others. augment(mod2) %&gt;% head() ## .rownames prestige income education type .fitted .se.fit .resid ## 1 accountant 82 62 86 prof 83.21783 2.352262 -1.217831 ## 2 pilot 83 72 76 prof 85.74010 2.674659 -2.740102 ## 3 architect 90 75 92 prof 93.05785 2.755775 -3.057851 ## 4 author 76 55 90 prof 80.41628 2.589351 -4.416282 ## 5 chemist 90 64 86 prof 84.41292 2.360632 5.587076 ## 6 minister 87 21 84 prof 58.02779 4.260837 28.972214 ## .hat .sigma .cooksd .std.resid ## 1 0.05827491 9.866259 0.0002052803 -0.1287893 ## 2 0.07534370 9.857751 0.0013936701 -0.2924366 ## 3 0.07998300 9.855093 0.0018611391 -0.3271700 ## 4 0.07061418 9.841004 0.0033585648 -0.4701256 ## 5 0.05869037 9.825129 0.0043552315 0.5909809 ## 6 0.19120532 8.412639 0.5168053288 3.3061127 .fitted: the model predictions for all observations .se.fit: the estandard error of the predictions .resid: the residuals of the predictions (acual - predicted values) .sigma: is the standard error of the prediction. The other columns—.hat, .cooksd, and .std.resid are used in regression diagnostics. 13.5 Plotting Fitted Regression Results Consider the regression of prestige on income, mod3 &lt;- lm(prestige ~ income, data = Duncan) This creates a new dataset with the column income and 100 observations between the min and maximum observed incomes in the Duncan dataset. mod3_newdata &lt;- data_frame(income = seq(min(Duncan$income), max(Duncan$income), length.out = 100)) We will calculate fitted values for all these values of income. ggplot() + geom_point(data = Duncan, mapping = aes(x = income, y = prestige), colour = &quot;gray75&quot;) + geom_line(data = augment(mod3, newdata = mod3_newdata), mapping = aes(x = income, y = .fitted)) + ylab(&quot;Prestige&quot;) + xlab(&quot;Income&quot;) + theme_minimal() Now plot something similar, but for a regression with income interacted with type, mod4 &lt;- lm(prestige ~ income * type, data = Duncan) We want to create a dataset which has, (1) each value of type in the Duncan data, and (2) values spanning the range of income in the Duncan data. The function expand.grid creates a data frame with all combinations of vectors given to it (Cartesian product). mod4_newdata &lt;- expand.grid(income = seq(min(Duncan$income), max(Duncan$income), length.out = 100), type = unique(Duncan$type)) Now plot the fitted values evaluated at each of these values along wite original values in the data, ggplot() + geom_point(data = Duncan, mapping = aes(x = income, y = prestige, color = type)) + geom_line(data = augment(mod4, newdata = mod4_newdata), mapping = aes(x = income, y = .fitted, color = type)) + ylab(&quot;Prestige&quot;) + xlab(&quot;Income&quot;) + theme_minimal() Running geom_smooth with method = &quot;lm&quot; gives similar results. However, note that geom_smooth with run a separate regression for each group. ggplot(data = Duncan, aes(x = income, y = prestige, color = type)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + ylab(&quot;Prestige&quot;) + xlab(&quot;Income&quot;) + theme_minimal() "],
["formatting-tables.html", "Chapter 14 Formatting Tables 14.1 Overview of Packages 14.2 Summary Statistic Table Example 14.3 Regression Table Example", " Chapter 14 Formatting Tables 14.1 Overview of Packages R has multiple packages and functions for directly producing formatted tables for LaTeX, HTML, and other output formats. Given the See the Reproducible Research Task View for an overview of various options. xtable is a general purpose package for creating LaTeX, HTML, or plain text tables in R. texreg is more specifically geared to regression tables. It also outputs results in LaTeX (texreg), HTML (texreg), and plain text. The packages stargazer and apsrtable are other popular packages for formatting regression output. However, they are less-well maintained and have less functionality than texreg. For example, apsrtable hasn’t been updated since 2012, stargazer since 2015. The texreg vignette is a good introduction to texreg, and also discusses the These blog posts by Will Lowe cover many of the options. Additionally, for simple tables, knitr, the package which provides the heavy lifting for R markdown, has a function knitr. knitr also has the ability to customize how R objects are printed with the knit_print function. Other notable packages are: pander creates output in markdown for export to other formats. tables uses a formula syntax to define tables ReportR has the most complete support for creating Word documents, but is likely too much. For a political science perspective on why automating the research process is important see: Nicholas Eubank Embrace Your Fallibility: Thoughts on Code Integrity, based on this article Matthew Gentzkow Jesse M. Shapiro.Code and Data for the Social Sciences: A Practitioner’s Guide. March 10, 2014. Political Methodologist issue on Workflow Management 14.2 Summary Statistic Table Example The xtable package has methods to convert many types of R objects to tables. library(&quot;gapminder&quot;) gapminder_summary &lt;- gapminder %&gt;% # Keep numeric variables select_if(is.numeric) %&gt;% # gather variables gather(variable, value) %&gt;% # Summarize by variable group_by(variable) %&gt;% # summarise all columns summarise(n = sum(!is.na(value)), `Mean` = mean(value), `Std. Dev.` = sd(value), `Median` = median(value), `Min.` = min(value), `Max.` = max(value)) gapminder_summary ## # A tibble: 4 × 7 ## variable n Mean `Std. Dev.` Median Min. ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 gdpPercap 1704 7.215327e+03 9.857455e+03 3531.8470 241.1659 ## 2 lifeExp 1704 5.947444e+01 1.291711e+01 60.7125 23.5990 ## 3 pop 1704 2.960121e+07 1.061579e+08 7023595.5000 60011.0000 ## 4 year 1704 1.979500e+03 1.726533e+01 1979.5000 1952.0000 ## # ... with 1 more variables: Max. &lt;dbl&gt; Now that we have a data frame with the table we want, use xtable to create it: library(&quot;xtable&quot;) foo &lt;- xtable(gapminder_summary, digits = 0) %&gt;% print(type = &quot;html&quot;, html.table.attributes = &quot;&quot;, include.rownames = FALSE, format.args = list(big.mark = &quot;,&quot;)) variable n Mean Std. Dev. Median Min. Max. gdpPercap 1,704 7,215 9,857 3,532 241 113,523 lifeExp 1,704 59 13 61 24 83 pop 1,704 29,601,212 106,157,897 7,023,596 60,011 1,318,683,096 year 1,704 1,980 17 1,980 1,952 2,007 Note that there we two functions to get HTML. The function xtable creates an xtable R object, and the function xtable (called as print()), which prints the xtable object as HTML (or LaTeX). The default HTML does not look nice, and would need to be formatted with CSS. If you are copy and pasting it into Word, you would do some post-processing cleanup anyways. Another alternative is the knitr function in the knitr package, which outputs R markdown tables. knitr::kable(gapminder_summary) variable n Mean Std. Dev. Median Min. Max. gdpPercap 1704 7.215327e+03 9.857455e+03 3531.8470 241.1659 1.135231e+05 lifeExp 1704 5.947444e+01 1.291711e+01 60.7125 23.5990 8.260300e+01 pop 1704 2.960121e+07 1.061579e+08 7023595.5000 60011.0000 1.318683e+09 year 1704 1.979500e+03 1.726533e+01 1979.5000 1952.0000 2.007000e+03 This is useful for producing quick tables. Finally, htmlTables package unsurprisingly produces HTML tables. library(&quot;htmlTable&quot;) htmlTable(txtRound(gapminder_summary, 0), align = &quot;lrrrr&quot;) variable n Mean Std. Dev. Median Min. Max. 1 gdpPercap 1704 7 10 3532 241 1 2 lifeExp 1704 6 1 61 24 8 3 pop 1704 3 1 7023596 60011 1 4 year 1704 2 2 1980 1952 2 It has more features for producing HTML tables than xtable, but does not output LaTeX. 14.3 Regression Table Example library(&quot;tidyverse&quot;) library(&quot;texreg&quot;) We will run several regression models with the Duncan data Prestige &lt;- car::Prestige Since I’m running several regressions, I will save them to a list. If you know that you will be creating multiple objects, and programming with them, always put them in a list. First, create a list of the regression formulas, formulae &lt;- list( prestige ~ type, prestige ~ income, prestige ~ education, prestige ~ type + education + income ) Write a function to run a single model, Now use map to run a regression with each of these formulae, and save them to a list, prestige_mods &lt;- map(formulae, ~ lm(.x, data = Prestige, model = FALSE)) This is a list of lm objects, map(prestige_mods, class) ## [[1]] ## [1] &quot;lm&quot; ## ## [[2]] ## [1] &quot;lm&quot; ## ## [[3]] ## [1] &quot;lm&quot; ## ## [[4]] ## [1] &quot;lm&quot; We can look at the first model, prestige_mods[[1]] ## ## Call: ## lm(formula = .x, data = Prestige, model = FALSE) ## ## Coefficients: ## (Intercept) typeprof typewc ## 35.527 32.321 6.716 Now we can format the regression table in HTML using htmlreg. The first argument of htmlreg is a list of models: htmlreg(prestige_mods) Statistical models Model 1 Model 2 Model 3 Model 4 (Intercept) 35.53*** 27.14*** -10.73** -0.62 (1.43) (2.27) (3.68) (5.23) typeprof 32.32*** 6.04 (2.23) (3.87) typewc 6.72** -2.74 (2.44) (2.51) income 0.00*** 0.00*** (0.00) (0.00) education 5.36*** 3.67*** (0.33) (0.64) R2 0.70 0.51 0.72 0.83 Adj. R2 0.69 0.51 0.72 0.83 Num. obs. 98 102 102 98 RMSE 9.50 12.09 9.10 7.09 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 By default, htmlreg() prints out HTML, which is exactly what I want in an R markdown document. To save the output to a file, specify a non-null file argument. For example, to save the table to the file prestige.html, htmlreg(prestige_mods, file = &quot;prestige.html&quot;) Since this function outputs HTML directly to the console, it can be hard to tell what’s going on. If you want to preview the table in RStudio while working on it, this snippet of code uses htmltools package to do so: library(&quot;htmltools&quot;) htmlreg(prestige_mods) %&gt;% HTML() %&gt;% browsable() The htmlreg function has many options to adjust the table formatting. Below, I clean up the table. I remove stars using stars = NULL. It is a growing convention to avoid the use of stars indicating significance in regression tables (see AJPS and Political Analysis guidelines). The arguments doctype, html.tag, head.tag, body.tag control what sort of HTML is created. Generally all these functions (whether LaTeX or HTML output) have some arguments that determine whether it is creating a standalone, complete document, or a fragment that will be copied into another dcoument. The arguments include.rsquared, include.adjrs, and include.nobs are passed to the function extract() which determines what information the texreg package extracts from a model to put into the table. I get rid of \\(R^2\\), but keep adjusted \\(R^2\\), and the number of observations. library(&quot;stringr&quot;) coefnames &lt;- c(&quot;Professional&quot;, &quot;Working Class&quot;, &quot;Income&quot;, &quot;Education&quot;) note &lt;- &quot;OLS regressions with prestige as the response variable.&quot; htmlreg(prestige_mods, stars = NULL, custom.model.names = str_c(&quot;(&quot;, seq_along(prestige_mods), &quot;)&quot;), custom.coef.names = coefnames, custom.note = str_c(&quot;Note: &quot;, note), omit.coef = &quot;(Intercept)&quot;, caption.above = TRUE, caption = &quot;Regressions of Occupational Prestige&quot;, # better for markdown doctype = FALSE, html.tag = FALSE, head.tag = FALSE, body.tag = FALSE, # passed to extract() method for &quot;lm&quot; include.adjr = TRUE, include.rsquared = FALSE, include.rmse = FALSE, include.nobs = TRUE) Regressions of Occupational Prestige (1) (2) (3) (4) Professional 32.32 6.04 (2.23) (3.87) Working Class 6.72 -2.74 (2.44) (2.51) Income 0.00 0.00 (0.00) (0.00) Education 5.36 3.67 (0.33) (0.64) Adj. R2 0.69 0.51 0.72 0.83 Num. obs. 98 102 102 98 Note: OLS regressions with prestige as the response variable. Once you find a set of options that are common across your tables, make a function so you con’t need to retype them. my_reg_table &lt;- function(mods, ..., note = NULL) { htmlreg(mods, stars = NULL, custom.note = if (!is.null(note)) str_c(&quot;Note: &quot;, note) else NULL, caption.above = TRUE, # better for markdown doctype = FALSE, html.tag = FALSE, head.tag = FALSE) } my_reg_table(prestige_mods, custom.model.names = str_c(&quot;(&quot;, seq_along(prestige_mods), &quot;)&quot;), custom.coef.names = coefnames, note = note, # put intercept at the bottom reorder.coef = c(2, 3, 4, 5, 1), caption = &quot;Regressions of Occupational Prestige&quot;) Statistical models Model 1 Model 2 Model 3 Model 4 (Intercept) 35.53 27.14 -10.73 -0.62 (1.43) (2.27) (3.68) (5.23) typeprof 32.32 6.04 (2.23) (3.87) typewc 6.72 -2.74 (2.44) (2.51) income 0.00 0.00 (0.00) (0.00) education 5.36 3.67 (0.33) (0.64) R2 0.70 0.51 0.72 0.83 Adj. R2 0.69 0.51 0.72 0.83 Num. obs. 98 102 102 98 RMSE 9.50 12.09 9.10 7.09 Note: OLS regressions with prestige as the response variable. Note that I didn’t include every option in my_reg_table, only those arguments that will be common across tables. I use ... to pass arguments to htmlreg. Then when I call my_reg_table the only arguments are those specific to the content of the table, not the formatting, making it easier to understand what each table is saying. Of course, texreg also produces LaTeX output, with the function texreg. Almost all the options are the same as htmlreg. "],
["reproducible-research.html", "Chapter 15 Reproducible Research", " Chapter 15 Reproducible Research "],
["writing-resources.html", "Chapter 16 Writing Resources 16.1 Writing and Organizing Papers 16.2 Finding Research Ideas 16.3 Replications", " Chapter 16 Writing Resources 16.1 Writing and Organizing Papers Here are a few useful resources for writing papers: Chris Adolph. Writing Empirical Papers: 6 Rules &amp; 12 Recommendations Barry R. Weingast. 2015. Caltech Rules for Writing Papers: How to Structure Your Paper and Write an Introduction The Science of Scientific Writing American Scientist Deidre McCloskey. Economical Writing William Thompson. A Guide for the Young Economist. “Chapter 2: Writing Papers.” Stephen Van Evera. Guide to Methods for Students of Political Science. Appendix. Joseph M. Williams and Joseph Bizup. Style: Lessons in Clarity and Grace Strunk and White. The Elements of Style Chicago Manual of Style and APSA Style Manual for Political Science for editorial and style issues. How to construct a Nature summary paragraph. Though specifi to Nature is good advice for structuring abstracts or introductions. Ezra Klein. How researchers are terrible communications, and how they can do better. The advice in the AJPS Instructions for Submitting Authors is a concise description of how to write an abstract: The abstract should provide a very concise descriptive summary of the research stream to which the manuscript contributes, the specific research topic it addresses, the research strategy employed for the analysis, the results obtained from the analysis, and the implications of the findings. Concrete Advice for Writing Informative Abstracts and pHow to Carefully Choose Useless Titles for Academic Writing](http://www.socialsciencespace.com/2014/03/how-to-carefully-choose-useless-titles-for-academic-writing/) 16.2 Finding Research Ideas Paul Krugman How I Work Hal Varian. How to build an Economic Model in your spare time Greg Mankiw, My Rules of Thumb: links in Advice for Grad Students 16.3 Replications Gary King has advice on how to turn a replication into a publishable paper: Gary King How to Write a Publishable Paper as a Class Project Gary King. 2006. “Publication, Publication.” PS: Political Science and Politics. Political Science Should Not Stop Young Researchers from Replicating from the Political Science Replication blog. And see the examples of students replications from his Harvard course at https://politicalsciencereplication.wordpress.com/ Famous replications. David Broockman, Joahua Kalla, and Peter Aronow. 2015. Irregularities in LaCour (2014). Homas Herndon, Michael Ash &amp; Robert Pollin (2013). Does High Public Debt Consistently Stifle Economic Growth? A Critique of Reinhart and Rogoff. Working Paper Series 322. Political Economy Research Institute. [URL] However, although those replications are famous for finding fraud or obvious errors in the analysis, replications can lead to extensions and generate new ideas. This was the intent of Brookman, Kalla, and Aronow when starting the replication. "],
["multivariate-normal-distribution.html", "A Multivariate Normal Distribution", " A Multivariate Normal Distribution library(&quot;tidyverse&quot;) library(&quot;viridis&quot;) library(&quot;mvtnorm&quot;) The multivariate normal distribution is the generalization of the univariate normal distribution to more than one dimension.6 The random variable, \\(\\vec{x}\\), is a length \\(k\\) vector. The \\(k\\) length vector \\(\\vec{\\mu}\\) are the means of \\(\\vec{x}\\), and the \\(k \\times k\\) matrix, \\(\\mat{\\Sigma}\\), is the variance-covariance matrix, \\[ \\begin{aligned}[t] \\vec{x} &amp;\\sim \\dmvnorm{k}\\left(\\vec{\\mu}, \\mat{\\Sigma} \\right) \\\\ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_k \\end{bmatrix} &amp; \\sim \\dmvnorm{k} \\left( \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_k \\end{bmatrix}, \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{1,2} &amp; \\cdots &amp; \\sigma_{1, k} \\\\ \\sigma_{2,1} &amp; \\sigma_2^2 &amp; \\cdots &amp; \\sigma_{2, k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{k,1} &amp; \\sigma_{k,2} &amp; \\cdots &amp; \\sigma_{k, k} \\end{bmatrix} \\right) \\end{aligned} \\] The density function of the multivariate normal is, \\[ p(\\vec{x}; \\vec{\\mu}, \\mat{\\Sigma}) = (2 k)^{-\\frac{k}{2}} \\left| \\mat{\\Sigma} \\right|^{-\\frac{1}{2}} \\exp \\left( -\\frac{1}{2} (\\vec{x} - \\vec{\\mu})\\T \\mat{\\Sigma}^{-1} (\\vec{x} - \\vec{\\mu}) \\right) . \\] You can sample from and calculate the density for the multivariate normal distribution with the functions dmvnorm and rmvnorm from the package mvtnorm. Density plots of different bivariate normal distributions, See Multivariate normal distribution and references therein.↩ "],
["references-1.html", "References", " References "]
]
