[
["index.html", "Data Analysis Notes Chapter 1 Introduction", " Data Analysis Notes Jeffrey B. Arnold 2017-04-12 Chapter 1 Introduction Notes used when teaching “POLS/CS&amp;SS 501: Advanced Political Research Design and Analysis” and “POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology” at the University of Washington. \\[ \\] "],
["regression.html", "Chapter 2 Regression 2.1 Joint vs. Conditional models 2.2 Conditional expectation function", " Chapter 2 Regression library(&quot;tidyverse&quot;) library(&quot;modelr&quot;) library(&quot;stringr&quot;) library(&quot;HistData&quot;) 2.1 Joint vs. Conditional models In most problems, the researcher is concerned with relationships between multiple variables. For example, suppose that we want to model the relationship between two variables, \\(Y\\) and \\(X\\). There are two main approaches to modeling this relationship. joint model: Jointly model \\(Y\\) and \\(X\\) as \\(f(Y, X)\\). For example, we can model \\(Y\\) and \\(X\\) as coming from a bivariate normal distribution.1 conditional model: Model \\(Y\\) as a conditional function of \\(X\\). This means we calculate a function of \\(Y\\) for each value of \\(X\\).2 Most often we focus on modeling a conditional statistic of \\(Y\\), and linear regression will focus on modeling the conditional mean of \\(Y\\), \\(\\E(Y | X)\\). regression (conditional) model \\(p(y | x_1, \\dots, x_k) = f(x_1, \\dots, x_k)\\) and \\(x_1, \\dots, x_k\\) are given. joint model \\(p(y, x_1, \\dots, x_k) = f(y, x_1, \\dots, x_k)\\) If we knew the joint model we could calculate the conditional model, \\[ (y | x_1, \\dots, x_k) = \\frac{p(y, x_1, \\dots, x_k)}{p(x_1, \\dots, x_k)} . \\] However, especially when there are specific outcome variables of interest, the conditional model, i.e. regression, is easier because the analyst can focus on modeling how \\(Y\\) varies with respect to \\(X\\), without necessarily having to model the process by which \\(X\\) is generated. However, that very convenience of not modeling the process which generates \\(X\\) will be the problem when regression is used for causal inference on observational data. At its most general, “regression analysis, broadly construed, traces the distribution of a response variable (denoted by \\(Y\\))—or some characteristic of this distribution (such as its mean)—as a function of one of more explanatory variables (Fox 2016, 15).” is a procedure that is used to summarize conditional relationships. That is, the average value of an outcome variable conditional on different values of one or more explanatory variables. While this section will generally cover linear regression, it is not the only form of regression. So let’s start with a definition of regression. Most generally, a regression represents a function of a variable, \\(Y\\) as a function of another variable or variables, \\(X\\), and and error. \\[ g(Y_i) = f(X_i) + \\text{error}_i \\] The conditional expectation function (CEF) or regression function of \\(Y\\) given \\(X\\) is denoted, \\[ \\mu(x) = \\E\\left[Y | X = x\\right] \\] But if regression represents \\(Y\\) as a function of \\(X\\), what’s the alternative? Instead of modeling \\(Y\\) as a function of \\(X\\), we could jointly model both \\(Y\\) and \\(X\\). A regression model of \\(Y\\) and \\(X\\) would be multivariate function, \\(f(Y, X)\\). In machine learning these approaches are sometimes called descriminative (regression) and generative (joint models) models. 2.2 Conditional expectation function 2.2.1 Discrete Covariates Before turning to considering continuous variable, it is useful to consider the conditional expectation function for a discrete \\(Y\\) and \\(X\\). Consider the datasets dataset included in the recommended R package datasets. It is a cross-tabulation of the survival of the 2,201 passengers in the sinking of the Titanic in 1912, as well as characteristics of those passengers: passenger class, gender, and age. Titanic &lt;- as_tibble(datasets::Titanic) %&gt;% mutate(Survived = (Survived == &quot;Yes&quot;)) The proportion of passengers who survived was summarise(Titanic, prop_survived = sum(n * Survived) / sum(n)) ## # A tibble: 1 × 1 ## prop_survived ## &lt;dbl&gt; ## 1 0.323035 Since Survived is a A conditional expectation function is a function that calculates the mean of Y for different values of X. For example, the conditional expectation function for Calculate the CEF for Survived conditional on Age, Titanic %&gt;% group_by(Age) %&gt;% summarise(prop_survived = sum(n * Survived) / sum(n)) ## # A tibble: 2 × 2 ## Age prop_survived ## &lt;chr&gt; &lt;dbl&gt; ## 1 Adult 0.3126195 ## 2 Child 0.5229358 conditional on Sex, Titanic %&gt;% group_by(Sex) %&gt;% summarise(prop_survived = sum(n * Survived) / sum(n)) ## # A tibble: 2 × 2 ## Sex prop_survived ## &lt;chr&gt; &lt;dbl&gt; ## 1 Female 0.7319149 ## 2 Male 0.2120162 conditional on Class, Titanic %&gt;% group_by(Class) %&gt;% summarise(prop_survived = sum(n * Survived) / sum(n)) ## # A tibble: 4 × 2 ## Class prop_survived ## &lt;chr&gt; &lt;dbl&gt; ## 1 1st 0.6246154 ## 2 2nd 0.4140351 ## 3 3rd 0.2521246 ## 4 Crew 0.2395480 finally, conditional on all combinations of the other variables (Age, Sex, Class), titanic_cef_3 &lt;- Titanic %&gt;% group_by(Class, Age, Sex) %&gt;% summarise(prop_survived = sum(n * Survived) / sum(n)) titanic_cef_3 ## Source: local data frame [16 x 4] ## Groups: Class, Age [?] ## ## Class Age Sex prop_survived ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1st Adult Female 0.97222222 ## 2 1st Adult Male 0.32571429 ## 3 1st Child Female 1.00000000 ## 4 1st Child Male 1.00000000 ## 5 2nd Adult Female 0.86021505 ## 6 2nd Adult Male 0.08333333 ## 7 2nd Child Female 1.00000000 ## 8 2nd Child Male 1.00000000 ## 9 3rd Adult Female 0.46060606 ## 10 3rd Adult Male 0.16233766 ## 11 3rd Child Female 0.45161290 ## 12 3rd Child Male 0.27083333 ## 13 Crew Adult Female 0.86956522 ## 14 Crew Adult Male 0.22273782 ## 15 Crew Child Female NaN ## 16 Crew Child Male NaN The CEF can be used to predict outcome variables given \\(X\\) variables. What is the predicted probability of survival for each of these characters from the movie Titanic? Rose (Kate Winslet): 1st class, adult female (survived) Jack (Leonardo DiCaprio): 3rd class, adult male (did not survive) Cal (Billy Zane) : 1st class, adult male (survived) titanic_chars &lt;- tribble( ~ name, ~ Class, ~ Age, ~ Sex, ~ Survived, &quot;Rose&quot;, &quot;1st&quot;, &quot;Adult&quot;, &quot;Female&quot;, TRUE, &quot;Jack&quot;, &quot;3rd&quot;, &quot;Adult&quot;, &quot;Male&quot;, FALSE, &quot;Cal&quot;, &quot;1st&quot;, &quot;Adult&quot;, &quot;Male&quot;, TRUE ) left_join(titanic_chars, titanic_cef_3, by = c(&quot;Class&quot;, &quot;Age&quot;, &quot;Sex&quot;)) ## # A tibble: 3 × 6 ## name Class Age Sex Survived prop_survived ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 Rose 1st Adult Female TRUE 0.9722222 ## 2 Jack 3rd Adult Male FALSE 0.1623377 ## 3 Cal 1st Adult Male TRUE 0.3257143 Rose was predicted to survive 97% of 1st class adult females survived, and she did. Jack was not predicted to survive (only 16% of 3rd class adult males survived, and he did not.3 Cal was not predicted to survive (33% of 1st class adult males survived), but he did, though through less than honorable means in the movie. Note that we haven’t made any assumptions about distributions of the variables. In this case, the outcome variable in the CEF was a binary variable, and we calculated a proportion. However, the proportion is the expected value (mean) of a binary variable, so the calculation of the CEF wouldn’t change. If we continued to condition on more discrete variables, the number of observed cell sizes would get smaller and smaller (and possibly zero), with larger standard errors. 2.2.2 Continuous Covariates But what happens if the conditioning variables are continuous? Galton (1886) examined the joint distribution of the heights of parents and their children. He was estimating the average height of children conditional upon the height of their parents. He found that this relationship was approximately linear with a slope of 2/3. This means that on average taller parents had taller children, but the children of taller parents were on average shorter than they were, and the children of shorter parents were on average taller than they were. In other words, the children’s height was more average than parent’s height. This phenomenon was called regression to the mean, and the term regression is now used to describe conditional relationships (Hansen 2010). His key insight was that if the marginal distributions of two variables are the same, then the linear slope will be less than one. He also found that when the variables are standardized, the slope of the regression of \\(y\\) on \\(x\\) and \\(x\\) on \\(y\\) are the same. They are both the correlation between \\(x\\) and \\(y\\), and they both show regression to the mean. Galton &lt;- as_tibble(Galton) Galton ## # A tibble: 928 × 2 ## parent child ## &lt;dbl&gt; &lt;dbl&gt; ## 1 70.5 61.7 ## 2 68.5 61.7 ## 3 65.5 61.7 ## 4 64.5 61.7 ## 5 64.0 61.7 ## 6 67.5 62.2 ## 7 67.5 62.2 ## 8 67.5 62.2 ## 9 66.5 62.2 ## 10 66.5 62.2 ## # ... with 918 more rows Calculate the regression of children’s heights on parents. Interpret the regression. child_reg &lt;- lm(child ~ parent, data=Galton) child_reg ## ## Call: ## lm(formula = child ~ parent, data = Galton) ## ## Coefficients: ## (Intercept) parent ## 23.9415 0.6463 Calculate the regression of parent’s heights on children’s heights. Interpret the regression. parent_reg &lt;- lm(parent ~ child, data=Galton) parent_reg ## ## Call: ## lm(formula = parent ~ child, data = Galton) ## ## Coefficients: ## (Intercept) child ## 46.1353 0.3256 Regression calculates the conditional expectation function, \\(f(Y, X) = \\E(Y | X) + \\epsilon\\), but we could instead jointly model \\(Y\\) and \\(X\\). This is a topic for multivariate statistics (principal components, factor analysis, clustering). In this case, an alternative would be to model the heights of fathers and sons as a bivariate normal distribution. ggplot(Galton, aes(y = child, x = parent)) + geom_jitter() + geom_density2d() # covariance matrix Galton_mean &lt;- c(mean(Galton$parent), mean(Galton$child)) # variance covariance matrix Galton_cov &lt;- cov(Galton) Galton_cov ## parent child ## parent 3.194561 2.064614 ## child 2.064614 6.340029 var(Galton$parent) ## [1] 3.194561 var(Galton$child) ## [1] 6.340029 cov(Galton$parent, Galton$child) ## [1] 2.064614 Calculate density for a multivariate normal distribution library(&quot;mvtnorm&quot;) Galton_mvnorm &lt;- function(parent, child) { # mu and Sigma will use the values calculated earlier dmvnorm(cbind(parent, child), mean = Galton_mean, sigma = Galton_cov) } Galton_mvnorm(Galton$parent[1], Galton$child[1]) ## [1] 4.272599e-05 Galton_dist &lt;- Galton %&gt;% modelr::data_grid(parent = seq_range(parent, 50), child = seq_range(child, 50)) %&gt;% mutate(dens = map2_dbl(parent, child, Galton_mvnorm)) Why don’t I calculate the mean and density using the data grid? library(&quot;viridis&quot;) ggplot(Galton_dist, aes(x = parent, y = child)) + geom_raster(mapping = aes(fill = dens)) + #geom_contour(mapping = aes(z = dens), colour = &quot;white&quot;, alpha = 0.3) + #geom_jitter(data = Galton, colour = &quot;white&quot;, alpha = 0.2) + scale_fill_viridis() + theme_minimal() + theme(panel.grid = element_blank()) + labs(y = &quot;Parent height (in)&quot;, x = &quot;Child height (in)&quot;) Using the plotly library we can make an interactive 3D plot: x &lt;- unique(Galton_dist$parent) y &lt;- unique(Galton_dist$child) z &lt;- Galton_dist %&gt;% arrange(child, parent) %&gt;% spread(parent, dens) %&gt;% select(-child) %&gt;% as.matrix() plotly::plot_ly(z = z, type = &quot;surface&quot;) But with regression we are calculating only one margin. Galton_means &lt;- Galton %&gt;% group_by(parent) %&gt;% summarise(child = mean(child)) ggplot(Galton, aes(x = factor(parent), y = child)) + geom_jitter(width = 0) + geom_point(data = Galton_means, colour = &quot;red&quot;) Note that in this example, it doesn’t really matter since a bivariate normal distribution happens to describe the data very well. This is not true in general, and we are simplifying our analysis by calculating the CEF rather than jointly modeling both. References "],
["interpreting-coefficients.html", "Chapter 3 Interpreting Coefficients 3.1 Interactions and Polynomials 3.2 Average Marginal Effects 3.3 Standardized Coefficients", " Chapter 3 Interpreting Coefficients That the coefficients are the marginal effects of each predictor makes linear regression particularly easy to interpret. However, this interpretation of predictors becomes more complicated once a variable is included in multiple terms through interactions or nonlinear functions, such as polynomials. Consider the regression, \\[ Y_i = \\beta_0 + \\beta_1 X + \\beta_2 Z + \\varepsilon \\] The regression coefficient \\(\\beta_1\\) it the change in the expected value of \\(Y\\) associated with a one-unit change in \\(X\\) holding \\(Z\\) constant, \\[ \\begin{aligned}[t] E(Y | X = x, Z = z) - E(Y | X = x + 1, Z = z) &amp;= (\\beta_0 + \\beta_1 X + \\beta_2 z) - (\\beta_0 + \\beta_1 (x + 1) + \\beta_2 z) \\\\ &amp;= \\beta_1 x - \\beta_1 (x + 1) \\\\ &amp;= \\beta_1 (x - x + 1) \\\\ &amp;= \\beta_1 \\end{aligned} \\] More formally, the coefficient \\(\\beta_k\\) is the partial derivative of \\(E(Y | X)\\) with respect to \\(X_k\\), \\[ \\begin{aligned}[t] \\frac{\\partial\\,E(Y | X)}{\\partial\\,X_k} = \\frac{\\partial}{\\partial\\,X_k} \\left( \\beta_0 + \\sum_{k = 1}^K \\beta_k X_k) \\right) = \\beta_k \\end{aligned} \\] Implications: If \\(X\\) is multiplied by a constant scalar \\(a\\), \\[ E(Y | X) = \\tilde{\\beta}_0 + \\tilde{\\beta}_1 a X = \\beta_0 + (a \\beta_1) X . \\] If \\(X_k\\) has a scalar \\(a\\) added to it, \\[ E(Y | X) = \\tilde{\\beta}_0 + \\tilde{\\beta}_1 (X + a) = (\\beta_0 + \\tilde{\\beta}_1 a) + \\tilde{\\beta}_1 X \\] Thus, \\(\\tilde{\\beta}_0 = (\\beta_0 + \\beta_1 a)\\) and \\(\\tilde{\\beta}_1 = \\beta_1\\). Consider the regression model \\[ \\vec{Y} = \\vec{\\beta} \\mat{X} + \\vec{\\epsilon} \\] Rather than staring by asking what do the regression coefficients, \\(\\vec{\\beta}\\), mean, we should start by asking what we want to estimate (i.e. the estimand) and then figure out how to extract that from the regression model. Let’s start with what it that we want to calculate. We want to calculate the “marginal effect” of changing the \\(j\\)th predictors while holding other predictors constant. In particular, one common estimand is the predicted change in the expected value of \\(Y\\) from a change in the \\(j\\)th predictor variable while holding the other predictors constant. The regression model is a model of the expected value of \\(Y\\) as a function of \\(\\mat{X}\\), \\[ \\hat{\\vec{y}} = \\E(Y) = \\hat{\\vec{beta}} \\mat{X} \\] For a continuous variable, \\(\\vec{x}_j\\), this is called the “marginal effect” and it is the partial derivative of the regression line with respect to \\(\\vec{x}_j\\), \\[ ME_{i,j} = \\frac{\\partial \\E( Y_i | x_{i,j}, x_{i,-j})}{\\partial x_{i,j}} \\] For a discrete change in \\(x_j\\), this is called the “partial effect” or “first difference”, and is simply the difference of predicted values, \\[ ME_{i,j} = \\E(Y_i | x_{i,j}, x_{i,-j}) - \\E(Y_i | x_{i,j} + \\Delta x_{i,j}, x_{i,-j}) \\] Now consider the linear regression with two predictors for a change in \\(x_1\\), \\[ \\begin{aligned}[t] ME_j &amp;= E(y | x_1, \\tilde{x}_2) - E(y | x_1 + \\Delta x_1, \\tilde{x}_2) \\end{aligned} \\] Since the linear regression equation is \\(E(y | x)\\), this simplifies to \\[ \\begin{aligned}[t] ME_j &amp;= (\\beta_0 + \\beta_1 x_1 + \\tilde{x}_2) - (\\beta_0 + \\beta_1 (x_1 + \\Delta x_1) \\tilde{x}_2) \\\\ &amp;= \\beta_1 \\Delta x_1 \\end{aligned} \\] or as \\(\\Delta x_1 \\to 0\\), this simplifies to the coefficient itself. \\[ \\begin{aligned}[t] ME_j &amp;= \\frac{\\partial E(y | x_1, x_2)}{\\partial x_1} \\\\ \\frac{\\partial (\\beta_0 + \\beta_1 x_1 + \\tilde{x}_2)}{\\partial x_1} &amp;= \\beta_1 \\end{aligned} \\] All of the previous equations were at the population level. The sample estimate for the marginal effect is \\[ \\widehat{ME}_j = \\hat{\\beta}_1 \\] So, for a linear regression, the marginal effect of \\(x_j\\), defined as the change in the expected value of \\(y\\) for a small a unit of \\(j\\) The equation presented above is not causal, it is simply a function derived from the population or estimated equation. If population equation is not the as the linear regression, \\(\\hat{\\beta_j}\\) can still be viewed as an estimator of \\(ME_j\\). In OLS, the \\(ME_j\\) is weighted by observations with the most variation in \\(x_j\\), after accounting for the parts of \\(x_j\\) and \\(y\\) predicted by the other predictors. See the discussion Angrist and Pischke (2009) and Aronow and Samii (2015). For regressions other than OLS, the coefficients are not the \\(ME_j\\). It is a luxury that the coefficients happen to have a nice interpretation in OLS. In most other regressions, the coefficients are not directly useful. This is yet another reason to avoid the mindless presentation of tables and star-worshiping. The researcher should focus on inference about the research quantity of interest, whether or not that happens to be conveniently provided as a parameter of the model that was estimated. 3.1 Interactions and Polynomials Even for OLS, if \\(x_j\\) is included as part of a function, e.g. a polynomial or an interaction, then its coefficient cannot be interpreted as the marginal effect. Suppose that the regression equation is \\[ \\vec{y} = \\vec{\\beta}_0 + \\vec{\\beta}_1 x_1 + \\vec{\\beta}_2 x_1^2 + \\vec{\\beta}_3 x_2, \\] then the marginal effect of \\(x_1\\) is, \\[ \\begin{aligned}[t] ME_j &amp;= \\frac{\\partial E(y | x_1, x_2)}{\\partial x_1} \\\\ &amp;= \\frac{\\partial (\\beta_0 + \\beta_1 x_1 + \\beta_1 x_1^2 + \\beta_3 \\tilde{x}_2)}{\\partial x_1} \\\\ &amp;= \\beta_1 + 2 \\beta_2 x_1 \\end{aligned} \\] Note that the marginal effect of \\(x_1\\) is not, \\(\\beta_1\\). That would require a change in \\(x_1\\) while holding \\(x_1 ^ 2\\) constant, which is a logical impossibility. Instead, the marginal effect of \\(x_1\\) depends on the value of \\(x_2\\) at which it is evaluated, and, thus, observations will have different marginal effects. Similarly, if there is an interaction between \\(x_1\\) and \\(x_2\\), the coefficient of a predictor is not its marginal effect. For example, in \\[ y = \\vec{\\beta}_0 + \\vec{\\beta}_1 x_1 + \\vec{\\beta}_2 x_1 + \\vec{\\beta}_3 x_1 x_2 \\] the marginal effect of \\(x_1\\) is \\[ \\begin{aligned}[t] ME_j &amp;= \\frac{\\partial E(y | x_1, x_2)}{\\partial x_1} \\\\ &amp;= \\frac{\\partial (\\beta_0 + \\beta_1 x_1 + \\beta_1 x_1^2 + \\beta_2 \\tilde{x}_2)}{\\partial x_1} \\\\ &amp;= \\beta_1 + \\beta_3 x_2 \\end{aligned} \\] Now the marginal effect of \\(x_1\\) is a function of another variable \\(x_2\\). 3.2 Average Marginal Effects For marginal effects that are functions of the data, there are multiple ways to calculate them. They include, AME: Average Marginal Effect. Average the marginal effects at each observed \\(x\\). MEM: Marginal Effect at the mean. Calculate the marginal effect with all observations at their means or other central values. MER: Marginal Effect at a representative value. Similar to MEM but with another meaningful value. Of these, the AME is the preferred one; marginal effects should be calculated for all observations, and then averaged (Hanmer and Kalkan 2012). When it is discrete change in \\(x\\), it is called a partial effect (APE) or a first difference. The difference in the expected value of y, given a change in \\(x_j\\) from \\(x^*\\) to \\(x^* + \\Delta\\) is \\(\\beta_j \\Delta\\), and the standard error can be calculated analytically by the Delta method, \\[ \\se(\\hat{\\beta}_j \\Delta x_j) = \\sqrt{\\Var\\hat{\\beta}_j (\\Delta x_j)^2} = \\se\\hat{\\beta}_j \\Delta x_j. \\] The Delta method can be used to analytically derive approximations of the standard errors for other nonlinear functions and interaction in regression, but it scales poorly, and it is often easier to use bootstrapping or software than calculate it by hand. See the margins package. 3.3 Standardized Coefficients A standardized coefficient is the coefficient on \\(X\\), when \\(X\\) is standardized so that \\(\\mean(X) = 0\\) and \\(\\Var(X) = 1\\). In that case, \\(\\beta_1\\) is the change in \\(\\E(Y)\\) associated with a one standard deviation change in \\(X\\). Additionally, if all predictors are set so that \\(\\mean(X) = 0\\), \\(\\beta_0\\) is the expected value of \\(Y\\) when all \\(X\\) are at their means. However, if any variables appear in multiple terms, then the standardized coefficients are not particularly useful. Standardized coefficients are generally not used in political science. (King How Not to Lie with Statistics, p. 669) More often, the effects of variables are compared by the first difference between the value of the variable at the mean, and a one standard deviation change. While, this is equivalent to the standardized coefficient Note, that standardizing variables can help computationally in some cases. In OLS, there is a closed-form solution, so iterative optimization algorithms are not needed in to find the best parameters. However, in more complicated models which require iterative optimization, standardizing variables can often improve the performance of the optimization. Thus standardizing variables before analysis is common in machine learning. However, the purpose is for ease of computation, not for ease of interpretation. References "],
["multiple-testing.html", "Chapter 4 Multiple Testing 4.1 Setup 4.2 Multiple Testing 4.3 Data snooping", " Chapter 4 Multiple Testing 4.1 Setup library(&quot;tidyverse&quot;) library(&quot;broom&quot;) library(&quot;stringr&quot;) library(&quot;magrittr&quot;) 4.2 Multiple Testing What happens if we run multiple regressions? What do p-values mean in that context? Simulate data where \\(Y\\) and \\(X\\) are all simulated from i.i.d. standard normal distributions, \\(Y_i \\sim N(0, 1)\\) and \\(X_{i,j} \\sim N(0, 1)\\). This means that \\(Y\\) and \\(X\\) are not associated. sim_reg_nothing &lt;- function(n, k, sigma = 1, .id = NULL) { .data &lt;- rnorm(n * k, mean = 0, sd = 1) %&gt;% matrix(nrow = n, ncol = k) %&gt;% set_colnames(str_c(&quot;X&quot;, seq_len(k))) %&gt;% as_tibble() .data$y &lt;- rnorm(n, mean = 0, sd = 1) # Run first regression .formula1 &lt;- as.formula(str_c(&quot;y&quot;, &quot;~&quot;, str_c(&quot;X&quot;, seq_len(k), collapse = &quot;+&quot;))) mod &lt;- lm(.formula1, data = .data, model = FALSE) df &lt;- tidy(mod) df[[&quot;.id&quot;]] &lt;- .id df } Here is an example with of running one regression: n &lt;- 1000 k &lt;- 19 results_sim &lt;- sim_reg_nothing(n, k) How many coefficients are significant at the 5% level? alpha &lt;- 0.05 arrange(results_sim, p.value) %&gt;% select(term, estimate, statistic, p.value) %&gt;% head(n = 20) ## term estimate statistic p.value ## 1 (Intercept) -0.057671023 -1.76761794 0.07743594 ## 2 X11 -0.048028515 -1.50903711 0.13161162 ## 3 X14 0.048992567 1.48746690 0.13721321 ## 4 X16 0.046164679 1.41286413 0.15801318 ## 5 X10 -0.044591056 -1.39512286 0.16329487 ## 6 X6 -0.034591848 -1.10784039 0.26820258 ## 7 X17 0.033580690 1.06105001 0.28892859 ## 8 X3 0.027898979 0.85598092 0.39221757 ## 9 X7 0.026048269 0.79321403 0.42784513 ## 10 X1 -0.021855838 -0.68917119 0.49087868 ## 11 X2 -0.021550954 -0.64034712 0.52209664 ## 12 X18 -0.020022613 -0.63555967 0.52521184 ## 13 X13 0.016805579 0.52445681 0.60007946 ## 14 X8 -0.015532376 -0.48393610 0.62853934 ## 15 X4 0.013266040 0.41097637 0.68117971 ## 16 X19 0.010020257 0.31736246 0.75103619 ## 17 X12 -0.010034560 -0.31199476 0.75511087 ## 18 X9 -0.009682185 -0.29624723 0.76710405 ## 19 X15 0.009669889 0.29186436 0.77045210 ## 20 X5 -0.002942765 -0.09249518 0.92632353 Is this surprising? No. Since the null hypothesis is true for all coefficients (\\(\\beta_j = 0\\)), a \\(p\\)-value of 5% means that 5% of the tests will be false positives (Type I error). Let’s confirm that with a larger number of simulations and also use it to calculate some other values. Run 1,024 simulations and save the results to a data frame. number_sims &lt;- 1024 sims &lt;- map_df(seq_len(number_sims), function(i) { sim_reg_nothing(n, k, .id = i) }) Calculate the number significant at the 5% level in each regression. n_sig &lt;- sims %&gt;% group_by(.id) %&gt;% summarise(num_sig = sum(p.value &lt; alpha)) %&gt;% count(num_sig) %&gt;% ungroup() %&gt;% mutate(p = n / sum(n)) Overall, we expect 5% to be significant at the 5 percent level. sims %&gt;% summarise(num_sig = sum(p.value &lt; alpha), n = n()) %&gt;% ungroup() %&gt;% mutate(p = num_sig / n) ## num_sig n p ## 1 1015 20480 0.04956055 What about the distribution of statistically significant coefficients in each regression? ggplot(n_sig, aes(x = num_sig, y = p)) + geom_bar(stat = &quot;identity&quot;) + scale_x_continuous(&quot;Number of significant coefs&quot;, breaks = unique(n_sig$num_sig)) + labs(y = &quot;Pr(reg has k signif coef)&quot;) What’s the probability that a regression will have no significant coefficients, \\(1 - (1 - \\alpha) ^ {k - 1}\\), (1 - (1 - alpha) ^ (k + 1)) ## [1] 0.6415141 What’s the take-away? Don’t be too impressed by statistical significance when many tests are run. Note that multiple hypothesis tests occur both within papers … and within literatures. 4.3 Data snooping A not-uncommon practice is to run a regresion, filter out variables with “insignificant” coefficients, and then run and report a regression with only the smaller number of “significant” variables. Most explicitly, this occurs with stepwise regression, the problems of which are well known (when used for inference). However, this can even occur in cases where the hypotheses are not specified in advance and there is no explicit stepwise function used. To see the issues with this method, let’s consider the worst case scenario, when there is no relationship between \\(Y\\) and \\(X\\). Suppose \\(Y_i\\) is sampled from a i.i.d. standard normal distributions, \\(Y_i \\sim N(0, 1)\\). Suppose that the design matrix, \\(\\mat{X}\\), consists of 50 variables, each sampled from i.i.d. standard normal distributions, \\(X_{i,k} \\sim N(0, 1)\\) for \\(i \\in 1:100\\), \\(k \\in 1:50\\). Given this, the \\(R^2\\) for these regressions should be approximately 0.50. As shown in the previous section, it will not be uncommon to have several “statistically” significant coefficients at the 5 percent level. The sim_datasnoop function simulates data, and runs two regressions: Regress \\(Y\\) on \\(X\\) Keep all variables in \\(X\\) with \\(p &lt; .25\\). Regress \\(Y\\) on the subset of \\(X\\), keeping only those variables that were significant in step 2. sim_datasnoop &lt;- function(n = 100, k = 50, p = 0.10) { .data &lt;- rnorm(n * k, mean = 0, sd = 1) %&gt;% matrix(nrow = n, ncol = k) %&gt;% set_colnames(str_c(&quot;X&quot;, seq_len(k))) %&gt;% as_tibble() .data$y &lt;- rnorm(n, mean = 0, sd = 1) # Run first regression .formula1 &lt;- as.formula(str_c(&quot;y&quot;, &quot;~&quot;, str_c(&quot;X&quot;, seq_len(k), collapse = &quot;+&quot;))) mod1 &lt;- lm(.formula1, data = .data, model = FALSE) # Select model with only significant values (ignoring intercept) signif_x &lt;- tidy(mod1) %&gt;% filter(p.value &lt; p, term != &quot;(Intercept)&quot;) %&gt;% `[[`(&quot;term&quot;) if (length(signif_x &gt; 0)) { .formula2 &lt;- str_c(str_c(&quot;y&quot;, &quot;~&quot;, str_c(signif_x, collapse = &quot;+&quot;))) mod2 &lt;- lm(.formula2, data = .data, model = FALSE) } else { mod2 &lt;- NULL } tibble(mod1 = list(mod1), mod2 = list(mod2)) } Now repeat this simulation 1,024 times, calculate the \\(R^2\\) and number of statistically signifcant coefficients at \\(\\alpha = .05\\). n_sims &lt;- 1024 alpha &lt;- 0.05 sims &lt;- rerun(n_sims, sim_datasnoop()) %&gt;% bind_rows() %&gt;% mutate( r2_1 = map_dbl(mod1, ~ glance(.x)$r.squared), r2_2 = map_dbl(mod2, function(x) if (is.null(x)) NA_real_ else glance(x)$r.squared), pvalue_1 = map_dbl(mod1, ~ glance(.x)$p.value), pvalue_2 = map_dbl(mod2, function(x) if (is.null(x)) NA_real_ else glance(x)$p.value), sig_1 = map_dbl(mod1, ~ nrow(filter(tidy(.x), term != &quot;(Intercept)&quot;, p.value &lt; alpha))), sig_2 = map_dbl(mod2, function(x) { if (is.null(x)) NA_real_ else nrow(filter(tidy(x), term != &quot;(Intercept)&quot;, p.value &lt; alpha)) }) ) select(sims, r2_1, r2_2, pvalue_1, pvalue_2, sig_1, sig_2) %&gt;% summarise_all(funs(mean(., na.rm = TRUE))) ## # A tibble: 1 × 6 ## r2_1 r2_2 pvalue_1 pvalue_2 sig_1 sig_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.5036307 0.1622974 0.5070183 0.03709078 2.523438 2.51002 While the average \\(R\\) squared of the second stage regressions are less, the average \\(p\\)-values of the F-test that all coefficients are zero are much less. The number of statistically significant coefficients in the first and second regressions are approximately the same, which the second regression being slightly What happens if the number of obs, number of variables, and filtering significance level are adjusted? So why are the significance levels of the overall \\(F\\) test incorrect? For a p-value to be correct, it has to have the correct sampling distribution of the observed data. Even though in this simulation we are sampling the data in the first stage from a model that satisfies the assumptions of the F-test, the second stage does not account for the original filtering. This example is known as Freedman’s Paradox (Freedman 1983). References "],
["weighted-regression.html", "Chapter 5 Weighted Regression 5.1 Weighted Least Squares (WLS) 5.2 When should you use WLS? 5.3 Correcting for Known Heteroskedasticity 5.4 Sampling Weights 5.5 References", " Chapter 5 Weighted Regression 5.1 Weighted Least Squares (WLS) Ordinary least squares estimates coefficients by finding the coefficients that minimize the sum of squared errors, \\[ \\hat{\\vec\\beta}_{OLS} = \\argmin_{\\vec{b}} \\sum_{i = 1}^N (y_i - \\vec{x}\\T \\vec{b})^2 . \\] Note that the objective function treats all observations equally; an error in one is as good as any other. However, there are several situations where we care more about minimizing some errors more than others. The next situation will discuss the reasons to use WLS, but Weighted least squares (WLS) requires only a small change to the OLS objective function. Each observation is given a weight, \\(w_i\\), and the weighted sum of squared errors is minimized, \\[ \\begin{aligned}[t] \\hat{\\vec\\beta}_{WLS} = \\argmin_{\\vec{b}} \\sum_{i = 1}^N w_i (y_i - \\vec{x}\\T \\vec{b})^2 \\end{aligned} \\hat{\\beta}_{WLS} = \\argmin_{\\vec{b}} \\sum_{i = 1}^N w_i (y_i - \\vec{x}\\T \\vec{b})^2 . \\] The weights \\(w_i\\) are provided by the analyst and are not estimated. Note that OLS is a special case of WLS where \\(w_i = 1\\) for all the observations. In order to minimize the errors, WLS will have to fit the line closer to observations with higher weights You can estimate WLS by using the weights argument to rdoc(&quot;stats&quot;, &quot;lm&quot;). 5.2 When should you use WLS? The previous section showed what WLS is, but when should you use weighted regression? It depends on the purpose of your analysis: If you are estimating population descriptive statistics, then weighting is needed to ensure that the sample is representative of the population. If you are concerned with causal inference, then weighting is more nuanced. You may or may not need to weight, and it will often be unclear which is better. There are three reasons for weighting in causal inference (Solon, Haider, and Wooldridge 2015): To correct standard errors for heteroskedasticity Get consistent estimates by correcting for endogenous sampling Identify average partial effects when there is unmodeled heterogeneity in the effects. Heteroskedasticity: Estimate OLS and WLS. If the model is misspecified or there is endogenous selection, then OLS and WLS have different probability limits. The contrast between OLS and WLS estimates is a diagnostic for model misspecification or endogenous sampling. Always use robust standard errors. Endogenous sampling: If the sample weights vary exogenously instead of endogenously, then weighting may be harmful for precision. The OLS still specifies the conditional mean. Sampling is exogenous if the sampling probabilities are independent of the error - e.g. if they are only functions of the explanatory variables. If the probabilities are a function of the dependent variable, then they are endogenous. if sampling rate is endogenous, weight by inverse selection. use robust standard errors. if the sampling rate is exogenous, then OLS and WLS are consistent. Use OLS and WLS as test of model misspecification. Heterogeneous effects: Identifying average partial effects. WLS estimates the linear regression of the population, but this is not the same as the average partial effects. But that is because OLS does not estimate the average partial effect, but weights according to the variance in X. Angrist and Pischke (2009, 92) suggest weighting when “they make it more likely that the regression you are estimaing is close to the population target you are trying to estimate”. sampling weights: yes grouped data (sums, averages): yes heteroskedasticity: no (just use robust standard errors) WLS can be more efficient than OLS if variance model is correct If \\(E(e_i | X)\\) is a poor approximation or measurements are noisy, WLS has bad finite sample properties If the CEF is not linear, then OLS and WLS are both wrong, but OLS still interpretable as the minimum mean squared error approximation of the CEF. The WLS is also an approx of CEF, but approx is a function of the weights. Advice of (Cameron and Trivedi 2010, 113). There are two approaches to using weights. Census parameter: Reweight regression to try to get the population regression estimates. Control function approach: Assuming that \\(\\E(\\epsilon_i | \\vec{x}_i) = 0\\), then weights are not needed. WLS is consistent for any weights, and OLS is more efficient. This means if we control for all covariates relevant to sampling probabilities, there is no need to weight. This works as long as sampling probabilities are a function of \\(x\\) and not of \\(y\\). It seems that Cameron and Trivedi (2010) “census parameter” approach is what Angrist and Pischke (2009) interprets it as, but it supports the model based approach. Weights should be used for predictions and computing average MEs. (Cameron and Trivedi 2010, 114–15). Fox (2016, 461): inverse probability weights are different than weights in heteroskedasticity, and WLS cannot be used. It will give the wrong SEs but correct point estimates. Seems to suggest using bootstrapping to get standard errors instead (Fox 2016, 661, 666). 5.3 Correcting for Known Heteroskedasticity Most generally, heteroskedasticity is “unknown” and robust standard errors should be used. However, there are some cases where heteroskedasticity is “known”. For example: The outcome variable consists of measurements with a given measurement error - perhaps they are estimates themselves. The error of the output depends on input variables in known ways. For example, the sampling error of polls. Examples: \\(\\E(\\epsilon)_i^2 \\propto z_i^2\\) where \\(a\\) is some observated variable. Then \\(w_i = z_i\\). \\(\\E(\\epsilon)_^2\\) is an average of values. Then \\(\\sigma^2_i = \\omega^2 / n_i\\). In WLS, \\(w_i = 1 / \\sqrt{n_i}\\). \\(\\E(\\epsilon)_^2\\) is the sum of values. Then $^2_i = n_i ^2 $. In WLS, \\(w_i = \\sqrt{n_i}\\). If \\(p_i^{-1}\\) is the inverse-sampling probability weight, then weight by \\(w_i\\) Suppose that the heteroskedasticity is known up to a multiplicative constant, \\[ \\Var(\\varepsilon_i | \\mat{X}) = a_i \\sigma^2 , \\] where \\(a_i = a_i \\vec{x}_i\\T\\) is a positive and known function of \\(\\vec{x}_i\\). Define the weighting matrix, \\[ \\mat{W} = \\begin{bmatrix} 1 / \\sqrt{a_1} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 / \\sqrt{a_2} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; 0 \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 / \\sqrt{a_N} \\end{bmatrix}, \\] and run the regression, \\[ \\begin{aligned}[t] \\mat{W} y &amp;= \\mat{W} \\mat{X} \\vec{\\beta} + \\mat{W} \\vec\\varepsilon \\\\ \\vec{y}^* &amp;= \\mat{X}^* \\vec{\\beta} + \\vec{\\varepsilon}^* . \\end{aligned} \\] Run the regression of \\(\\vec{y}^*\\) on \\(\\mat{X}^*\\), and the Gauss-Markov assumptions are satisfied. Then using the usual OLS formula, \\[ \\hat{\\vec\\beta}_{WLS} = ((\\mat{X}^*)&#39; \\mat{X}^*) (\\mat{X}^*)&#39; \\vec{y}^* = (\\mat{X}&#39; \\mat{W}&#39; \\mat{W} \\mat{X})^{-1} \\mat{X}&#39; \\mat{W}&#39; \\mat{W} \\vec{y} . \\] 5.4 Sampling Weights Sampling weights are the inverse probabilities of selection that are used to weight a sample to be representative of population (as if were a random draw from the population). In this situation, whether to use sampling weights depends on whether you are calculating If you are calculating a descriptive statistic from the sample as an estimator of a population parameter, you need to use weights if sample weights are a function of \\(X\\) only, estimates are unbiased and more efficient without weighting if the sample weights are a function of \\(Y | X\\), then use the weights With fixed \\(X\\), regression does not require random sampling, so the sampling weights of the \\(X\\) are irrelevant. If the original unweighted data are homoskedastic, then sampling weights induces heteroskedasticity. Suppose the true model is, \\[ Y_i = \\vec{x}\\T \\vec{\\beta} + \\varepsilon_i \\] where \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\). Then the weighted model is, \\[ \\sqrt{w_i} Y_i = \\sqrt{w_i} \\vec{x}\\T \\vec{\\beta} + \\sqrt{w_i} \\varepsilon_i \\] and now \\(\\sqrt{w_i} \\varepsilon_i \\sim N(0, w_i \\sigma^2)\\). If the sampling weights are only a function of the \\(X\\), then controlling for \\(X\\) is sufficient. In fact, OLS is preferred to WLS, and will produce unbiased and efficient estimates. The choice between OLS and WLS is a choice between different distributions of \\(\\mat{X}\\). However, if the model is specified correctly the coefficients should be the same, regardless of the distribution of \\(\\mat{X}\\). Thus, if the estimates of OLS and WLS differ, then it is evidence that the model is misspecified. Winship and Radbill (1994) suggest using the method of Dumouchel and Duncan (1983) to test whether the OLS and WLS are difference. Estimate \\(E(Y) = \\mat{X} \\beta\\) Estimate \\(E(Y) = \\mat{X} \\beta + \\delta \\vec{w} + \\vec{\\gamma} \\vec{w} \\mat{X}\\), where all \\(X\\) Test regression 1 vs. regression 2 using an F test. If the F-test is significant, then the weights are not simply a function of \\(X\\). Either try to respecify the model or use WLS with robust standard errors. If the F-test is insignificant, then the weights are simply a function of \\(X\\). Use OLS. Modern survey often use complex multi-stage sampling designs. Like clustering generally, this will affect the standard errors of these regressions. Clustering by primary sampling units is a good approximation of the standard errors from multistage sampling. 5.5 References The WLS derivation can be found in Fox (2016, 304–6, 335–36, 461). Other textbook discussions: Angrist and Pischke (2009, 91–94), Angrist and Pischke (2014), [p. 202-203], Davidson and MacKinnon (2004, 261–62), Wooldridge (2012, 409–13). Solon, Haider, and Wooldridge (2015) is a good (and recent) overview with practical advice of when to weight and when not-to weight linear regressions. Also see the advice from the World Bank blog. See also Deaton (1997), Dumouchel and Duncan (1983), and Wissoker (1999). Gelman (2007b), in the context of post-stratification, proposes controlling for variables related to selection into the sample instead of using survey weights; also see the responses (Bell and Cohen 2007; Breidt and Opsomer 2007; Little 2007; Pfeffermann 2007), and rejoinder (Gelman 2007a) and blog post. Gelman’s approach is similar to that earlier suggested by Winship and Radbill (1994). For survey weighting, see the R package survey. References "],
["rs-forumula-syntax.html", "Chapter 6 R’s Forumula Syntax 6.1 Setup 6.2 Introduction to Formula Objects 6.3 Programming with Formulas", " Chapter 6 R’s Forumula Syntax These notes build off of the topics discussed in the chapter Many Models in R for Data Science. It uses the functionals (map() function) for iteration, string functions, and list columns in data frames. 6.1 Setup data(&quot;Duncan&quot;, package = &quot;car&quot;) library(&quot;tidyverse&quot;) library(&quot;stringr&quot;) library(&quot;broom&quot;) 6.2 Introduction to Formula Objects Many R functions, especially those for statistical models such as lm(), use a convenient syntax to compactly specify the outcome and predictor variables. This format is described in more detail in the stats. Formula objects in R are created with the tilde operator (~), and can be either one- or two-sided. prestige ~ type + income * education ~ prestige + type + income Formula are used in a variety of different contexts in R, e.g. ggplot2, but are commonly associated with statistical modeling functions as a compact way to specify the outcome and design matrix. lm(formula = prestige ~ type + income * education, data = Duncan) estimates this, \\[ \\mathtt{prestige} = \\beta_0 + \\beta_1 \\mathtt{income} + \\beta_2 \\mathtt{education} + \\beta_3 \\mathtt{income} \\times \\mathtt{education} . \\] Note that in a formula, the operators + and * do not refer to addition or multiplication. Instead adding + adds terms to the regression, and * creates interactions. Symbol Example Meaning + +x Include \\(x\\) - -x Exclude \\(x\\) : x : z Include \\(x z\\) as a predictor * x * z Include \\(x\\), \\(z\\), and their interaction \\(x z\\) as predictors ^ (x + w + z) ^ 3 Include variables and interactions up to three way: \\(x\\), \\(z\\), \\(w\\), \\(xz\\), \\(xw\\), \\(zw\\), \\(xzw\\). I I(x ^ 2) as is: include a new variable with \\(x ^ 2\\). 1 -1 Intercept; Use -1 to delete, and +1 to include Formula Equation y ~ x + y + z \\(y = \\beta_0 + beta_1 x + beta_2 y + beta_3 z\\) y ~ x + y - 1 \\(y = \\beta_1 x + \\beta_2 y\\) y ~ 1 \\(y = \\beta_0\\) y ~ x:z \\(y = \\beta_0 + \\beta_1 xz\\) y ~ x*z \\(y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 xz\\) y ~ x*z - x \\(y = \\beta_0 + \\beta_1 z + \\beta_2 xz\\) y ~ (x + z)^2 \\(y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3\\) y ~ (x + z + w)^2 \\(y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 w + \\beta_4 w + \\beta_5 xz + \\beta_6 xw + \\beta_7 zw\\) y ~ (x + z + w)^2 \\(y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 w + \\beta_4 w + \\beta_5 xz + \\beta_6 xw + \\beta_7 zw + \\beta_8 xzw\\) y ~ x + I(x ^ 2) \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\) y ~ poly(x, 2) \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\) y ~ I(x * z) \\(y = \\beta_0 + \\beta_1 xz\\) y ~ I(x + z) + I(x - z) \\(y = \\beta_0 + \\beta_1 (x + z) + \\beta_2 (x - z)\\) y ~ log(x) \\(y = \\beta_0 + \\beta_1 \\log(x)\\) y ~ x / z \\(y = \\beta_0 + \\beta_1 (x / z)\\) y ~ x + 0 \\(y = \\beta_1 x\\) Formula objects provide a convenient means of specifying the statistical model and also generating temporary variables that we only needed in the model. For example, if we want to include \\(\\log(x)\\), \\(z\\), their interaction, and the square of \\(z\\), we could write y ~ log(x) * y + I(y ^ 2) instead of of having to create new columns with the log, square, and interaction variables before running the regression. This becomes especially useful if you need to include large polynomials, splines, interaction, or similarly complicated functional forms. Warning: Often you will want to include polynomials in a regression. However, ^ already has a special meaning in R’s syntax, so you cannot use x ^ 2. There are two ways to specify polynomials. - Use the as is operator, I(). For example, I(x ^ 2) includes \\(x^2\\) in the regression. - Use the poly function to generate polynomials. For example, I(x ^ 2) will include \\(x\\) and \\(x^2\\) in the regression. This can be more convenient and clear than writing x + I(x ^ 2)$. Warning: R’s formula syntax is flexible and includes more features than what was covered in this section. What was described in this section were features that lm() uses. Other functions may have more features or the parts will have different meanings. However, they will be mostly be similar to what was described here. An example of a function that has slightly different syntax for its formula is lme4 which adds notation for random and fixed effects. Some functions like stats use a formula syntax even though they aren’t statistical modeling functions. The package Formula provides an even more powerful Formula syntax than that included with base R. If you have to write a function or package that uses a formula, use that package. Inspired by this handout from Richard Hahn and the handout form the NPS. 6.3 Programming with Formulas In these examples, we’ll use the car dataset in the car package. Prestige &lt;- car::Prestige Each observation is an occupation, and contains the prestige score of the occupation from a survey, and the average education, income, percentage of women, and type of occupation. glimpse(Prestige) ## Observations: 102 ## Variables: 6 ## $ education &lt;dbl&gt; 13.11, 12.26, 12.77, 11.42, 14.62, 15.64, 15.09, 15.... ## $ income &lt;int&gt; 12351, 25879, 9271, 8865, 8403, 11030, 8258, 14163, ... ## $ women &lt;dbl&gt; 11.16, 4.02, 15.70, 9.11, 11.68, 5.13, 25.65, 2.69, ... ## $ prestige &lt;dbl&gt; 68.8, 69.1, 63.4, 56.8, 73.5, 77.6, 72.6, 78.1, 73.1... ## $ census &lt;int&gt; 1113, 1130, 1171, 1175, 2111, 2113, 2133, 2141, 2143... ## $ type &lt;fctr&gt; prof, prof, prof, prof, prof, prof, prof, prof, pro... We will run several regressions with prestige as the outcome variable, and the over variables are explanatory variables. In R, the formulas are objects (of class &quot;formula&quot;). That means we can program on them, and importantly, perhaps avoid excessive copying and pasting if we run multiple models. A formula object is created with the ~ operator: f &lt;- prestige ~ type + education class(f) ## [1] &quot;formula&quot; f ## prestige ~ type + education A useful function for working with formulas is update. The update function allows you to easily update a formula object # the . is replaced by the original formula values update(f, . ~ income) ## prestige ~ income update(f, income ~ .) ## income ~ type + education update(f, . ~ . + type + women) ## prestige ~ type + education + women Also note that many types of models have update method which will rerun the model with a new formula. Sometimes this can help computational time if the model is able to reuse some previous results or data. You can also create formula objects from a character vector as.formula(&quot;prestige ~ income + education&quot;) ## prestige ~ income + education This means that you can create model formula objects programmatically which is useful if you are running many models, or simply to keep the logic of your code clear. xvars &lt;- c(&quot;type&quot;, &quot;income&quot;, &quot;education&quot;) as.formula(str_c(&quot;prestige&quot;, &quot;~&quot;, str_c(xvars, collapse = &quot; + &quot;))) ## prestige ~ type + income + education Often you will need to run multiple models. Since most often the only thing that changes between models is the formula (the outcome or response variables), storing the formula in a list, and then running the models by iterating through the list is a clean strategy for estimating your models. xvar_list &lt;- list(c(&quot;type&quot;), c(&quot;income&quot;), c(&quot;education&quot;), c(&quot;type&quot;, &quot;income&quot;), c(&quot;type&quot;, &quot;income&quot;, &quot;education&quot;)) formulae &lt;- vector(&quot;list&quot;, length(xvar_list)) for (i in seq_along(xvar_list)) { formulae[[i]] &lt;- str_c(&quot;prestige ~ &quot;, str_c(xvar_list[[i]], collapse = &quot; + &quot;)) } formulae ## [[1]] ## [1] &quot;prestige ~ type&quot; ## ## [[2]] ## [1] &quot;prestige ~ income&quot; ## ## [[3]] ## [1] &quot;prestige ~ education&quot; ## ## [[4]] ## [1] &quot;prestige ~ type + income&quot; ## ## [[5]] ## [1] &quot;prestige ~ type + income + education&quot; Alternatively, create this list of formula objects with a functional, make_mod_f &lt;- function(x) { str_c(&quot;prestige ~ &quot;, str_c(x, collapse = &quot; + &quot;)) } formulae &lt;- map(xvar_list, make_mod_f) Now that we have a list with formula objects for each model that we want to run, we can loop over the list and run each model. But first, we need to create a function that runs a single model that returns a data frame with a single row and a column named mod, which is a list column with an lm object containing the fitted model. In this function, I set model = FALSE because by default an lm model stores the data used to estimate. This is convenient, but if you are estimating many models, this can consume much memory. run_reg &lt;- function(f) { mod &lt;- lm(f, data = Prestige, model = FALSE) data_frame(mod = list(mod)) } ret &lt;- run_reg(formulae[[1]]) ret[[&quot;mod&quot;]][[1]] ## ## Call: ## lm(formula = f, data = Prestige, model = FALSE) ## ## Coefficients: ## (Intercept) typeprof typewc ## 35.527 32.321 6.716 Since each data frame has only one row, it is not particularly useful on its own, but it will be convenient to keep all the models in a data frame. Now, run run_reg for each formula in formulae using map_df to return the results as a data frame with a list column, mod, containing the lm objects. prestige_fits &lt;- map_df(formulae, run_reg, .id = &quot;.id&quot;) prestige_fits ## # A tibble: 5 × 2 ## .id mod ## &lt;chr&gt; &lt;list&gt; ## 1 1 &lt;S3: lm&gt; ## 2 2 &lt;S3: lm&gt; ## 3 3 &lt;S3: lm&gt; ## 4 4 &lt;S3: lm&gt; ## 5 5 &lt;S3: lm&gt; To extract the original formulas and add them to the data set, run formula() on each lm object using map, and then convert it to a character string using deparse: prestige_fits &lt;- prestige_fits %&gt;% mutate(formula = map_chr(mod, ~ deparse(formula(.x)))) prestige_fits$formula ## [1] &quot;prestige ~ type&quot; ## [2] &quot;prestige ~ income&quot; ## [3] &quot;prestige ~ education&quot; ## [4] &quot;prestige ~ type + income&quot; ## [5] &quot;prestige ~ type + income + education&quot; Get a data frame of the coefficients for all models using tidy and tidyr: mutate(prestige_fits, x = map(mod, tidy)) %&gt;% unnest(x) ## # A tibble: 16 × 7 ## .id formula term estimate ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 prestige ~ type (Intercept) 35.527272727 ## 2 1 prestige ~ type typeprof 32.321114370 ## 3 1 prestige ~ type typewc 6.716205534 ## 4 2 prestige ~ income (Intercept) 27.141176368 ## 5 2 prestige ~ income income 0.002896799 ## 6 3 prestige ~ education (Intercept) -10.731981968 ## 7 3 prestige ~ education education 5.360877731 ## 8 4 prestige ~ type + income (Intercept) 27.997056941 ## 9 4 prestige ~ type + income typeprof 25.055473883 ## 10 4 prestige ~ type + income typewc 7.167155112 ## 11 4 prestige ~ type + income income 0.001401196 ## 12 5 prestige ~ type + income + education (Intercept) -0.622929165 ## 13 5 prestige ~ type + income + education typeprof 6.038970651 ## 14 5 prestige ~ type + income + education typewc -2.737230718 ## 15 5 prestige ~ type + income + education income 0.001013193 ## 16 5 prestige ~ type + income + education education 3.673166052 ## # ... with 3 more variables: std.error &lt;dbl&gt;, statistic &lt;dbl&gt;, ## # p.value &lt;dbl&gt; Get a data frame of model summary statistics for all models using glance, mutate(prestige_fits, x = map(mod, glance)) %&gt;% unnest(x) ## # A tibble: 5 × 14 ## .id mod formula r.squared ## &lt;chr&gt; &lt;list&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 &lt;S3: lm&gt; prestige ~ type 0.6976287 ## 2 2 &lt;S3: lm&gt; prestige ~ income 0.5110901 ## 3 3 &lt;S3: lm&gt; prestige ~ education 0.7228007 ## 4 4 &lt;S3: lm&gt; prestige ~ type + income 0.7764569 ## 5 5 &lt;S3: lm&gt; prestige ~ type + income + education 0.8348574 ## # ... with 10 more variables: adj.r.squared &lt;dbl&gt;, sigma &lt;dbl&gt;, ## # statistic &lt;dbl&gt;, p.value &lt;dbl&gt;, df &lt;int&gt;, logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, ## # BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt; "],
["duncan-occupational-prestige.html", "Chapter 7 Duncan Occupational Prestige 7.1 Setup 7.2 Coefficients, Standard errors 7.3 Residuals, Fitted Values, 7.4 Broom 7.5 Plotting Fitted Regression Results", " Chapter 7 Duncan Occupational Prestige 7.1 Setup library(&quot;tidyverse&quot;) library(&quot;broom&quot;) This example makes use of the Duncan Occpuational prestige included in the car package. This is data from a classic sociology paper and contains data on the prestige and other characteristics of 45 U.S. occupations in 1950. data(&quot;Duncan&quot;, package = &quot;car&quot;) The dataset Duncan contains four variables: type, income, education, and prestige, glimpse(Duncan) ## Observations: 45 ## Variables: 4 ## $ type &lt;fctr&gt; prof, prof, prof, prof, prof, prof, prof, prof, wc,... ## $ income &lt;int&gt; 62, 72, 75, 55, 64, 21, 64, 80, 67, 72, 42, 76, 76, ... ## $ education &lt;int&gt; 86, 76, 92, 90, 86, 84, 93, 100, 87, 86, 74, 98, 97,... ## $ prestige &lt;int&gt; 82, 83, 90, 76, 90, 87, 93, 90, 52, 88, 57, 89, 97, ... You run a regression in R using the function lm. This runs a linear regression of occupational prestige on income, lm(prestige ~ income, data = Duncan) ## ## Call: ## lm(formula = prestige ~ income, data = Duncan) ## ## Coefficients: ## (Intercept) income ## 2.457 1.080 This estimates the linear regression \\[ \\mathtt{prestige} = \\beta_0 + \\beta_1 \\mathtt{income} \\] In R, \\(\\beta_0\\) is named (Intercept), and the other coefficients are named after the associated predictor. The function lm returns an lm object that can be used in future computations. Instead of printing the regression result to the screen, save it to the variable mod1, mod1 &lt;- lm(prestige ~ income, data = Duncan) We can print this object print(mod1) ## ## Call: ## lm(formula = prestige ~ income, data = Duncan) ## ## Coefficients: ## (Intercept) income ## 2.457 1.080 Somewhat counterintuitively, the summary function returns more information about a regression, summary(mod1) ## ## Call: ## lm(formula = prestige ~ income, data = Duncan) ## ## Residuals: ## Min 1Q Median 3Q Max ## -46.566 -9.421 0.257 9.167 61.855 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.4566 5.1901 0.473 0.638 ## income 1.0804 0.1074 10.062 7.14e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.4 on 43 degrees of freedom ## Multiple R-squared: 0.7019, Adjusted R-squared: 0.695 ## F-statistic: 101.3 on 1 and 43 DF, p-value: 7.144e-13 The summary function also returns an object that we can use later, summary_mod1 &lt;- summary(mod1) summary_mod1 ## ## Call: ## lm(formula = prestige ~ income, data = Duncan) ## ## Residuals: ## Min 1Q Median 3Q Max ## -46.566 -9.421 0.257 9.167 61.855 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.4566 5.1901 0.473 0.638 ## income 1.0804 0.1074 10.062 7.14e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.4 on 43 degrees of freedom ## Multiple R-squared: 0.7019, Adjusted R-squared: 0.695 ## F-statistic: 101.3 on 1 and 43 DF, p-value: 7.144e-13 Now lets estimate a multiple linear regression, mod2 &lt;- lm(prestige ~ income + education + type, data = Duncan) mod2 ## ## Call: ## lm(formula = prestige ~ income + education + type, data = Duncan) ## ## Coefficients: ## (Intercept) income education typeprof typewc ## -0.1850 0.5975 0.3453 16.6575 -14.6611 7.2 Coefficients, Standard errors Coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\): coef(mod2) ## (Intercept) income education typeprof typewc ## -0.1850278 0.5975465 0.3453193 16.6575134 -14.6611334 Variance-covariance matrix of the coefficients, \\(\\Var{\\hat{\\boldsymbol{\\beta}}}\\): vcov(mod2) ## (Intercept) income education typeprof typewc ## (Intercept) 13.7920916 -0.115636760 -0.257485549 14.0946963 7.9021988 ## income -0.1156368 0.007984369 -0.002924489 -0.1260105 -0.1090485 ## education -0.2574855 -0.002924489 0.012906986 -0.6166508 -0.3881200 ## typeprof 14.0946963 -0.126010517 -0.616650831 48.9021401 30.2138627 ## typewc 7.9021988 -0.109048528 -0.388119979 30.2138627 37.3171167 The standard errors of the coefficients, \\(\\se{\\hat{\\boldsymbol{\\beta}}}\\), are the square root diagonal of the vcov matrix, sqrt(diag(vcov(mod2))) ## (Intercept) income education typeprof typewc ## 3.7137705 0.0893553 0.1136089 6.9930065 6.1087737 This can be confirmed by comparing their values to those in the summary table, summary(mod2) ## ## Call: ## lm(formula = prestige ~ income + education + type, data = Duncan) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.890 -5.740 -1.754 5.442 28.972 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.18503 3.71377 -0.050 0.96051 ## income 0.59755 0.08936 6.687 5.12e-08 *** ## education 0.34532 0.11361 3.040 0.00416 ** ## typeprof 16.65751 6.99301 2.382 0.02206 * ## typewc -14.66113 6.10877 -2.400 0.02114 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.744 on 40 degrees of freedom ## Multiple R-squared: 0.9131, Adjusted R-squared: 0.9044 ## F-statistic: 105 on 4 and 40 DF, p-value: &lt; 2.2e-16 7.3 Residuals, Fitted Values, To get the fitted or predicted values (\\(\\hat{\\mathbf{y}} = \\mathbf{X} \\hat{\\boldsymbol\\beta}\\)) from a regression, mod1_fitted &lt;- fitted(mod1) head(mod1_fitted) ## accountant pilot architect author chemist minister ## 69.44073 80.24463 83.48580 61.87801 71.60151 25.14476 or mod1_predict &lt;- predict(mod1) head(mod1_predict) ## accountant pilot architect author chemist minister ## 69.44073 80.24463 83.48580 61.87801 71.60151 25.14476 The difference between predict and fitted is how they handle missing values in the data. Fitted values will not include predictions for missing values in the data, while predict will include values for Using predict, we can also predict values for new data. For example, create a data frame with each category of type, and in which income and education are set to their mean values. Duncan_at_means &lt;- data.frame(type = unique(Duncan$type), income = mean(Duncan$income), education = mean(Duncan$education)) Duncan_at_means ## type income education ## 1 prof 41.86667 52.55556 ## 2 wc 41.86667 52.55556 ## 3 bc 41.86667 52.55556 Now use this with the newdata argument, predict(mod2, newdata = Duncan_at_means) ## 1 2 3 ## 59.63821 28.31957 42.98070 To get the residuals (\\(\\hat{\\boldsymbol{\\epsilon}} = \\mathbf{y} - \\hat{\\mathbf{y}}\\)). mod1_resid &lt;- residuals(mod1) head(mod1_resid) ## accountant pilot architect author chemist minister ## 12.559266 2.755369 6.514200 14.121993 18.398486 61.855242 7.4 Broom The package broom has some functions that reformat the results of statistical modeling functions (t.test, lm, etc.) to data frames that work nicer with ggplot2, dplyr, and friends. The broom package has three main functions: glance: Information about the model. tidy: Information about the estimated parameters augment: The original data with estimates of the model. glance: Always return a one-row data.frame that is a summary of the model: e.g. R2, adjusted R2, etc. glance(mod2) ## r.squared adj.r.squared sigma statistic p.value df logLik ## 1 0.9130657 0.9043723 9.744171 105.0294 1.170871e-20 5 -163.6522 ## AIC BIC deviance df.residual ## 1 339.3045 350.1444 3797.955 40 tidy: Transforms into a ready-to-go data.frame the coefficients, SEs (and CIs if given), critical values, and p-values in statistical tests’ outputs tidy(mod2) ## term estimate std.error statistic p.value ## 1 (Intercept) -0.1850278 3.7137705 -0.0498221 9.605121e-01 ## 2 income 0.5975465 0.0893553 6.6873093 5.123720e-08 ## 3 education 0.3453193 0.1136089 3.0395443 4.164463e-03 ## 4 typeprof 16.6575134 6.9930065 2.3820246 2.206245e-02 ## 5 typewc -14.6611334 6.1087737 -2.4000125 2.114015e-02 augment: Add columns to the original data that was modeled. This includes predictions, estandard error of the predictions, residuals, and others. augment(mod2) %&gt;% head() ## .rownames prestige income education type .fitted .se.fit .resid ## 1 accountant 82 62 86 prof 83.21783 2.352262 -1.217831 ## 2 pilot 83 72 76 prof 85.74010 2.674659 -2.740102 ## 3 architect 90 75 92 prof 93.05785 2.755775 -3.057851 ## 4 author 76 55 90 prof 80.41628 2.589351 -4.416282 ## 5 chemist 90 64 86 prof 84.41292 2.360632 5.587076 ## 6 minister 87 21 84 prof 58.02779 4.260837 28.972214 ## .hat .sigma .cooksd .std.resid ## 1 0.05827491 9.866259 0.0002052803 -0.1287893 ## 2 0.07534370 9.857751 0.0013936701 -0.2924366 ## 3 0.07998300 9.855093 0.0018611391 -0.3271700 ## 4 0.07061418 9.841004 0.0033585648 -0.4701256 ## 5 0.05869037 9.825129 0.0043552315 0.5909809 ## 6 0.19120532 8.412639 0.5168053288 3.3061127 .fitted: the model predictions for all observations .se.fit: the estandard error of the predictions .resid: the residuals of the predictions (acual - predicted values) .sigma: is the standard error of the prediction. The other columns—.hat, .cooksd, and .std.resid are used in regression diagnostics. 7.5 Plotting Fitted Regression Results Consider the regression of prestige on income, mod3 &lt;- lm(prestige ~ income, data = Duncan) This creates a new dataset with the column income and 100 observations between the min and maximum observed incomes in the Duncan dataset. mod3_newdata &lt;- data_frame(income = seq(min(Duncan$income), max(Duncan$income), length.out = 100)) We will calculate fitted values for all these values of income. ggplot() + geom_point(data = Duncan, mapping = aes(x = income, y = prestige), colour = &quot;gray75&quot;) + geom_line(data = augment(mod3, newdata = mod3_newdata), mapping = aes(x = income, y = .fitted)) + ylab(&quot;Prestige&quot;) + xlab(&quot;Income&quot;) + theme_minimal() Now plot something similar, but for a regression with income interacted with type, mod4 &lt;- lm(prestige ~ income * type, data = Duncan) We want to create a dataset which has, (1) each value of type in the Duncan data, and (2) values spanning the range of income in the Duncan data. The function expand.grid creates a data frame with all combinations of vectors given to it (Cartesian product). mod4_newdata &lt;- expand.grid(income = seq(min(Duncan$income), max(Duncan$income), length.out = 100), type = unique(Duncan$type)) Now plot the fitted values evaluated at each of these values along wite original values in the data, ggplot() + geom_point(data = Duncan, mapping = aes(x = income, y = prestige, color = type)) + geom_line(data = augment(mod4, newdata = mod4_newdata), mapping = aes(x = income, y = .fitted, color = type)) + ylab(&quot;Prestige&quot;) + xlab(&quot;Income&quot;) + theme_minimal() Running geom_smooth with method = &quot;lm&quot; gives similar results. However, note that geom_smooth with run a separate regression for each group. ggplot(data = Duncan, aes(x = income, y = prestige, color = type)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + ylab(&quot;Prestige&quot;) + xlab(&quot;Income&quot;) + theme_minimal() "],
["yule-pauperism-data.html", "Chapter 8 Yule Pauperism Data 8.1 Summary Statistics 8.2 Regressions 8.3 Summary Statistics 8.4 Pauperism 8.5 Specifications 8.6 Multiple regression anatomy", " Chapter 8 Yule Pauperism Data library(&quot;tidyverse&quot;) library(&quot;modelr&quot;) ## ## Attaching package: &#39;modelr&#39; ## The following object is masked from &#39;package:broom&#39;: ## ## bootstrap library(&quot;viridis&quot;) library(&quot;broom&quot;) The Yule Pauperism data is included in the package datums at jrnold/datums. Yule (1899) was the published example multiple regression analysis in its modern form.4 Yule wrote this paper to analyze the effect of policy changes and implementation on pauperism (poor receiving benefits) in England. under the https://en.wikipedia.org/wiki/English_Poor_Laws. In 1834, a new poor law was passed that established a national welfare system in England and Wales. The New Poor Law created new administrative districts, Poor Law Unions, to adminster the law. Most importantly, it attempted to standardize the provision of aid to the poor. There were two types of aid provided: in-relief, aid provided to paupers in workhouses where they resided, and out-relief, aid provided to paupers residing at home. The New Poor Law wanted to decrease in-relief and increase out-relief in the belief that out-relief, in particular the quality of life in workhouses, was a deterrence to poverty and an encouragement for the poor to work harder to avoid poverty. The data in datums is It consists of two datasets: pauperism_plu contains data on the Poor Law Unions, while pauperism_year has the PLU, year as the unit of observation and contains data on the levels of pauperism in 1871, 1881, and 1891 in each PLU. library(&quot;tidyverse&quot;) # install_github/jrnold pauperism_plu &lt;- datums::pauperism_plu pauperism_year &lt;- datums::pauperism_year glimpse(pauperism_year) Instead of taking differences, or percentages. Yule worked with “percent ratio differences”, \\(100 \\times x_t / x_{t - 1}\\), because he did not want to work with negative signs, presumably a concern at the because he was doing arithmetic by hand and this would make calculations more tedious or error-prone. pctratiodiff &lt;- function(x) { z &lt;- 100 * (x / lag(x)) z[is.infinite(z)] &lt;- NA_real_ z } pauperism &lt;- pauperism_year %&gt;% mutate(Popn65 = F65 + M65, Prop65 = Popn65 / Popn, year = as.integer(year)) %&gt;% arrange(ID, year) %&gt;% group_by(ID) %&gt;% mutate(Prop65_diff = pctratiodiff(Prop65)) %&gt;% left_join(pauperism_plu, by = &quot;ID&quot;) %&gt;% filter(Type != &quot;NaN&quot;) %&gt;% ungroup() Table 1. of pauperism pauperism %&gt;% filter(year &gt;= 1881) %&gt;% select(ID, year, Type, paupratiodiff, outratiodiff, Prop65_diff, popratiodiff) %&gt;% drop_na() %&gt;% count(year, Type) ## Source: local data frame [8 x 3] ## Groups: year [?] ## ## year Type n ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 1881 Metropolitan 32 ## 2 1881 Mixed 217 ## 3 1881 Rural 238 ## 4 1881 Urban 101 ## 5 1891 Metropolitan 32 ## 6 1891 Mixed 218 ## 7 1891 Rural 239 ## 8 1891 Urban 103 The number of districts is different, but I’m not sure why. Table 2: Metropolitan Group, 1871-1881 filter(pauperism, Type == &quot;Metropolitan&quot;) %&gt;% filter(year == 1881) %&gt;% select(ID, Union, paupratiodiff, outratiodiff, Prop65_diff, popratiodiff) %&gt;% arrange(ID) %&gt;% ungroup() %&gt;% select(-ID) %&gt;% knitr::kable() Union paupratiodiff outratiodiff Prop65_diff popratiodiff Kensington 27.02455 4.709511 104.24513 135.62124 Fulham 30.55314 20.691155 85.41113 173.89046 Paddington 47.13245 12.820014 115.12124 110.74752 Chelsea 64.00997 20.957361 80.78122 123.96855 StGeorge 46.26695 18.696592 113.18960 96.03171 Westminster 52.01266 26.439934 104.88340 90.94977 StMarylebone 81.13188 35.865384 99.68465 97.27228 StJohnHampstead 60.85194 38.687664 102.89827 140.80109 StPancras 60.55554 34.762259 101.01971 106.67961 StMaryIslington 59.22427 34.920438 100.83279 132.31717 Hackney 33.04587 21.644359 91.27279 149.22810 StGGBloomsbury 75.50703 29.319882 102.77106 84.73747 Strand 63.99014 26.916741 96.73724 81.41880 Holborn 78.52338 33.517874 94.42741 93.09890 CityLondon 78.66401 64.365584 112.37948 67.80155 StLeonardShoreditch 51.70655 20.521901 108.23606 99.54940 BethnalGreen 46.38214 19.192527 101.73429 105.70922 Whitechapel 35.30723 6.246978 92.03899 94.45547 StGeorgeEast 36.67330 6.135770 99.03527 98.13743 Stepney 33.86640 9.516070 87.06120 101.47859 MileEndOldTown 42.59398 13.266601 102.07680 113.37706 Poplar 38.50528 19.930617 102.62700 134.48649 StSaviourSouthwark 52.46445 22.451416 99.60833 111.49107 StOlaveSouthwark 56.83153 32.200945 102.36756 109.99526 Lambeth 56.46941 38.253650 98.30586 121.77045 WandsworthandClapham 23.21714 17.946851 90.69907 168.26643 Camberwell 29.90475 13.835835 82.55591 167.63966 Greenwich 54.65175 37.920863 93.50556 130.45030 Woolwich 76.97900 15.186659 118.71024 110.17307 Lewisham 39.55357 27.421469 101.25774 140.71416 Croydon 47.72780 29.186036 101.44845 142.12133 WestHam 38.40110 48.914488 85.53301 202.69714 Table 3 pauperism %&gt;% filter(year &gt;= 1881) %&gt;% select(year, Type, paupratiodiff, outratiodiff, Prop65_diff, popratiodiff) %&gt;% drop_na() %&gt;% gather(variable, value, -year, -Type) %&gt;% group_by(year, Type, variable) %&gt;% summarise_at(vars(value), funs(mean, sd)) %&gt;% knitr::kable() year Type variable mean sd 1881 Metropolitan outratiodiff 25.07636 12.979986 1881 Metropolitan paupratiodiff 50.61654 16.303219 1881 Metropolitan popratiodiff 119.90865 30.090299 1881 Metropolitan Prop65_diff 99.13926 9.176468 1881 Mixed outratiodiff 70.81556 27.968580 1881 Mixed paupratiodiff 66.39643 17.478868 1881 Mixed popratiodiff 110.71895 14.749708 1881 Mixed Prop65_diff 100.75271 21.403143 1881 Rural outratiodiff 72.19569 27.100992 1881 Rural paupratiodiff 66.18899 15.492810 1881 Rural popratiodiff 97.61334 7.550125 1881 Rural Prop65_diff 103.72348 8.713096 1881 Urban outratiodiff 64.57293 25.469154 1881 Urban paupratiodiff 73.85648 26.387059 1881 Urban popratiodiff 123.32655 34.457872 1881 Urban Prop65_diff 106.89375 76.126030 1891 Metropolitan outratiodiff 90.84679 42.912778 1891 Metropolitan paupratiodiff 104.67066 29.836570 1891 Metropolitan popratiodiff 111.31099 24.296891 1891 Metropolitan Prop65_diff 107.59174 5.524179 1891 Mixed outratiodiff 109.63868 45.965573 1891 Mixed paupratiodiff 91.09768 19.467070 1891 Mixed popratiodiff 108.46061 12.832222 1891 Mixed Prop65_diff 106.87802 18.705844 1891 Rural outratiodiff 116.88662 48.513561 1891 Rural paupratiodiff 91.59067 20.409368 1891 Rural popratiodiff 97.44270 6.741761 1891 Rural Prop65_diff 107.54229 8.369285 1891 Urban outratiodiff 97.18114 33.819458 1891 Urban paupratiodiff 86.23436 29.512905 1891 Urban popratiodiff 119.53807 31.484939 1891 Urban Prop65_diff 116.51129 95.171090 Table 4: Correlations of Percentage Ratios pauperism %&gt;% filter(year &gt;= 1881) %&gt;% select(year, Type, paupratiodiff, outratiodiff, Prop65_diff, popratiodiff) %&gt;% drop_na() %&gt;% group_by(year, Type) %&gt;% do({ cor(.[ , c(&quot;paupratiodiff&quot;, &quot;outratiodiff&quot;, &quot;Prop65_diff&quot;, &quot;popratiodiff&quot;)]) %&gt;% tidy() %&gt;% gather(.colnames, value, -.rownames) %&gt;% filter(.rownames &lt; .colnames) %&gt;% unite(variable, .rownames, .colnames) %&gt;% spread(variable, value) }) ## Source: local data frame [8 x 8] ## Groups: year, Type [8] ## ## year Type outratiodiff_paupratiodiff outratiodiff_popratiodiff ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1881 Metropolitan 0.5731816 -0.0092212754 ## 2 1881 Mixed 0.3173784 0.0776315386 ## 3 1881 Rural 0.4140595 -0.0602981249 ## 4 1881 Urban 0.6561705 0.0898857241 ## 5 1891 Metropolitan 0.5151787 0.2268864595 ## 6 1891 Mixed 0.3717615 0.0488693203 ## 7 1891 Rural 0.5034687 0.0003006584 ## 8 1891 Urban 0.2814298 -0.0159747065 ## # ... with 4 more variables: outratiodiff_Prop65_diff &lt;dbl&gt;, ## # paupratiodiff_popratiodiff &lt;dbl&gt;, paupratiodiff_Prop65_diff &lt;dbl&gt;, ## # popratiodiff_Prop65_diff &lt;dbl&gt; \\[ \\begin{aligned}[t] \\Delta\\mathtt{Paup} &amp;= \\beta_0 \\\\ &amp;+ \\beta_1 \\Delta\\mathtt{Out} \\\\ &amp;+ \\beta_2 \\Delta\\mathtt{Old} \\\\ &amp;+ \\beta_3 \\Delta\\mathtt{Pop} + \\varepsilon \\end{aligned} \\] 8.1 Summary Statistics filter(pauperism, year &gt; 1871) %&gt;% ungroup() %&gt;% count(year, Type) ## Source: local data frame [8 x 3] ## Groups: year [?] ## ## year Type n ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 1881 Metropolitan 32 ## 2 1881 Mixed 220 ## 3 1881 Rural 240 ## 4 1881 Urban 103 ## 5 1891 Metropolitan 32 ## 6 1891 Mixed 220 ## 7 1891 Rural 240 ## 8 1891 Urban 103 filter(pauperism, year &gt; 1871) %&gt;% filter(Type != &quot;NaN&quot;) %&gt;% group_by(year, Type) %&gt;% select(paupratiodiff, outratiodiff, Prop65_diff, popratiodiff) %&gt;% gather(variable, value, -Type, -year) %&gt;% group_by(variable, year, Type) %&gt;% summarize(mean = mean(value, na.rm = TRUE), sd = sd(value, na.rm = TRUE)) %&gt;% knitr::kable() ## Adding missing grouping variables: `year`, `Type` variable year Type mean sd outratiodiff 1881 Metropolitan 25.07636 12.979986 outratiodiff 1881 Mixed 70.71569 27.942993 outratiodiff 1881 Rural 72.24664 27.055468 outratiodiff 1881 Urban 64.57293 25.469154 outratiodiff 1891 Metropolitan 90.84679 42.912778 outratiodiff 1891 Mixed 109.82761 45.945173 outratiodiff 1891 Rural 116.73896 48.465973 outratiodiff 1891 Urban 97.18114 33.819458 paupratiodiff 1881 Metropolitan 50.61654 16.303219 paupratiodiff 1881 Mixed 66.46855 17.471028 paupratiodiff 1881 Rural 66.18899 15.492810 paupratiodiff 1881 Urban 73.85648 26.387059 paupratiodiff 1891 Metropolitan 104.67066 29.836570 paupratiodiff 1891 Mixed 91.09768 19.467070 paupratiodiff 1891 Rural 91.59067 20.409368 paupratiodiff 1891 Urban 86.23436 29.512905 popratiodiff 1881 Metropolitan 119.90865 30.090299 popratiodiff 1881 Mixed 110.65842 14.706062 popratiodiff 1881 Rural 97.55477 7.546160 popratiodiff 1881 Urban 124.81552 36.422994 popratiodiff 1891 Metropolitan 111.31099 24.296891 popratiodiff 1891 Mixed 108.01843 14.739470 popratiodiff 1891 Rural 97.03669 9.209995 popratiodiff 1891 Urban 119.53807 31.484939 Prop65_diff 1881 Metropolitan 99.13926 9.176468 Prop65_diff 1881 Mixed 100.71849 21.359748 Prop65_diff 1881 Rural 103.74897 8.703695 Prop65_diff 1881 Urban 106.89375 76.126030 Prop65_diff 1891 Metropolitan 107.59174 5.524179 Prop65_diff 1891 Mixed 106.87802 18.705844 Prop65_diff 1891 Rural 107.54229 8.369285 Prop65_diff 1891 Urban 116.51129 95.171090 8.2 Regressions lm(pauper ~ outratio, data = pauperism) ## ## Call: ## lm(formula = pauper ~ outratio, data = pauperism) ## ## Coefficients: ## (Intercept) outratio ## 0.024637 0.002408 lm(pauper ~ year + Type + outratio, data = pauperism) ## ## Call: ## lm(formula = pauper ~ year + Type + outratio, data = pauperism) ## ## Coefficients: ## (Intercept) year TypeMixed TypeRural TypeUrban ## 1.8736845 -0.0009799 -0.0014571 0.0038099 -0.0083008 ## outratio ## 0.0014681 lm(pauper ~ year + Type + outratio + Prop65 + Popn65, data = pauperism) ## ## Call: ## lm(formula = pauper ~ year + Type + outratio + Prop65 + Popn65, ## data = pauperism) ## ## Coefficients: ## (Intercept) year TypeMixed TypeRural TypeUrban ## 1.999e+00 -1.052e-03 -1.405e-02 -1.600e-02 -1.099e-02 ## outratio Prop65 Popn65 ## 1.417e-03 4.421e-01 -1.301e-06 lm(pauper ~ Type * (year + outratio + Prop65 + Popn65), data = pauperism) ## ## Call: ## lm(formula = pauper ~ Type * (year + outratio + Prop65 + Popn65), ## data = pauperism) ## ## Coefficients: ## (Intercept) TypeMixed TypeRural ## 1.515e+00 3.711e-01 1.021e+00 ## TypeUrban year outratio ## -3.650e-01 -7.964e-04 2.063e-03 ## Prop65 Popn65 TypeMixed:year ## 5.683e-01 -1.790e-06 -2.010e-04 ## TypeRural:year TypeUrban:year TypeMixed:outratio ## -5.580e-04 1.958e-04 -8.536e-04 ## TypeRural:outratio TypeUrban:outratio TypeMixed:Prop65 ## -5.522e-04 -6.385e-04 -8.643e-02 ## TypeRural:Prop65 TypeUrban:Prop65 TypeMixed:Popn65 ## 6.153e-02 -3.845e-01 -2.140e-06 ## TypeRural:Popn65 TypeUrban:Popn65 ## 2.853e-06 4.012e-07 ggplot(filter(pauperism, !is.na(pauper), !is.na(outratio)), aes(x = outratio, y = pauper)) + geom_point(colour = &quot;gray&quot;) + geom_smooth(method = &quot;lm&quot;, colour = &quot;black&quot;, se = FALSE) + facet_grid(Type ~ year) + theme_minimal() pauperism_diff &lt;- pauperism %&gt;% filter(year &gt; 1871) %&gt;% mutate(year = as.factor(year)) %&gt;% select(ID, Union, Type, year, paupratiodiff, outratiodiff, popratiodiff, Prop65_diff) %&gt;% drop_na() lm(paupratiodiff ~ outratiodiff, data = pauperism_diff) ## ## Call: ## lm(formula = paupratiodiff ~ outratiodiff, data = pauperism_diff) ## ## Coefficients: ## (Intercept) outratiodiff ## 51.8476 0.3062 lm(paupratiodiff ~ Type * year + outratiodiff, data = pauperism_diff) ## ## Call: ## lm(formula = paupratiodiff ~ Type * year + outratiodiff, data = pauperism_diff) ## ## Coefficients: ## (Intercept) TypeMixed TypeRural ## 44.9546 5.4525 4.9335 ## TypeUrban year1891 outratiodiff ## 14.3221 39.2040 0.2258 ## TypeMixed:year1891 TypeRural:year1891 TypeUrban:year1891 ## -23.2685 -23.8930 -34.1886 lm(paupratiodiff ~ Type * year + outratiodiff + popratiodiff + Prop65_diff, data = pauperism_diff) ## ## Call: ## lm(formula = paupratiodiff ~ Type * year + outratiodiff + popratiodiff + ## Prop65_diff, data = pauperism_diff) ## ## Coefficients: ## (Intercept) TypeMixed TypeRural ## 76.82129 3.25804 -0.21806 ## TypeUrban year1891 outratiodiff ## 15.33249 37.36729 0.22808 ## popratiodiff Prop65_diff TypeMixed:year1891 ## -0.23418 -0.03878 -21.81233 ## TypeRural:year1891 TypeUrban:year1891 ## -22.05079 -32.94109 lm(paupratiodiff ~ (Type * year) * (outratiodiff + Prop65_diff + popratiodiff), data = pauperism_diff) ## ## Call: ## lm(formula = paupratiodiff ~ (Type * year) * (outratiodiff + ## Prop65_diff + popratiodiff), data = pauperism_diff) ## ## Coefficients: ## (Intercept) TypeMixed ## 5.777e+01 -6.535e+00 ## TypeRural TypeUrban ## -8.292e+01 -2.639e+00 ## year1891 outratiodiff ## -7.264e+01 7.082e-01 ## Prop65_diff popratiodiff ## 1.082e-01 -2.973e-01 ## TypeMixed:year1891 TypeRural:year1891 ## 1.607e+02 7.619e+01 ## TypeUrban:year1891 TypeMixed:outratiodiff ## 1.310e+02 -5.095e-01 ## TypeRural:outratiodiff TypeUrban:outratiodiff ## -4.775e-01 -2.505e-02 ## TypeMixed:Prop65_diff TypeRural:Prop65_diff ## -9.975e-02 3.717e-01 ## TypeUrban:Prop65_diff TypeMixed:popratiodiff ## -1.675e-01 2.994e-01 ## TypeRural:popratiodiff TypeUrban:popratiodiff ## 5.524e-01 1.428e-01 ## year1891:outratiodiff year1891:Prop65_diff ## -3.761e-01 1.102e+00 ## year1891:popratiodiff TypeMixed:year1891:outratiodiff ## -6.927e-02 3.356e-01 ## TypeRural:year1891:outratiodiff TypeUrban:year1891:outratiodiff ## 3.612e-01 -7.082e-02 ## TypeMixed:year1891:Prop65_diff TypeRural:year1891:Prop65_diff ## -1.292e+00 -9.311e-01 ## TypeUrban:year1891:Prop65_diff TypeMixed:year1891:popratiodiff ## -1.073e+00 -3.577e-01 ## TypeRural:year1891:popratiodiff TypeUrban:year1891:popratiodiff ## -8.724e-04 -1.660e-01 8.3 Summary Statistics 8.3.1 Outratio ggplot(select(filter(pauperism, !is.na(outratio)), outratio, ID, year, Type), aes(x = outratio, y = ..density..)) + geom_histogram(binwidth = 2) + facet_grid(year ~ Type) ggplot(select(filter(pauperism, !is.na(outratiodiff)), outratiodiff, ID, year, Type), aes(x = outratiodiff, y = ..density..)) + geom_histogram(binwidth = 20) + facet_grid(year ~ Type) 8.4 Pauperism ggplot(select(filter(pauperism, !is.na(pauper)), pauper, ID, year, Type), aes(x = pauper, y = ..density..)) + geom_histogram(binwidth = .01) + facet_grid(year ~ Type) There appear to be some big outliers in the ratio difference in pauperism, ggplot(select(filter(pauperism, !is.na(paupratiodiff)), paupratiodiff, ID, year, Type), aes(x = paupratiodiff, y = ..density..)) + geom_histogram(binwidth = 15) + facet_grid(year ~ Type) 8.5 Specifications Dependent variable proportion paupers number paupers log(paupers) log(proportion paupers) Controlling for various: outratio, inratio population number log(number) proportion 65+ proportion number log(number) datums::pauperism_year %&gt;% filter(year == 1871) %&gt;% select(outratio, pauper2) %&gt;% drop_na() %&gt;% ggplot(aes(x = outratio, pauper2)) + geom_point() + geom_smooth(method = &quot;lm&quot;) datums::pauperism_year %&gt;% filter(year == 1871) %&gt;% select(outratio, pauper2) %&gt;% drop_na() %&gt;% cor() ## outratio pauper2 ## outratio 1.0000000 0.2105665 ## pauper2 0.2105665 1.0000000 Regressions for each year datums::pauperism_year %&gt;% group_by(year) %&gt;% summarise(mod = list(lm(outratio ~ pauper2, data = .))) ## # A tibble: 3 × 2 ## year mod ## &lt;int&gt; &lt;list&gt; ## 1 1871 &lt;S3: lm&gt; ## 2 1881 &lt;S3: lm&gt; ## 3 1891 &lt;S3: lm&gt; datums::pauperism_year %&gt;% filter(year == 1871) %&gt;% select(outratio, pauper2) %&gt;% drop_na() %&gt;% lm(pauper2 ~ outratio, data = .) ## ## Call: ## lm(formula = pauper2 ~ outratio, data = .) ## ## Coefficients: ## (Intercept) outratio ## 0.043637 0.001218 mod_1871 &lt;- datums::pauperism_year %&gt;% filter(year == 1871) %&gt;% select(outratio, pauper2) %&gt;% {lm(pauper2 ~ outratio, data = .)} ggplot(augment(mod_1871), aes(x = outratio)) + geom_ref_line(v = mean(filter(pauperism_year, year == 1871)$outratio, na.rm = TRUE)) + geom_point(mapping = aes(y = pauper2, size = .hat, colour = .hat)) + geom_path(mapping = aes(y = .fitted)) + scale_color_viridis() The observations that have the highest weight in determining the fitted values are those furthest from the mean of X. Hat values in multidimensional space: # MDS pauperism_mds &lt;- pauperism %&gt;% filter(year == 1891) %&gt;% select(outratiodiff, popratiodiff, Prop65_diff) %&gt;% drop_na() %&gt;% dist() %&gt;% cmdscale() %&gt;% tidy() lm(paupratiodiff ~ outratiodiff + popratiodiff + Prop65_diff, data = filter(pauperism, year == 1891)) %&gt;% augment() %&gt;% bind_cols(pauperism_mds) %&gt;% ggplot(aes(x = X1, y = X2, size = .hat, colour = .hat)) + geom_ref_line(h = 0) + geom_ref_line(v = 0) + geom_point() 8.6 Multiple regression anatomy \\[ \\beta_k = \\frac{\\Cov(Y_i, \\tilde{X}_{k,i})}{\\Var{\\tilde{X}_{ki}}} \\] where \\(\\tidle{X}_{ki}\\) is the residual of the regression of \\(X\\) on all the variables other than \\(K\\). Regress paupratio on Prop65_diff: pauperism_diff &lt;- select(pauperism, paupratiodiff, outratiodiff, Prop65_diff, popratiodiff) %&gt;% drop_na() mod_prop65_diff &lt;- lm(paupratiodiff ~ Prop65_diff, data = pauperism_diff) pauperism_diff$.fitted1 &lt;- fitted(mod_prop65_diff) pauperism_diff$.resid1 &lt;- residuals(mod_prop65_diff) mod_popratiodiff &lt;- lm(.resid1 ~ popratiodiff, data = pauperism_diff) pauperism_diff$.fitted2 &lt;- fitted(mod_popratiodiff) pauperism_diff$.resid2 &lt;- residuals(mod_popratiodiff) Regress residuals on outratiodiff mod_popratiodiff &lt;- lm(.resid2 ~ outratiodiff, data = pauperism_diff) pauperism_diff$.fitted3 &lt;- fitted(mod_popratiodiff) pauperism_diff$.resid3 &lt;- residuals(mod_popratiodiff) Relationship between the omitted variables \\(Z\\) and \\(X\\) Relationship between the omitted variable and \\(Y\\) OVB formula. Angrist and Pischke (2014, 70) Effect of \\(X\\) in short = Effect of \\(X\\) in long + (Relationship between included and omitted) x effect of omitted in long. OVB = (Relationship between \\(X\\) and \\(Z\\)) x (effect of \\(Z\\) in long) References "],
["formatting-tables.html", "Chapter 9 Formatting Tables 9.1 Overview of Packages 9.2 Summary Statistic Table Example 9.3 Regression Table Example", " Chapter 9 Formatting Tables 9.1 Overview of Packages R has multiple packages and functions for directly producing formatted tables for LaTeX, HTML, and other output formats. Given the See the Reproducible Research Task View for an overview of various options. xtable is a general purpose package for creating LaTeX, HTML, or plain text tables in R. texreg is more specifically geared to regression tables. It also outputs results in LaTeX (texreg), HTML (texreg), and plain text. The packages stargazer and apsrtable are other popular packages for formatting regression output. However, they are less-well maintained and have less functionality than texreg. For example, apsrtable hasn’t been updated since 2012, stargazer since 2015. The texreg vignette is a good introduction to texreg, and also discusses the These blog posts by Will Lowe cover many of the options. Additionally, for simple tables, knitr, the package which provides the heavy lifting for R markdown, has a function knitr. knitr also has the ability to customize how R objects are printed with the knit_print function. Other notable packages are: pander creates output in markdown for export to other formats. tables uses a formula syntax to define tables ReportR has the most complete support for creating Word documents, but is likely too much. For a political science perspective on why automating the research process is important see: Nicholas Eubank Embrace Your Fallibility: Thoughts on Code Integrity, based on this article Matthew Gentzkow Jesse M. Shapiro.Code and Data for the Social Sciences: A Practitioner’s Guide. March 10, 2014. Political Methodologist issue on Workflow Management 9.2 Summary Statistic Table Example The xtable package has methods to convert many types of R objects to tables. library(&quot;gapminder&quot;) gapminder_summary &lt;- gapminder %&gt;% # Keep numeric variables select_if(is.numeric) %&gt;% # gather variables gather(variable, value) %&gt;% # Summarize by variable group_by(variable) %&gt;% # summarise all columns summarise(n = sum(!is.na(value)), `Mean` = mean(value), `Std. Dev.` = sd(value), `Median` = median(value), `Min.` = min(value), `Max.` = max(value)) gapminder_summary ## # A tibble: 4 × 7 ## variable n Mean `Std. Dev.` Median Min. ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 gdpPercap 1704 7.215327e+03 9.857455e+03 3531.8470 241.1659 ## 2 lifeExp 1704 5.947444e+01 1.291711e+01 60.7125 23.5990 ## 3 pop 1704 2.960121e+07 1.061579e+08 7023595.5000 60011.0000 ## 4 year 1704 1.979500e+03 1.726533e+01 1979.5000 1952.0000 ## # ... with 1 more variables: Max. &lt;dbl&gt; Now that we have a data frame with the table we want, use xtable to create it: library(&quot;xtable&quot;) foo &lt;- xtable(gapminder_summary, digits = 0) %&gt;% print(type = &quot;html&quot;, html.table.attributes = &quot;&quot;, include.rownames = FALSE, format.args = list(big.mark = &quot;,&quot;)) variable n Mean Std. Dev. Median Min. Max. gdpPercap 1,704 7,215 9,857 3,532 241 113,523 lifeExp 1,704 59 13 61 24 83 pop 1,704 29,601,212 106,157,897 7,023,596 60,011 1,318,683,096 year 1,704 1,980 17 1,980 1,952 2,007 Note that there we two functions to get HTML. The function xtable creates an xtable R object, and the function xtable (called as print()), which prints the xtable object as HTML (or LaTeX). The default HTML does not look nice, and would need to be formatted with CSS. If you are copy and pasting it into Word, you would do some post-processing cleanup anyways. Another alternative is the knitr function in the knitr package, which outputs R markdown tables. knitr::kable(gapminder_summary) variable n Mean Std. Dev. Median Min. Max. gdpPercap 1704 7.215327e+03 9.857455e+03 3531.8470 241.1659 1.135231e+05 lifeExp 1704 5.947444e+01 1.291711e+01 60.7125 23.5990 8.260300e+01 pop 1704 2.960121e+07 1.061579e+08 7023595.5000 60011.0000 1.318683e+09 year 1704 1.979500e+03 1.726533e+01 1979.5000 1952.0000 2.007000e+03 This is useful for producing quick tables. Finally, htmlTables package unsurprisingly produces HTML tables. library(&quot;htmlTable&quot;) htmlTable(txtRound(gapminder_summary, 0), align = &quot;lrrrr&quot;) variable n Mean Std. Dev. Median Min. Max. 1 gdpPercap 1704 7 10 3532 241 1 2 lifeExp 1704 6 1 61 24 8 3 pop 1704 3 1 7023596 60011 1 4 year 1704 2 2 1980 1952 2 It has more features for producing HTML tables than xtable, but does not output LaTeX. 9.3 Regression Table Example library(&quot;tidyverse&quot;) library(&quot;texreg&quot;) We will run several regression models with the Duncan data Prestige &lt;- car::Prestige Since I’m running several regressions, I will save them to a list. If you know that you will be creating multiple objects, and programming with them, always put them in a list. First, create a list of the regression formulas, formulae &lt;- list( prestige ~ type, prestige ~ income, prestige ~ education, prestige ~ type + education + income ) Write a function to run a single model, Now use map to run a regression with each of these formulae, and save them to a list, prestige_mods &lt;- map(formulae, ~ lm(.x, data = Prestige, model = FALSE)) This is a list of lm objects, map(prestige_mods, class) ## [[1]] ## [1] &quot;lm&quot; ## ## [[2]] ## [1] &quot;lm&quot; ## ## [[3]] ## [1] &quot;lm&quot; ## ## [[4]] ## [1] &quot;lm&quot; We can look at the first model, prestige_mods[[1]] ## ## Call: ## lm(formula = .x, data = Prestige, model = FALSE) ## ## Coefficients: ## (Intercept) typeprof typewc ## 35.527 32.321 6.716 Now we can format the regression table in HTML using htmlreg. The first argument of htmlreg is a list of models: htmlreg(prestige_mods) Statistical models Model 1 Model 2 Model 3 Model 4 (Intercept) 35.53*** 27.14*** -10.73** -0.62 (1.43) (2.27) (3.68) (5.23) typeprof 32.32*** 6.04 (2.23) (3.87) typewc 6.72** -2.74 (2.44) (2.51) income 0.00*** 0.00*** (0.00) (0.00) education 5.36*** 3.67*** (0.33) (0.64) R2 0.70 0.51 0.72 0.83 Adj. R2 0.69 0.51 0.72 0.83 Num. obs. 98 102 102 98 RMSE 9.50 12.09 9.10 7.09 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 By default, htmlreg() prints out HTML, which is exactly what I want in an R markdown document. To save the output to a file, specify a non-null file argument. For example, to save the table to the file prestige.html, htmlreg(prestige_mods, file = &quot;prestige.html&quot;) Since this function outputs HTML directly to the console, it can be hard to tell what’s going on. If you want to preview the table in RStudio while working on it, this snippet of code uses htmltools package to do so: library(&quot;htmltools&quot;) htmlreg(prestige_mods) %&gt;% HTML() %&gt;% browsable() The htmlreg function has many options to adjust the table formatting. Below, I clean up the table. I remove stars using stars = NULL. It is a growing convention to avoid the use of stars indicating significance in regression tables (see AJPS and Political Analysis guidelines). The arguments doctype, html.tag, head.tag, body.tag control what sort of HTML is created. Generally all these functions (whether LaTeX or HTML output) have some arguments that determine whether it is creating a standalone, complete document, or a fragment that will be copied into another dcoument. The arguments include.rsquared, include.adjrs, and include.nobs are passed to the function extract() which determines what information the texreg package extracts from a model to put into the table. I get rid of \\(R^2\\), but keep adjusted \\(R^2\\), and the number of observations. library(&quot;stringr&quot;) coefnames &lt;- c(&quot;Professional&quot;, &quot;Working Class&quot;, &quot;Income&quot;, &quot;Education&quot;) note &lt;- &quot;OLS regressions with prestige as the response variable.&quot; htmlreg(prestige_mods, stars = NULL, custom.model.names = str_c(&quot;(&quot;, seq_along(prestige_mods), &quot;)&quot;), custom.coef.names = coefnames, custom.note = str_c(&quot;Note: &quot;, note), omit.coef = &quot;(Intercept)&quot;, caption.above = TRUE, caption = &quot;Regressions of Occupational Prestige&quot;, # better for markdown doctype = FALSE, html.tag = FALSE, head.tag = FALSE, body.tag = FALSE, # passed to extract() method for &quot;lm&quot; include.adjr = TRUE, include.rsquared = FALSE, include.rmse = FALSE, include.nobs = TRUE) Regressions of Occupational Prestige (1) (2) (3) (4) Professional 32.32 6.04 (2.23) (3.87) Working Class 6.72 -2.74 (2.44) (2.51) Income 0.00 0.00 (0.00) (0.00) Education 5.36 3.67 (0.33) (0.64) Adj. R2 0.69 0.51 0.72 0.83 Num. obs. 98 102 102 98 Note: OLS regressions with prestige as the response variable. Once you find a set of options that are common across your tables, make a function so you con’t need to retype them. my_reg_table &lt;- function(mods, ..., note = NULL) { htmlreg(mods, stars = NULL, custom.note = if (!is.null(note)) str_c(&quot;Note: &quot;, note) else NULL, caption.above = TRUE, # better for markdown doctype = FALSE, html.tag = FALSE, head.tag = FALSE) } my_reg_table(prestige_mods, custom.model.names = str_c(&quot;(&quot;, seq_along(prestige_mods), &quot;)&quot;), custom.coef.names = coefnames, note = note, # put intercept at the bottom reorder.coef = c(2, 3, 4, 5, 1), caption = &quot;Regressions of Occupational Prestige&quot;) Statistical models Model 1 Model 2 Model 3 Model 4 (Intercept) 35.53 27.14 -10.73 -0.62 (1.43) (2.27) (3.68) (5.23) typeprof 32.32 6.04 (2.23) (3.87) typewc 6.72 -2.74 (2.44) (2.51) income 0.00 0.00 (0.00) (0.00) education 5.36 3.67 (0.33) (0.64) R2 0.70 0.51 0.72 0.83 Adj. R2 0.69 0.51 0.72 0.83 Num. obs. 98 102 102 98 RMSE 9.50 12.09 9.10 7.09 Note: OLS regressions with prestige as the response variable. Note that I didn’t include every option in my_reg_table, only those arguments that will be common across tables. I use ... to pass arguments to htmlreg. Then when I call my_reg_table the only arguments are those specific to the content of the table, not the formatting, making it easier to understand what each table is saying. Of course, texreg also produces LaTeX output, with the function texreg. Almost all the options are the same as htmlreg. "],
["reproducible-research.html", "Chapter 10 Reproducible Research", " Chapter 10 Reproducible Research "],
["writing-resources.html", "Chapter 11 Writing Resources 11.1 Writing and Organizing Papers 11.2 Finding Research Ideas 11.3 Replications", " Chapter 11 Writing Resources 11.1 Writing and Organizing Papers Here are a few useful resources for writing papers: Chris Adolph. Writing Empirical Papers: 6 Rules &amp; 12 Recommendations Barry R. Weingast. 2015. Caltech Rules for Writing Papers: How to Structure Your Paper and Write an Introduction The Science of Scientific Writing American Scientist Deidre McCloskey. Economical Writing William Thompson. A Guide for the Young Economist. “Chapter 2: Writing Papers.” Stephen Van Evera. Guide to Methods for Students of Political Science. Appendix. Joseph M. Williams and Joseph Bizup. Style: Lessons in Clarity and Grace Strunk and White. The Elements of Style Chicago Manual of Style and APSA Style Manual for Political Science for editorial and style issues. How to construct a Nature summary paragraph. Though specifi to Nature is good advice for structuring abstracts or introductions. Ezra Klein. How researchers are terrible communications, and how they can do better. The advice in the AJPS Instructions for Submitting Authors is a concise description of how to write an abstract: The abstract should provide a very concise descriptive summary of the research stream to which the manuscript contributes, the specific research topic it addresses, the research strategy employed for the analysis, the results obtained from the analysis, and the implications of the findings. Concrete Advice for Writing Informative Abstracts and pHow to Carefully Choose Useless Titles for Academic Writing](http://www.socialsciencespace.com/2014/03/how-to-carefully-choose-useless-titles-for-academic-writing/) 11.2 Finding Research Ideas Paul Krugman How I Work Hal Varian. How to build an Economic Model in your spare time Greg Mankiw, My Rules of Thumb: links in Advice for Grad Students 11.3 Replications Gary King has advice on how to turn a replication into a publishable paper: Gary King How to Write a Publishable Paper as a Class Project Gary King. 2006. “Publication, Publication.” PS: Political Science and Politics. Political Science Should Not Stop Young Researchers from Replicating from the Political Science Replication blog. And see the examples of students replications from his Harvard course at https://politicalsciencereplication.wordpress.com/ Famous replications. David Broockman, Joahua Kalla, and Peter Aronow. 2015. Irregularities in LaCour (2014). Homas Herndon, Michael Ash &amp; Robert Pollin (2013). Does High Public Debt Consistently Stifle Economic Growth? A Critique of Reinhart and Rogoff. Working Paper Series 322. Political Economy Research Institute. [URL] However, although those replications are famous for finding fraud or obvious errors in the analysis, replications can lead to extensions and generate new ideas. This was the intent of Brookman, Kalla, and Aronow when starting the replication. "],
["multivariate-normal-distribution.html", "A Multivariate Normal Distribution", " A Multivariate Normal Distribution library(&quot;tidyverse&quot;) library(&quot;viridis&quot;) library(&quot;mvtnorm&quot;) The multivariate normal distribution is the generalization of the univariate normal distribution to more than one dimension.5 The random variable, \\(\\vec{x}\\), is a length \\(k\\) vector. The \\(k\\) length vector \\(\\vec{\\mu}\\) are the means of \\(\\vec{x}\\), and the \\(k \\times k\\) matrix, \\(\\mat{\\Sigma}\\), is the variance-covariance matrix, \\[ \\begin{aligned}[t] \\vec{x} &amp;\\sim \\dmvnorm{k}\\left(\\vec{\\mu}, \\mat{\\Sigma} \\right) \\\\ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_k \\end{bmatrix} &amp; \\sim \\dmvnorm{k} \\left( \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_k \\end{bmatrix}, \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{1,2} &amp; \\cdots &amp; \\sigma_{1, k} \\\\ \\sigma_{2,1} &amp; \\sigma_2^2 &amp; \\cdots &amp; \\sigma_{2, k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{k,1} &amp; \\sigma_{k,2} &amp; \\cdots &amp; \\sigma_{k, k} \\end{bmatrix} \\right) \\end{aligned} \\] The density function of the multivariate normal is, \\[ p(\\vec{x}; \\vec{\\mu}, \\mat{\\Sigma}) = (2 k)^{-\\frac{k}{2}} \\left| \\mat{\\Sigma} \\right|^{-\\frac{1}{2}} \\exp \\left( -\\frac{1}{2} (\\vec{x} - \\vec{\\mu})\\T \\mat{\\Sigma}^{-1} (\\vec{x} - \\vec{\\mu}) \\right) . \\] You can sample from and calculate the density for the multivariate normal distribution with the functions dmvnorm and rmvnorm from the package mvtnorm. Density plots of different bivariate normal distributions, See Multivariate normal distribution and references therein.↩ "],
["references-1.html", "References", " References "]
]
