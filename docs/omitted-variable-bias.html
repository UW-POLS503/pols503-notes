<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Data Analysis Notes</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>">
  <meta name="generator" content="bookdown 0.3.6 and GitBook 2.6.7">

  <meta property="og:title" content="Data Analysis Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>" />
  <meta name="github-repo" content="jrnold/intro-methods-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Data Analysis Notes" />
  
  <meta name="twitter:description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>" />
  

<meta name="author" content="Jeffrey B. Arnold">


<meta name="date" content="2017-04-29">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="regression-inference.html">
<link rel="next" href="outliers.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<link href="libs/plotlyjs-1.16.3/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.16.3/plotly-latest.min.js"></script>
<script src="libs/plotly-binding-4.5.6/plotly.js"></script>



<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intro Method Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="part"><span><b>I Exploratory Data Analysis</b></span></li>
<li class="part"><span><b>II Probability</b></span></li>
<li class="part"><span><b>III Inference</b></span></li>
<li class="part"><span><b>IV Linear Regresssion</b></span></li>
<li class="chapter" data-level="2" data-path="bivariate-ols.html"><a href="bivariate-ols.html"><i class="fa fa-check"></i><b>2</b> Bivariate OLS</a><ul>
<li class="chapter" data-level="2.0.1" data-path="bivariate-ols.html"><a href="bivariate-ols.html#ols-is-the-weighted-sum-of-outcomes"><i class="fa fa-check"></i><b>2.0.1</b> OLS is the weighted sum of outcomes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html"><i class="fa fa-check"></i><b>3</b> Goodness of Fit</a><ul>
<li class="chapter" data-level="3.1" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html#root-mean-squared-error-and-standard-error"><i class="fa fa-check"></i><b>3.1</b> Root Mean Squared Error and Standard Error</a></li>
<li class="chapter" data-level="3.2" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html#r-squared"><i class="fa fa-check"></i><b>3.2</b> R squared</a></li>
<li class="chapter" data-level="3.3" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html#maximum-likelihood"><i class="fa fa-check"></i><b>3.3</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="3.4" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html#regression-line"><i class="fa fa-check"></i><b>3.4</b> Regression Line</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>4</b> Multiple Regression</a></li>
<li class="chapter" data-level="5" data-path="what-is-regression.html"><a href="what-is-regression.html"><i class="fa fa-check"></i><b>5</b> What is Regression?</a><ul>
<li class="chapter" data-level="5.1" data-path="what-is-regression.html"><a href="what-is-regression.html#what-is-regression-and-what-is-it-used-for"><i class="fa fa-check"></i><b>5.1</b> What is Regression and What is it Used For ?</a></li>
<li class="chapter" data-level="5.2" data-path="what-is-regression.html"><a href="what-is-regression.html#joint-vs.conditional-models"><i class="fa fa-check"></i><b>5.2</b> Joint vs.Â Conditional models</a></li>
<li class="chapter" data-level="5.3" data-path="what-is-regression.html"><a href="what-is-regression.html#conditional-expectation-function"><i class="fa fa-check"></i><b>5.3</b> Conditional expectation function</a><ul>
<li class="chapter" data-level="5.3.1" data-path="what-is-regression.html"><a href="what-is-regression.html#discrete-covariates"><i class="fa fa-check"></i><b>5.3.1</b> Discrete Covariates</a></li>
<li class="chapter" data-level="5.3.2" data-path="what-is-regression.html"><a href="what-is-regression.html#continuous-covariates"><i class="fa fa-check"></i><b>5.3.2</b> Continuous Covariates</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="interpreting-coefficients.html"><a href="interpreting-coefficients.html"><i class="fa fa-check"></i><b>6</b> Interpreting Coefficients</a><ul>
<li class="chapter" data-level="6.1" data-path="interpreting-coefficients.html"><a href="interpreting-coefficients.html#interactions-and-polynomials"><i class="fa fa-check"></i><b>6.1</b> Interactions and Polynomials</a></li>
<li class="chapter" data-level="6.2" data-path="interpreting-coefficients.html"><a href="interpreting-coefficients.html#average-marginal-effects"><i class="fa fa-check"></i><b>6.2</b> Average Marginal Effects</a></li>
<li class="chapter" data-level="6.3" data-path="interpreting-coefficients.html"><a href="interpreting-coefficients.html#standardized-coefficients"><i class="fa fa-check"></i><b>6.3</b> Standardized Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="regression-inference.html"><a href="regression-inference.html"><i class="fa fa-check"></i><b>7</b> Regression Inference</a><ul>
<li class="chapter" data-level="7.1" data-path="regression-inference.html"><a href="regression-inference.html#prerequisites"><i class="fa fa-check"></i><b>7.1</b> Prerequisites</a></li>
<li class="chapter" data-level="7.2" data-path="regression-inference.html"><a href="regression-inference.html#sampling-distribution-and-standard-errors-of-coefficients"><i class="fa fa-check"></i><b>7.2</b> Sampling Distribution and Standard Errors of Coefficients</a></li>
<li class="chapter" data-level="7.3" data-path="regression-inference.html"><a href="regression-inference.html#single-coefficient"><i class="fa fa-check"></i><b>7.3</b> Single Coefficient</a><ul>
<li class="chapter" data-level="7.3.1" data-path="regression-inference.html"><a href="regression-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>7.3.1</b> Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="regression-inference.html"><a href="regression-inference.html#multiple-coefficients"><i class="fa fa-check"></i><b>7.4</b> Multiple Coefficients</a><ul>
<li class="chapter" data-level="7.4.1" data-path="regression-inference.html"><a href="regression-inference.html#f-test"><i class="fa fa-check"></i><b>7.4.1</b> F-test</a></li>
<li class="chapter" data-level="7.4.2" data-path="regression-inference.html"><a href="regression-inference.html#confidence-regions"><i class="fa fa-check"></i><b>7.4.2</b> Confidence Regions</a></li>
<li class="chapter" data-level="7.4.3" data-path="regression-inference.html"><a href="regression-inference.html#linear-hypothesis-tests"><i class="fa fa-check"></i><b>7.4.3</b> Linear Hypothesis Tests</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="regression-inference.html"><a href="regression-inference.html#linear-and-non-linear-confidence-intervals"><i class="fa fa-check"></i><b>7.5</b> Linear and Non-Linear Confidence Intervals</a></li>
<li class="chapter" data-level="7.6" data-path="regression-inference.html"><a href="regression-inference.html#multiple-testing"><i class="fa fa-check"></i><b>7.6</b> Multiple Testing</a></li>
<li class="chapter" data-level="7.7" data-path="regression-inference.html"><a href="regression-inference.html#data-snooping"><i class="fa fa-check"></i><b>7.7</b> Data snooping</a></li>
<li class="chapter" data-level="7.8" data-path="regression-inference.html"><a href="regression-inference.html#power"><i class="fa fa-check"></i><b>7.8</b> Power</a></li>
<li class="chapter" data-level="7.9" data-path="regression-inference.html"><a href="regression-inference.html#prediction-intervals"><i class="fa fa-check"></i><b>7.9</b> Prediction Intervals</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html"><i class="fa fa-check"></i><b>8</b> Omitted Variable Bias</a><ul>
<li class="chapter" data-level="8.1" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#prerequisites-1"><i class="fa fa-check"></i><b>8.1</b> Prerequisites</a></li>
<li class="chapter" data-level="8.2" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#simpsons-paradox"><i class="fa fa-check"></i><b>8.2</b> Simpsonâs Paradox</a></li>
<li class="chapter" data-level="8.3" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#omitted-variable-bias-1"><i class="fa fa-check"></i><b>8.3</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="8.4" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#measurement-error"><i class="fa fa-check"></i><b>8.4</b> Measurement Error</a><ul>
<li class="chapter" data-level="8.4.1" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#whats-the-problem"><i class="fa fa-check"></i><b>8.4.1</b> Whatâs the problem?</a></li>
<li class="chapter" data-level="8.4.2" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#what-to-do-about-it"><i class="fa fa-check"></i><b>8.4.2</b> What to do about it?</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#more-information"><i class="fa fa-check"></i><b>8.5</b> More Information</a><ul>
<li class="chapter" data-level="8.5.1" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#simpsons-paradox-1"><i class="fa fa-check"></i><b>8.5.1</b> Simpsonâs Paradox</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="outliers.html"><a href="outliers.html"><i class="fa fa-check"></i><b>9</b> Outliers</a><ul>
<li class="chapter" data-level="9.1" data-path="outliers.html"><a href="outliers.html#iver-and-soskice-data"><i class="fa fa-check"></i><b>9.1</b> Iver and Soskice Data</a></li>
<li class="chapter" data-level="9.2" data-path="outliers.html"><a href="outliers.html#influential-observations"><i class="fa fa-check"></i><b>9.2</b> Influential Observations</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="problems-with-errors.html"><a href="problems-with-errors.html"><i class="fa fa-check"></i><b>10</b> Problems with Errors</a><ul>
<li class="chapter" data-level="10.1" data-path="problems-with-errors.html"><a href="problems-with-errors.html#prerequisites-2"><i class="fa fa-check"></i><b>10.1</b> Prerequisites</a></li>
<li class="chapter" data-level="10.2" data-path="problems-with-errors.html"><a href="problems-with-errors.html#heteroskedasticity"><i class="fa fa-check"></i><b>10.2</b> Heteroskedasticity</a><ul>
<li class="chapter" data-level="10.2.1" data-path="problems-with-errors.html"><a href="problems-with-errors.html#example-duncans-occupation-data"><i class="fa fa-check"></i><b>10.2.1</b> Example: Duncanâs Occupation Data</a></li>
<li class="chapter" data-level="10.2.2" data-path="problems-with-errors.html"><a href="problems-with-errors.html#notes"><i class="fa fa-check"></i><b>10.2.2</b> Notes</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="problems-with-errors.html"><a href="problems-with-errors.html#non-normal-errors"><i class="fa fa-check"></i><b>10.3</b> Non-normal Errors</a></li>
<li class="chapter" data-level="10.4" data-path="problems-with-errors.html"><a href="problems-with-errors.html#clustered-standard-errors"><i class="fa fa-check"></i><b>10.4</b> Clustered Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="weighted-regression.html"><a href="weighted-regression.html"><i class="fa fa-check"></i><b>11</b> Weighted Regression</a><ul>
<li class="chapter" data-level="11.1" data-path="weighted-regression.html"><a href="weighted-regression.html#weighted-least-squares-wls"><i class="fa fa-check"></i><b>11.1</b> Weighted Least Squares (WLS)</a></li>
<li class="chapter" data-level="11.2" data-path="weighted-regression.html"><a href="weighted-regression.html#when-should-you-use-wls"><i class="fa fa-check"></i><b>11.2</b> When should you use WLS?</a></li>
<li class="chapter" data-level="11.3" data-path="weighted-regression.html"><a href="weighted-regression.html#correcting-for-known-heteroskedasticity"><i class="fa fa-check"></i><b>11.3</b> Correcting for Known Heteroskedasticity</a></li>
<li class="chapter" data-level="11.4" data-path="weighted-regression.html"><a href="weighted-regression.html#sampling-weights"><i class="fa fa-check"></i><b>11.4</b> Sampling Weights</a></li>
<li class="chapter" data-level="11.5" data-path="weighted-regression.html"><a href="weighted-regression.html#references"><i class="fa fa-check"></i><b>11.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="discrete-outcome-variables.html"><a href="discrete-outcome-variables.html"><i class="fa fa-check"></i><b>12</b> Discrete Outcome Variables</a><ul>
<li class="chapter" data-level="12.1" data-path="discrete-outcome-variables.html"><a href="discrete-outcome-variables.html#linear-probability-model"><i class="fa fa-check"></i><b>12.1</b> Linear Probability Model</a></li>
<li class="chapter" data-level="12.2" data-path="discrete-outcome-variables.html"><a href="discrete-outcome-variables.html#logit-model"><i class="fa fa-check"></i><b>12.2</b> Logit Model</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="robust-regression.html"><a href="robust-regression.html"><i class="fa fa-check"></i><b>13</b> Robust Regression</a><ul>
<li class="chapter" data-level="13.1" data-path="robust-regression.html"><a href="robust-regression.html#prerequites"><i class="fa fa-check"></i><b>13.1</b> Prerequites</a></li>
<li class="chapter" data-level="13.2" data-path="robust-regression.html"><a href="robust-regression.html#examples"><i class="fa fa-check"></i><b>13.2</b> Examples</a></li>
<li class="chapter" data-level="13.3" data-path="robust-regression.html"><a href="robust-regression.html#notes-1"><i class="fa fa-check"></i><b>13.3</b> Notes</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="bootstrapping.html"><a href="bootstrapping.html"><i class="fa fa-check"></i><b>14</b> Bootstrapping</a></li>
<li class="chapter" data-level="15" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html"><i class="fa fa-check"></i><b>15</b> Prediction and Model Comparison</a><ul>
<li class="chapter" data-level="15.1" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#prerequisites-3"><i class="fa fa-check"></i><b>15.1</b> Prerequisites</a></li>
<li class="chapter" data-level="15.2" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#measures-of-prediction"><i class="fa fa-check"></i><b>15.2</b> Measures of Prediction</a></li>
<li class="chapter" data-level="15.3" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#model-comparison"><i class="fa fa-check"></i><b>15.3</b> Model Comparison</a></li>
<li class="chapter" data-level="15.4" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#example-predicting-the-price-of-wine"><i class="fa fa-check"></i><b>15.4</b> Example: Predicting the Price of Wine</a></li>
<li class="chapter" data-level="15.5" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#cross-validation"><i class="fa fa-check"></i><b>15.5</b> Cross-Validation</a></li>
<li class="chapter" data-level="15.6" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#out-of-sample-error"><i class="fa fa-check"></i><b>15.6</b> Out of Sample Error</a><ul>
<li class="chapter" data-level="15.6.1" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#held-out-data"><i class="fa fa-check"></i><b>15.6.1</b> Held-out data</a></li>
<li class="chapter" data-level="15.6.2" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>15.6.2</b> Leave-One-Out Cross-Validation</a></li>
<li class="chapter" data-level="15.6.3" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>15.6.3</b> k-fold Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#analytic-covariance-methods"><i class="fa fa-check"></i><b>15.7</b> Analytic Covariance Methods</a></li>
<li class="chapter" data-level="15.8" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#further-resources"><i class="fa fa-check"></i><b>15.8</b> Further Resources</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="miscellaneous-regression-stuff.html"><a href="miscellaneous-regression-stuff.html"><i class="fa fa-check"></i><b>16</b> Miscellaneous Regression Stuff</a><ul>
<li class="chapter" data-level="16.1" data-path="miscellaneous-regression-stuff.html"><a href="miscellaneous-regression-stuff.html#anscombe-quartet"><i class="fa fa-check"></i><b>16.1</b> Anscombe quartet</a></li>
<li class="chapter" data-level="16.2" data-path="miscellaneous-regression-stuff.html"><a href="miscellaneous-regression-stuff.html#correlation-plots"><i class="fa fa-check"></i><b>16.2</b> Correlation Plots</a></li>
</ul></li>
<li class="part"><span><b>V Programming</b></span></li>
<li class="chapter" data-level="17" data-path="rs-forumula-syntax.html"><a href="rs-forumula-syntax.html"><i class="fa fa-check"></i><b>17</b> Râs Forumula Syntax</a><ul>
<li class="chapter" data-level="17.1" data-path="rs-forumula-syntax.html"><a href="rs-forumula-syntax.html#setup"><i class="fa fa-check"></i><b>17.1</b> Setup</a></li>
<li class="chapter" data-level="17.2" data-path="rs-forumula-syntax.html"><a href="rs-forumula-syntax.html#introduction-to-formula-objects"><i class="fa fa-check"></i><b>17.2</b> Introduction to Formula Objects</a></li>
<li class="chapter" data-level="17.3" data-path="rs-forumula-syntax.html"><a href="rs-forumula-syntax.html#programming-with-formulas"><i class="fa fa-check"></i><b>17.3</b> Programming with Formulas</a></li>
</ul></li>
<li class="part"><span><b>VI Examples</b></span></li>
<li class="chapter" data-level="18" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html"><i class="fa fa-check"></i><b>18</b> Duncan Occupational Prestige</a><ul>
<li class="chapter" data-level="18.1" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#setup-1"><i class="fa fa-check"></i><b>18.1</b> Setup</a></li>
<li class="chapter" data-level="18.2" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#coefficients-standard-errors"><i class="fa fa-check"></i><b>18.2</b> Coefficients, Standard errors</a></li>
<li class="chapter" data-level="18.3" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#residuals-fitted-values"><i class="fa fa-check"></i><b>18.3</b> Residuals, Fitted Values,</a></li>
<li class="chapter" data-level="18.4" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#broom"><i class="fa fa-check"></i><b>18.4</b> Broom</a></li>
<li class="chapter" data-level="18.5" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#plotting-fitted-regression-results"><i class="fa fa-check"></i><b>18.5</b> Plotting Fitted Regression Results</a></li>
</ul></li>
<li class="part"><span><b>VII Presentation</b></span></li>
<li class="chapter" data-level="19" data-path="formatting-tables.html"><a href="formatting-tables.html"><i class="fa fa-check"></i><b>19</b> Formatting Tables</a><ul>
<li class="chapter" data-level="19.1" data-path="formatting-tables.html"><a href="formatting-tables.html#overview-of-packages"><i class="fa fa-check"></i><b>19.1</b> Overview of Packages</a></li>
<li class="chapter" data-level="19.2" data-path="formatting-tables.html"><a href="formatting-tables.html#summary-statistic-table-example"><i class="fa fa-check"></i><b>19.2</b> Summary Statistic Table Example</a></li>
<li class="chapter" data-level="19.3" data-path="formatting-tables.html"><a href="formatting-tables.html#regression-table-example"><i class="fa fa-check"></i><b>19.3</b> Regression Table Example</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="reproducible-research.html"><a href="reproducible-research.html"><i class="fa fa-check"></i><b>20</b> Reproducible Research</a></li>
<li class="chapter" data-level="21" data-path="writing-resources.html"><a href="writing-resources.html"><i class="fa fa-check"></i><b>21</b> Writing Resources</a><ul>
<li class="chapter" data-level="21.1" data-path="writing-resources.html"><a href="writing-resources.html#writing-and-organizing-papers"><i class="fa fa-check"></i><b>21.1</b> Writing and Organizing Papers</a></li>
<li class="chapter" data-level="21.2" data-path="writing-resources.html"><a href="writing-resources.html#finding-research-ideas"><i class="fa fa-check"></i><b>21.2</b> Finding Research Ideas</a></li>
<li class="chapter" data-level="21.3" data-path="writing-resources.html"><a href="writing-resources.html#replications"><i class="fa fa-check"></i><b>21.3</b> Replications</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="multivariate-normal-distribution.html"><a href="multivariate-normal-distribution.html"><i class="fa fa-check"></i><b>A</b> Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
\[
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Bias}{Bias}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\RMSE}{RMSE}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\T}{'}

\newcommand{\distr}[1]{\mathcal{#1}}
\newcommand{\dnorm}{\distr{N}}
\newcommand{\dmvnorm}[1]{\distr{N}_{#1}}
\newcommand{\dt}[1]{\distr{T}_{#1}}

\newcommand{\cia}{\perp\!\!\!\perp}
\DeclareMathOperator*{\plim}{plim}
\]
<div id="omitted-variable-bias" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Omitted Variable Bias</h1>
<div id="prerequisites-1" class="section level2">
<h2><span class="header-section-number">8.1</span> Prerequisites</h2>
<p>This chapter uses the <a href="https://www.rdocumentation.org/packages/car/topics/Duncan">car</a> data set in the <strong><a href="https://cran.r-project.org/package=car">car</a></strong> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;car&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;tidyverse&quot;</span>)</code></pre></div>
</div>
<div id="simpsons-paradox" class="section level2">
<h2><span class="header-section-number">8.2</span> Simpsonâs Paradox</h2>
<p>Before considering the more general phenomena of omitted variable bias, weâll discuss <a href="https://en.wikipedia.org/wiki/Simpson%27s_paradox">Simpsonâs Paradox</a>.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> This when a trend or relationship appears when data is disaggregated into groups, but that trend or relationship either disappears or reverses when the data are aggregated.</p>
<p>A famous real-world case of this <span class="citation">Bickel, Hammel, and OâConnell (<a href="#ref-BickelHammelOConnell1975a">1975</a>)</span>, which analyzes a claim of sex bias in graduate admissions against UC-Berkeley in the 1970s. In 1973, 8,442 men and 4,321 women applied for admission to Berkeley graduate programs. In aggregate, UC Berkeley admitted 44% of men and 35% of women applicants, seemingly supporting that claim. However, when the admissions rates were disaggregated by graduate department, the acceptance rates by department were not, on average, different. What is going on? On average, more women applied to more selective (higher rejection rate) departments than men.</p>
<p>The dataset <a href="https://www.rdocumentation.org/packages/datasets/topics/UCBAdmissions">datasets</a> in the <strong><a href="https://cran.r-project.org/package=datasets">datasets</a></strong> package contains data for the largest 6 programs.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">&quot;UCBAdmissions&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;datasets&quot;</span>)
admissions &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(UCBAdmissions) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spread</span>(Admit, n) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">applicants =</span> Admitted <span class="op">+</span><span class="st"> </span>Rejected,
         <span class="dt">accepted =</span> Admitted <span class="op">/</span><span class="st"> </span>applicants)

<span class="kw">ggplot</span>(admissions, <span class="kw">aes</span>(<span class="dt">x =</span> Dept, <span class="dt">y =</span> accepted, <span class="dt">size =</span> applicants, <span class="dt">colour =</span> Gender)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="ovb_files/figure-html/unnamed-chunk-3-1.svg" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">select</span>(admissions, Dept, Gender, applicants, accepted) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(Dept, Gender)</code></pre></div>
<pre><code>## # A tibble: 12 Ã 4
##     Dept Gender applicants   accepted
##    &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;
## 1      A Female        108 0.82407407
## 2      A   Male        825 0.62060606
## 3      B Female         25 0.68000000
## 4      B   Male        560 0.63035714
## 5      C Female        593 0.34064081
## 6      C   Male        325 0.36923077
## 7      D Female        375 0.34933333
## 8      D   Male        417 0.33093525
## 9      E Female        393 0.23918575
## 10     E   Male        191 0.27748691
## 11     F Female        341 0.07038123
## 12     F   Male        373 0.05898123</code></pre>
<p>An interactive visualization is this UC Berkeley VUD Lab visualization: <a href="http://vudlab.com/simpsons/">Simpsonâs Paradox</a>. <strong>Stop. Go to that link. Explore that visualization, and build your intution</strong></p>
<p>For some other examples see <span class="citation">Moore (<a href="#ref-Moore2005a">2005</a>)</span>, <span class="citation">Wagner (<a href="#ref-Wagner1982a">1982</a>)</span>, <a href="https://en.wikipedia.org/wiki/Simpson%27s_paradox">Wikipedia</a>, <span class="citation">Julious and Mullee (<a href="#ref-JuliousMullee1994a">1994</a>)</span> (Kidney stone treatment).</p>
</div>
<div id="omitted-variable-bias-1" class="section level2">
<h2><span class="header-section-number">8.3</span> Omitted Variable Bias</h2>
<p>Suppose that the population model is, <span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + \beta_2 Z_i + \epsilon_i ,
\]</span> but given a sample, we run a regression with only <span class="math inline">\(\vec{x}\)</span> and not <span class="math inline">\(\vec{z}\)</span>. <span class="math display">\[
y_i = \hat{\beta}_0 + \hat{\beta}_1 x_i + \hat{\epsilon}_i .
\]</span></p>
<p>What is the relationship between <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>? Is <span class="math inline">\(\hat{\beta}_1\)</span> an unbiased estimator of <span class="math inline">\(\beta_1\)</span> ?</p>
<p><span class="math display">\[
\text{omitted variable bias} =  (\text{effect of $Z_i$ on $Y_i$}) \times (\text{effect of $X_i$ on $Z_i$})
\]</span></p>
<p>What does the omitted variable bias An <em>irrelevant variable</em> is one that is uncorrelated with <span class="math inline">\(Y_i\)</span>, meaning that its population coefficient is 0. Suppose <span class="math inline">\(Z_i\)</span> is an irrelevant variable, <span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + 0 \times Z_i = \epsilon_i
\]</span></p>
<p>In this case OLS is unbiased â¦ <span class="math display">\[
\begin{aligned}[t]
\E(\hat\beta_0) &amp;= \beta_0 \\
\E(\hat\beta_1) &amp;= \beta_1 \\
\E(\hat\beta_2) &amp;= 0
\end{aligned}
\]</span></p>
<p>However, including an irrelevant variable will increase the standard errors for <span class="math inline">\(\hat{\beta}_1\)</span>. Why?</p>
<p>Consider the linear regression model, <span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i.
\]</span> What if we included <span class="math inline">\(X_i\)</span> twice? <span class="math display">\[
Y_i = \tilde\beta_0 + \tilde\beta_1 X_i + \tilde\beta_2 X_i + \epsilon_i.
\]</span> Clearly, any combination of <span class="math inline">\(\tilde\beta_1\)</span> and <span class="math inline">\(\tilde\beta_2\)</span> where <span class="math display">\[
\tilde\beta_1 + \tilde\beta_2 =\beta_1 
\]</span> will fit the model as well as any other.</p>
<p>Consider cases of</p>
<ul>
<li>bivariate OLS with âeffectiveâ number of observations</li>
<li>continuous OLS</li>
</ul>
<table>
<colgroup>
<col width="23%" />
<col width="26%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(\Cov(X_1, X_2)\)</span></th>
<th align="left"><span class="math inline">\(\Cov(X_2, Y) &gt; 0\)</span></th>
<th><span class="math inline">\(\Cov(X_2, Y) = 0\)</span></th>
<th><span class="math inline">\(\Cov(X_2, Y) &lt; 0\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(&gt; 0\)</span> <span class="math inline">\(0\)</span> <span class="math inline">\(&lt; 0\)</span></td>
<td align="left">+ 0 -</td>
<td>0 0 0</td>
<td>- 0 +</td>
</tr>
</tbody>
</table>
<p>In practice, this is the primary problem of many papers and papers. That is because it biases the coefficient of interest. Reviewers and discussants will often ask about whether you have considered âcontrollingâ for <em>insert variable here</em>.</p>
<p>Although these may be legitimate concerns, not all reviewerss understand the purpose of controls variables so some of these may not be legitimate, and in fact harmful. There two arguments to consider when controlling for a variable.</p>
<ol style="list-style-type: decimal">
<li><p>The omitted variable has to plausibly be correlated with <em>both</em> the variable of interest <em>and</em> the outcome variable, and the burden is on the reviewers to provide at a confounding variable and plausible relationships. Simply stating that there <em>could</em> be an unobservable variable is trivially true, uninteresting, and not a fatal critique. That said, the plausibility of a causal claim would be higher if with methods less susceptible to unobserved confounders, such as experiments, instrumental variables, regression discontinuity, and difference-in-differences.</p></li>
<li><p>The omitted variable should be not be âpost treatmentâ variable. If the omitted variable should not be one of the causal pathways by which <span class="math inline">\(X\)</span> affects <span class="math inline">\(Y\)</span>, it should not be controlled for. If <span class="math inline">\(Z\)</span> affects the values of <span class="math inline">\(X\)</span> and also affects <span class="math inline">\(Y\)</span>, then it needs to be controlled for.</p></li>
</ol>
<p>How to assess the potential magnitude of omitted variable bias?</p>
<ol style="list-style-type: decimal">
<li><p><strong>Informal method</strong>. This is the methods that you see in many empirical papers. They estimate the model including different control variables. The less sensitive the coefficient(s) of the variables of interest are to the inclusion of control variables, the more plausible it is that the variable of interest also not sensitive to unobserved confounders <span class="citation">(Angrist and Pischke <a href="#ref-AngristPischke2014a">2014</a>)</span>. <span class="citation">Oster (<a href="#ref-Oster2016a">2016</a>)</span> states</p>
<blockquote>
<p>A common heuristic for evaluating the robustness of a result to omitted variable bias concerns is to look at the sensitivity of the treatment effect to inclusion of observed controls. In three top general interest economics journals in 2012, 75% of non-experimental empirical papers included such sensitivity analysis. The intuitive appeal of this approach lies in the idea that the bias arising from the observed controls is informative about the bias that arises from the unobserved ones.</p>
</blockquote>
<p>Note that what is important is that the magnitude of the <em>coefficient</em> is stable to the inclusion of controls, not that the coefficient remains statistically significant.</p></li>
<li><p><strong>Formal methods:</strong> <span class="citation">Bellows and Miguel (<a href="#ref-BellowsMiguel2009a">2009</a>)</span> propose the following simple statistic to assess the magnitude of omitted variable bias: <span class="math display">\[
\delta = \frac{\hat{\beta}_F}{\hat{\beta}_R - \hat{\beta}_C},
\]</span> The statistic <span class="math inline">\(\delta\)</span> is interpreted as the magnitude of covariance between the unobserved part of the controls and the treatment variable necessary to explain away the entire treatment effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>. A larger ratio suggests it is implausible that omitted variable bias could explain away the entire observed effect. See <span class="citation">Bellows and Miguel (<a href="#ref-BellowsMiguel2009a">2009</a> Appendix A)</span> for the derivation. <span class="citation">Nunn and Wantchekon (<a href="#ref-NunnWantchekon2011a">2011</a>)</span> provides a clear explanation and application of the statistic.</p></li>
</ol>
<p>Often you will see works that add regressors sequentially and make some sort of implicit coefficient stability argument. That is not useful. The important comparison is between the coefficient when nothing (or only a small subset of covariates) is controlled for, and the full set of controls.</p>
<p><span class="citation">Bellows and Miguel (<a href="#ref-BellowsMiguel2009a">2009</a>)</span> themselves generalize <span class="citation">Altonji, Elder, and Taber (<a href="#ref-AltonjiElderTaber2005a">2005</a>)</span> from binary to continuous treatment variables. <span class="citation">Oster (<a href="#ref-Oster2016a">2016</a>)</span> further generalizes the estimator. <span class="citation">Pei, Pischke, and Schwandt (<a href="#ref-PeiPischkeSchwandt2017a">2017</a>)</span> show that if the covariates are measured with error, a âbalancing testâ (regressing the confounder on the treatment) is more powerful.</p>
<p>Methods such as <em>matching</em>, <em>propensity scores</em>, or <em>inverse weighting</em> still depend on assumptions about selection on observables. They may be less sensitive to âomitted variable biasâ due to The differ from regression in the estimand or their sensitivity to model misspecification.</p>
<p>The preference for âdesign basedâ inference is mostly driven by a desire to find situations (designs) where other assumptions can substitute for the nigh impossible to test âselection on observablesâ assumption. Apart from experiments, these include instrumental variables, regression discontinuity, and difference-in-differences.</p>
</div>
<div id="measurement-error" class="section level2">
<h2><span class="header-section-number">8.4</span> Measurement Error</h2>
<div id="whats-the-problem" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Whatâs the problem?</h3>
<p>It biases coefficients:</p>
<ol style="list-style-type: decimal">
<li>Variable with measurement error: biases <span class="math inline">\(\beta\)</span> towards zero (<strong>attenuation bias</strong>)</li>
<li>Other variables: Biases <span class="math inline">\(\beta\)</span> similarly to omitted variable bias. In other words, when a variable has measurement error it is an imperfect control. You can think of omitted variables as the limit of the effect of measurement error as it increases.</li>
</ol>
</div>
<div id="what-to-do-about-it" class="section level3">
<h3><span class="header-section-number">8.4.2</span> What to do about it?</h3>
<p>Thereâs no easy fix within the OLS framework.</p>
<ol style="list-style-type: decimal">
<li>If the measurement error is in the variable of interest, then the variable will be biased towards zero, and your estimate is too large.</li>
<li>Find better measures with lower measurement errors. If the variable is the variable of interest, then perhaps combine multiple variables into a single index. If the measurement error is in the control variables, then include several measures. That these measure correlate closely increases their standard errors, but the control variables are not the object of the inferential analysis.</li>
<li>More complicated methods: errors in variable models, structural equation models, instrumental variable (IV) models, and Bayesian methods.</li>
</ol>
<p><span class="citation">Blackwell, Honaker, and King (<a href="#ref-BlackwellHonakerKing2015a">2015</a>)</span> note that the easiest way to handle measurement error in the predictors is to treat them as missing data where you have extra information about their range. Suppose a covariate is observed as <span class="math inline">\(x \sim(x^*, \delta^2)\)</span>, where <span class="math inline">\(x^*\)</span> is the true value, and <span class="math inline">\(\delta\)</span> is the scale of the measurement error. Then missing values are the case when <span class="math inline">\(\delta \to \infty\)</span>. So missing values are special, extreme, case of measurement error. This means that we can use multiple imputation methods for dealing with <a href="missing_values.html">missing values</a> where we add additional information to restrict the plausible range of imputated values. The <strong><a href="https://cran.r-project.org/package=Amelia">Amelia</a></strong> has built-in support for this, but the general idea could be adapted to other multiple imputation methods.</p>
</div>
</div>
<div id="more-information" class="section level2">
<h2><span class="header-section-number">8.5</span> More Information</h2>
<div id="simpsons-paradox-1" class="section level3">
<h3><span class="header-section-number">8.5.1</span> Simpsonâs Paradox</h3>
<ul>
<li>See <span class="citation">Samuels (<a href="#ref-Samuels1993a">1993</a>)</span> for more discussion of Simpsonâs Paradox</li>
<li><span class="citation">Moore (<a href="#ref-Moore2005a">2005</a>)</span> collects and succinctly describes several examples of Simpsonâs Paradox</li>
<li>An interactive visualization of the <a href="http://vudlab.com/simpsons/">Simpsonâs Paradox</a></li>
<li>Horton. 2015. <a href="http://blog.revolutionanalytics.com/2015/11/fun-with-simpsons-paradox-simulating-confounders.html">Fun with Simpsonâs Paradox: Simulating Confounders</a></li>
<li>Horton. 2012. <a href="https://www.r-bloggers.com/example-9-20-visualizing-simpsons-paradox/">Example 9.20: visualizing Simpsonâs paradox</a></li>
<li>See the <a href="https://en.wikipedia.org/wiki/Simpson%27s_paradox">Wikipedia Page</a></li>
<li><a href="https://economix.blogs.nytimes.com/2013/05/01/can-every-group-be-worse-than-average-yes/">US Median Wage by Education Level</a>. Overall wages have risen, but within every group, the wage has fallen.</li>
<li>Nielsen. <a href="http://michaelnielsen.org/reinventing_explanation/index.html">Reinventing Explanation</a> has a visual explanation of the Simpsonâs paradox</li>
<li>Gelman. <a href="http://andrewgelman.com/2014/04/08/understanding-simpsons-paradox-using-graph/">Understanding Simpsonâs Praradox Using a Graph</a>. April 8, 2014. Discusses the Nielsen post, provides other visualizations, and notes how aggregation problems arise even in non-causal cases.</li>
<li><span class="citation">Armstrong and Wattenberg (<a href="#ref-ArmstrongWattenberg2014a">2014</a>)</span> introduce the Comet Chart for visualizing Simpsonâs Paradoxes. See this <a href="https://www.zanarmstrong.com/#/research-1/">page</a> for code and examples, including an <a href="https://gist.github.com/zanarmstrong/6c2855a34f504029847485c690692e75">R implementation</a>.</li>
</ul>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-BickelHammelOConnell1975a">
<p>Bickel, P. J., E. A. Hammel, and J. W. OâConnell. 1975. âSex Bias in Graduate Admissions: Data from Berkeley.â <em>Science</em> 187 (4175). American Association for the Advancement of Science (AAAS): 398â404. doi:<a href="https://doi.org/10.1126/science.187.4175.398">10.1126/science.187.4175.398</a>.</p>
</div>
<div id="ref-Moore2005a">
<p>Moore, Tom. 2005. âSimpson or Simpson-Like Paradox Examples.â <a href="http://www.math.grinnell.edu/~mooret/reports/SimpsonExamples.pdf" class="uri">http://www.math.grinnell.edu/~mooret/reports/SimpsonExamples.pdf</a>.</p>
</div>
<div id="ref-Wagner1982a">
<p>Wagner, Clifford H. 1982. âSimpsonâs Paradox in Real Life.â <em>The American Statistician</em> 36 (1). Informa UK Limited: 46â48. doi:<a href="https://doi.org/10.1080/00031305.1982.10482778">10.1080/00031305.1982.10482778</a>.</p>
</div>
<div id="ref-JuliousMullee1994a">
<p>Julious, S. A., and M. A. Mullee. 1994. âConfounding and Simpsonâs Paradox.â <em>BMJ</em> 309 (6967). BMJ: 1480â1. doi:<a href="https://doi.org/10.1136/bmj.309.6967.1480">10.1136/bmj.309.6967.1480</a>.</p>
</div>
<div id="ref-AngristPischke2014a">
<p>Angrist, Joshua D., and JÃ¶rn-Steffen Pischke. 2009. <em>Mostly Harmless Econometrics: An Empiricistâs Companion</em>. Pr.</p> 2014. <em>Mastering âMetrics</em>. Princeton UP.</p>
</div>
<div id="ref-Oster2016a">
<p>Oster, Emily. 2016. âUnobservable Selection and Coefficient Stability: Theory and Evidence.â <em>Journal of Business &amp; Economic Statistics</em>, September. Informa UK Limited, 0â0. doi:<a href="https://doi.org/10.1080/07350015.2016.1227711">10.1080/07350015.2016.1227711</a>.</p>
</div>
<div id="ref-BellowsMiguel2009a">
<p>Bellows, John, and Edward Miguel. 2009. âWar and Local Collective Action in Sierra Leone.â <em>Journal of Public Economics</em> 93 (11â12): 1144â57. doi:<a href="https://doi.org/10.1016/j.jpubeco.2009.07.012">10.1016/j.jpubeco.2009.07.012</a>.</p>
</div>
<div id="ref-NunnWantchekon2011a">
<p>Nunn, Nathan, and Leonard Wantchekon. 2011. âThe Slave Trade and the Origins of Mistrust in Africa.â <em>American Economic Review</em> 101 (7): 3221â52. doi:<a href="https://doi.org/10.1257/aer.101.7.3221">10.1257/aer.101.7.3221</a>.</p>
</div>
<div id="ref-AltonjiElderTaber2005a">
<p>Altonji, Joseph G., Todd E. Elder, and Christopher R. Taber. 2005. âSelection on Observed and Unobserved Variables: Assessing the Effectiveness of Catholic Schools.â <em>Journal of Political Economy</em> 113 (1). University of Chicago Press: 151â84. doi:<a href="https://doi.org/10.1086/426036">10.1086/426036</a>.</p>
</div>
<div id="ref-PeiPischkeSchwandt2017a">
<p>Pei, Zhuan, JÃ¶rn-Steffen Pischke, and Hannes Schwandt. 2017. âPoorly Measured Confounders Are More Useful on the Left Than on the Right.â National Bureau of Economic Research. doi:<a href="https://doi.org/10.3386/w23232">10.3386/w23232</a>.</p>
</div>
<div id="ref-BlackwellHonakerKing2015a">
<p>Blackwell, M., J. Honaker, and G. King. 2015. âA Unified Approach to Measurement Error and Missing Data: Overview and Applications.â <em>Sociological Methods &amp; Research</em>, June. SAGE Publications. doi:<a href="https://doi.org/10.1177/0049124115585360">10.1177/0049124115585360</a>.</p>
</div>
<div id="ref-Samuels1993a">
<p>Samuels, Myra L. 1993. âSimpsonâs Paradox and Related Phenomena.â <em>Journal of the American Statistical Association</em> 88 (421). Informa UK Limited: 81â88. doi:<a href="https://doi.org/10.1080/01621459.1993.10594297">10.1080/01621459.1993.10594297</a>.</p>
</div>
<div id="ref-ArmstrongWattenberg2014a">
<p>Armstrong, Zan, and Martin Wattenberg. 2014. âVisualizing Statistical Mix Effects and Simpson&amp;#x0027<span class="math inline">\(\mathsemicolon\)</span>s Paradox.â <em>IEEE Transactions on Visualization and Computer Graphics</em> 20 (12). Institute of Electrical; Electronics Engineers (IEEE): 2132â41. doi:<a href="https://doi.org/10.1109/tvcg.2014.2346297">10.1109/tvcg.2014.2346297</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="7">
<li id="fn7"><p>However, omitted variable bias only makes sense as a concept when the regression has a causal or structural interpretation. Simpsonâs paradox is an aggregation affect that can occur with conditioning even when there is no causal interpretation of the model.<a href="omitted-variable-bias.html#fnref7">â©</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression-inference.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="outliers.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/jrnold/intro-method-notes/edit/master/ovb.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
