<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>POLS 503: Advanced Quantitative Political Methodology: The Notes</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>">
  <meta name="generator" content="bookdown 0.0.60 and GitBook 2.6.7">

  <meta property="og:title" content="POLS 503: Advanced Quantitative Political Methodology: The Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>" />
  <meta name="github-repo" content="UW-POLS503/pols503-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="POLS 503: Advanced Quantitative Political Methodology: The Notes" />
  
  <meta name="twitter:description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>" />
  

<meta name="author" content="Jeffrey B. Arnold">

<meta name="date" content="2016-04-19">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="ols-troubleshooting-and-diagnostics.html">

<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />











<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>

$$
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Bias}{Bias}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\T}{'}

\newcommand{\distr}[1]{\mathcal{#1}}
\newcommand{\dnorm}{\distr{N}}
\newcommand{\dmvnorm}[1]{\distr{N}_{#1}}
$$

  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="linear-regression-and-the-ordinary-least-squares-ols-estimator.html"><a href="linear-regression-and-the-ordinary-least-squares-ols-estimator.html"><i class="fa fa-check"></i><b>2</b> Linear Regression and the Ordinary Least Squares (OLS) Estimator</a><ul>
<li class="chapter" data-level="2.1" data-path="linear-regression-and-the-ordinary-least-squares-ols-estimator.html"><a href="linear-regression-and-the-ordinary-least-squares-ols-estimator.html#linear-regression-function"><i class="fa fa-check"></i><b>2.1</b> Linear Regression Function</a></li>
<li class="chapter" data-level="2.2" data-path="linear-regression-and-the-ordinary-least-squares-ols-estimator.html"><a href="linear-regression-and-the-ordinary-least-squares-ols-estimator.html#ordinary-least-squares"><i class="fa fa-check"></i><b>2.2</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="2.3" data-path="linear-regression-and-the-ordinary-least-squares-ols-estimator.html"><a href="linear-regression-and-the-ordinary-least-squares-ols-estimator.html#properties-of-the-ols-estimator"><i class="fa fa-check"></i><b>2.3</b> Properties of the OLS Estimator</a><ul>
<li class="chapter" data-level="2.3.1" data-path="linear-regression-and-the-ordinary-least-squares-ols-estimator.html"><a href="linear-regression-and-the-ordinary-least-squares-ols-estimator.html#what-makes-an-estimator-good"><i class="fa fa-check"></i><b>2.3.1</b> What makes an estimator good?</a></li>
<li class="chapter" data-level="2.3.2" data-path="linear-regression-and-the-ordinary-least-squares-ols-estimator.html"><a href="linear-regression-and-the-ordinary-least-squares-ols-estimator.html#properties-of-ols"><i class="fa fa-check"></i><b>2.3.2</b> Properties of OLS</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="linear-regression-and-the-ordinary-least-squares-ols-estimator.html"><a href="linear-regression-and-the-ordinary-least-squares-ols-estimator.html#references"><i class="fa fa-check"></i><b>2.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ols-troubleshooting-and-diagnostics.html"><a href="ols-troubleshooting-and-diagnostics.html"><i class="fa fa-check"></i><b>3</b> OLS Troubleshooting and Diagnostics</a><ul>
<li class="chapter" data-level="3.1" data-path="ols-troubleshooting-and-diagnostics.html"><a href="ols-troubleshooting-and-diagnostics.html#multi-collinearity"><i class="fa fa-check"></i><b>3.1</b> Multi-Collinearity</a></li>
<li class="chapter" data-level="3.2" data-path="ols-troubleshooting-and-diagnostics.html"><a href="ols-troubleshooting-and-diagnostics.html#omitted-variable-bias"><i class="fa fa-check"></i><b>3.2</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="3.3" data-path="ols-troubleshooting-and-diagnostics.html"><a href="ols-troubleshooting-and-diagnostics.html#measurement-error"><i class="fa fa-check"></i><b>3.3</b> Measurement Error</a></li>
<li class="chapter" data-level="3.4" data-path="ols-troubleshooting-and-diagnostics.html"><a href="ols-troubleshooting-and-diagnostics.html#non-linearity"><i class="fa fa-check"></i><b>3.4</b> Non-linearity</a></li>
<li class="chapter" data-level="3.5" data-path="ols-troubleshooting-and-diagnostics.html"><a href="ols-troubleshooting-and-diagnostics.html#heteroskedasticity-and-auto-correlation"><i class="fa fa-check"></i><b>3.5</b> Heteroskedasticity and Auto-correlation</a></li>
<li class="chapter" data-level="3.6" data-path="ols-troubleshooting-and-diagnostics.html"><a href="ols-troubleshooting-and-diagnostics.html#non-constant-variance-heteroskedasticity"><i class="fa fa-check"></i><b>3.6</b> Non-constant variance (Heteroskedasticity)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>4</b> Appendix</a><ul>
<li class="chapter" data-level="4.1" data-path="appendix.html"><a href="appendix.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>4.1</b> Multivariate Normal Distribution</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">POLS 503: Advanced Quantitative Political Methodology: The Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression-and-the-ordinary-least-squares-ols-estimator" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Linear Regression and the Ordinary Least Squares (OLS) Estimator</h1>
<p>Since we will largely be concerned with using linear regression for inference, we will start by discussion the population parameter of interest (population linear regression function), then the sample statistic (sample linear regression function) and estimator (ordinary least squares).</p>
<p>We will then consider the properties of the OLS estimator.</p>
<div id="linear-regression-function" class="section level2">
<h2><span class="header-section-number">2.1</span> Linear Regression Function</h2>
<p>The <strong>population linear regression function</strong> is <span class="math display">\[
r(x) = \E[Y | X = x] = \beta_0 + \sum_{k = 1}^{K} \beta_{k} x_k .
\]</span> The population linear regression function is defined for random variables, and will be the object to be estimated.</p>
<p>Names for <span class="math inline">\(\vec{y}\)</span></p>
<ul>
<li>dependent variable</li>
<li>explained variable</li>
<li>response variable</li>
<li>predicted variable</li>
<li>regressand</li>
<li>outcome variable</li>
</ul>
<p>Names for <span class="math inline">\(\mat{X}\)</span>,</p>
<ul>
<li>indpendent variables</li>
<li>explanatory varaibles</li>
<li>treatment and control variables</li>
<li>predictor variables</li>
<li>covariates</li>
<li>regressors</li>
</ul>
<p>To estimate the unkonwn population linear regression, we will use the <strong>sample linear regression function</strong>, <span class="math display">\[
\hat{r}(x_i) = \hat{y}_i = \hat\beta_0 + \sum_{k = 1}^{K} \hat\beta_{k} x_k .
\]</span> However, we</p>
<p><span class="math inline">\(\hat{Y}_i\)</span> are the fitted or predicted value The <strong>residuals</strong> or <strong>errors</strong> are the prediction errors of the estimates <span class="math display">\[
\hat{\epsilon}_i = y_i - \hat{y}_i
\]</span></p>
<p><span class="math inline">\(\vec{\beta}\)</span> are the parameters; <span class="math inline">\(\beta_0\)</span> is called the <em>intercept</em>, and <span class="math inline">\(\beta_{1}, \dots, \beta_{K}\)</span> are called the <em>slope parameters</em>, or <em>coefficients</em>.</p>
<p>The linear regression function can be written as a scalar function for each observation, <span class="math inline">\(i = 1, \dots, N\)</span>, <span class="math display">\[
\begin{aligned}[t]
y_i &amp;= \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \cdots + \beta_{K,i} + \varepsilon_i \\
 &amp;= \beta_0 + \sum_{k = 1}^{K} \beta_k x_{k,i} + \varepsilon_i \\
&amp;= \sum_{k = 0}^{K} \beta_k x_{k,i} + \varepsilon_i
\end{aligned}
\]</span> where <span class="math inline">\(x_{0,i} = 1\)</span> for all <span class="math inline">\(i \in 1:N\)</span>.</p>
<p>The linear regression can be more compactly written in matrix form, <span class="math display">\[
\begin{aligned}[t]
  \begin{bmatrix}
    y_1 \\
    y_2 \\
    \vdots \\
    y_N
  \end{bmatrix} &amp;=
  \begin{bmatrix}
    1 &amp; x_{1,1} &amp; x_{2,1} &amp; \cdots &amp; x_{K,1} \\
    1 &amp; x_{1,2} &amp; x_{2,2} &amp; \cdots &amp; x_{K,2} \\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    1 &amp; x_{1,N}&amp; x_{2,n} &amp; \cdots &amp; x_{K,N}
  \end{bmatrix}
  \begin{bmatrix}
    \beta_0 \\
    \beta_1 \\
    \beta_2 \\
    \vdots \\
    \beta_K
    \end{bmatrix}
  +
  \begin{bmatrix}
    \varepsilon_1 \\
    \varepsilon_2 \\
    \vdots \\
    \varepsilon_N
  \end{bmatrix}
\end{aligned} .
\]</span> More compactly, the linear regression model can be written as, <span class="math display">\[
\begin{aligned}[t]
  \underbrace{\vec{y}}_{N \times 1} &amp;=
  \underbrace{\mat{X}}_{N \times K} \,\,
  \underbrace{\vec{\beta}}_{K \times 1} +
  \underbrace{\vec{\varepsilon}}_{N \times 1} .
\end{aligned}
\]</span> The matrix <span class="math inline">\(\mat{X}\)</span> is called the <em>design</em> matrix. Its rows are each observation in the data. Its columns are the intercept, a column vector of 1’s, and the values of each predictor.</p>
</div>
<div id="ordinary-least-squares" class="section level2">
<h2><span class="header-section-number">2.2</span> Ordinary Least Squares</h2>
<p>Ordinary least squares (OLS) is an estimator of the slope and statistic of the regression line<a href="appendix.html#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. OLS finds values of the intercept and slope coefficients by minimizing the squared errors, <span class="math display">\[
\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_K
=
\argmin_{b_0, b_1, \dots, b_k} \sum_{i = 1}^{N}  \underbrace{{\left(y_i - b_0 - \sum_{k = 1}^{K} b_k x_{i,k} \right)}^2}_{\text{squared error}},
\]</span> or, in matrix notation, <span class="math display">\[
\begin{aligned}[t]
\hat{\vec{\beta}} &amp;= \argmin_{\vec{b}} \sum_{i = 1}^N (y_i - \vec{b}\T \vec{x}_i)^2 \\
&amp;= \argmin_{\vec{b}} \sum_{i = 1}^N u_i^2 \\
&amp;= \argmin_{\vec{b}} \vec{u}&#39; \vec{u}
\end{aligned}
\]</span> where <span class="math inline">\(\vec{u} = \vec{y} - \mat{X} \vec{\beta}\)</span>.</p>
<p>In most statistical models, including even genalized linear models such as logit, the solution to this minimization problem would be solved with optimization methods that require interation. One nice feature of OLS is that there is a closed form solution for <span class="math inline">\(\hat{\beta}\)</span> even in the multiple regression case, so no iterative optimization methods need to be used.</p>
<p>In the bivariate regression case, the OLS estimators for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are <span class="math display">\[
\begin{aligned}[t]
\hat{\beta}_0 &amp;= \bar{y} - \hat\beta_1 \bar{x} \\
\hat{\beta}_1 7= \frac{\sum_{i = 1}^N (x_i - \bar{x}) (y_i - \bar{y})}{\sum_{i = 1}^N (x_i - \bar{x})^2} \\
&amp;= \frac{\Cov(\vec{x} \vec{y})}{\Var{\vec{x}}}
&amp;= \frac{\text{Sample covariance betweeen $\vec{x}$ and $\vec{y}$}}{\text{Sample variance of $\vec{x}$}} .
\end{aligned}
\]</span> In the multiple regression case, the OLS estimator for <span class="math inline">\(\hat{\vec{\beta}}\)</span> is <span class="math display">\[
\hat{\vec{\beta}} = \left( \mat{X}&#39; \mat{X} \right)^{-1} \mat{X}&#39; \vec{y} .
\]</span> The term <span class="math inline">\(\mat{X}&#39; \mat{X}\)</span> is similar to the variance of <span class="math inline">\(\vec{x}\)</span> in the bivariate case. The term <span class="math inline">\(\mat{X}&#39; \vec{y}\)</span> is similar to the covariance between <span class="math inline">\(\mat{X}\)</span> and <span class="math inline">\(\vec{y}\)</span> in the bivariate case.</p>
<p>The sample linear regression function estimated by OLS has the following properties:</p>
<ol style="list-style-type: decimal">
<li>Residuals sum to zero, <span class="math display">\[
\sum_{i = 1}^N \hat{\epsilon}_i = 0 .
\]</span> This implies that the mean of residuals is also 0.</li>
<li>The regression function passes through the point <span class="math inline">\((\bar{\vec{y}}, \bar{\vec{x}}_1, \dots, \bar{\vec{x}_K})\)</span>. In other words, the following is always true, <span class="math display">\[
\bar{\vec{y}} = \hat\beta_0 + \sum_{k = 1}^K \hat\beta_k \bar{\vec{x}}_k .
\]</span></li>
<li>The resisuals are uncorrelated with the predictor <span class="math display">\[
\sum_{i = 1}^N x_i \hat{\epsilon}_i = 0
\]</span></li>
<li>The residuals are uncorrelated with the fitted values <span class="math display">\[
\sum_{i = 1}^N \hat{y}_i \hat{\varepsilon}_i = 0
\]</span></li>
</ol>
</div>
<div id="properties-of-the-ols-estimator" class="section level2">
<h2><span class="header-section-number">2.3</span> Properties of the OLS Estimator</h2>
<div id="what-makes-an-estimator-good" class="section level3">
<h3><span class="header-section-number">2.3.1</span> What makes an estimator good?</h3>
<p>Estimators are evaluated not on how close an estimate in a given sample is to the population, but how their sampling distributions compare to the population. In other words, judge the <em>methodology</em> (estimator), not the <em>result</em> (estimate).[^ols-properties-references]</p>
<p>Let <span class="math inline">\(\theta\)</span> be the population parameter, and <span class="math inline">\(\hat\theta\)</span> be an estimator of that population parameter.</p>
<dl>
<dt>Bias</dt>
<dd><p>The bias of an estimator is the difference between the mean of its sampling distribution and the population parameter, <span class="math display">\[\Bias(\hat\theta) = \E(\hat\theta) - \theta .\]</span></p>
</dd>
<dt>Variance</dt>
<dd><p>The variance of the estimator is the variance of its sampling distribution, <span class="math inline">\(\Var(\theta)\)</span>.</p>
</dd>
<dt>Efficiency (Mean squared error)</dt>
<dd><p>An efficient estimator is one that minimizes a given “loss function”, which is a penalty for missing the population average. The most common loss function is squared loss, which gives the <em>Mean Squared Error (MSE)</em> of an estimator.</p>
</dd>
<dd><span class="math display">\[\MSE(\hat\theta) = \E\left[{(\hat\theta - \theta)}^{2}\right] =  (\E(\hat\theta) - \theta)^2 + \E(\hat\theta - \E(\hat\theta))^2 = \Bias(\hat\theta)^2 + \Var(\hat\theta)\]</span>
</dd>
<dd>The mean squared error is a function of both the bias and variance of an estimator.
</dd>
<dd>This means that some biased estimators can be more efficient : than unbiased estimators if their variance offsets their bias.<a href="appendix.html#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>
</dd>
</dl>
<table>
<caption><span id="tab:unnamed-chunk-1">Table 2.1: </span>Examples of clocks as “estimators” of the time<a href="appendix.html#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">Biased</th>
<th align="left">Variance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Stopped clock</td>
<td align="left">Yes</td>
<td align="left">High</td>
</tr>
<tr class="even">
<td align="left">Random clock</td>
<td align="left">No</td>
<td align="left">High</td>
</tr>
<tr class="odd">
<td align="left">Clock that is “a lot” fast</td>
<td align="left">Yes</td>
<td align="left">Low</td>
</tr>
<tr class="even">
<td align="left">Clock that is “a little” fast</td>
<td align="left">Yes</td>
<td align="left">Low</td>
</tr>
<tr class="odd">
<td align="left">Atomic clock</td>
<td align="left">No</td>
<td align="left">Low</td>
</tr>
</tbody>
</table>
<p>Another property is <strong>consistency</strong>. Consistency is an asymptotic property<a href="appendix.html#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>, that roughly states that an estimator converges to the truth as the number of obserservations grows, <span class="math inline">\(\E(\hat\theta - \theta) \to 0\)</span> as <span class="math inline">\(N \to \infty\)</span>. Roughly, this means that if you had enough (infinite) data, the estimator will give you the true value of the parameter.</p>
</div>
<div id="properties-of-ols" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Properties of OLS</h3>
<ul>
<li>When is OLS unbiased?</li>
<li>When is OLS consistent?</li>
<li>When is OLS efficient?</li>
</ul>
<ol style="list-style-type: decimal">
<li><p><strong>Linearity</strong> The popluation model is <span class="math display">\[
   \vec{y} = \mat{X} \vec{\beta} + \vec{\varepsilon}
   \]</span> where <span class="math inline">\(\vec{\varepsilon}\)</span> is an unobserved random error or disturbance term with <span class="math inline">\(\E(\varepsilon) = 0\)</span>.</p></li>
<li>Random/iid sample. <span class="math inline">\((y_i, \vec{x}_i&#39;)\)</span> are a random sample from the population.</li>
<li><strong>No Perfect Collinearity</strong>. There is no exact <em>linear</em> relationships among the independent variables. <span class="math inline">\(\mat{X}\)</span> is a <span class="math inline">\(N \times K\)</span> matrix with rank <span class="math inline">\(K\)</span>.</li>
<li>Zero conditional mean. The error, <span class="math inline">\(\varepsilon\)</span>, has an expected value of zero, conditional on the predictors. <span class="math display">\[
\E(\varepsilon | X) = 0
\]</span></li>
<li><p>Constant variance (Homoskedasticity). The error has the same variance conditional on the predictors, for all observations, <span class="math display">\[
\E(\epsilon_i | \vec{x}_i) = \sigma^2) \text{ for all $i$}
\]</span></p></li>
<li><p>Fixed <span class="math inline">\(\mat{X}\)</span> or <span class="math inline">\(\mat{X}\)</span> measured without error and independent of the error.</p></li>
<li><p>Normal disturbances: <span class="math inline">\(\epsilon_i|\mat{X} \sim N(0, \sigma^2)\)</span>.</p></li>
</ol>
<p>What do these assumptions give us?</p>
<ul>
<li>Identification of OLS: Under Assumption 1, OLS can be estimated. In other words, there is a <em>unique</em> <span class="math inline">\(\hat\beta\)</span> that minimizes the sum of squared errors.</li>
<li>Unbiasedness of OLS: Under Assumptions 1–4, OLS is unbiased. <span class="math display">\[
\E(\hat{\vec{\beta}}) = \vec{\beta}
\]</span></li>
<li>Gauss-Markov theorem. Under Assumptions 1–5, OLS is the best linear unbiased estimator of <span class="math inline">\(\vec{\beta}\)</span>. <em>Linear</em> means that the estimates can be written as a linear functions of the outcomes, <span class="math display">\[
  \tilde{\beta}_j = \sum_{i = 1}^n w_{i,j} y_i
  \]</span> <em>Best</em> means that it has the smallest variance. This means for any unbiased and linear estimator, <span class="math inline">\(\tilde{\beta}\)</span>, the OLS estimator, <span class="math inline">\(\hat{\beta}_{OLS}\)</span>, has a smaller variance, <span class="math display">\[
  \Var(\tilde{\beta}) &gt; \Var(\hat{\beta}_{OLS})
  \]</span> Not that this does not imply that OLS has the lowest MSE of any estimator, since a biased estimator could have a lower MSE. In fact, for any regression with three or more variables, there is a ridge estimator with a lower MSE.</li>
</ul>
<table style="width:93%;">
<colgroup>
<col width="29%" />
<col width="29%" />
<col width="34%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Assumption</th>
<th align="left">Formal statement</th>
<th align="left">Consequence of violation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">No (perfect) collinearity</td>
<td align="left"><span class="math inline">\(\rank(\mat{X}) = K, K &lt; N\)</span></td>
<td align="left">Coefficients unidentified</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\mat{X}\)</span> is exogenous</td>
<td align="left"><span class="math inline">\(\E(\mat{X} \vec{\varepsilon}) = 0\)</span></td>
<td align="left">Biased, even as <span class="math inline">\(N \to \infty\)</span></td>
</tr>
<tr class="odd">
<td align="left">Disturbances have mean 0</td>
<td align="left"><span class="math inline">\(\E(\varepsilon) = 0\)</span></td>
<td align="left">Biased, even as <span class="math inline">\(N \to \infty\)</span></td>
</tr>
<tr class="even">
<td align="left">No serial correlation</td>
<td align="left"><span class="math inline">\(\E(\varepsilon_i \varepsilon_j) = 0\)</span>, <span class="math inline">\(i \neq j\)</span></td>
<td align="left">Unbiased, wrong se</td>
</tr>
<tr class="odd">
<td align="left">Homoskedastic errors</td>
<td align="left"><span class="math inline">\(\E(\vec{\varepsilon}\T \vec{\varepsilon})\)</span></td>
<td align="left">Unbiased, wrong se</td>
</tr>
<tr class="even">
<td align="left">Gaussian errors</td>
<td align="left"><span class="math inline">\(\varepsilon \sim \dnorm(0, \sigma^2)\)</span></td>
<td align="left">Unbiased, se wrong unless <span class="math inline">\(N \to \infty\)</span></td>
</tr>
</tbody>
</table>
<!--
1. Nonlinearity
    - Result: biased/inconsistent estimates
    - Diagnose: scatterplots, added variable plots, component-plus-residual plots
    - Correct: transformations, polynomials, different model
2. iid/random sample
    - Result: no bias with appropriate alternative assumptions (structured dependence)
    - Result (ii): violations imply heteroskedasticity
    - Result (iii): outliers from different distributions can cause inefficiency/bias
    - Diagnose/Correct: next week!
3. Perfect collinearity
    - Result: can't run OLS
    - Diagnose/correct: drop one collinear term
4. Zero conditional mean error
    - Result: biased/inconsistent estimates
    - Diagnose: very difficult
    - Correct: instrumental variables (Gov 2002)
5. Heteroskedasticity
    - Result: SEs are biased (usually downward)
    - Diagnose/correct: next week!
6. Non-Normality
    - Result: critical values for $t$ and $F$ tests wrong
    - Diagnose: checking the (studentized) residuals, QQ-plots, etc
    - Correct: transformations, add variables to $\X$, different model
-->
<p>Note that these assumptions can be sometimes be written in largely equivalent, but slightly different forms.</p>
</div>
</div>
<div id="references" class="section level2">
<h2><span class="header-section-number">2.4</span> References</h2>
<ul>
<li>Wooldrige, Ch 3.</li>
<li>Fox, Ch 6, 9.</li>
</ul>
<!-- Footnotes -->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ols-troubleshooting-and-diagnostics.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/UW-POLS503/pols503-notes/edit/gh-pages/ols-estimator.Rmd",
"text": "Edit"
},
"download": ["pols503-notes.pdf", "pols503-notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>


</body>

</html>
