<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>POLS 503: Advanced Quantitative Political Methodology: The Notes</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>">
  <meta name="generator" content="bookdown 0.0.60 and GitBook 2.6.7">

  <meta property="og:title" content="POLS 503: Advanced Quantitative Political Methodology: The Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>" />
  <meta name="github-repo" content="UW-POLS503/pols503-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="POLS 503: Advanced Quantitative Political Methodology: The Notes" />
  
  <meta name="twitter:description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>" />
  

<meta name="author" content="Jeffrey B. Arnold">

<meta name="date" content="2016-04-18">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="appendix.html">

<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />











<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>

$$
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Bias}{Bias}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\T}{'}

\newcommand{\distr}[1]{\mathcal{#1}}
\newcommand{\dnorm}{\distr{N}}
\newcommand{\dmvnorm}[1]{\distr{N}_{#1}}
$$

  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>2</b> Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="linear-regression.html"><a href="linear-regression.html#population-linear-regression-function"><i class="fa fa-check"></i><b>2.1</b> Population Linear Regression Function</a></li>
<li class="chapter" data-level="2.2" data-path="linear-regression.html"><a href="linear-regression.html#sample-linear-regression-function"><i class="fa fa-check"></i><b>2.2</b> Sample Linear Regression Function</a></li>
<li class="chapter" data-level="2.3" data-path="linear-regression.html"><a href="linear-regression.html#matrix-representation"><i class="fa fa-check"></i><b>2.3</b> Matrix Representation</a></li>
<li class="chapter" data-level="2.4" data-path="linear-regression.html"><a href="linear-regression.html#assumptions"><i class="fa fa-check"></i><b>2.4</b> Assumptions</a></li>
<li class="chapter" data-level="2.5" data-path="linear-regression.html"><a href="linear-regression.html#errors-in-linear-regression"><i class="fa fa-check"></i><b>2.5</b> Errors in Linear Regression</a></li>
<li class="chapter" data-level="2.6" data-path="linear-regression.html"><a href="linear-regression.html#non-constant-variance-heteroskedasticity"><i class="fa fa-check"></i><b>2.6</b> Non-constant variance (Heteroskedasticity)</a></li>
<li class="chapter" data-level="2.7" data-path="linear-regression.html"><a href="linear-regression.html#why-control-for-variables"><i class="fa fa-check"></i><b>2.7</b> Why Control for Variables</a></li>
<li class="chapter" data-level="2.8" data-path="linear-regression.html"><a href="linear-regression.html#multicollinearity"><i class="fa fa-check"></i><b>2.8</b> Multicollinearity</a></li>
<li class="chapter" data-level="2.9" data-path="linear-regression.html"><a href="linear-regression.html#most-general-ols-assumptions"><i class="fa fa-check"></i><b>2.9</b> Most general OLS assumptions</a></li>
<li class="chapter" data-level="2.10" data-path="linear-regression.html"><a href="linear-regression.html#no-perfect-collinearity"><i class="fa fa-check"></i><b>2.10</b> No perfect collinearity</a></li>
<li class="chapter" data-level="2.11" data-path="linear-regression.html"><a href="linear-regression.html#expected-values-of-vectors"><i class="fa fa-check"></i><b>2.11</b> Expected values of vectors</a></li>
<li class="chapter" data-level="2.12" data-path="linear-regression.html"><a href="linear-regression.html#ols-is-unbiased"><i class="fa fa-check"></i><b>2.12</b> OLS is unbiased</a></li>
<li class="chapter" data-level="2.13" data-path="linear-regression.html"><a href="linear-regression.html#variance-covariance-matrix-of-random-vectors"><i class="fa fa-check"></i><b>2.13</b> Variance-covariance matrix of random vectors</a></li>
<li class="chapter" data-level="2.14" data-path="linear-regression.html"><a href="linear-regression.html#matrix-version-of-homoskedasticity"><i class="fa fa-check"></i><b>2.14</b> Matrix version of homoskedasticity</a></li>
<li class="chapter" data-level="2.15" data-path="linear-regression.html"><a href="linear-regression.html#sampling-variance-for-ols-estimates"><i class="fa fa-check"></i><b>2.15</b> Sampling variance for OLS estimates</a></li>
<li class="chapter" data-level="2.16" data-path="linear-regression.html"><a href="linear-regression.html#inference-in-the-general-setting"><i class="fa fa-check"></i><b>2.16</b> Inference in the general setting</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>3</b> Appendix</a><ul>
<li class="chapter" data-level="3.1" data-path="appendix.html"><a href="appendix.html#covariancevariance-interpretation-of-matrix-ols"><i class="fa fa-check"></i><b>3.1</b> Covariance/variance interpretation of matrix OLS</a></li>
<li class="chapter" data-level="3.2" data-path="appendix.html"><a href="appendix.html#violations-of-the-assumptions"><i class="fa fa-check"></i><b>3.2</b> Violations of the assumptions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>4</b> OLS</a></li>
<li class="chapter" data-level="5" data-path="what-makes-an-estimator-good.html"><a href="what-makes-an-estimator-good.html"><i class="fa fa-check"></i><b>5</b> What makes an estimator good?</a><ul>
<li class="chapter" data-level="5.1" data-path="what-makes-an-estimator-good.html"><a href="what-makes-an-estimator-good.html#assumptions-1"><i class="fa fa-check"></i><b>5.1</b> Assumptions</a></li>
<li class="chapter" data-level="5.2" data-path="what-makes-an-estimator-good.html"><a href="what-makes-an-estimator-good.html#references"><i class="fa fa-check"></i><b>5.2</b> References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="appendix-1.html"><a href="appendix-1.html"><i class="fa fa-check"></i><b>6</b> Appendix</a><ul>
<li class="chapter" data-level="6.1" data-path="appendix-1.html"><a href="appendix-1.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.1</b> Multivariate Normal Distribution</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">POLS 503: Advanced Quantitative Political Methodology: The Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Linear Regression</h1>
<p>Since we will largely be concerned with using linear regression for inference, we will start by discussion the population parameter of interest (population linear regression function), then the sample statistic (sample linear regression function) and estimator (ordinary least squares).</p>
<div id="population-linear-regression-function" class="section level2">
<h2><span class="header-section-number">2.1</span> Population Linear Regression Function</h2>
<p>As with</p>
<p>The population linear regression function</p>
</div>
<div id="sample-linear-regression-function" class="section level2">
<h2><span class="header-section-number">2.2</span> Sample Linear Regression Function</h2>
</div>
<div id="matrix-representation" class="section level2">
<h2><span class="header-section-number">2.3</span> Matrix Representation</h2>
<p>The linear regression function can be written as a scalar function for each observation, <span class="math inline">\(i = 1, \dots, N\)</span>, <span class="math display">\[
\begin{aligned}[t]
y_i &amp;= \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \cdots + \beta_{K,i} + \varepsilon_i \\
 &amp;= \beta_0 + \sum_{k = 1}^{K} \beta_k x_{k,i} + \varepsilon_i \\
&amp;= \sum_{k = 0}^{K} \beta_k x_{k,i} + \varepsilon_i
\end{aligned}
\]</span> where <span class="math inline">\(x_{0,i} = 1\)</span> for all <span class="math inline">\(i \in 1:N\)</span>.</p>
<p>The linear regression can be more compactly written in matrix form, <span class="math display">\[
\begin{aligned}[t]
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_N
\end{bmatrix} &amp;=
\begin{bmatrix} 
1 &amp; x_{1,1} &amp; x_{2,1} &amp; \cdots &amp; x_{K,1} \\
1 &amp; x_{1,2} &amp; x_{2,2} &amp; \cdots &amp; x_{K,2} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_{1,N}&amp; x_{2,n} &amp; \cdots &amp; x_{K,N}
\end{bmatrix}
\begin{bmatrix} 
\beta_0 \\
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_K
\end{bmatrix}
+ 
\begin{bmatrix}
\varepsilon_1 \\
\varepsilon_2 \\
\vdots \\
\varepsilon_N
\end{bmatrix}
\\
\underbrace{\vec{y}}_{N \times 1} &amp;= \underbrace{\mat{X}}_{N \times K} \,\, \underbrace{\vec{\beta}}_{K \times 1} + \underbrace{\vec{\varepsilon}}_{N \times 1}
\end{aligned}
\]</span> The matrix <span class="math inline">\(\mat{X}\)</span> is called the <em>design</em> matrix. Its rows are each observation in the data. Its columns are the intercept, a column vector of 1’s, and the values of each predictor.</p>
<p>The mean of the disturbance vector is 0, <span class="math display">\[
\E(\epsilon) = 0
\]</span></p>
</div>
<div id="assumptions" class="section level2">
<h2><span class="header-section-number">2.4</span> Assumptions</h2>
<p>With these assumptions</p>
<ul>
<li>When is OLS unbiased?</li>
<li>When is OLS efficient?</li>
<li>When are the OLS standard errors correct?</li>
</ul>
<table style="width:93%;">
<colgroup>
<col width="29%" />
<col width="29%" />
<col width="34%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Assumption</th>
<th align="left">Formal statement</th>
<th align="left">Consequence of violation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">No (perfect) collinearity</td>
<td align="left"><span class="math inline">\(\rank(\mat{X}) = K, K &lt; N\)</span></td>
<td align="left">Coefficients unidentified</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\mat{X}\)</span> is exogenous</td>
<td align="left"><span class="math inline">\(\E(\mat{X} \vec{\varepsilon}) = 0\)</span></td>
<td align="left">Biased, even as <span class="math inline">\(N \to \infty\)</span></td>
</tr>
<tr class="odd">
<td align="left">Disturbances have mean 0</td>
<td align="left"><span class="math inline">\(\E(\varepsilon) = 0\)</span></td>
<td align="left">Biased, even as <span class="math inline">\(N \to \infty\)</span></td>
</tr>
<tr class="even">
<td align="left">No serial correlation</td>
<td align="left"><span class="math inline">\(\E(\varepsilon_i \varepsilon_j) = 0\)</span>, <span class="math inline">\(i \neq j\)</span></td>
<td align="left">Unbiased, wrong se</td>
</tr>
<tr class="odd">
<td align="left">Homoskedastic errors</td>
<td align="left"><span class="math inline">\(\E(\vec{\varepsilon}\T \vec{\varepsilon})\)</span></td>
<td align="left">Unbiased, wrong se</td>
</tr>
<tr class="even">
<td align="left">Gaussian errors</td>
<td align="left"><span class="math inline">\(\varepsilon \sim \dnorm(0, \sigma^2)\)</span></td>
<td align="left">Unbiased, se wrong unless <span class="math inline">\(N \to \infty\)</span></td>
</tr>
</tbody>
</table>
<p>If assumptions 1–5, then OLS is the best linear unbiased estimator (BLUE) estimator.</p>
<div class="figure">
<img src="img/tobias-funke-blue.jpeg" alt="OLS is BLUE" />
<p class="caption">OLS is BLUE</p>
</div>
<p>However, 1–5 does not impy that OLS has the lowest MSE.</p>
<p>But if assumptions 1–6 hold, then OLS is the the minimum-variance unbiased (MVU) estimator. This means that for all estimators that are unbiased, OLS has the least variance in its sampling distribution.</p>
</div>
<div id="errors-in-linear-regression" class="section level2">
<h2><span class="header-section-number">2.5</span> Errors in Linear Regression</h2>
<p>Note, that OLS assumes that the variance of the the disturbances is constant <span class="math inline">\(\hat{Y} - Y = \varepsilon = \sigma^2\)</span>. What happens if it isn’t?</p>
<p><span class="math display">\[
\mat{\Sigma} =
\begin{bmatrix}
\Var(\varepsilon_1) &amp; \Cov(\varepsilon_1, \varepsilon_2) &amp; \cdots &amp; \Cov(\varepsilon_1, \varepsilon_N) \\
\Var(\varepsilon_2, \varepsilon_1) &amp; \Var(\varepsilon_2) &amp; \cdots &amp; \Cov(\varepsilon_2, \varepsilon_N) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\Cov(\varepsilon_N, \varepsilon_1) &amp; \Cov(\varepsilon_N, \varepsilon_2) &amp; \cdots &amp; \Cov(\varepsilon_N) \\
\end{bmatrix} \\
\Sigma =
\begin{bmatrix}
\E(\varepsilon_1^2) &amp; \E(\varepsilon_1 \varepsilon_2) &amp; \cdots &amp; \E(\varepsilon_1 \varepsilon_N) \\
\E(\varepsilon_2 \varepsilon_1) &amp; \E(\varepsilon_2^2) &amp; \cdots &amp; \E(\varepsilon_2 \varepsilon_N) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\E(\varepsilon_N \varepsilon_1) &amp; \E(\varepsilon_N \varepsilon_2) &amp; \cdots &amp; \E(\varepsilon_N^2) \\
\end{bmatrix} \\
\]</span> The matrix can be written more compactly as, <span class="math display">\[
\mat{\Sigma} = \E(\vec{\varepsilon} \vec{\varepsilon}\T)
\]</span></p>
<p>An assumption is that errors are independent, <span class="math inline">\(\E(\epsilon_i \epsilon_j)\)</span> for all <span class="math inline">\(i \neq j\)</span>. This means that all off-diagonal elements of <span class="math inline">\(\mat{\Sigma}\)</span> are 0$. Additionally, all <span class="math inline">\(\epsilon_i\)</span> are assumed to have the same variance, <span class="math inline">\(\sigma^2\)</span>. Thus, the variance-covariance matrix of the errors is a assumed to have a diagonal matrix with the form, <span class="math display">\[
\mat{\Sigma} = 
\begin{bmatrix}
\sigma^2 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma^2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \sigma^2
\end{bmatrix} 
= \sigma^2 \mat{I}_N
\]</span> If these assumptions of the errors do not hold, then <span class="math inline">\(\Sigma\)</span> does not take this form, and more complicated models than OLS need to be used to get correct standard errors.</p>
</div>
<div id="non-constant-variance-heteroskedasticity" class="section level2">
<h2><span class="header-section-number">2.6</span> Non-constant variance (Heteroskedasticity)</h2>
<p>The homoskedastic case assumes that each eror term has its own variance. In the heteroskedastic case, each distrurbance may have its own variance, but they are still uncorrelated (<span class="math inline">\(\mat{\Sigma}\)</span> is diagonal) <span class="math display">\[
\mat{\Sigma} = 
\begin{bmatrix}
\sigma_1^2 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma_2^2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \sigma_N^2
\end{bmatrix}
\]</span> The problem is that now there are <span class="math inline">\(N\)</span> variance parameters to estimate, in addition to the <span class="math inline">\(K\)</span> slope coefficients. Now, there are more parameters than we can estimate. With heteroskedasticity, OLS with be unbiased, but the standard errors will be incorrect.</p>
<p>More general case allows for heteroskedasticity, and autocorrelation (<span class="math inline">\(\Cov(\varepsilon_i, \varepsilon_j) \neq 0\)</span>), <span class="math display">\[
\mat{\Sigma} = 
\begin{bmatrix}
\sigma_1^2 &amp; \sigma_{1,2} &amp; \cdots &amp; \sigma_{1,N} \\
\sigma_{2,1} &amp; \sigma_2^2 &amp; \cdots &amp; \sigma_{2,N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\sigma_{N,1} &amp; \sigma_{N,2} &amp; \cdots &amp; \sigma_N^2
\end{bmatrix} 
\]</span> As with heteroskedasticity, OLS with be unbiased, but the standard errors will be incorrect.</p>
<p><strong>TODO</strong> Different views on heteroskedasticity.</p>
</div>
<div id="why-control-for-variables" class="section level2">
<h2><span class="header-section-number">2.7</span> Why Control for Variables</h2>
<p>The reason for controlling for variables depends on the purpose of regresion</p>
<dl>
<dt>Description</dt>
<dd><p>Get a sense of the relationships in the data</p>
</dd>
<dt>Prediction</dt>
<dd><p>More variables may improve prediction</p>
</dd>
<dt>Causal</dt>
<dd><p>Blocks <strong>confounding</strong>, which is when <span class="math inline">\(X\)</span> does not cause <span class="math inline">\(Y\)</span>, but is correlated</p>
</dd>
<dd>with <span class="math inline">\(Y\)</span> because another variable, <span class="math inline">\(Z\)</span>, causes <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.
</dd>
</dl>
<p>Multiple regression vs. bivariate regression</p>
<ol style="list-style-type: decimal">
<li>Slopes are predicted differences <em>conditional</em> on the other predictors.</li>
<li>Estimates are still found by minimizing the squared residuals</li>
<li>Regression with multiple covariates is equivalent to sequentially running multiple OLS regressions, each time removing the part explained by that predictor.</li>
<li>Adding or omitting variables in a regression can affect the bias and variance of the OLS estimator.</li>
</ol>
</div>
<div id="multicollinearity" class="section level2">
<h2><span class="header-section-number">2.8</span> Multicollinearity</h2>
</div>
<div id="most-general-ols-assumptions" class="section level2">
<h2><span class="header-section-number">2.9</span> Most general OLS assumptions</h2>
<ol style="list-style-type: decimal">
<li>Linearity: <span class="math inline">\(\vec{y} = \mat{X}\vec{\beta} + \vec{\epsilon}\)</span></li>
<li>Random/iid sample: <span class="math inline">\((y_i, \mat{X}&#39;_i)\)</span> are a iid sample from the population.</li>
<li>No perfect collinearity: <span class="math inline">\(\mat{X}\)</span> is an <span class="math inline">\(n \times (K+1)\)</span> matrix with rank <span class="math inline">\(K+1\)</span></li>
<li>Zero conditional mean: <span class="math inline">\(\E[\vec{\epsilon}|\mat{X}] = \vec{0}\)</span></li>
<li>Homoskedasticity: <span class="math inline">\(\text{var}(\vec{\epsilon}|\mat{X}) = \sigma_u^2 \mat{I}_n\)</span></li>
<li>Normality: <span class="math inline">\(\vec{\epsilon}|\mat{X} \sim N(\vec{0}, \sigma^2_u\mat{I}_n)\)</span></li>
</ol>
</div>
<div id="no-perfect-collinearity" class="section level2">
<h2><span class="header-section-number">2.10</span> No perfect collinearity</h2>
<ul>
<li>In matrix form: <span class="math inline">\(\mat{X}\)</span> is an <span class="math inline">\(n \times (K+1)\)</span> matrix with rank <span class="math inline">\(K+1\)</span></li>
<li><strong>Definition</strong> The <strong>rank</strong> of a matrix is the maximum number of linearly independent columns.</li>
<li>If <span class="math inline">\(\mat{X}\)</span> has rank <span class="math inline">\(K+1\)</span>, then all of its columns are linearly independent</li>
<li>… and none of its columns are linearly dependent <span class="math inline">\(\implies\)</span> no perfect collinearity</li>
<li><span class="math inline">\(\mat{X}\)</span> has rank <span class="math inline">\(K+1 \implies (\mat{X}&#39;\mat{X})\)</span> is invertible.</li>
<li>Just like variation in <span class="math inline">\(X\)</span> led us to be able to divide by the variance in simple OLS</li>
</ul>
</div>
<div id="expected-values-of-vectors" class="section level2">
<h2><span class="header-section-number">2.11</span> Expected values of vectors</h2>
<ul>
<li>The expected value of the vector is just the expected value of its entries.</li>
<li>Using the zero mean conditional error assumptions: <span class="math display">\[\E[\vec{\epsilon}|\mat{X}] = \left[\begin{array}{c} \E[u_1 | \mat{X}] \\ \E[u_2|\mat{X}] \\ \vdots \\ \E[u_n|\mat{X}] \end{array} \right] = \left[\begin{array}{c} 0 \\ 0 \\ \vdots \\ 0 \end{array} \right] = \vec{0}\]</span></li>
</ul>
</div>
<div id="ols-is-unbiased" class="section level2">
<h2><span class="header-section-number">2.12</span> OLS is unbiased</h2>
<ul>
<li>Under matrix assumptions 1-4, OLS is unbiased for <span class="math inline">\(\vec{\beta}\)</span>: <span class="math display">\[\E[\widehat{\vec{\beta}}] = \vec{\beta}\]</span></li>
</ul>
</div>
<div id="variance-covariance-matrix-of-random-vectors" class="section level2">
<h2><span class="header-section-number">2.13</span> Variance-covariance matrix of random vectors</h2>
<ul>
<li>The homoskedasticity assumption is different: <span class="math inline">\(\text{var}(\vec{\epsilon}|\mat{X}) = \sigma_u^2 \mat{I}_n\)</span></li>
<li>In order to investigate this, we need to know what the variance of a vector is.</li>
<li>The variance of a vector is actually a matrix:</li>
</ul>
<p><span class="math display">\[\Var[\vec{\epsilon}] = \Sigma_u = \left[ 
\begin{bmatrix}
\Var(u_1) &amp; \Cov(u_1,u_2) &amp; \dots &amp; \Cov(u_1, u_n) \\
\Cov(u_2,u_1) &amp; \Var(u_2) &amp; \dots &amp; \Cov(u_2,u_n) \\
 \vdots &amp; &amp; \ddots &amp; \\
\Cov(u_n,u_1) &amp; \Cov(u_n,u_2) &amp; \dots &amp; \Var(u_{n})
\end{bmatrix}\]</span></p>
<ul>
<li>This matrix is symmetric since <span class="math inline">\(\text{cov}(u_i,u_j) = \text{cov}(u_i,u_j)\)</span></li>
</ul>
</div>
<div id="matrix-version-of-homoskedasticity" class="section level2">
<h2><span class="header-section-number">2.14</span> Matrix version of homoskedasticity</h2>
<ul>
<li>Once again: <span class="math inline">\(\text{var}(\vec{\epsilon}|\mat{X}) = \sigma_u^2 \mat{I}_n\)</span></li>
<li>Visually:</li>
</ul>
<p><span class="math display">\[\text{var}[\vec{\epsilon}] =  \sigma^2_u \mat{I}_n=
\left[
\begin{array}{ccccc}
\sigma_u^2 &amp; 0 &amp; 0 &amp; \dots &amp; 0 \\
 0 &amp; \sigma_u^2 &amp; 0 &amp; \dots &amp; 0 \\
 &amp; &amp; &amp; \vdots &amp; \\
 0 &amp; 0 &amp; 0 &amp; \dots &amp; \sigma_u^2
 \end{array}
\right]\]</span></p>
<ul>
<li>In less matrix notation:
<ul>
<li><span class="math inline">\(\text{var}(u_i) = \sigma^2_u\)</span> for all <span class="math inline">\(i\)</span> (constant variance)</li>
<li><span class="math inline">\(\text{cov}(u_i,u_j) = 0\)</span> for all <span class="math inline">\(i \neq j\)</span> (implied by iid)</li>
</ul></li>
</ul>
</div>
<div id="sampling-variance-for-ols-estimates" class="section level2">
<h2><span class="header-section-number">2.15</span> Sampling variance for OLS estimates</h2>
<p>Under assumptions 1-5, the sampling variance of the OLS estimator can be written in matrix form as the following: <span class="math display">\[\text{var}[\widehat{\vec{\beta}}] = \sigma^2_u(\mat{X}&#39;\mat{X})^{-1}\]</span></p>
<ul>
<li>This matrix looks like this:</li>
</ul>
\begin{center}
\begin{tabular}{c|ccccc}
           &amp;    $\widehat{\beta}_0$ &amp;    $\widehat{\beta}_1$ &amp;    $\widehat{\beta}_2$ &amp;  $\cdots$   &amp;    $\widehat{\beta}_K$ \\
\hline
   $\widehat{\beta}_0$ &amp; $\text{var}[\widehat{\beta}_0]$ &amp; $\text{cov}[\widehat{\beta}_0,\widehat{\beta}_1]$ &amp;
   $\text{cov}[\widehat{\beta}_0,\widehat{\beta}_2]$ &amp; $\cdots$ &amp;  $\text{cov}[\widehat{\beta}_0,\widehat{\beta}_K]$   \\

   $\widehat{\beta}_1$ &amp; $\text{cov}[\widehat{\beta}_0,\widehat{\beta}_1]$ &amp; $\text{var}[\widehat{\beta}_1]$ &amp;
    $\text{cov}[\widehat{\beta}_1,\widehat{\beta}_2]$ &amp;  $\cdots$    &amp;  $\text{cov}[\widehat{\beta}_1,\widehat{\beta}_K]$ \\

   $\widehat{\beta}_2$ &amp; $\text{cov}[\widehat{\beta}_0,\widehat{\beta}_2]$ &amp; $\text{cov}[\widehat{\beta}_1,\widehat{\beta}_2]$ &amp; $\text{var}[\widehat{\beta}_2]$ &amp;   $\cdots$  &amp;   $\text{cov}[\widehat{\beta}_2,\widehat{\beta}_K]$  \\
    $\vdots$    &amp;  $\vdots$  &amp; $\vdots$    &amp;  $\vdots$    &amp;    $\ddots$        &amp;    $\vdots$        \\
   $\widehat{\beta}_K$ &amp; $\text{cov}[\widehat{\beta}_0,\widehat{\beta}_K]$ &amp; $\text{cov}[\widehat{\beta}_K,\widehat{\beta}_1]$ &amp; $\text{cov}[\widehat{\beta}_K,\widehat{\beta}_2]$ &amp;     $\cdots$       &amp; $\text{var}[\widehat{\beta}_K]$ \\
\end{tabular}
\end{center}
</div>
<div id="inference-in-the-general-setting" class="section level2">
<h2><span class="header-section-number">2.16</span> Inference in the general setting</h2>
<ul>
<li><p>Under assumption 1-5 in large samples: <span class="math display">\[\frac{\widehat{\beta}_k - \beta_k}{\widehat{SE}[\widehat{\beta}_k]} \sim N(0,1)\]</span></p></li>
<li><p>In small samples, under assumptions 1-6,</p></li>
</ul>
<p><span class="math display">\[\frac{\widehat{\beta}_k - \beta_k}{\widehat{SE}[\widehat{\beta}_k]} \sim t_{n - (K+1)}\]</span></p>
<ul>
<li><p>Thus, under the null of <span class="math inline">\(H_0: \beta_k = 0\)</span>, we know that <span class="math display">\[\frac{\widehat{\beta}_k}{\widehat{SE}[\widehat{\beta}_k]} \sim t_{n - (K+1)}\]</span></p></li>
<li><p>Here, the estimated SEs come from: <span class="math display">\[\begin{aligned}
\widehat{\text{var}}[\widehat{\vec{\beta}}] &amp;= \widehat{\sigma}^2_u(\mat{X}&#39;\mat{X})^{-1} \\
 \widehat{\sigma}^2_u &amp;= \frac{\widehat{\vec{\epsilon}}&#39;\widehat{\vec{\epsilon}}}{n-(k+1)}
\end{aligned}\]</span></p></li>
</ul>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendix.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/UW-POLS503/pols503-notes/edit/gh-pages/ols-estimator.Rmd",
"text": "Edit"
},
"download": ["pols503-notes.pdf", "pols503-notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>


</body>

</html>
