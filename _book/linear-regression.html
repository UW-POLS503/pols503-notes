<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>POLS 503: Advanced Quantitative Political Methodology: The Notes</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>">
  <meta name="generator" content="bookdown 0.0.60 and GitBook 2.6.7">

  <meta property="og:title" content="POLS 503: Advanced Quantitative Political Methodology: The Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>" />
  <meta name="github-repo" content="UW-POLS503/pols503-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="POLS 503: Advanced Quantitative Political Methodology: The Notes" />
  
  <meta name="twitter:description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>" />
  

<meta name="author" content="Jeffrey B. Arnold">

<meta name="date" content="2016-04-18">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="what-makes-an-estimator-good.html">

<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />











<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>

$$
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Bias}{Bias}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\T}{'}

\newcommand{\distr}[1]{\mathcal{#1}}
\newcommand{\dnorm}{\distr{N}}
\newcommand{\dmvnorm}[1]{\distr{N}_{#1}}
$$

  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>2</b> Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="linear-regression.html"><a href="linear-regression.html#matrix-representation"><i class="fa fa-check"></i><b>2.1</b> Matrix Representation</a></li>
<li class="chapter" data-level="2.2" data-path="linear-regression.html"><a href="linear-regression.html#estimating-ols"><i class="fa fa-check"></i><b>2.2</b> Estimating OLS</a></li>
<li class="chapter" data-level="2.3" data-path="linear-regression.html"><a href="linear-regression.html#assumptions"><i class="fa fa-check"></i><b>2.3</b> Assumptions</a></li>
<li class="chapter" data-level="2.4" data-path="linear-regression.html"><a href="linear-regression.html#errors-in-linear-regression"><i class="fa fa-check"></i><b>2.4</b> Errors in Linear Regression</a></li>
<li class="chapter" data-level="2.5" data-path="linear-regression.html"><a href="linear-regression.html#non-constant-variance-heteroskedasticity"><i class="fa fa-check"></i><b>2.5</b> Non-constant variance (Heteroskedasticity)</a></li>
<li class="chapter" data-level="2.6" data-path="linear-regression.html"><a href="linear-regression.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>2.6</b> Multivariate Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="what-makes-an-estimator-good.html"><a href="what-makes-an-estimator-good.html"><i class="fa fa-check"></i><b>3</b> What makes an estimator good?</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">POLS 503: Advanced Quantitative Political Methodology: The Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Linear Regression</h1>
<div id="matrix-representation" class="section level2">
<h2><span class="header-section-number">2.1</span> Matrix Representation</h2>
<p>The linear regression function can be written as a scalar function for each observation, <span class="math inline">\(i = 1, \dots, N\)</span>, <span class="math display">\[
\begin{aligned}[t]
y_i &amp;= \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \cdots + \beta_{K,i} + \varepsilon_i \\
 &amp;= \beta_0 + \sum_{k = 1}^{K} \beta_k x_{k,i} + \varepsilon_i \\
&amp;= \sum_{k = 0}^{K} \beta_k x_{k,i} + \varepsilon_i
\end{aligned}
\]</span> where <span class="math inline">\(x_{0,i} = 1\)</span> for all <span class="math inline">\(i \in 1:N\)</span>.</p>
<p>The linear regression can be more compactly written in matrix form, <span class="math display">\[
\begin{aligned}[t]
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_N
\end{bmatrix} &amp;=
\begin{bmatrix} 
1 &amp; x_{1,1} &amp; x_{2,1} &amp; \cdots &amp; x_{K,1} \\
1 &amp; x_{1,2} &amp; x_{2,2} &amp; \cdots &amp; x_{K,2} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_{1,N}&amp; x_{2,n} &amp; \cdots &amp; x_{K,N}
\end{bmatrix}
\begin{bmatrix} 
\beta_0 \\
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_K
\end{bmatrix}
+ 
\begin{bmatrix}
\varepsilon_1 \\
\varepsilon_2 \\
\vdots \\
\varepsilon_N
\end{bmatrix}
\\
\underbrace{\vec{y}}_{N \times 1} &amp;= \underbrace{\mat{X}}_{N \times K} \,\, \underbrace{\vec{\beta}}_{K \times 1} + \underbrace{\vec{\varepsilon}}_{N \times 1}
\end{aligned}
\]</span> The matrix <span class="math inline">\(\mat{X}\)</span> is called the <em>design</em> matrix. Its rows are each observation in the data. Its columns are the intercept, a column vector of 1’s, and the values of each predictor.</p>
<p>The mean of the disturbance vector is 0, <span class="math display">\[
\E(\epsilon) = 0
\]</span></p>
</div>
<div id="estimating-ols" class="section level2">
<h2><span class="header-section-number">2.2</span> Estimating OLS</h2>
<p>The OLS estimator finds estimates of a paramters</p>
</div>
<div id="assumptions" class="section level2">
<h2><span class="header-section-number">2.3</span> Assumptions</h2>
<p>With these assumptions</p>
<ul>
<li>When is OLS unbiased?</li>
<li>When is OLS efficient?</li>
<li>When are the OLS standard errors correct?</li>
</ul>
<table style="width:93%;">
<colgroup>
<col width="29%" />
<col width="29%" />
<col width="34%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Assumption</th>
<th align="left">Formal statement</th>
<th align="left">Consequence of violation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">No (perfect) collinearity</td>
<td align="left"><span class="math inline">\(\rank(\mat{X}) = K, K &lt; N\)</span></td>
<td align="left">Coefficients unidentified</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\mat{X}\)</span> is exogenous</td>
<td align="left"><span class="math inline">\(\E(\mat{X} \vec{\varepsilon}) = 0\)</span></td>
<td align="left">Biased, even as <span class="math inline">\(N \to \infty\)</span></td>
</tr>
<tr class="odd">
<td align="left">Disturbances have mean 0</td>
<td align="left"><span class="math inline">\(\E(\varepsilon) = 0\)</span></td>
<td align="left">Biased, even as <span class="math inline">\(N \to \infty\)</span></td>
</tr>
<tr class="even">
<td align="left">No serial correlation</td>
<td align="left"><span class="math inline">\(\E(\varepsilon_i \varepsilon_j) = 0\)</span>, <span class="math inline">\(i \neq j\)</span></td>
<td align="left">Unbiased, wrong se</td>
</tr>
<tr class="odd">
<td align="left">Homoskedastic errors</td>
<td align="left"><span class="math inline">\(\E(\vec{\varepsilon}\T \vec{\varepsilon})\)</span></td>
<td align="left">Unbiased, wrong se</td>
</tr>
<tr class="even">
<td align="left">Gaussian errors</td>
<td align="left"><span class="math inline">\(\varepsilon \sim \dnorm(0, \sigma^2)\)</span></td>
<td align="left">Unbiased, se wrong unless <span class="math inline">\(N \to \infty\)</span></td>
</tr>
</tbody>
</table>
<p>If assumptions 1–5, then OLS is the best linear unbiased estimator (BLUE) estimator.</p>
<div class="figure">
<img src="tobias-funke-blue.jpeg" alt="OLS is BLUE" />
<p class="caption">OLS is BLUE</p>
</div>
<p>However, 1–5 does not impy that OLS has the lowest MSE.</p>
<p>But if assumptions 1–6 hold, then OLS is the the minimum-variance unbiased (MVU) estimator. This means that for all estimators that are unbiased, OLS has the least variance in its sampling distribution.</p>
</div>
<div id="errors-in-linear-regression" class="section level2">
<h2><span class="header-section-number">2.4</span> Errors in Linear Regression</h2>
<p>Note, that OLS assumes that the variance of the the disturbances is constant <span class="math inline">\(\hat{Y} - Y = \varepsilon = \sigma^2\)</span>. What happens if it isn’t?</p>
<p><span class="math display">\[
\mat{\Sigma} =
\begin{bmatrix}
\Var(\varepsilon_1) &amp; \Cov(\varepsilon_1, \varepsilon_2) &amp; \cdots &amp; \Cov(\varepsilon_1, \varepsilon_N) \\
\Var(\varepsilon_2, \varepsilon_1) &amp; \Var(\varepsilon_2) &amp; \cdots &amp; \Cov(\varepsilon_2, \varepsilon_N) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\Cov(\varepsilon_N, \varepsilon_1) &amp; \Cov(\varepsilon_N, \varepsilon_2) &amp; \cdots &amp; \Cov(\varepsilon_N) \\
\end{bmatrix} \\
\Sigma =
\begin{bmatrix}
\E(\varepsilon_1^2) &amp; \E(\varepsilon_1 \varepsilon_2) &amp; \cdots &amp; \E(\varepsilon_1 \varepsilon_N) \\
\E(\varepsilon_2 \varepsilon_1) &amp; \E(\varepsilon_2^2) &amp; \cdots &amp; \E(\varepsilon_2 \varepsilon_N) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\E(\varepsilon_N \varepsilon_1) &amp; \E(\varepsilon_N \varepsilon_2) &amp; \cdots &amp; \E(\varepsilon_N^2) \\
\end{bmatrix} \\
\]</span> The matrix can be written more compactly as, <span class="math display">\[
\mat{\Sigma} = \E(\vec{\varepsilon} \vec{\varepsilon}\T)
\]</span></p>
<p>An assumption is that errors are independent, <span class="math inline">\(\E(\epsilon_i \epsilon_j)\)</span> for all <span class="math inline">\(i \neq j\)</span>. This means that all off-diagonal elements of <span class="math inline">\(\mat{\Sigma}\)</span> are 0$. Additionally, all <span class="math inline">\(\epsilon_i\)</span> are assumed to have the same variance, <span class="math inline">\(\sigma^2\)</span>. Thus, the variance-covariance matrix of the errors is a assumed to have a diagonal matrix with the form, <span class="math display">\[
\mat{\Sigma} = 
\begin{bmatrix}
\sigma^2 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma^2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \sigma^2
\end{bmatrix} 
= \sigma^2 \mat{I}_N
\]</span> If these assumptions of the errors do not hold, then <span class="math inline">\(\Sigma\)</span> does not take this form, and more complicated models than OLS need to be used to get correct standard errors.</p>
</div>
<div id="non-constant-variance-heteroskedasticity" class="section level2">
<h2><span class="header-section-number">2.5</span> Non-constant variance (Heteroskedasticity)</h2>
<p>The homoskedastic case assumes that each eror term has its own variance. In the heteroskedastic case, each distrurbance may have its own variance, but they are still uncorrelated (<span class="math inline">\(\mat{\Sigma}\)</span> is diagonal) <span class="math display">\[
\mat{\Sigma} = 
\begin{bmatrix}
\sigma_1^2 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma_2^2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \sigma_N^2
\end{bmatrix}
\]</span> The problem is that now there are <span class="math inline">\(N\)</span> variance parameters to estimate, in addition to the <span class="math inline">\(K\)</span> slope coefficients. Now, there are more parameters than we can estimate. With heteroskedasticity, OLS with be unbiased, but the standard errors will be incorrect.</p>
<p>More general case allows for heteroskedasticity, and autocorrelation (<span class="math inline">\(\Cov(\varepsilon_i, \varepsilon_j) \neq 0\)</span>), <span class="math display">\[
\mat{\Sigma} = 
\begin{bmatrix}
\sigma_1^2 &amp; \sigma_{1,2} &amp; \cdots &amp; \sigma_{1,N} \\
\sigma_{2,1} &amp; \sigma_2^2 &amp; \cdots &amp; \sigma_{2,N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\sigma_{N,1} &amp; \sigma_{N,2} &amp; \cdots &amp; \sigma_N^2
\end{bmatrix} 
\]</span> As with heteroskedasticity, OLS with be unbiased, but the standard errors will be incorrect.</p>
<p><strong>TODO</strong> Different views on heteroskedasticity.</p>
</div>
<div id="multivariate-normal-distribution" class="section level2">
<h2><span class="header-section-number">2.6</span> Multivariate Normal Distribution</h2>
<p>The multivariate normal distribution is the generalization of the univariate normal distribution to more than one dimension.<a href="what-makes-an-estimator-good.html#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> The random variable, <span class="math inline">\(\vec{x}\)</span>, is a length <span class="math inline">\(k\)</span> vector. The <span class="math inline">\(k\)</span> length vector <span class="math inline">\(\vec{\mu}\)</span> are the means of <span class="math inline">\(\vec{x}\)</span>, and the <span class="math inline">\(k \times k\)</span> matrix, <span class="math inline">\(\mat{\Sigma}\)</span>, is the variance-covariance matrix, <span class="math display">\[
\begin{aligned}[t]
\vec{x} &amp;\sim \dmvnorm{k}\left(\vec{\mu}, \mat{\Sigma} \right) \\
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_k
\end{bmatrix}
&amp; \sim
\dmvnorm{k}
\left(
  \begin{bmatrix}
  \mu_1 \\
  \mu_2 \\
  \vdots \\
  \mu_k
  \end{bmatrix},
  \begin{bmatrix}
  \sigma_1^2 &amp; \sigma_{1,2} &amp; \cdots &amp; \sigma_{1, k} \\
  \sigma_{2,1} &amp; \sigma_2^2 &amp; \cdots &amp; \sigma_{2, k} \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  \sigma_{k,1} &amp; \sigma_{k,2} &amp; \cdots &amp; \sigma_{k, k}
  \end{bmatrix}
\right)
\end{aligned}
\]</span> <span class="math display">\[
p(\vec{x}; \vec{\mu}, \mat{\Sigma}) =
(2 k)^{-\frac{k}{2}}
\left| \mat{\Sigma} \right|^{-\frac{1}{2}}
\exp \left( -\frac{1}{2} (\vec{x} - \vec{\mu})\T \mat{\Sigma}^{-1} (\vec{x} - \vec{\mu}) \right) .
\]</span></p>
<p>You can sample from and calculate the density for the multivariate normal distribution with the functions <code>dmvnorm</code> and <code>rmvnorm</code> from the package <strong>mvtnorm</strong>.</p>
<pre><code>## 
## Attaching package: &#39;purrr&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     order_by</code></pre>
<p><span class="math display">\[
\begin{bmatrix}
  x_1 \\
  x_2
\end{bmatrix}
\sim \dmvnorm{2}
\left(
  \begin{bmatrix}
    0 \\
    0
  \end{bmatrix},
  \begin{bmatrix}
    1 &amp; 0 \\
    0 &amp; 1
  \end{bmatrix}
\right)
\]</span></p>
<p><span class="math display">\[
\begin{bmatrix}
  x_1 \\
  x_2
\end{bmatrix}
\sim \dmvnorm{2}
\left(
  \begin{bmatrix}
    0 \\
    2
  \end{bmatrix},
  \begin{bmatrix}
    1 &amp; 0 \\
    0 &amp; 1
  \end{bmatrix}
\right)
\]</span></p>
<p><span class="math display">\[
\begin{bmatrix}
  x_1 \\
  x_2
\end{bmatrix}
\sim \dmvnorm{2}
\left(
  \begin{bmatrix}
    2 \\
    2
  \end{bmatrix},
  \begin{bmatrix}
    1 &amp; 0 \\
    0 &amp; 1
  \end{bmatrix}
\right)
\]</span></p>
<p><span class="math display">\[
\begin{bmatrix}
  x_1 \\
  x_2
\end{bmatrix}
\sim \dmvnorm{2}
\left(
  \begin{bmatrix}
    0 \\
    0
  \end{bmatrix},
  \begin{bmatrix}
    0.33 &amp; 0 \\
    0 &amp; 1
  \end{bmatrix}
\right)
\]</span></p>
<p><span class="math display">\[
\begin{bmatrix}
  x_1 \\
  x_2
\end{bmatrix}
\sim \dmvnorm{2}
\left(
  \begin{bmatrix}
    0 \\
    0
  \end{bmatrix},
  \begin{bmatrix}
    0.33 &amp; 0 \\
    0 &amp; 3
  \end{bmatrix}
\right)
\]</span></p>
<p><span class="math display">\[
\begin{bmatrix}
  x_1 \\
  x_2
\end{bmatrix}
\sim \dmvnorm{2}
\left(
  \begin{bmatrix}
    0 \\
    0
  \end{bmatrix},
  \begin{bmatrix}
    0.33 &amp; 0 \\
    0 &amp; 0.33
  \end{bmatrix}
\right)
\]</span></p>
<p><span class="math display">\[
\begin{bmatrix}
  x_1 \\
  x_2
\end{bmatrix}
\sim \dmvnorm{2}
\left(
  \begin{bmatrix}
    0 \\
    0
  \end{bmatrix},
  \begin{bmatrix}
    1 &amp; 0.8 \\
    0.8 &amp; 1
  \end{bmatrix}
\right)
\]</span></p>
<p><span class="math display">\[
\begin{bmatrix}
  x_1 \\
  x_2
\end{bmatrix}
\sim \dmvnorm{2}
\left(
  \begin{bmatrix}
    0 \\
    0
  \end{bmatrix},
  \begin{bmatrix}
    1 &amp; -0.8 \\
    -0.8 &amp; 1
  \end{bmatrix}
\right)
\]</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="what-makes-an-estimator-good.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/UW-POLS503/pols503-notes/edit/gh-pages/ols-estimator.Rmd",
"text": "Edit"
},
"download": ["pols503-notes.pdf", "pols503-notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>


</body>

</html>
