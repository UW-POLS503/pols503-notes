<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>POLS 503: Advanced Quantitative Political Methodology: The Notes</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>">
  <meta name="generator" content="bookdown 0.0.60 and GitBook 2.6.7">

  <meta property="og:title" content="POLS 503: Advanced Quantitative Political Methodology: The Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>" />
  <meta name="github-repo" content="UW-POLS503/pols503-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="POLS 503: Advanced Quantitative Political Methodology: The Notes" />
  
  <meta name="twitter:description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>" />
  

<meta name="author" content="Jeffrey B. Arnold">

<meta name="date" content="2016-04-13">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="review-of-statistics.html">
<link rel="next" href="multiple-regression.html">

<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    extensions: ["autoload-all.js"]
  }
});
</script>

$$
\usepackage{booktabs}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\cor}{Cor}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\T}{'}

\newcommand{\distr}[1]{\ensuremath{\mathcal{#1}}}
\newcommand{\dnorm}{\distr{N}}
$$



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="review-of-statistics.html"><a href="review-of-statistics.html"><i class="fa fa-check"></i><b>2</b> Review of Statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="review-of-statistics.html"><a href="review-of-statistics.html#terms"><i class="fa fa-check"></i><b>2.1</b> Terms</a><ul>
<li class="chapter" data-level="2.1.1" data-path="review-of-statistics.html"><a href="review-of-statistics.html#statistic"><i class="fa fa-check"></i><b>2.1.1</b> Statistic</a></li>
<li class="chapter" data-level="2.1.2" data-path="review-of-statistics.html"><a href="review-of-statistics.html#parameter"><i class="fa fa-check"></i><b>2.1.2</b> Parameter</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="review-of-statistics.html"><a href="review-of-statistics.html#estimators-and-estimates"><i class="fa fa-check"></i><b>2.2</b> Estimators and Estimates</a><ul>
<li class="chapter" data-level="2.2.1" data-path="review-of-statistics.html"><a href="review-of-statistics.html#sampling-distribution"><i class="fa fa-check"></i><b>2.2.1</b> Sampling distribution</a></li>
<li class="chapter" data-level="2.2.2" data-path="review-of-statistics.html"><a href="review-of-statistics.html#standard-error"><i class="fa fa-check"></i><b>2.2.2</b> Standard Error</a></li>
<li class="chapter" data-level="2.2.3" data-path="review-of-statistics.html"><a href="review-of-statistics.html#methods-for-evaluating-statistics"><i class="fa fa-check"></i><b>2.2.3</b> Methods for evaluating statistics</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="review-of-statistics.html"><a href="review-of-statistics.html#example"><i class="fa fa-check"></i><b>2.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>3</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#representing-the-relationship-between-two-variables"><i class="fa fa-check"></i><b>3.1</b> Representing the Relationship between two variables</a></li>
<li class="chapter" data-level="3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#conditional-expectation-function"><i class="fa fa-check"></i><b>3.2</b> Conditional Expectation Function</a><ul>
<li class="chapter" data-level="3.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#three-uses-of-regression"><i class="fa fa-check"></i><b>3.2.1</b> Three uses of regression</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#population-linear-regression-function"><i class="fa fa-check"></i><b>3.3</b> Population Linear Regression Function</a></li>
<li class="chapter" data-level="3.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#sample-linear-regression-function"><i class="fa fa-check"></i><b>3.4</b> Sample Linear Regression Function</a></li>
<li class="chapter" data-level="3.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#ordinary-least-squares"><i class="fa fa-check"></i><b>3.5</b> Ordinary Least Squares</a><ul>
<li class="chapter" data-level="3.5.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#mechanical-properties-of-the-ols-statistic"><i class="fa fa-check"></i><b>3.5.1</b> Mechanical properties of the OLS statistic</a></li>
<li class="chapter" data-level="3.5.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#ols-slope-is-a-weighted-sum-of-the-outcomes"><i class="fa fa-check"></i><b>3.5.2</b> OLS slope is a weighted sum of the outcomes</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#properties-of-the-ols-estimator"><i class="fa fa-check"></i><b>3.6</b> Properties of the OLS Estimator</a></li>
<li class="chapter" data-level="3.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#properties-of-the-estimator"><i class="fa fa-check"></i><b>3.7</b> Properties of the Estimator</a></li>
<li class="chapter" data-level="3.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#assumptions-for-unbiasedness-and-consistency-of-ols"><i class="fa fa-check"></i><b>3.8</b> Assumptions for unbiasedness and consistency of OLS</a></li>
<li class="chapter" data-level="3.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#hypothesis-tests-for-regression"><i class="fa fa-check"></i><b>3.9</b> Hypothesis Tests for Regression</a></li>
<li class="chapter" data-level="3.10" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#confidence-intervals-for-regression"><i class="fa fa-check"></i><b>3.10</b> Confidence Intervals for Regression</a></li>
<li class="chapter" data-level="3.11" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#goodness-of-fit"><i class="fa fa-check"></i><b>3.11</b> Goodness of Fit</a></li>
<li class="chapter" data-level="3.12" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#references"><i class="fa fa-check"></i><b>3.12</b> References</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>4</b> Multiple Regression</a><ul>
<li class="chapter" data-level="4.1" data-path="multiple-regression.html"><a href="multiple-regression.html#multiple-linear-regression-in-matrix-form"><i class="fa fa-check"></i><b>4.1</b> Multiple linear regression in matrix form</a><ul>
<li class="chapter" data-level="4.1.1" data-path="multiple-regression.html"><a href="multiple-regression.html#residuals"><i class="fa fa-check"></i><b>4.1.1</b> Residuals</a></li>
<li class="chapter" data-level="4.1.2" data-path="multiple-regression.html"><a href="multiple-regression.html#ols-estimator-in-matrix-form"><i class="fa fa-check"></i><b>4.1.2</b> OLS estimator in matrix form</a></li>
<li class="chapter" data-level="4.1.3" data-path="multiple-regression.html"><a href="multiple-regression.html#scalar-inverses"><i class="fa fa-check"></i><b>4.1.3</b> Scalar inverses</a></li>
<li class="chapter" data-level="4.1.4" data-path="multiple-regression.html"><a href="multiple-regression.html#matrix-inverses"><i class="fa fa-check"></i><b>4.1.4</b> Matrix inverses</a></li>
<li class="chapter" data-level="4.1.5" data-path="multiple-regression.html"><a href="multiple-regression.html#inference"><i class="fa fa-check"></i><b>4.1.5</b> Inference</a></li>
<li class="chapter" data-level="4.1.6" data-path="multiple-regression.html"><a href="multiple-regression.html#back-to-ols"><i class="fa fa-check"></i><b>4.1.6</b> Back to OLS</a></li>
<li class="chapter" data-level="4.1.7" data-path="multiple-regression.html"><a href="multiple-regression.html#intuition-for-the-ols-in-matrix-form"><i class="fa fa-check"></i><b>4.1.7</b> Intuition for the OLS in matrix form</a></li>
<li class="chapter" data-level="4.1.8" data-path="multiple-regression.html"><a href="multiple-regression.html#most-general-ols-assumptions"><i class="fa fa-check"></i><b>4.1.8</b> Most general OLS assumptions</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="multiple-regression.html"><a href="multiple-regression.html#no-perfect-collinearity"><i class="fa fa-check"></i><b>4.2</b> No perfect collinearity</a></li>
<li class="chapter" data-level="4.3" data-path="multiple-regression.html"><a href="multiple-regression.html#expected-values-of-vectors"><i class="fa fa-check"></i><b>4.3</b> Expected values of vectors</a></li>
<li class="chapter" data-level="4.4" data-path="multiple-regression.html"><a href="multiple-regression.html#ols-is-unbiased"><i class="fa fa-check"></i><b>4.4</b> OLS is unbiased</a></li>
<li class="chapter" data-level="4.5" data-path="multiple-regression.html"><a href="multiple-regression.html#variance-covariance-matrix-of-random-vectors"><i class="fa fa-check"></i><b>4.5</b> Variance-covariance matrix of random vectors</a><ul>
<li class="chapter" data-level="4.5.1" data-path="multiple-regression.html"><a href="multiple-regression.html#matrix-version-of-homoskedasticity"><i class="fa fa-check"></i><b>4.5.1</b> Matrix version of homoskedasticity</a></li>
<li class="chapter" data-level="4.5.2" data-path="multiple-regression.html"><a href="multiple-regression.html#sampling-variance-for-ols-estimates"><i class="fa fa-check"></i><b>4.5.2</b> Sampling variance for OLS estimates</a></li>
<li class="chapter" data-level="4.5.3" data-path="multiple-regression.html"><a href="multiple-regression.html#inference-in-the-general-setting"><i class="fa fa-check"></i><b>4.5.3</b> Inference in the general setting</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="multiple-regression.html"><a href="multiple-regression.html#matrix-algebra-review"><i class="fa fa-check"></i><b>4.6</b> Matrix algebra review</a><ul>
<li class="chapter" data-level="4.6.1" data-path="multiple-regression.html"><a href="multiple-regression.html#matrices-and-vectors"><i class="fa fa-check"></i><b>4.6.1</b> Matrices and vectors</a></li>
<li class="chapter" data-level="4.6.2" data-path="multiple-regression.html"><a href="multiple-regression.html#examples-of-matrices"><i class="fa fa-check"></i><b>4.6.2</b> Examples of matrices</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="multiple-regression.html"><a href="multiple-regression.html#vectors"><i class="fa fa-check"></i><b>4.7</b> Vectors</a></li>
<li class="chapter" data-level="4.8" data-path="multiple-regression.html"><a href="multiple-regression.html#vector-examples"><i class="fa fa-check"></i><b>4.8</b> Vector examples</a></li>
<li class="chapter" data-level="4.9" data-path="multiple-regression.html"><a href="multiple-regression.html#transpose"><i class="fa fa-check"></i><b>4.9</b> Transpose</a></li>
<li class="chapter" data-level="4.10" data-path="multiple-regression.html"><a href="multiple-regression.html#transposing-vectors"><i class="fa fa-check"></i><b>4.10</b> Transposing vectors</a></li>
<li class="chapter" data-level="4.11" data-path="multiple-regression.html"><a href="multiple-regression.html#write-matrices-as-vectors"><i class="fa fa-check"></i><b>4.11</b> Write matrices as vectors</a></li>
<li class="chapter" data-level="4.12" data-path="multiple-regression.html"><a href="multiple-regression.html#addition-and-subtraction"><i class="fa fa-check"></i><b>4.12</b> Addition and subtraction</a></li>
<li class="chapter" data-level="4.13" data-path="multiple-regression.html"><a href="multiple-regression.html#scalar-multiplication"><i class="fa fa-check"></i><b>4.13</b> Scalar multiplication</a></li>
<li class="chapter" data-level="4.14" data-path="multiple-regression.html"><a href="multiple-regression.html#the-linear-model-with-new-notation"><i class="fa fa-check"></i><b>4.14</b> The linear model with new notation</a></li>
<li class="chapter" data-level="4.15" data-path="multiple-regression.html"><a href="multiple-regression.html#matrix-multiplication-by-a-vector"><i class="fa fa-check"></i><b>4.15</b> Matrix multiplication by a vector</a></li>
<li class="chapter" data-level="4.16" data-path="multiple-regression.html"><a href="multiple-regression.html#matrix-multiplication"><i class="fa fa-check"></i><b>4.16</b> Matrix multiplication</a></li>
<li class="chapter" data-level="4.17" data-path="multiple-regression.html"><a href="multiple-regression.html#special-multiplications"><i class="fa fa-check"></i><b>4.17</b> Special multiplications</a></li>
<li class="chapter" data-level="4.18" data-path="multiple-regression.html"><a href="multiple-regression.html#special-matrices-and-jargon"><i class="fa fa-check"></i><b>4.18</b> Special matrices and jargon</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">POLS 503: Advanced Quantitative Political Methodology: The Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple-linear-regression" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Simple Linear Regression</h1>
<div id="representing-the-relationship-between-two-variables" class="section level2">
<h2><span class="header-section-number">3.1</span> Representing the Relationship between two variables</h2>
<p>Many theories in social science is how two variables are related</p>
<p>– Does turnout vary by types of mailers received? – Is the quality of political institutions related to average incomes? - Does conflict mediation help reduce civil conflict?</p>
<ul>
<li><p>Y - the dependent variable or outcome or regressand or left-hand-side variable or response – Voter turnout – Log GDP per capita – Number of battle deaths</p></li>
<li><p>X - the independent variable or explanatory variable or regressor or righthand-side variable or treatment or predictor – Social pressure mailer versus Civic Duty Mailer – Average Expropriation Risk – Presence of conflict mediation</p></li>
</ul>
<p>A common goal of these analyses is to understand how <span class="math inline">\(Y\)</span> varies as a function of <span class="math inline">\(X\)</span>,</p>
</div>
<div id="conditional-expectation-function" class="section level2">
<h2><span class="header-section-number">3.2</span> Conditional Expectation Function</h2>
<p>There are many ways to summarize the relationship between two variables: visually with scatterplots, covariance, and correlation.</p>
<p>When we want to describe an outcome or estimate the effect of a covariate on an outcome, one way to model this is through a conditional expectation—what is the expected value (mean) of the outcome for a given value of the covariate(s).</p>
<p>The <strong>conditional expectation function (CEF)</strong>, also called the <strong>regression function</strong> of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> is the function that gives the expected value (mean) of <span class="math inline">\(Y\)</span> at various values of <span class="math inline">\(x\)</span>. It is denoted <span class="math inline">\(\E(Y | X = x)\)</span>.</p>
<p>Note that the CEF is a function of the population. In the same way that the expected value of a distribution is a parameter, the conditional expected values of the distribution given values of <span class="math inline">\(x\)</span> is also a parameter.</p>
<p>Like other parameters, the CEF throws away a lot of data. It does not describe all the ways that the distribution of <span class="math inline">\(Y\)</span> changes as a function of <span class="math inline">\(x\)</span>; it only describes how the <em>mean</em> of <span class="math inline">\(Y\)</span> changes as a function of <span class="math inline">\(x\)</span>.</p>
<div id="three-uses-of-regression" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Three uses of regression</h3>
<ol style="list-style-type: decimal">
<li>Description - parsimonious summary of the data</li>
<li>Prediction/Estimation/Inference - learn about parameters of the joint distribution of the data</li>
<li>Causal Inference - evaluate counterfactuals</li>
</ol>
</div>
</div>
<div id="population-linear-regression-function" class="section level2">
<h2><span class="header-section-number">3.3</span> Population Linear Regression Function</h2>
<p>The <em>simple</em> (<em>bivariate</em>) linear regression, regression is linear regression with one outcome variable and one covariate, <span class="math display">\[
r(x) = \E[Y | X = x] = \beta_0 + \beta_1 X
\]</span></p>
<p>This describes a data generating process in the population where</p>
<ul>
<li>Y : dependent variable (random variable)</li>
<li>X : independent variable (random variable)</li>
<li><span class="math inline">\(\beta_0\)</span>: population intercept</li>
<li><span class="math inline">\(\beta_1\)</span>: popluation slope</li>
</ul>
<p>We will want to estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
</div>
<div id="sample-linear-regression-function" class="section level2">
<h2><span class="header-section-number">3.4</span> Sample Linear Regression Function</h2>
<p>The sample linear regression function is <span class="math display">\[
\hat{r}(x_i) = \hat{y}_i = \hat\beta_0 + \hat\beta_1 x_i
\]</span> - _i : fitted or predicted value - _i - x_i : observed values of the predictors - <span class="math inline">\(\hat\beta_0\)</span>: estimated intercept - <span class="math inline">\(\hat\beta_1\)</span>: estimated slope - This also yields the residuals <span class="math inline">\(\hat\epsilon_i\)</span>, which are the difference between the observed value of <span class="math inline">\(y_i\)</span> and the predicted value <span class="math inline">\(\hat{y}_i\)</span>, <span class="math display">\[
    \hat\epsilon_i = y_i - \hat{y}_i
    \]</span></p>
</div>
<div id="ordinary-least-squares" class="section level2">
<h2><span class="header-section-number">3.5</span> Ordinary Least Squares</h2>
<p>Ordinary least squares is an estimator of the slope an intercept of the regression line. It finds the slope and intercept that minimizes the sum of the squared residuals. The estimates <span class="math display">\[
\begin{aligned}[t]
(\hat{\beta}_0, \hat{\beta}_1) &amp;= \argmin_{b_0, b_1} \sum_{i = 1}^N (y_i - b_0 - b_1 x_i)^2 \\
&amp;= \argmin_{b_0, b_1} \sum_{i = 1}^N (y_i - \hat{y_i})^2 \\
&amp;= \argmin_{b_0, b_1} \sum_{i = 1}^N \hat{\epsilon}_i^2
\end{aligned}
\]</span> where, <span class="math inline">\(\hat{y}_i\)</span> are the fitted values, <span class="math display">\[
\hat{y}_i = b_0 + b_1 x_i
\]</span> and <span class="math inline">\(\hat{\epsilon}_i\)</span> are the esimated residuals, <span class="math display">\[
\hat{\epsilon}_i = y_i - \hat{y}_i .
\]</span></p>
<p>INSERT PLOT</p>
<p>A nice feature of OLS is that that the solution to the minimization problem can be found analytically.</p>
<p>The solutions to OLS are <span class="math display">\[
\begin{aligned}[t]
\hat{\beta}_0 &amp;= \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat{\beta_1} &amp;= \frac{\sum_{i = 1}^n (x_i - \bar{x})^2 (y_i - \bar{y})}{\sum_{i = 1}^n (x_i - \bar{x})^2} = \frac{\cov(x, y)}{\Var{x}}
\end{aligned}
\]</span></p>
<p>In other words the regression slop is the covariance between the outcome and predictor variable, scaled by the variance of the predictor variable. Since <span class="math inline">\(\cor(x, y) = \cov(x, y) / (\sd{x} \sd{y})\)</span>, the regression slope can be expressed as a function of the correlation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, <span class="math display">\[
\hat{y} = \cor(x, y) \cdot \frac{\sd{x}}{\sd{y}} .
\]</span></p>
<p>Questions:</p>
<ul>
<li>How does the slope of the regression vary with the sample correlation? covariance?</li>
<li>What happens when <span class="math inline">\(x\)</span> doesn’t vary? In other words <span class="math inline">\(x_1 = x_2 = \dots = x_n\)</span>. Answer this using the equation, and by sketching what this data and the regression line would look like?</li>
<li>What happens when <span class="math inline">\(y\)</span> doesn’t vary?</li>
</ul>
<p>In more complicated problems, the solutions have to be found through an iterative optimization problem.</p>
<div id="mechanical-properties-of-the-ols-statistic" class="section level3">
<h3><span class="header-section-number">3.5.1</span> Mechanical properties of the OLS statistic</h3>
<p>These properties are related to the algorithm used the estimate OLS, and are not directly related to how well OLS works as an estimator. These properties follow directly from the first-order conditions used to find the OLS estimates.</p>
<ol style="list-style-type: decimal">
<li><p>Residuals are zero on average. And by implications, the residuals sum to zero.</p>
<p><span class="math display">\[
\frac{1}{N} \sum_{i = 1}^N \hat{\epsilon}_i = 0
\]</span></p></li>
<li><p>The residuals are uncorrelated with the covariate,</p>
<p><span class="math display">\[
\cov(x_i, \hat{\epsilon}_i) = 0
\]</span></p></li>
<li><p>The residuals are uncorrelated with the fitted values</p>
<p><span class="math display">\[
\cov(\hat{y}_i, \hat{\epsilon}_i) = 0
\]</span></p></li>
<li><p>The regression line goes through <span class="math inline">\((\bar{y}, \bar{x})\)</span>.</p></li>
</ol>
<p>Note that 2 and 3 are properites of the estimated residuals, not the population residuals. If the population residuals are correlated with the covariate or outcome, then OLS estimator will have problems.</p>
</div>
<div id="ols-slope-is-a-weighted-sum-of-the-outcomes" class="section level3">
<h3><span class="header-section-number">3.5.2</span> OLS slope is a weighted sum of the outcomes</h3>
<p>The OLS estimator for the slope, <span class="math inline">\(\hat{\beta}_1\)</span>, can be written as a weighted sum of the outcomes, <span class="math display">\[
\hat{\beta}_1 = \sum_{i = 1}^n w_i y_i
\]</span> where <span class="math display">\[
w_i = \frac{(x_i - \bar{x})^2}{\sum_{i = 1}^n (x_i - \bar{x})^2}
\]</span></p>
<p>This is important for several reasons. Even though the regression line includes all observations in its calculation, not all observations are equally important in determining the regression line. Those observations far from the average value of <span class="math inline">\(x\)</span> are given more weight.[^ch2-outlier]</p>
<p>See the paper by XXXX on panel data and regression weighting.</p>
<p>This also implies that <span class="math inline">\(\hat{\beta}\)</span> is a random variable, since it is the sum of random variables, <span class="math inline">\(Y_i\)</span>.</p>
<p>For the derivation, see <a href="http://www.mattblackwell.org/files/teaching/s07-simple-regression.pdf" class="uri">http://www.mattblackwell.org/files/teaching/s07-simple-regression.pdf</a></p>
</div>
</div>
<div id="properties-of-the-ols-estimator" class="section level2">
<h2><span class="header-section-number">3.6</span> Properties of the OLS Estimator</h2>
<p>Remember, OLS is a statistic—a mechanical function of the data. We plug in data and get a result. It may be more complicated than sample means, variances, medians, or other summary stats seen earlier, but it is analgous.</p>
<p>Since it is a function of the sample data, an the sample is a random variable, the OLS statistic has a sampling distribution. This sampling distribution will determine how well the OLS statistic works as an estimator of the population linear regression line.</p>
<p>INSERT FAKE DATA EXAMPLE</p>
</div>
<div id="properties-of-the-estimator" class="section level2">
<h2><span class="header-section-number">3.7</span> Properties of the Estimator</h2>
<ul>
<li>Unbiased</li>
<li>Consistent</li>
<li>Asymptotically normal</li>
<li>Efficiency. An efficient estimator is the estimator with the lowest <em>standard error</em>. OLS is BLUE—the Best Linear Unbiased Estimator. This is proven by the <em>Gauss-Markov Theorem</em>. For the set of the class of linear, unbiased, estimators (with a few regularity conditions), OLS is the efficient estimator.</li>
</ul>
<div class="figure">
<img src="img/tobias-funke-blue.jpeg" alt="OLS Is BLUE (Best Linear Unbiased Estimator)" />
<p class="caption">OLS Is BLUE (Best Linear Unbiased Estimator)</p>
</div>
</div>
<div id="assumptions-for-unbiasedness-and-consistency-of-ols" class="section level2">
<h2><span class="header-section-number">3.8</span> Assumptions for unbiasedness and consistency of OLS</h2>
<ol style="list-style-type: decimal">
<li>Linearity</li>
<li>Random (i.i.d.) sample</li>
<li>Variation in <span class="math inline">\(x_i\)</span></li>
<li>Zero conditional mean of the errors</li>
</ol>
<p>These assumptions are about using the OLS statistic as an estimator of the population linear regression line. The only assumption that is necessary to calculate a sample regression line is variation in <span class="math inline">\(x\)</span>. If the other assumptions are not met, then the sample OLS statistic can be calculated, but it may be a poor estimator of the population regression line.</p>
</div>
<div id="hypothesis-tests-for-regression" class="section level2">
<h2><span class="header-section-number">3.9</span> Hypothesis Tests for Regression</h2>
</div>
<div id="confidence-intervals-for-regression" class="section level2">
<h2><span class="header-section-number">3.10</span> Confidence Intervals for Regression</h2>
</div>
<div id="goodness-of-fit" class="section level2">
<h2><span class="header-section-number">3.11</span> Goodness of Fit</h2>
<p>How well does the regression line fit the data? There are two commonly used statistics to judge this.</p>
<ul>
<li>prediction error, or the standard error of the regression</li>
<li><span class="math inline">\(R^2\)</span> (R-squared) and adjusted <span class="math inline">\(R^2\)</span></li>
</ul>
<p><span class="math display">\[
SST = \sum_{i = 1}^n (y_i - \bar{y})^2
\]</span> <span class="math display">\[
SSR = \sum_{i = 1}^n (y_i - \hat{y})^2
\]</span></p>
<p><span class="math inline">\(R^2\)</span> is a form of model comparison. It compares the regression against a benchmark null model, which in this case is a model with only the intercept <span class="math inline">\(y_i = \beta_0 = \bar{y}\)</span>.</p>
<p>Some propertie of <span class="math inline">\(R\)</span>-squared:</p>
<ul>
<li><span class="math inline">\(R^2\)</span> implies no relationship. If <span class="math inline">\(R^2 = 0\)</span>, then <span class="math inline">\(\beta_0 = 0\)</span>.</li>
<li><span class="math inline">\(R^2\)</span> implies a perfect linear fit</li>
<li><span class="math inline">\(R^2\)</span> is the correlation squared.</li>
</ul>
</div>
<div id="references" class="section level2">
<h2><span class="header-section-number">3.12</span> References</h2>
<ul>
<li>Much of this is taken from Matthew Blackwell lecture notes for Govt 2002 <a href="http://www.mattblackwell.org/files/teaching/s07-simple-regression.pdf" class="uri">http://www.mattblackwell.org/files/teaching/s07-simple-regression.pdf</a></li>
<li>Tobias Funke meme <a href="http://memegenerator.net/instance2/5082792" class="uri">http://memegenerator.net/instance2/5082792</a>. Generated by Jeffrey Arnold. Hat-tip to Brenton Kenkel for the refence. <a href="http://bkenkel.com/psci8357/notes/02-reintroduction.pdf" class="uri">http://bkenkel.com/psci8357/notes/02-reintroduction.pdf</a>.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="review-of-statistics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multiple-regression.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/UW-POLS503/pols503-notes/edit/gh-pages/02-simple-linear-regression.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>


</body>

</html>
