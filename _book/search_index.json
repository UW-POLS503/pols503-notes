[
["index.html", "POLS 503: Advanced Quantitative Political Methodology: The Notes Chapter 1 Introduction", " POLS 503: Advanced Quantitative Political Methodology: The Notes Jeffrey B. Arnold 2016-04-19   Chapter 1 Introduction hello, world!  "],
["linear-regression.html", "Chapter 2 Linear Regression 2.1 Matrix Representation 2.2 Estimating OLS 2.3 Assumptions 2.4 Errors in Linear Regression 2.5 Non-constant variance (Heteroskedasticity) 2.6 Multivariate Normal Distribution", " Chapter 2 Linear Regression  2.1 Matrix Representation The linear regression function can be written as a scalar function for each observation, \\(i = 1, \\dots, N\\), \\[ \\begin{aligned}[t] y_i &amp;= \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\cdots + \\beta_{K,i} + \\varepsilon_i \\\\  &amp;= \\beta_0 + \\sum_{k = 1}^{K} \\beta_k x_{k,i} + \\varepsilon_i \\\\ &amp;= \\sum_{k = 0}^{K} \\beta_k x_{k,i} + \\varepsilon_i \\end{aligned} \\] where \\(x_{0,i} = 1\\) for all \\(i \\in 1:N\\). The linear regression can be more compactly written in matrix form, \\[ \\begin{aligned}[t] \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix} &amp;= \\begin{bmatrix}  1 &amp; x_{1,1} &amp; x_{2,1} &amp; \\cdots &amp; x_{K,1} \\\\ 1 &amp; x_{1,2} &amp; x_{2,2} &amp; \\cdots &amp; x_{K,2} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{1,N}&amp; x_{2,n} &amp; \\cdots &amp; x_{K,N} \\end{bmatrix} \\begin{bmatrix}  \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_K \\end{bmatrix} +  \\begin{bmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_N \\end{bmatrix} \\\\ \\underbrace{\\vec{y}}_{N \\times 1} &amp;= \\underbrace{\\mat{X}}_{N \\times K} \\,\\, \\underbrace{\\vec{\\beta}}_{K \\times 1} + \\underbrace{\\vec{\\varepsilon}}_{N \\times 1} \\end{aligned} \\] The matrix \\(\\mat{X}\\) is called the design matrix. Its rows are each observation in the data. Its columns are the intercept, a column vector of 1’s, and the values of each predictor. The mean of the disturbance vector is 0, \\[ \\E(\\epsilon) = 0 \\]   2.2 Estimating OLS The OLS estimator finds estimates of a paramters   2.3 Assumptions With these assumptions  When is OLS unbiased? When is OLS efficient? When are the OLS standard errors correct?          Assumption Formal statement Consequence of violation     No (perfect) collinearity \\(\\rank(\\mat{X}) = K, K &lt; N\\) Coefficients unidentified   \\(\\mat{X}\\) is exogenous \\(\\E(\\mat{X} \\vec{\\varepsilon}) = 0\\) Biased, even as \\(N \\to \\infty\\)   Disturbances have mean 0 \\(\\E(\\varepsilon) = 0\\) Biased, even as \\(N \\to \\infty\\)   No serial correlation \\(\\E(\\varepsilon_i \\varepsilon_j) = 0\\), \\(i \\neq j\\) Unbiased, wrong se   Homoskedastic errors \\(\\E(\\vec{\\varepsilon}\\T \\vec{\\varepsilon})\\) Unbiased, wrong se   Gaussian errors \\(\\varepsilon \\sim \\dnorm(0, \\sigma^2)\\) Unbiased, se wrong unless \\(N \\to \\infty\\)    If assumptions 1–5, then OLS is the best linear unbiased estimator (BLUE) estimator.   OLS is BLUE  However, 1–5 does not impy that OLS has the lowest MSE. But if assumptions 1–6 hold, then OLS is the the minimum-variance unbiased (MVU) estimator. This means that for all estimators that are unbiased, OLS has the least variance in its sampling distribution.   2.4 Errors in Linear Regression Note, that OLS assumes that the variance of the the disturbances is constant \\(\\hat{Y} - Y = \\varepsilon = \\sigma^2\\). What happens if it isn’t? \\[ \\mat{\\Sigma} = \\begin{bmatrix} \\Var(\\varepsilon_1) &amp; \\Cov(\\varepsilon_1, \\varepsilon_2) &amp; \\cdots &amp; \\Cov(\\varepsilon_1, \\varepsilon_N) \\\\ \\Var(\\varepsilon_2, \\varepsilon_1) &amp; \\Var(\\varepsilon_2) &amp; \\cdots &amp; \\Cov(\\varepsilon_2, \\varepsilon_N) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\Cov(\\varepsilon_N, \\varepsilon_1) &amp; \\Cov(\\varepsilon_N, \\varepsilon_2) &amp; \\cdots &amp; \\Cov(\\varepsilon_N) \\\\ \\end{bmatrix} \\\\ \\Sigma = \\begin{bmatrix} \\E(\\varepsilon_1^2) &amp; \\E(\\varepsilon_1 \\varepsilon_2) &amp; \\cdots &amp; \\E(\\varepsilon_1 \\varepsilon_N) \\\\ \\E(\\varepsilon_2 \\varepsilon_1) &amp; \\E(\\varepsilon_2^2) &amp; \\cdots &amp; \\E(\\varepsilon_2 \\varepsilon_N) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\E(\\varepsilon_N \\varepsilon_1) &amp; \\E(\\varepsilon_N \\varepsilon_2) &amp; \\cdots &amp; \\E(\\varepsilon_N^2) \\\\ \\end{bmatrix} \\\\ \\] The matrix can be written more compactly as, \\[ \\mat{\\Sigma} = \\E(\\vec{\\varepsilon} \\vec{\\varepsilon}\\T) \\] An assumption is that errors are independent, \\(\\E(\\epsilon_i \\epsilon_j)\\) for all \\(i \\neq j\\). This means that all off-diagonal elements of \\(\\mat{\\Sigma}\\) are 0$. Additionally, all \\(\\epsilon_i\\) are assumed to have the same variance, \\(\\sigma^2\\). Thus, the variance-covariance matrix of the errors is a assumed to have a diagonal matrix with the form, \\[ \\mat{\\Sigma} =  \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma^2 \\end{bmatrix}  = \\sigma^2 \\mat{I}_N \\] If these assumptions of the errors do not hold, then \\(\\Sigma\\) does not take this form, and more complicated models than OLS need to be used to get correct standard errors.   2.5 Non-constant variance (Heteroskedasticity) The homoskedastic case assumes that each eror term has its own variance. In the heteroskedastic case, each distrurbance may have its own variance, but they are still uncorrelated (\\(\\mat{\\Sigma}\\) is diagonal) \\[ \\mat{\\Sigma} =  \\begin{bmatrix} \\sigma_1^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma_2^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma_N^2 \\end{bmatrix} \\] The problem is that now there are \\(N\\) variance parameters to estimate, in addition to the \\(K\\) slope coefficients. Now, there are more parameters than we can estimate. With heteroskedasticity, OLS with be unbiased, but the standard errors will be incorrect. More general case allows for heteroskedasticity, and autocorrelation (\\(\\Cov(\\varepsilon_i, \\varepsilon_j) \\neq 0\\)), \\[ \\mat{\\Sigma} =  \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{1,2} &amp; \\cdots &amp; \\sigma_{1,N} \\\\ \\sigma_{2,1} &amp; \\sigma_2^2 &amp; \\cdots &amp; \\sigma_{2,N} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{N,1} &amp; \\sigma_{N,2} &amp; \\cdots &amp; \\sigma_N^2 \\end{bmatrix}  \\] As with heteroskedasticity, OLS with be unbiased, but the standard errors will be incorrect. TODO Different views on heteroskedasticity.   2.6 Multivariate Normal Distribution The multivariate normal distribution is the generalization of the univariate normal distribution to more than one dimension.1 The random variable, \\(\\vec{x}\\), is a length \\(k\\) vector. The \\(k\\) length vector \\(\\vec{\\mu}\\) are the means of \\(\\vec{x}\\), and the \\(k \\times k\\) matrix, \\(\\mat{\\Sigma}\\), is the variance-covariance matrix, \\[ \\begin{aligned}[t] \\vec{x} &amp;\\sim \\dmvnorm{k}\\left(\\vec{\\mu}, \\mat{\\Sigma} \\right) \\\\ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_k \\end{bmatrix} &amp; \\sim \\dmvnorm{k} \\left(   \\begin{bmatrix}   \\mu_1 \\\\   \\mu_2 \\\\   \\vdots \\\\   \\mu_k   \\end{bmatrix},   \\begin{bmatrix}   \\sigma_1^2 &amp; \\sigma_{1,2} &amp; \\cdots &amp; \\sigma_{1, k} \\\\   \\sigma_{2,1} &amp; \\sigma_2^2 &amp; \\cdots &amp; \\sigma_{2, k} \\\\   \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\   \\sigma_{k,1} &amp; \\sigma_{k,2} &amp; \\cdots &amp; \\sigma_{k, k}   \\end{bmatrix} \\right) \\end{aligned} \\] \\[ p(\\vec{x}; \\vec{\\mu}, \\mat{\\Sigma}) = (2 k)^{-\\frac{k}{2}} \\left| \\mat{\\Sigma} \\right|^{-\\frac{1}{2}} \\exp \\left( -\\frac{1}{2} (\\vec{x} - \\vec{\\mu})\\T \\mat{\\Sigma}^{-1} (\\vec{x} - \\vec{\\mu}) \\right) . \\] You can sample from and calculate the density for the multivariate normal distribution with the functions dmvnorm and rmvnorm from the package mvtnorm. ##  ## Attaching package: &#39;purrr&#39; ## The following object is masked from &#39;package:dplyr&#39;: ##  ##     order_by \\[ \\begin{bmatrix}   x_1 \\\\   x_2 \\end{bmatrix} \\sim \\dmvnorm{2} \\left(   \\begin{bmatrix}     0 \\\\     0   \\end{bmatrix},   \\begin{bmatrix}     1 &amp; 0 \\\\     0 &amp; 1   \\end{bmatrix} \\right) \\] \\[ \\begin{bmatrix}   x_1 \\\\   x_2 \\end{bmatrix} \\sim \\dmvnorm{2} \\left(   \\begin{bmatrix}     0 \\\\     2   \\end{bmatrix},   \\begin{bmatrix}     1 &amp; 0 \\\\     0 &amp; 1   \\end{bmatrix} \\right) \\] \\[ \\begin{bmatrix}   x_1 \\\\   x_2 \\end{bmatrix} \\sim \\dmvnorm{2} \\left(   \\begin{bmatrix}     2 \\\\     2   \\end{bmatrix},   \\begin{bmatrix}     1 &amp; 0 \\\\     0 &amp; 1   \\end{bmatrix} \\right) \\] \\[ \\begin{bmatrix}   x_1 \\\\   x_2 \\end{bmatrix} \\sim \\dmvnorm{2} \\left(   \\begin{bmatrix}     0 \\\\     0   \\end{bmatrix},   \\begin{bmatrix}     0.33 &amp; 0 \\\\     0 &amp; 1   \\end{bmatrix} \\right) \\] \\[ \\begin{bmatrix}   x_1 \\\\   x_2 \\end{bmatrix} \\sim \\dmvnorm{2} \\left(   \\begin{bmatrix}     0 \\\\     0   \\end{bmatrix},   \\begin{bmatrix}     0.33 &amp; 0 \\\\     0 &amp; 3   \\end{bmatrix} \\right) \\] \\[ \\begin{bmatrix}   x_1 \\\\   x_2 \\end{bmatrix} \\sim \\dmvnorm{2} \\left(   \\begin{bmatrix}     0 \\\\     0   \\end{bmatrix},   \\begin{bmatrix}     0.33 &amp; 0 \\\\     0 &amp; 0.33   \\end{bmatrix} \\right) \\] \\[ \\begin{bmatrix}   x_1 \\\\   x_2 \\end{bmatrix} \\sim \\dmvnorm{2} \\left(   \\begin{bmatrix}     0 \\\\     0   \\end{bmatrix},   \\begin{bmatrix}     1 &amp; 0.8 \\\\     0.8 &amp; 1   \\end{bmatrix} \\right) \\] \\[ \\begin{bmatrix}   x_1 \\\\   x_2 \\end{bmatrix} \\sim \\dmvnorm{2} \\left(   \\begin{bmatrix}     0 \\\\     0   \\end{bmatrix},   \\begin{bmatrix}     1 &amp; -0.8 \\\\     -0.8 &amp; 1   \\end{bmatrix} \\right) \\]   "],
["what-makes-an-estimator-good.html", "Chapter 3 What makes an estimator good?", " Chapter 3 What makes an estimator good? Estimators are evaluated not on how close an estimate in a given sample is to the population, but how their sampling distributions compare to the population. In other words, judge the methodology (estimator), not the result (estimate).[^ols-properties-references] Let \\(\\theta\\) be the population parameter, and \\(\\hat\\theta\\) be an estimator of that population parameter.  Bias The bias of an estimator is the difference between the mean of its sampling distribution and the population parameter, \\[\\Bias(\\hat\\theta) = \\E(\\hat\\theta) - \\theta .\\]  Variance The variance of the estimator is the variance of its sampling distribution, \\(\\Var(\\theta)\\).  Efficiency (Mean squared error) An efficient estimator is one that minimizes a given “loss function”, which is a penalty for missing the population average. The most common loss function is squared loss, which gives the Mean Squared Error (MSE) of an estimator.  \\[\\MSE(\\hat\\theta) = \\E\\left[{(\\hat\\theta - \\theta)}^{2}\\right] =  (\\E(\\hat\\theta) - \\theta)^2 + \\E(\\hat\\theta - \\E(\\hat\\theta))^2 = \\Bias(\\hat\\theta)^2 + \\Var(\\hat\\theta)\\]  The mean squared error is a function of both the bias and variance of an estimator.  This means that some biased estimators can be more efficient : than unbiased estimators if their variance offsets their bias.2    Table 3.1: Examples of clocks as “estimators” of the time3    Biased Variance     Stopped clock Yes High   Random clock No High   Clock that is “a lot” fast Yes Low   Clock that is “a little” fast Yes Low   Atomic clock No Low    Another property is consistency. Consistency is an asymptotic property4, that roughly states that an estimator converges to the truth as the number of obserservations grows, \\(\\E(\\hat\\theta - \\theta) \\to 0\\) as \\(N \\to \\infty\\). Roughly, this means that if you had enough (infinite) data, the estimator will give you the true value of the parameter.          See Multivariate normal distribution and references therein.↩ It follows from the definition of MSE, that biased estimator, \\(\\hat\\theta_{B}\\), has a lower MSE than an unbiased estimator, \\(\\hat\\theta_{U}\\), if \\(\\Bias(\\theta_B)^2 &lt; \\Var(\\theta_U) - \\Var(\\theta_B)\\).↩ Example from Chris Adolph↩ As the number of observations goes to infinity.↩  "]
]
