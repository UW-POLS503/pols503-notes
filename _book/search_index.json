[
["index.html", "POLS 503: Advanced Quantitative Political Methodology: The Notes Chapter 1 Introduction", " POLS 503: Advanced Quantitative Political Methodology: The Notes Jeffrey B. Arnold 2016-04-19   Chapter 1 Introduction hello, world!  "],
["linear-regression-and-the-ordinary-least-squares-ols-estimator.html", "Chapter 2 Linear Regression and the Ordinary Least Squares (OLS) Estimator 2.1 Linear Regression Function 2.2 Ordinary Least Squares 2.3 Properties of the OLS Estimator 2.4 References", " Chapter 2 Linear Regression and the Ordinary Least Squares (OLS) Estimator Since we will largely be concerned with using linear regression for inference, we will start by discussion the population parameter of interest (population linear regression function), then the sample statistic (sample linear regression function) and estimator (ordinary least squares). We will then consider the properties of the OLS estimator.  2.1 Linear Regression Function The population linear regression function is \\[ r(x) = \\E[Y | X = x] = \\beta_0 + \\sum_{k = 1}^{K} \\beta_{k} x_k . \\] The population linear regression function is defined for random variables, and will be the object to be estimated. Names for \\(\\vec{y}\\)  dependent variable explained variable response variable predicted variable regressand outcome variable  Names for \\(\\mat{X}\\),  indpendent variables explanatory varaibles treatment and control variables predictor variables covariates regressors  To estimate the unkonwn population linear regression, we will use the sample linear regression function, \\[ \\hat{r}(x_i) = \\hat{y}_i = \\hat\\beta_0 + \\sum_{k = 1}^{K} \\hat\\beta_{k} x_k . \\] However, we \\(\\hat{Y}_i\\) are the fitted or predicted value The residuals or errors are the prediction errors of the estimates \\[ \\hat{\\epsilon}_i = y_i - \\hat{y}_i \\] \\(\\vec{\\beta}\\) are the parameters; \\(\\beta_0\\) is called the intercept, and \\(\\beta_{1}, \\dots, \\beta_{K}\\) are called the slope parameters, or coefficients. We will then consider the properties of the OLS estimator. The linear regression can be more compactly written in matrix form, \\[ \\begin{aligned}[t]   \\begin{bmatrix}     y_1 \\\\     y_2 \\\\     \\vdots \\\\     y_N   \\end{bmatrix} &amp;=   \\begin{bmatrix}     1 &amp; x_{1,1} &amp; x_{2,1} &amp; \\cdots &amp; x_{K,1} \\\\     1 &amp; x_{1,2} &amp; x_{2,2} &amp; \\cdots &amp; x_{K,2} \\\\     \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\     1 &amp; x_{1,N}&amp; x_{2,n} &amp; \\cdots &amp; x_{K,N}   \\end{bmatrix}   \\begin{bmatrix}     \\beta_0 \\\\     \\beta_1 \\\\     \\beta_2 \\\\     \\vdots \\\\     \\beta_K     \\end{bmatrix}   +   \\begin{bmatrix}     \\varepsilon_1 \\\\     \\varepsilon_2 \\\\     \\vdots \\\\     \\varepsilon_N   \\end{bmatrix} \\end{aligned} . \\] More compactly, the linear regression model can be written as, \\[ \\begin{aligned}[t]   \\underbrace{\\vec{y}}_{N \\times 1} &amp;=   \\underbrace{\\mat{X}}_{N \\times K} \\,\\,   \\underbrace{\\vec{\\beta}}_{K \\times 1} +   \\underbrace{\\vec{\\varepsilon}}_{N \\times 1} . \\end{aligned} \\] The matrix \\(\\mat{X}\\) is called the design matrix. Its rows are each observation in the data. Its columns are the intercept, a column vector of 1’s, and the values of each predictor.   2.2 Ordinary Least Squares Ordinary least squares (OLS) is an estimator of the slope and statistic of the regression line1. OLS finds values of the intercept and slope coefficients by minimizing the squared errors, \\[ \\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_K = \\argmin_{b_0, b_1, \\dots, b_k} \\sum_{i = 1}^{N}  \\underbrace{{\\left(y_i - b_0 - \\sum_{k = 1}^{K} b_k x_{i,k} \\right)}^2}_{\\text{squared error}}, \\] or, in matrix notation, \\[ \\begin{aligned}[t] \\hat{\\vec{\\beta}} &amp;= \\argmin_{\\vec{b}} \\sum_{i = 1}^N (y_i - \\vec{b}\\T \\vec{x}_i)^2 \\\\ &amp;= \\argmin_{\\vec{b}} \\sum_{i = 1}^N u_i^2 \\\\ &amp;= \\argmin_{\\vec{b}} \\vec{u}&#39; \\vec{u} \\end{aligned} \\] where \\(\\vec{u} = \\vec{y} - \\mat{X} \\vec{\\beta}\\). In most statistical models, including even generalized linear models such as logit, the solution to this minimization problem would be solved with optimization methods that require interation. One nice feature of OLS is that there is a closed form solution for \\(\\hat{\\beta}\\) even in the multiple regression case, so no iterative optimization methods need to be used. In the bivariate regression case, the OLS estimators for \\(\\beta_0\\) and \\(\\beta_1\\) are \\[ \\begin{aligned}[t] \\hat{\\beta}_0 &amp;= \\bar{y} - \\hat\\beta_1 \\bar{x} \\\\ \\hat{\\beta}_1 7= \\frac{\\sum_{i = 1}^N (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum_{i = 1}^N (x_i - \\bar{x})^2} \\\\ &amp;= \\frac{\\Cov(\\vec{x} \\vec{y})}{\\Var{\\vec{x}}} &amp;= \\frac{\\text{Sample covariance betweeen $\\vec{x}$ and $\\vec{y}$}}{\\text{Sample variance of $\\vec{x}$}} . \\end{aligned} \\] In the multiple regression case, the OLS estimator for \\(\\hat{\\vec{\\beta}}\\) is \\[ \\hat{\\vec{\\beta}} = \\left( \\mat{X}&#39; \\mat{X} \\right)^{-1} \\mat{X}&#39; \\vec{y} . \\] The term \\(\\mat{X}&#39; \\mat{X}\\) is similar to the variance of \\(\\vec{x}\\) in the bivariate case. The term \\(\\mat{X}&#39; \\vec{y}\\) is similar to the covariance between \\(\\mat{X}\\) and \\(\\vec{y}\\) in the bivariate case. The sample linear regression function estimated by OLS has the following properties:  Residuals sum to zero, \\[ \\sum_{i = 1}^N \\hat{\\epsilon}_i = 0 . \\] This implies that the mean of residuals is also 0. The regression function passes through the point \\((\\bar{\\vec{y}}, \\bar{\\vec{x}}_1, \\dots, \\bar{\\vec{x}_K})\\). In other words, the following is always true, \\[ \\bar{\\vec{y}} = \\hat\\beta_0 + \\sum_{k = 1}^K \\hat\\beta_k \\bar{\\vec{x}}_k . \\] The resisuals are uncorrelated with the predictor \\[ \\sum_{i = 1}^N x_i \\hat{\\epsilon}_i = 0 \\] The residuals are uncorrelated with the fitted values \\[ \\sum_{i = 1}^N \\hat{y}_i \\hat{\\varepsilon}_i = 0 \\]    2.3 Properties of the OLS Estimator  2.3.1 What makes an estimator good? Estimators are evaluated not on how close an estimate in a given sample is to the population, but how their sampling distributions compare to the population. In other words, judge the methodology (estimator), not the result (estimate).[^ols-properties-references] Let \\(\\theta\\) be the population parameter, and \\(\\hat\\theta\\) be an estimator of that population parameter.  Bias The bias of an estimator is the difference between the mean of its sampling distribution and the population parameter, \\[\\Bias(\\hat\\theta) = \\E(\\hat\\theta) - \\theta .\\]  Variance The variance of the estimator is the variance of its sampling distribution, \\(\\Var(\\theta)\\).  Efficiency (Mean squared error) An efficient estimator is one that minimizes a given “loss function”, which is a penalty for missing the population average. The most common loss function is squared loss, which gives the Mean Squared Error (MSE) of an estimator.  \\[\\MSE(\\hat\\theta) = \\E\\left[{(\\hat\\theta - \\theta)}^{2}\\right] =  (\\E(\\hat\\theta) - \\theta)^2 + \\E(\\hat\\theta - \\E(\\hat\\theta))^2 = \\Bias(\\hat\\theta)^2 + \\Var(\\hat\\theta)\\]  The mean squared error is a function of both the bias and variance of an estimator.  This means that some biased estimators can be more efficient : than unbiased estimators if their variance offsets their bias.2   Consistency is an asymptotic property3, that roughly states that an estimator converges to the truth as the number of observations grows, \\(\\E(\\hat\\theta - \\theta) \\to 0\\) as \\(N \\to \\infty\\). Roughly, this means that if you had enough (infinite) data, the estimator will give you the true value of the parameter.   2.3.2 Properties of OLS  When is OLS unbiased? When is OLS consistent? When is OLS efficient?          Assumption Formal statement Consequence of violation     No (perfect) collinearity \\(\\rank(\\mat{X}) = K, K &lt; N\\) Coefficients unidentified   \\(\\mat{X}\\) is exogenous \\(\\E(\\mat{X} \\vec{\\varepsilon}) = 0\\) Biased, even as \\(N \\to \\infty\\)   Disturbances have mean 0 \\(\\E(\\varepsilon) = 0\\) Biased, even as \\(N \\to \\infty\\)   No serial correlation \\(\\E(\\varepsilon_i \\varepsilon_j) = 0\\), \\(i \\neq j\\) Unbiased, wrong se   Homoskedastic errors \\(\\E(\\vec{\\varepsilon}\\T \\vec{\\varepsilon})\\) Unbiased, wrong se   Gaussian errors \\(\\varepsilon \\sim \\dnorm(0, \\sigma^2)\\) Unbiased, se wrong unless \\(N \\to \\infty\\)     Note that these assumptions can be sometimes be written in largely equivalent, but slightly different forms.    2.4 References  Wooldrige, Ch 3. Fox, Ch 6, 9.     "],
["ols-troubleshooting-and-diagnostics.html", "Chapter 3 OLS Troubleshooting and Diagnostics 3.1 Multi-Collinearity 3.2 Omitted Variable Bias 3.3 Measurement Error 3.4 Non-linearity 3.5 Heteroskedasticity and Auto-correlation 3.6 Non-constant variance (Heteroskedasticity)", " Chapter 3 OLS Troubleshooting and Diagnostics  3.1 Multi-Collinearity   3.2 Omitted Variable Bias   3.3 Measurement Error   3.4 Non-linearity   3.5 Heteroskedasticity and Auto-correlation Note, that OLS assumes that the variance of the the disturbances is constant \\(\\hat{Y} - Y = \\varepsilon = \\sigma^2\\). What happens if it isn’t? \\[ \\mat{\\Sigma} = \\begin{bmatrix} \\Var(\\varepsilon_1) &amp; \\Cov(\\varepsilon_1, \\varepsilon_2) &amp; \\cdots &amp; \\Cov(\\varepsilon_1, \\varepsilon_N) \\\\ \\Var(\\varepsilon_2, \\varepsilon_1) &amp; \\Var(\\varepsilon_2) &amp; \\cdots &amp; \\Cov(\\varepsilon_2, \\varepsilon_N) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\Cov(\\varepsilon_N, \\varepsilon_1) &amp; \\Cov(\\varepsilon_N, \\varepsilon_2) &amp; \\cdots &amp; \\Cov(\\varepsilon_N) \\\\ \\end{bmatrix} \\\\ \\Sigma = \\begin{bmatrix} \\E(\\varepsilon_1^2) &amp; \\E(\\varepsilon_1 \\varepsilon_2) &amp; \\cdots &amp; \\E(\\varepsilon_1 \\varepsilon_N) \\\\ \\E(\\varepsilon_2 \\varepsilon_1) &amp; \\E(\\varepsilon_2^2) &amp; \\cdots &amp; \\E(\\varepsilon_2 \\varepsilon_N) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\E(\\varepsilon_N \\varepsilon_1) &amp; \\E(\\varepsilon_N \\varepsilon_2) &amp; \\cdots &amp; \\E(\\varepsilon_N^2) \\\\ \\end{bmatrix} \\\\ \\] The matrix can be written more compactly as, \\[ \\mat{\\Sigma} = \\E(\\vec{\\varepsilon} \\vec{\\varepsilon}\\T) \\] An assumption is that errors are independent, \\(\\E(\\epsilon_i \\epsilon_j)\\) for all \\(i \\neq j\\). This means that all off-diagonal elements of \\(\\mat{\\Sigma}\\) are 0$. Additionally, all \\(\\epsilon_i\\) are assumed to have the same variance, \\(\\sigma^2\\). Thus, the variance-covariance matrix of the errors is a assumed to have a diagonal matrix with the form, \\[ \\mat{\\Sigma} =  \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma^2 \\end{bmatrix}  = \\sigma^2 \\mat{I}_N \\] If these assumptions of the errors do not hold, then \\(\\Sigma\\) does not take this form, and more complicated models than OLS need to be used to get correct standard errors.   3.6 Non-constant variance (Heteroskedasticity) The homoskedastic case assumes that each eror term has its own variance. In the heteroskedastic case, each distrurbance may have its own variance, but they are still uncorrelated (\\(\\mat{\\Sigma}\\) is diagonal) \\[ \\mat{\\Sigma} =  \\begin{bmatrix} \\sigma_1^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma_2^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma_N^2 \\end{bmatrix} \\] The problem is that now there are \\(N\\) variance parameters to estimate, in addition to the \\(K\\) slope coefficients. Now, there are more parameters than we can estimate. With heteroskedasticity, OLS with be unbiased, but the standard errors will be incorrect. More general case allows for heteroskedasticity, and autocorrelation (\\(\\Cov(\\varepsilon_i, \\varepsilon_j) \\neq 0\\)), \\[ \\mat{\\Sigma} =  \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{1,2} &amp; \\cdots &amp; \\sigma_{1,N} \\\\ \\sigma_{2,1} &amp; \\sigma_2^2 &amp; \\cdots &amp; \\sigma_{2,N} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{N,1} &amp; \\sigma_{N,2} &amp; \\cdots &amp; \\sigma_N^2 \\end{bmatrix}  \\] As with heteroskedasticity, OLS with be unbiased, but the standard errors will be incorrect.   "],
["appendix.html", "Chapter 4 Appendix 4.1 Multivariate Normal Distribution", " Chapter 4 Appendix  4.1 Multivariate Normal Distribution The multivariate normal distribution is the generalization of the univariate normal distribution to more than one dimension.4 The random variable, \\(\\vec{x}\\), is a length \\(k\\) vector. The \\(k\\) length vector \\(\\vec{\\mu}\\) are the means of \\(\\vec{x}\\), and the \\(k \\times k\\) matrix, \\(\\mat{\\Sigma}\\), is the variance-covariance matrix, \\[ \\begin{aligned}[t] \\vec{x} &amp;\\sim \\dmvnorm{k}\\left(\\vec{\\mu}, \\mat{\\Sigma} \\right) \\\\ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_k \\end{bmatrix} &amp; \\sim \\dmvnorm{k} \\left(   \\begin{bmatrix}   \\mu_1 \\\\   \\mu_2 \\\\   \\vdots \\\\   \\mu_k   \\end{bmatrix},   \\begin{bmatrix}   \\sigma_1^2 &amp; \\sigma_{1,2} &amp; \\cdots &amp; \\sigma_{1, k} \\\\   \\sigma_{2,1} &amp; \\sigma_2^2 &amp; \\cdots &amp; \\sigma_{2, k} \\\\   \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\   \\sigma_{k,1} &amp; \\sigma_{k,2} &amp; \\cdots &amp; \\sigma_{k, k}   \\end{bmatrix} \\right) \\end{aligned} \\] The density function of the multivariate normal is, \\[ p(\\vec{x}; \\vec{\\mu}, \\mat{\\Sigma}) = (2 k)^{-\\frac{k}{2}} \\left| \\mat{\\Sigma} \\right|^{-\\frac{1}{2}} \\exp \\left( -\\frac{1}{2} (\\vec{x} - \\vec{\\mu})\\T \\mat{\\Sigma}^{-1} (\\vec{x} - \\vec{\\mu}) \\right) . \\] You can sample from and calculate the density for the multivariate normal distribution with the functions dmvnorm and rmvnorm from the package mvtnorm. Density plots of different bivariate normal distributions,            Ordinary least squares is distinguished from generalized least squares (GLS).↩ It follows from the definition of MSE, that biased estimator, \\(\\hat\\theta_{B}\\), has a lower MSE than an unbiased estimator, \\(\\hat\\theta_{U}\\), if \\(\\Bias(\\theta_B)^2 &lt; \\Var(\\theta_U) - \\Var(\\theta_B)\\).↩ As the number of observations goes to infinity.↩ See Multivariate normal distribution and references therein.↩  "]
]
