[
["index.html", "POLS 503: Advanced Quantitative Political Methodology: The Notes Chapter 1 Introduction", " POLS 503: Advanced Quantitative Political Methodology: The Notes Jeffrey B. Arnold 2016-04-18   Chapter 1 Introduction hello, world!  "],
["linear-regression.html", "Chapter 2 Linear Regression 2.1 Matrix Representation 2.2 Estimating OLS 2.3 Assumptions 2.4 Errors in Linear Regression 2.5 Non-constant variance (Heteroskedasticity) 2.6 Why Control for Variables 2.7 Multicollinearity 2.8 Omitted Variable Bias", " Chapter 2 Linear Regression  2.1 Matrix Representation The linear regression function can be written as a scalar function for each observation, \\(i = 1, \\dots, N\\), \\[ \\begin{aligned}[t] y_i &amp;= \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\cdots + \\beta_{K,i} + \\varepsilon_i \\\\  &amp;= \\beta_0 + \\sum_{k = 1}^{K} \\beta_k x_{k,i} + \\varepsilon_i \\\\ &amp;= \\sum_{k = 0}^{K} \\beta_k x_{k,i} + \\varepsilon_i \\end{aligned} \\] where \\(x_{0,i} = 1\\) for all \\(i \\in 1:N\\). The linear regression can be more compactly written in matrix form, \\[ \\begin{aligned}[t] \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix} &amp;= \\begin{bmatrix}  1 &amp; x_{1,1} &amp; x_{2,1} &amp; \\cdots &amp; x_{K,1} \\\\ 1 &amp; x_{1,2} &amp; x_{2,2} &amp; \\cdots &amp; x_{K,2} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{1,N}&amp; x_{2,n} &amp; \\cdots &amp; x_{K,N} \\end{bmatrix} \\begin{bmatrix}  \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_K \\end{bmatrix} +  \\begin{bmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_N \\end{bmatrix} \\\\ \\underbrace{\\vec{y}}_{N \\times 1} &amp;= \\underbrace{\\mat{X}}_{N \\times K} \\,\\, \\underbrace{\\vec{\\beta}}_{K \\times 1} + \\underbrace{\\vec{\\varepsilon}}_{N \\times 1} \\end{aligned} \\] The matrix \\(\\mat{X}\\) is called the design matrix. Its rows are each observation in the data. Its columns are the intercept, a column vector of 1’s, and the values of each predictor. The mean of the disturbance vector is 0, \\[ \\E(\\epsilon) = 0 \\]   2.2 Estimating OLS The OLS estimator finds estimates of a paramters   2.3 Assumptions With these assumptions  When is OLS unbiased? When is OLS efficient? When are the OLS standard errors correct?     Assumption Formal statement Consequence of violation     No (perfect) collinearity \\(\\rank(\\mat{X}) = K, K &lt; N\\) Coefficients unidentified   \\(\\mat{X}\\) is exogenous \\(\\E(\\mat{X} \\vec{\\varepsilon}) = 0\\) Biased, even as \\(N \\to \\infty\\)   Disturbances have mean 0 \\(\\E(\\varepsilon) = 0\\) Biased, even as \\(N \\to \\infty\\)   No serial correlation \\(\\E(\\varepsilon_i \\varepsilon_j) = 0\\), \\(i \\neq j\\) Unbiased, wrong se   Homoskedastic errors \\(\\E(\\vec{\\varepsilon}\\T \\vec{\\varepsilon})\\) Unbiased, wrong se   Gaussian errors \\(\\varepsilon \\sim \\dnorm(0, \\sigma^2)\\) Unbiased, se wrong unless \\(N \\to \\infty\\)    If assumptions 1–5, then OLS is the best linear unbiased estimator (BLUE) estimator.   OLS is BLUE  However, 1–5 does not impy that OLS has the lowest MSE. But if assumptions 1–6 hold, then OLS is the the minimum-variance unbiased (MVU) estimator. This means that for all estimators that are unbiased, OLS has the least variance in its sampling distribution.   2.4 Errors in Linear Regression Note, that OLS assumes that the variance of the the disturbances is constant \\(\\hat{Y} - Y = \\varepsilon = \\sigma^2\\). What happens if it isn’t? \\[ \\mat{\\Sigma} = \\begin{bmatrix} \\Var(\\varepsilon_1) &amp; \\Cov(\\varepsilon_1, \\varepsilon_2) &amp; \\cdots &amp; \\Cov(\\varepsilon_1, \\varepsilon_N) \\\\ \\Var(\\varepsilon_2, \\varepsilon_1) &amp; \\Var(\\varepsilon_2) &amp; \\cdots &amp; \\Cov(\\varepsilon_2, \\varepsilon_N) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\Cov(\\varepsilon_N, \\varepsilon_1) &amp; \\Cov(\\varepsilon_N, \\varepsilon_2) &amp; \\cdots &amp; \\Cov(\\varepsilon_N) \\\\ \\end{bmatrix} \\\\ \\Sigma = \\begin{bmatrix} \\E(\\varepsilon_1^2) &amp; \\E(\\varepsilon_1 \\varepsilon_2) &amp; \\cdots &amp; \\E(\\varepsilon_1 \\varepsilon_N) \\\\ \\E(\\varepsilon_2 \\varepsilon_1) &amp; \\E(\\varepsilon_2^2) &amp; \\cdots &amp; \\E(\\varepsilon_2 \\varepsilon_N) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\E(\\varepsilon_N \\varepsilon_1) &amp; \\E(\\varepsilon_N \\varepsilon_2) &amp; \\cdots &amp; \\E(\\varepsilon_N^2) \\\\ \\end{bmatrix} \\\\ \\] The matrix can be written more compactly as, \\[ \\mat{\\Sigma} = \\E(\\vec{\\varepsilon} \\vec{\\varepsilon}\\T) \\] An assumption is that errors are independent, \\(\\E(\\epsilon_i \\epsilon_j)\\) for all \\(i \\neq j\\). This means that all off-diagonal elements of \\(\\mat{\\Sigma}\\) are 0$. Additionally, all \\(\\epsilon_i\\) are assumed to have the same variance, \\(\\sigma^2\\). Thus, the variance-covariance matrix of the errors is a assumed to have a diagonal matrix with the form, \\[ \\mat{\\Sigma} =  \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma^2 \\end{bmatrix}  = \\sigma^2 \\mat{I}_N \\] If these assumptions of the errors do not hold, then \\(\\Sigma\\) does not take this form, and more complicated models than OLS need to be used to get correct standard errors.   2.5 Non-constant variance (Heteroskedasticity) The homoskedastic case assumes that each eror term has its own variance. In the heteroskedastic case, each distrurbance may have its own variance, but they are still uncorrelated (\\(\\mat{\\Sigma}\\) is diagonal) \\[ \\mat{\\Sigma} =  \\begin{bmatrix} \\sigma_1^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma_2^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma_N^2 \\end{bmatrix} \\] The problem is that now there are \\(N\\) variance parameters to estimate, in addition to the \\(K\\) slope coefficients. Now, there are more parameters than we can estimate. With heteroskedasticity, OLS with be unbiased, but the standard errors will be incorrect. More general case allows for heteroskedasticity, and autocorrelation (\\(\\Cov(\\varepsilon_i, \\varepsilon_j) \\neq 0\\)), \\[ \\mat{\\Sigma} =  \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{1,2} &amp; \\cdots &amp; \\sigma_{1,N} \\\\ \\sigma_{2,1} &amp; \\sigma_2^2 &amp; \\cdots &amp; \\sigma_{2,N} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{N,1} &amp; \\sigma_{N,2} &amp; \\cdots &amp; \\sigma_N^2 \\end{bmatrix}  \\] As with heteroskedasticity, OLS with be unbiased, but the standard errors will be incorrect. TODO Different views on heteroskedasticity.   2.6 Why Control for Variables The reason for controlling for variables depends on the purpose of regresion  Description Get a sense of the relationships in the data  Prediction More variables may improve prediction  Causal Blocks confounding, which is when \\(X\\) does not cause \\(Y\\), but is correlated  with \\(Y\\) because another variable, \\(Z\\), causes \\(X\\) and \\(Y\\).   Multiple regression vs. bivariate regression  Slopes are predicted differences conditional on the other predictors. Estimates are still found by minimizing the squared residuals Regression with multiple covariates is equivalent to sequentially running multiple OLS regressions, each time removing the part explained by that predictor. Adding or omitting variables in a regression can affect the bias and variance of the OLS estimator.    2.7 Multicollinearity   2.8 Omitted Variable Bias \\[ \\]   "],
["ols.html", "Chapter 3 OLS", " Chapter 3 OLS Names for \\(\\vec{y}\\)  dependent variable explained variable response variable predicted variable regressand outcome variable  Names for \\(\\mat{X}\\),  indpendent variables explanatory varaibles treatment and control variables predictor variables covariates regressors  \\(\\vec{\\beta}\\) are the parameters; \\(\\beta_0\\) is called the intercept, and \\(\\beta_{1}, \\dots, \\beta_{K}\\) are called the slope parameters, or coefficients. \\(\\epsilon\\) is the error term, or disturbance, "],
["what-makes-an-estimator-good.html", "Chapter 4 What makes an estimator good? 4.1 Assumptions 4.2 References", " Chapter 4 What makes an estimator good? Estimators are evaluated not on how close an estimate in a given sample is to the population, but how their sampling distributions compare to the population. In other words, judge the methodology (estimator), not the result (estimate).[^ols-properties-references] Let \\(\\theta\\) be the population parameter, and \\(\\hat\\theta\\) be an estimator of that population parameter.  Bias The bias of an estimator is the difference between the mean of its sampling distribution and the population parameter, \\[\\Bias(\\hat\\theta) = \\E(\\hat\\theta) - \\theta .\\]  Variance The variance of the estimator is the variance of its sampling distribution, \\(\\Var(\\theta)\\).  Efficiency (Mean squared error) An efficient estimator is one that minimizes a given “loss function”, which is a penalty for missing the population average. The most common loss function is squared loss, which gives the Mean Squared Error (MSE) of an estimator.  \\[\\MSE(\\hat\\theta) = \\E\\left[{(\\hat\\theta - \\theta)}^{2}\\right] =  (\\E(\\hat\\theta) - \\theta)^2 + \\E(\\hat\\theta - \\E(\\hat\\theta))^2 = \\Bias(\\hat\\theta)^2 + \\Var(\\hat\\theta)\\]  The mean squared error is a function of both the bias and variance of an estimator.  This means that some biased estimators can be more efficient : than unbiased estimators if their variance offsets their bias.1    Table 4.1: Examples of clocks as “estimators” of the time2    Biased Variance     Stopped clock Yes High   Random clock No High   Clock that is “a lot” fast Yes Low   Clock that is “a little” fast Yes Low   Atomic clock No Low    Another property is consistency. Consistency is an asymptotic property3, that roughly states that an estimator converges to the truth as the number of obserservations grows, \\(\\E(\\hat\\theta - \\theta) \\to 0\\) as \\(N \\to \\infty\\). Roughly, this means that if you had enough (infinite) data, the estimator will give you the true value of the parameter.  4.1 Assumptions  No Perfect Collinearity. There is no exact linear relationships among the independent variables. \\(\\mat{X}\\) is a \\(N \\times K\\) matrix with rank \\(K\\). Linearity The popluation model is \\[    \\vec{y} = \\mat{X} \\vec{\\beta} + \\vec{\\varepsilon}    \\] where \\(\\vec{\\varepsilon}\\) is an unobserved random error or disturbance term with \\(\\E(\\varepsilon) = 0\\). Random/iid sample. \\((y_i, \\vec{x}_i&#39;)\\) are a random sample from the population. Zero conditional mean. The error, \\(\\varepsilon\\), has an expected value of zero, conditional on the predictors. \\[ \\E(\\varepsilon | X) = 0 \\] Constant variance (Homoskedasticity). The error has the same variance conditional on the predictors, for all observations, \\[ \\E(\\epsilon_i | \\vec{x}_i) = \\sigma^2) \\text{ for all $i$} \\] Fixed \\(\\mat{X}\\) or \\(\\mat{X}\\) measured without error and independent of the error. Normality   Identification of OLS: Under Assumption 1, OLS can be estimated. In other words, there is a unique \\(\\hat\\beta\\) that minimizes the sum of squared errors. Unbiasedness of OLS: Under Assumptions 1–4, OLS is unbiased. \\[ \\E(\\hat{\\vec{\\beta}}) = \\vec{\\beta} \\] Gauss-Markov theorem. Under Assumptions 1–5, OLS is the best linear unbiased estimator of \\(\\vec{\\beta}\\). Linear means that the estimates can be written as a linear functions of the outcomes, \\[   \\tilde{\\beta}_j = \\sum_{i = 1}^n w_{i,j} y_i   \\] Best means that it has the smallest variance. This means for any unbiased and linear estimator, \\(\\tilde{\\beta}\\), the OLS estimator, \\(\\hat{\\beta}_{OLS}\\), has a smaller variance, \\[   \\Var(\\tilde{\\beta}) &gt; \\Var(\\hat{\\beta}_{OLS})   \\] Not that this does not imply that OLS has the lowest MSE of any estimator, since a biased estimator could have a lower MSE. In fact, for any regression with three or more variables, there is a ridge estimator with a lower MSE.  Note that these assumptions can be sometimes be written in slightly different forms. Linearity: The Population regresion function is linear in the parameters. \\[ \\begin{aligned}[t] y_i &amp;= \\beta_0 + \\sum \\beta_k \\vec{x}&#39;_i \\\\ \\vec{Y} &amp;= \\vec{\\beta} \\mat{X} \\end{aligned} \\]   4.2 References  Wooldrige, Ch 3. Fox, Ch 6, 9.     "],
["appendix.html", "Chapter 5 Appendix 5.1 Multivariate Normal Distribution", " Chapter 5 Appendix  5.1 Multivariate Normal Distribution The multivariate normal distribution is the generalization of the univariate normal distribution to more than one dimension.4 The random variable, \\(\\vec{x}\\), is a length \\(k\\) vector. The \\(k\\) length vector \\(\\vec{\\mu}\\) are the means of \\(\\vec{x}\\), and the \\(k \\times k\\) matrix, \\(\\mat{\\Sigma}\\), is the variance-covariance matrix, \\[ \\begin{aligned}[t] \\vec{x} &amp;\\sim \\dmvnorm{k}\\left(\\vec{\\mu}, \\mat{\\Sigma} \\right) \\\\ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_k \\end{bmatrix} &amp; \\sim \\dmvnorm{k} \\left(   \\begin{bmatrix}   \\mu_1 \\\\   \\mu_2 \\\\   \\vdots \\\\   \\mu_k   \\end{bmatrix},   \\begin{bmatrix}   \\sigma_1^2 &amp; \\sigma_{1,2} &amp; \\cdots &amp; \\sigma_{1, k} \\\\   \\sigma_{2,1} &amp; \\sigma_2^2 &amp; \\cdots &amp; \\sigma_{2, k} \\\\   \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\   \\sigma_{k,1} &amp; \\sigma_{k,2} &amp; \\cdots &amp; \\sigma_{k, k}   \\end{bmatrix} \\right) \\end{aligned} \\] The density function of the multivariate normal is, \\[ p(\\vec{x}; \\vec{\\mu}, \\mat{\\Sigma}) = (2 k)^{-\\frac{k}{2}} \\left| \\mat{\\Sigma} \\right|^{-\\frac{1}{2}} \\exp \\left( -\\frac{1}{2} (\\vec{x} - \\vec{\\mu})\\T \\mat{\\Sigma}^{-1} (\\vec{x} - \\vec{\\mu}) \\right) . \\] You can sample from and calculate the density for the multivariate normal distribution with the functions dmvnorm and rmvnorm from the package mvtnorm. Density plots of different bivariate normal distributions,            It follows from the definition of MSE, that biased estimator, \\(\\hat\\theta_{B}\\), has a lower MSE than an unbiased estimator, \\(\\hat\\theta_{U}\\), if \\(\\Bias(\\theta_B)^2 &lt; \\Var(\\theta_U) - \\Var(\\theta_B)\\).↩ Example from Chris Adolph↩ As the number of observations goes to infinity.↩ See Multivariate normal distribution and references therein.↩  "]
]
