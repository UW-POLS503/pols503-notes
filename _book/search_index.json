[
["rs-formula-syntax.html", "Chapter 6 R’s Formula Syntax", " Chapter 6 R’s Formula Syntax Many R functions, especially those for statistical models such as lm(), use a convenient syntax to compactly specify the outcome and predictor variables. This format is described in more detail in the formula help page. Formulae in R are denoted with a tilde (~), with the interpretation outcome ~ predictors For example, this lm(formula = prestige ~ type + income * education, data = Duncan) estimates this, \\[ \\mathtt{prestige} = \\beta_0 + \\beta_1 \\mathtt{income} + \\beta_2 \\mathtt{education} + \\beta_3 \\mathtt{income} \\times \\mathtt{education} . \\] Note that + and * are not directly interpreted as addition and multiplication, instead adding terms to the regression and interactions, respectively. In a formula, there is a special syntax in which some common operators have different meanings than in R. Symbol Example Meaning + +x Include \\(x\\) - -x Exclude \\(x\\) : x : z Include \\(x z\\) as a predictor * x * z Include \\(x\\), \\(z\\), and their interaction \\(x z\\) as predictors ^ (x + w + z) ^ 3 Include variables and interactions up to three way: \\(x\\), \\(z\\), \\(w\\), \\(xz\\), \\(xw\\), \\(zw\\), \\(xzw\\). I I(x ^ 2) as is: include a new variable with \\(x ^ 2\\). 1 -1 Intercept; Use -1 to delete, and +1 to include Formula Equation y ~ x + y + z \\(y = \\beta_0 + beta_1 x + beta_2 y + beta_3 z\\) y ~ x + y - 1 \\(y = \\beta_1 x + \\beta_2 y\\) y ~ 1 \\(y = \\beta_0\\) y ~ x:z \\(y = \\beta_0 + \\beta_1 xz\\) y ~ x*z \\(y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 xz\\) y ~ x*z - x \\(y = \\beta_0 + \\beta_1 z + \\beta_2 xz\\) y ~ (x + z)^2 \\(y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3\\) y ~ (x + z + w)^2 \\(y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 w + \\beta_4 w + \\beta_5 xz + \\beta_6 xw + \\beta_7 zw\\) y ~ (x + z + w)^2 \\(y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 w + \\beta_4 w + \\beta_5 xz + \\beta_6 xw + \\beta_7 zw + \\beta_8 xzw\\) y ~ x + I(x ^ 2) \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\) y ~ poly(x, 2) \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\) y ~ I(x * z) \\(y = \\beta_0 + \\beta_1 xz\\) y ~ I(x + z) + I(x - z) \\(y = \\beta_0 + \\beta_1 (x + z) + \\beta_2 (x - z)\\) y ~ log(x) \\(y = \\beta_0 + \\beta_1 \\log(x)\\) y ~ x / z \\(y = \\beta_0 + \\beta_1 (x / z)\\) y ~ x + 0 \\(y = \\beta_1 x\\) Formulae provide a convenient means of specifying the statistical model and also generating temporary variables that we only need for the model. For example, if we want to include \\(\\log(x)\\), \\(z\\), their interaction \\(\\log(x)\\), and the square of \\(z\\), we could write y ~ log(x) * y + I(y ^ 2) instaed of of having to create the log, square, and interaction variables before running the regression. This becomes especially useful if you need to include large polynomials, splines, interaction, or similarly complicated functional forms. Warning: Often you will want to include polynomials in a regression. However, ^ already has a special meaning in R’s syntax, so you cannot use x ^ 2. There are two ways to specify polynomials. - Use the as is operator, I(). For example, I(x ^ 2) includes \\(x^2\\) in the regression. - Use the poly function to generate polynomials. For example, I(x ^ 2) will include \\(x\\) and \\(x^2\\) in the regerssion. This can be more convenient and clear than writing x + I(x ^ 2)$. Warning: R’s formula syntax is flexible and includes more features than what ws covered in this section. What was described in this section were features that lm() uses. Other functions may have more features or the parts will have different meanings. However, they will be mostly be similar to what was described here. An example of a function that has slightly different syntax for its formula is lmer which adds notation for random and fixed effects. Some functions like xtabs use a formula syntax even though they aren’t statistical modeling functions. The package Formula provides an even more powerful Formula syntax than that included with base R. If you have to write a function or package that uses a formula, use that package. Inspired by this handout from Richard Hahn and the handout form the NPS. "],
["functional-form-and-non-linearity.html", "Chapter 7 Functional Form and Non-linearity 7.1 Non-linearity 7.2 Logarithm 7.3 Miscellaneous 7.4 Polynomials 7.5 Interactions 7.6 Flexible Functional Forms 7.7 References", " Chapter 7 Functional Form and Non-linearity 7.1 Non-linearity 7.1.1 What’s the problem? If the relationship between the regression surfacne and \\(E(Y | X)\\) is not captured well, then the results of the regression may be misleading, although this depends on the modeling approach regression is being used for. The extent of the problem varies with which variables are affected, and the purpose of the analysis. If the analysis is interested in the average marginal effect of the treatment variable, then using the OLS coefficient to estimate the AME is not a bad approximation. The values of the individual marginal effects will be incorrect, but the average should be a reasonable approximation. If you are interested in the AME of sub-populations or other estimands, then you will need to account for the non-linearity. If the non-linearity is in the control variables, then it is another form of omitted variable bias. 7.1.2 What to do about it? And How to Solve it? The general approaches to identifying non-linearity include: Residual plots with curvature tests: car function residualPlots. Added-variable (AV) plot: car function avPlots. Component+residual (CERES) plot: car functions crPlots and ceresPlots. Ramsay RESET test. lmtest function resettest Compare Robust SE and classical OLS SE. King and Roberts. In general, I think most of these approaches are time consuming, sub-optimal given new methods and computation, and open up the regression model to too many researcher degrees of freedom that will not be represented in the uncertainty of the model There now exist many models (notably semi-parametric and non-parametric) which allow for more flexible functional forms with less-model dependence. Some of these include: GAM and spline models K-nearest neighbor models Matching methods LASSO, Ridge and other Shrinkage Regression (especially with basis functions) 7.2 Logarithm 7.2.1 Examples of Relevant Theories Converts multiplicative theories to additive theories. Theories with diminishing returns to scale. Theories about percentage changes or growth. Most uses of (per capita) GDP, population: Cobb-Douglas growth \\[ y = \\alpha (K^(\\delta) L^(1 - \\delta))^{\\nu} \\] Linearized, \\[ \\log y = \\log \\alpha + \\log k \\] Gravity trade equation Lanchester law for casualties \\[ \\Delta x = \\alpha x^\\beta y^\\gamma \\] where \\(\\Delta x\\) are casualties per period, \\(x\\) is the initial size of forces forces, and \\(y\\) are opposing forces. This can be linearized and estimated with OLS, \\[ \\log \\Delta x = \\log \\alpha + \\beta \\log x + \\gamma \\log y \\] as long as \\(x, y &gt; 0\\) (and preferrably large). 7.3 Miscellaneous 7.3.1 Square Root and Variance Stabalizing Transformations 7.3.2 Power-Transformation 7.4 Polynomials 7.4.1 Squared 7.4.1.1 What theories? Kuznets curve: economic development and inequality Environmental Kuznets curve: environmental quality and economic development Democratic Civil Peace: intermediate regimes prone to civil war, democracies and autocracies are less prone to civil war. 7.4.2 Higher-Order Polynomials Time cubed Seat-Vote curves? Other old examples in Tufte 1975 These are generally 7.5 Interactions Standard errors are more difficult to calculate. See Golder’s page, and Aiken and West (1991). 7.5.1 Theories Golder et. al recommend that for simple interaction model such as: \\[ \\vec{y} = \\beta_0 + \\beta_x \\vec{x} + \\beta_z \\vec{z} + \\beta_{xz} \\vec{x} \\vec{z} + \\vec{varepsilon} \\] the reseearcher make as many of the following predictions as possible The marginal effect of \\(X\\) is (positive, negative, zero) when \\(Z\\) is at its lowest level. … when \\(Z\\) is at its highest level. The marginal effect of \\(Z\\) is (positive, negative, zero) when \\(X\\) is at its lowest level. … when \\(X\\) is at its highest level. The marginal effect of each \\(X\\) and \\(Z\\) is (positively, negatively) related to the other variable 7.5.2 Recommendations Golder et al recommend Use multiplicative interaction models for conditional hypotheses. Include all constituent terms of the interaction in the model. Do not interpret coefficients on terms seperately, or as if they are unconditional marginal effects. Calculate substantively meaningful marginal effects and their standard errors. 7.5.3 Plots Golder et al recommend: Construct marginal effect plots for both X and Z. The range of the horizontal axis should extend from the minimum to the maximum value of variable in the sample. The plot should include a frequency distribution of the variable of interest, as either a rug plot, histogram, or density. Report the product term coefficient and its t-statistic or standard error. 7.6 Flexible Functional Forms Splines GAM Gaussian Processes Random Forests Neural Networks 7.7 References Matt Golder Interactions Golder’s papers "]
]
