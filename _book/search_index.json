[
["ols-troubleshooting-and-diagnostics.html", "Chapter 3 OLS Troubleshooting and Diagnostics 3.1 References 3.2 Multi-Collinearity 3.3 Omitted Variable Bias 3.4 Measurement Error 3.5 Non-linearity 3.6 Non-constant Variances and Correlated Errors 3.7 Non-Normal Errors", " Chapter 3 OLS Troubleshooting and Diagnostics  3.1 References Several packages provide large collections:  lmtest car sandwich  Reading the vignettes or documentation of these packages is a good overview of available regression diagnostics. Also see the Econometrics Task View. The Fox textbook has a particularly extensive overview of regression diagnostics. Also see Farawy. Though for Stata, this tutorial has an overview of many regression diagnostics.   3.2 Multi-Collinearity  3.2.1 Perfect Collinearity In order to estimate unique \\(\\hat{\\beta}\\) OLS requires the that the columns of the design matrix \\(\\vec{X}\\) are linearly independent. Common examples of groups of variables that are not linearly independent:  Categorical variables in which there is no excluded category. You can also include all categories of a categorical variable if you exclude the intercept. Note that although they are not (often) used in political science, there are other methods of transforming categorical variables to ensure the columns in the design matrix are independent. A constant variable. This can happen in practice with dichotomous variables of rare events; if you drop some observations for whatever reason, you may end up dropping all the 1’s in the data. So although the variable is not constant in the population, in your sample it is constant and cannot be included in the regression. A variable that is a multiple of another variable. E.g. you cannot include \\(\\log(\\text{GDP in millions USD})\\) and \\(\\log({GDP in USD})\\) since \\(\\log(\\text{GDP in millions USD}) = \\log({GDP in USD}) / 1,000,000\\). in A variable that is the sum of two other variables. E.g. you cannot include \\(\\log(population)\\), \\(\\log(GDP)\\), \\(\\log(GDP per capita)\\) in a regression since \\[\\log(\\text{GDP per capita}) = \\log(\\text{GDP} / \\text{population}) = \\log(\\text{GDP}) - \\log(\\text{population})\\].   3.2.1.1 What to do about it? R and most statistical programs will run regressions with collinear variables, but will drop variables until only linearly independent columns in \\(\\mat{X}\\) remain. For example, consider the following code. The variable type is a categorical variable with categories “bc”, “wc”, and “prof”. It will data(Duncan, package = &quot;car&quot;) # Create dummy variables for each category Duncan &lt;- mutate(Duncan,                  bc = type == &quot;bc&quot;,                  wc = type == &quot;wc&quot;,                  prof = type == &quot;prof&quot;) lm(prestige ~ bc + wc + prof, data = Duncan) ##  ## Call: ## lm(formula = prestige ~ bc + wc + prof, data = Duncan) ##  ## Coefficients: ## (Intercept)       bcTRUE       wcTRUE     profTRUE   ##       80.44       -57.68       -43.78           NA R runs the regression, but coefficient and standard errors for prof are set to NA. You should not rely on the software to fix this for you; once you (or the software) notices the problem check the reasons it occurred. The rewrite your regression to remove whatever was creating linearly dependent variables in \\(\\mat{X}\\).    3.2.2 Less-than Perfect Collinearity What happens if variables are not linearly dependent, but nevertheless highly correlated? If \\(\\Cor(\\vec{x}_1, vec{x}_2) = 1\\), then they are linearly dependent and the regression cannot be estimated (see above). But if \\(\\Cor(\\vec{x}_1, vec{x}_2) = 0.99\\), the OLS can estimate unique values of of \\(\\hat\\beta\\). However, it everything was fine with OLS estimates until, suddenly, when there is linearly independence everything breaks. The answer is yes, and no. As \\(|\\Cor(\\vec{x}_1, \\vec{x}_2)| \\to 1\\) the standard errors on the coefficients of these variables increase, but OLS as an estimator works correctly; \\(\\hat\\beta\\) and \\(\\se{\\hat\\beta}\\) are unbiased. With multicollinearly, OLS gives you the “right” answer, but it cannot say much with certainty. Insert plot of highly correlated variables and their coefficients. Insert plot of uncorrelated variables and their coefficients.  3.2.2.1 What to do about it? Remember multicollinearity does not violate the assumptions of OLS. If all the other assumptions hold, then OLS is giving you unbiased coefficients and standard errors. What multicollinearity is indicating is that you may not be able to answer the question with the precision you would like.  If the variable(s) of interest are highly correlated with other variables, then it means that there is not enough variation, controlling for other factors. You may check that you are not controlling for “post-treatment” variables. Dropping control variables if they are correctly included will bias your estimates. But otherwise, there is little you can do other than get more data. You could re-consider your research design and question. What does it mean if there is that little variation in the treatment variable after controlling for other factors? If control variables are highly correlated with each other, it does not matter. You should not be interpreting their coefficients, so their standard errors do not matter. In fact, controlling for several similar, but correlated variables, may be useful in order to offset measurement error in any one of them.      3.3 Omitted Variable Bias          \\(\\Cov(X_1, X_2)\\) \\(\\Cov(X_2, Y) &gt; 0\\) \\(\\Cov(X_2, Y) = 0\\) \\(\\Cov(X_2, Y) &lt; 0\\)     \\(&gt; 0\\) \\(0\\) \\(&lt; 0\\) + 0 - 0 0 0 - 0 +     3.3.1 What’s the problem?   3.3.2 What to do about it? Summary:  OVB is intrinsic to observational methods relying on selection on observables—not just regression. Control for all plausible “pre-treatment” variables Reason about possible biases due to OVB Sensitivity of coefficients to inclusion of control variables is an indication of the plausibility of OVB. Altonji, Elder, and Taber (2005). formalize this.  In practice, this is a primary problem of many papers and papers; and for good reason, it biases the coefficient of interest. Reviewers and discussants will often ask about whether you have considered controlling for foo. Although these may be legitimate concerns, not all commenters understand the purpose of control variables. There two arguments to consider when addressing these arguments.  The omitted variable has to plausibly be correlated with both the variable of interest and the outcome variable, and the burden is on the commenter to provide at a confounding variable and plausible relationships. Simpy stating that there could be an unobservable variable is trivially true, uninteresting, and not a fatal critique. That said, the evidentiary content of your methods would be higher if you used methods less susceptible to potential unobserved confounders. The omitted variable should be a good control and not a “post treatment” variable. If the omitted variable should not be one of the causal pathways by which \\(X\\) affects \\(Y\\), it should not be controlled for. If \\(Z\\) affects the values of \\(X\\) and also affects \\(Y\\), then it needs to be controlled for.  There are two common ways of assessing plausibility.  Informal method. This is what you see in many empirical papers. Estimate the model including different control variables. The less sensitive the coefficient(s) of the variables of interest are to the inclusion of control variables, the more plausible it is that the variable of interest is not sensitive to unobserved variables (Angrist and Pischke 2014). Oster (2013) states  A common heuristic for evaluating the robustness of a result to omitted variable bias concerns is to look at the sensitivity of the treatment effect to inclusion of observed controls. In three top general interest economics journals in 2012, 75% of non-experimental empirical papers included such sensitivity analysis. The intuitive appeal of this approach lies in the idea that the bias arising from the observed controls is informative about the bias that arises from the unobserved ones.  Note that what is important is that the coefficient is stable to the inclusion of controls, not that the coefficient remains statistically significant (which seems to be what many authors focus on). Formal method Several papers, including Altonji, Elder, and Taber (2005), Bellows and Miguel (2009), and Oster (2013), formalize the intuition behind the heuristic of coefficient stability to assess the sensitivity of the treatment to OVB.  Altonji, Elder, and Taber (2005) propose a method for assessing the potential impact of the omitted variable bias as the importance of the omitted variable needed to explain away the entire effect. That work addresses the case for a dichotomous treatment variable, and assumed joint normality. Bellows and Miguel (2009) extend the method to continuous treatment variables. The statistic proposed by Bellows and Miguel (2009) is simple, \\[ \\delta = \\frac{\\hat{\\beta}_F}{\\hat{\\beta}_R - \\hat{\\beta}_C}, \\] where \\(\\delta\\) is \\[ \\Cov{x, \\tilde{w}}{x, w&#39; \\gamma} \\] \\(\\delta\\) is interpreted as the how strong the covariance between the unobserved part of the controls and the treatment variable must be relative to the covariance between the observed part of the controls and the treatment variable to explain away the entire effect of \\(x\\) on \\(y\\). A larger ratio suggests it is implausible that omitted variable bias could explain away the entire observed effect. Suppose we would like to estimate \\[ y = \\beta x + \\gamma z \\] If \\(z\\) is left out of the OLS estimation, then the estimates of \\(\\beta\\) will have omitted variable bias, \\[ \\plim \\hat\\beta_{OLS,NC} = \\beta + \\gamma \\frac{\\Cov(x, z)}{\\Var{x}} \\] Suppose that instead of \\(z\\) we observe a set of controls \\(w^*\\) that are related to the full set of controls, \\[ z = w&#39; \\beta + \\omega \\] See Appendix A of Bellows and Miguel (2009) for the derivation. TODO Insert path diagram. OVB is a intrinsic problem in observational research, and there is nothing you can do to ever ensure that you have controlled for all relevant variables (however, all inference is uncertain, even the designs discussed next, so people should learn to deal with uncertainty). Also, methods such as matching, propensity scores, or inverse weighting still depend on assumptions about selection on observables, even if they may be less sensitive to certain kinds of modeling assumptions. The alternative is to use designs which do not require directly controlling for observable differences. Examples of these designs include: experiments (obviously), natural experiments, instrumental variables, and regression discontinuity.    3.4 Measurement Error  3.4.1 What’s the problem? It biases coefficients:  Variable with measurement error: biases \\(\\beta\\) towards zero (attenuation bias) Other variables: Biases \\(\\beta\\) similarly to omitted variable bias. In other words, when a variable has measurement error it is an imperfect control. You can think of omitted variables as the limit of the effect of measurement error as it increases.    3.4.2 What to do about it? There’s no easy fix within the OLS framework.  If the measurement error is in the variable of interest, then the variable will be biased towards zero, and your estimate is too large. Find better measures with lower measurement errors. If the variable is the variable of interest, then perhaps combine multiple variables into a single index. If the measurement error is in the control variables, then include several measures. That these measure correlate closely increases their standard errors, but the control variables are not the object of the inferential analysis. More complicated methods: errors in variable models, structural equation models, instrumental variable (IV) models, and Bayesian methods.     3.5 Non-linearity  3.5.1 What’s the problem? If the relationship between the regression surfacne and \\(E(Y | X)\\) is not captured well, then the results of the regression may be misleading, although this depends on the modeling approach regression is being used for. The extent of the problem varies with which variables are affected, and the purpose of the analysis.  If the analysis is interested in the average marginal effect of the treatment variable, then using the OLS coefficient to estimate the AME is not a bad approximation. The values of the individual marginal effects will be incorrect, but the average should be a reasonable approximation. If you are interested in the AME of sub-populations or other estimands, then you will need to account for the non-linearity. If the non-linearity is in the control variables, then it is another form of omitted variable bias.    3.5.2 What to do about it? And How to Solve it? The general approaches to identifying non-linearity include:  Residual plots with curvature tests: car function residualPlots. Added-variable (AV) plot: car function avPlots. Component+residual (CERES) plot: car functions crPlots and ceresPlots. Ramsay RESET test. lmtest function resettest Compare Robust SE and classical OLS SE. King and Roberts.  In general, I think most of these approaches are time consuming, sub-optimal given new methods and computation, and open up the regression model to too many researcher degrees of freedom that will not be represented in the uncertainty of the model There now exist many models (notably semi-parametric and non-parametric) which allow for more flexible functional forms with less-model dependence. Some of these include:  GAM and spline models K-nearest neighbor models Matching methods LASSO, Ridge and other Shrinkage Regression (especially with basis functions)     3.6 Non-constant Variances and Correlated Errors The OLS coefficient standard errors, \\[ \\Var({\\hat{\\vec{\\beta}}}) = \\sigma^2 (\\mat{X}\\T \\mat{X})^{-1} \\] depends on the assumption of homoskedastic errors. Homoskedasticity has two components,  Disturbances have the same variance, \\(\\Var(\\varepsilon_i) = \\sigma^2\\) for all \\(i\\). No correlation between distrurbances, \\(\\Cov(\\varepsilon_i, \\varepsilon_j) = 0\\) for all \\(i \\neq j\\).  Either or both of these components can be violated, and when they are, the standard errors of the OLS estimator are incorrect. The general OLS variance-covariance matrix of the coefficients is, \\[ \\Var(\\hat{\\vec\\beta}) = (\\mat{X}\\T \\mat{X})^{-1} (\\mat{X} \\Sigma \\mat{X}) (\\mat{X}\\T \\mat{X})^{-1} \\] where \\(\\mat{Sigma}\\) is the correlation of the disturbances, \\(\\vec\\varepsilon\\), \\[ \\mat{\\Sigma} = \\vec{\\varepsilon}\\T \\vec{\\varepsilon} =  \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_1 \\sigma_2 &amp; \\cdots &amp; \\sigma_1 \\sigma_N \\\\ \\sigma_2 \\sigma_1 &amp; \\sigma_2^2 &amp; \\cdots &amp; \\sigma_2 \\sigma_N \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_N \\sigma_1 &amp; \\sigma_N \\sigma_2 &amp; \\cdots &amp; \\sigma_N^2 \\\\ \\end{bmatrix} \\] When we assume homoskedasticity, the variance-covariance matrix of \\(\\varepsilon\\) is \\[ \\mat{\\Sigma} =  \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma^2  \\end{bmatrix}   = \\sigma^2 \\begin{bmatrix} 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{bmatrix}  =  \\sigma^2 \\mat{I}_{N} \\], Under homoskedasticity, the sampling distribution of \\[ \\begin{aligned}[t] \\Var(\\beta | \\mat{X}) &amp;= (\\mat{X}&#39; \\mat{X})^{-1} \\mat{X}&#39; \\mat{\\Sigma} \\mat{X} (\\mat{X}&#39; \\mat{X})^{-1} \\\\ &amp;= (\\mat{X}&#39; \\mat{X})^{-1} \\mat{X}&#39; \\sigma^2 \\mat{I}_N \\mat{X} (\\mat{X} &#39;\\mat{X})^{-1} \\\\ &amp;= \\sigma^2 (\\mat{X}&#39; \\mat{X})^{-1} \\mat{X}&#39; \\mat{X} (\\mat{X} &#39;\\mat{X})^{-1} \\\\ &amp;= \\sigma^2 (\\mat{X}&#39; \\mat{X})^{-1} \\end{aligned} \\] To estimate \\(\\Var(\\hat{\\vec\\beta}|\\mat{X})\\), replace \\(\\sigma^2\\) with \\(\\hat{\\sigma}^2\\), where \\[ \\hat{\\sigma}^2 = \\frac{1}{N - K - 1} \\sum \\varepsilon_i^2 \\] Q. What if the assumption of homoskeasticity isn’t true? A. The coefficients \\(\\hat{\\vec\\beta}\\) are unbiased, but the standard errors \\(\\hat{\\se}(\\hat{\\vec{\\beta}})\\) are biased. Since we don’t know \\(\\mat{\\Sigma}\\), why not simply estimate the elements of \\(\\mat{\\Sigma}\\) along with \\(\\vec{\\beta}\\)? The problem is that there are \\(N\\) observations, and \\(\\mat\\Sigma\\) is an \\(N \\times N\\) matrix with \\((N * (N + 1)) / 2\\) elements since it is symmetric. So some structure needs to be put on \\(\\mat{\\Sigma}\\), i.e. we need to make additional assumptions, to estimate \\(\\mat{\\Sigma}\\). As we will see, we can make less restrictive assumptions than homoskedasticity, i.e. \\(\\mat{\\Sigma} = \\sigma^2 \\mat{I}\\), but will always have to assume some sort of structure in \\(\\mat{\\Sigma}\\). There’s only one way for homoskedasticity to be correct (\\(\\mat{\\Sigma} = \\sigma^2 \\mat{I}\\)), and many ways for it to be wrong. We’ll consider a few of the most common, and methods to deal with them.  Heteroskedasticity Autocorrelation Clustering   3.6.1 Heteroskedasticity The homoskedastic case assumes that each error term has its own variance. In the heteroskedastic case, each disturbance may have its own variance, but they are still uncorrelated (\\(\\mat{\\Sigma}\\) is diagonal) \\[ \\mat{\\Sigma} = \\begin{bmatrix} \\sigma_1^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma_2^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma_N^2 \\end{bmatrix} = \\sigam^2 \\mat{I}_N \\] With homoskedasticity the estimator of the variance covariance matrix takes a particularly simple form, \\[ \\Var(\\hat{\\beta} | \\mat{X}) &amp;= (\\mat{X}&#39; \\mat{X})^{-1} \\mat{X}&#39; \\mat{\\Sigma} \\mat{X} (\\mat{X} &#39;\\mat{X})^{-1} \\\\ &amp;= \\hat\\sigma^2 (\\mat{X}&#39; \\mat{X})^{-1} \\] where \\[ \\hat\\sigma^2 = \\frac{\\sum \\hat{\\epsilon}^2}{N - K - 1} \\] The consequences of violating homoskedasticity are,  Biased (often downward) standard errors, \\(\\E(\\se{\\hat\\beta}) \\neq \\sd(\\beta)\\). Test statistics do hot have \\(t\\) or \\(F\\) distributions. \\(\\alpha\\)-level tests have the wrong level of Type I error. E.g. a 5% test will not have 5% Type I errors. Confidence intervals do not have the correct coverage. E.g. a 95% confidence does not contain the true mean in 95% of samples. OLS is not BLUE. \\(\\hat{\\vec\\beta}\\) is still and unbiased and consistent estimator for \\(\\vec\\beta\\).  So why don’t we estimate all the elements of \\(\\mat\\Sigma\\) like we estimate \\(\\beta\\)? Since \\(\\mat\\Sigma\\) is an \\(N \\times N\\) symmetric matrix, it has $(N (N + 1)) / 2 $ elements. But we have only \\(N\\) data points to estimate them. In order to estimate \\(\\mat\\Sigma\\) we cannot estimate arbitrary correlations in \\(\\mat\\Sigma\\), but we need to apply some structure to the variance-covariance matrix in order to reduce the number of elements to estimate.  Heteroskedasticity Clustered Standard Errors Serial Correlation  In general there are two types of methods to deal with issues in the error,  New estimators that model the error process and estimate elements of \\(\\mat\\Sigma\\) simultaneously with the coefficients \\(\\beta\\). This includes weighted least squares (heteroskedasticity), Prais-Wiston (AR(1) errors). These methods produce \\(\\hat\\beta \\neq \\hat\\beta_{OLS}\\). Since OLS produces unbiased and consistent estimates of \\(\\hat\\beta\\), we keep the coefficient estimates, but correct the variance-covariance matrix \\(\\hat\\Var{\\beta}\\).    3.6.2 Heteroskedasticity In heteroskedasticity, the errors are still independent, i.e. \\(\\mat\\Sigma\\) is still diagonal, but the variance elements on the diagonal are not equal, \\[ \\Var(\\vec\\varepsilon|\\mat{X}) =  \\begin{bmatrix} \\sigma_1^2 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma_2^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; \\sigma_N^2 \\end{bmatrix} . \\] While each observation has a difference variance, they are still independent, \\(\\Cov(\\epsilon_i, \\epsilon_j | \\mat{X}) = 0\\), for all \\(i \\neq j\\).  Example in difference in means Example with a continous \\(X\\) Simulation of the effects of violations    3.6.3 Diagnostics  Plot residuals vs. fitted values Spread-level plots (car::spreadLevel), Compare Robust SE vs. non-robust SE. If they are differen Tests: Breusch-Pagan (lmtest::bptest, car::ncvTest),    3.6.4 Dealing with Heteroskedasticity  Transform the dependent variable. For example, \\(\\log\\) the dependent variable. Model heteroskedasticity using Weighted Least Squares (WLS) Use OLS with an estimator of \\(\\Var(\\hat{\\vec\\beta})\\) that is robust to heteroskedasticity Admit that OLS is insufficient, and use a different model   If the form of heteroskedasticity follows a particularly simple form, transform the dependent variable. For example, log the dependent variable. If the form of the heteroskedasticity is known: weighted least squares. lm() with the weights argument. If the form of the heteroskedasticity is unknown: Huber-White heteroskedasticity consisten standard errors. See sandwich package. You can calculate the heteroskedasticity correct covariance matrix using sandwich::vcovHC and then use lmtest::coeftest to calculate p-values and standard-errors.    3.6.5 Weighted Least Squares Suppose that the heteroskedasticity is known up to a multiplicative constant, \\[ \\Var(\\varepsilon_i | \\mat{X}) = a_i \\sigma^2 , \\] where \\(a_i = a_i \\vec{x}_i&#39;\\) is a positive and known funtion of \\(\\vec{x}_i\\). Then in weighted least squares multiply \\(y_i\\) by \\(1 / \\sqrt{a_i}\\), \\[ \\begin{aligned}[t] y_i &amp;= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_K x_K + \\varepsilon_i \\\\ y_i / \\sqrt{a_i} &amp;= \\beta_0 / \\sqrt{a_i} + \\beta_1 x_1 / \\sqrt{a_1} + \\beta_2 x_2 / \\sqrt{a_2} + \\dots + \\beta_K x_K / \\sqrt{a_K} + \\varepsilon_i^* \\end{aligned} \\] where \\(\\varepsilon_i^* \\sim N(0, \\sigma^2)\\). This rescales errors to \\(\\varepsilon_i / \\sqrt{a_i}\\) which keeps \\(\\E(\\varepsilon_i) = 0\\), but makes the variance constant, \\[ \\Var\\left( \\frac{1}{\\sqrt{a_i}} \\varepsilon_i | \\mat{X} \\right) = \\frac{1}{a_i} \\Var(\\varepsilon_i | \\mat{X}) = \\frac{1}{a_i} a_i \\sigma^2 = \\sigma^2 \\] If \\(a_i\\) is known, then the model is homoskedastic and the estimator is BLUE. Define the weighting matrix, \\[ \\mat{W} = \\begin{bmatrix} 1 / \\sqrt{a_1} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 / \\sqrt{a_2} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; 0 \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 / \\sqrt{a_N} \\end{bmatrix} . \\] Then run the regression, \\[ \\begin{aligned}[t] \\mat{W} y &amp;= \\mat{W} \\mat{X} \\vec{\\beta} + \\mat{W} \\vec\\varepsilon \\\\ \\vec{y}^* &amp;= \\mat{X}^* \\vec{\\beta} + \\vec{\\varepsilon}^* . \\end{aligned} \\] Run the regression of \\(\\vec{y}^*\\) on \\(\\mat{X}^*\\), and the Gauss-Markov assumptions are statisfied. Then using the usual OLS formula, \\[ \\hat{\\vec\\beta}_{WLS} = ((\\mat{X}^*)&#39; \\mat{X}^*) (\\mat{X}^*)&#39; \\vec{y}^* = (\\mat{X}&#39; \\mat{W}&#39; \\mat{W} \\mat{X})^{-1} \\mat{X}&#39; \\mat{W}&#39; \\mat{W} \\vec{y} . \\] In R Use lm() with the weights argment. TODO: when is WLS useful  3.6.5.1 Advice In practice, often diagnostics are not conducted and robust standard errors are used. This is partially due to the ease with which heteroskasticity consistent standard errors can be calculate in Stata (see , robust). Robust standard errors, especially when used with MLE estimators, is controversial.  See Freedman See King and Roberts  But this depends on how they are being used, see Angrist.    3.6.6 Clustered Standard Errors  Clusters: \\(g = 1, \\dots, G\\). Units: \\(i = 1, \\dots, N_g\\). \\(N_g\\) is the number of observations in cluster \\(g\\) \\(N = \\sum_g N_g\\) is the total observations Units (usually) belong to a single clustr voters in household individuals in states students in classes This is particularly important when the outcome varies at the unit-level, \\(y_ij\\) and the main independent variable varies at the cluster level. Ignoring clustering overstates the effective number of individuals in the data.  Clustered dependence \\[ \\begin{aligned}[t] y_{ig} &amp;= \\beta_0 + \\beta_1 x_{ig} +\\epsilon_{ig} \\\\ &amp;= \\beta_0 + \\beta_1 x_{ig} + \\nu_g + \\eta_{ig} \\end{aligned} \\] Then the cluster error is \\[ \\nu \\sim N(0, rho \\sigma^2), \\] and the individual error is \\[ \\eta_{ig} \\sim N(0, (1 - \\rho) \\sigma^2) . \\] The cluster and unit errors are assumed to be indendent of each other. \\(\\rho \\in (0, 1)\\) is the within-cluster correlation. If we ignore the cluster, and use \\(\\eta_{ig}\\) as the error, the variance is \\(\\sigma^2\\) \\[ \\Var(\\eta_{ig}) &amp;= \\Var(\\nu_g +\\eta_{ig}) \\\\ &amp;= \\Var(\\nu_g) + \\Var(\\varepsilon_{ig}) \\\\ &amp;= \\rho \\sigma^2 + (1 - \\rho) \\sigma^2 = \\sigma^2 \\] The Covariance between units in the same cluster is \\[ \\Cov(\\varepsilon_{ig}, \\varepsilon_{ig}) = \\rho \\sigma^2, \\] meaning that the correlation for units within a group is \\[ \\Cor(\\varepsilon_{ig}, \\varepsilon_{ig}) = rho . \\] But, there is zero covariance and correlation between units in different clusters. For example, the covariance matrix of \\[ \\vec\\varepsilon = \\begin{bmatrix} \\varepsilon_{1,1} &amp;  \\varepsilon_{2,1} &amp; \\varepsilon_{3,1} &amp; \\varepsilon_{4,2} &amp; \\varepsilon_{5,2} \\end{bmatrix}&#39; \\] is \\[ \\Var(\\vec\\varepsilon | \\mat{X}) = \\mat{\\Sigma} = \\begin{bmatrix} \\sigma^2 &amp; \\sigma \\rho &amp; \\sigma \\rho &amp; 0 &amp; 0 \\\\ \\sigma \\rho &amp; \\sigma^2 &amp; \\sigma \\rho &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\] More generally, the variance-covariance matrix of a the errors is block diagonal, \\[ \\Var(\\varepsilon | \\mat{X}) = \\mat{\\Sigma} = \\begin{bmatrix} \\mat{\\Sigma}_1 &amp; \\mat{0}_{N_1 \\times N_2} &amp; \\cdots &amp; \\mat{0}_{N_1 \\times N_G} \\\\ \\mat{0}_{N_2 \\times N_1} &amp; \\mat{\\Sigma}_2 &amp; \\cdots &amp; \\mat{0}_{N_2 \\times N_G} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mat{0}_{N_G \\times N_1} &amp; \\mat{0}_{N_G \\times N_2} &amp; \\cdots  &amp; \\mat{\\Sigma}_G \\end{bmatrix} \\] where \\(\\mat{Sigma}_g\\) are the covariance matrices of each cluster, and \\(\\mat{0}\\) are matrices of zeros of the appropriate sizes. There are several ways to address clustering, including:  Include an indicator variable for each cluster Random effects models Cluster-robust (“clustered”) standard error Aggregate data to the cluster data and use WLS with \\(\\bar{y}_g = \\frac{1}{N}_g \\sum_i y_{ig}\\) where the clusters are weighted by \\(N_g\\).  Cluster-robust standard errors uses the observed residuals, \\(\\hat\\varepsilon_i\\), to estimate a the variance-covariance matrix \\(\\hat\\Var{\\hat{\\vec{beta}}\\) which allows units to be independent across clusters and dependent within clusters.  CRSE do not change \\(\\hat\\vec{\\beta}\\). Thus they do not fix bias in the coefficients. CRSE is a consistent estiamtor of \\(\\Var{\\hat\\vec{\\beta}}\\) given the clustered dependence.  This relies on the assumption of independence between clusters Does not rely on the model form CRSE are usually larger than classic standard errors  Consistency of the CRSE are in the number of groups, not the number of individuals  CRSE work well when the number of clusters is large (&gt; 50) Alternative: use a block bootstrap   See the R package plr (Panel linear models in R). See Cameron and Miller, Practioner’s Guide to Cluster-Robust Inference.   3.6.7 Auto-correlation More general case allows for heteroskedasticity, and autocorrelation (\\(\\Cov(\\varepsilon_i, \\varepsilon_j) \\neq 0\\)), \\[ \\mat{\\Sigma} = \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{1,2} &amp; \\cdots &amp; \\sigma_{1,N} \\\\ \\sigma_{2,1} &amp; \\sigma_2^2 &amp; \\cdots &amp; \\sigma_{2,N} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{N,1} &amp; \\sigma_{N,2} &amp; \\cdots &amp; \\sigma_N^2 \\end{bmatrix} \\] As with heteroskedasticity, OLS with be unbiased, but the standard errors will be incorrect. Tests  Breusch-Godfrey Test (lmtest::bgtest)  Solution  If the form is known: Prais-Wiston, include lagged dependent variable. Huber-White Heteroskedasticity and Autocorrelation Robust standard errors. These are an extension of the heteroskedasticity robust standard errors to also include autocorrelation. See sandwich function hcacVCOV.    3.6.8 Clustered and Panel Standard Errors Panel Corrected Standard See the R package plm    3.7 Non-Normal Errors Non-normal errors pose several problems for OLS:  If the sample size is small, normal errors are required for correct confidence interval coverage and p-values in tests. In large samples, CLT properites of OLS kick in and the normality of errors assumption is not needed to justify the sampling distributions of the test statistics. Heavy-tailed errors threaten the efficiency of OLS estimate. Skewed or multimodal errors suggest that the conditional mean \\(E(Y | \\mat{X})\\), estimated by OLS may not be a good summay of the data.  How to diagnose? Plot the quantiles of the studentized residuals (see the section on Outliers) against the expected quantiles of a normal distribution using a QQ-plot. In general, non-normal errors are a minor issue, and towards the bottom in priority of problems in inference. How to fix? There are a couple of ways to fix this:  Transform the dependendt variable and use OLS Add different sets of covariates. This is especially likely with multimodal error distributions, which could suggest an omitted categorical variable. Use a different model other than OLS.  R Calculate studentized residuals with the function rstudent and a QQ-plot using stat_qq in ggplot. Diagnostics  QQ-plot of the studentized residuals  Important things to remember  The assumption is not that \\(Y\\) has a normal distribution, it is that the errors after including covariates are normal. While non-normal errors will not bias \\(\\beta\\) and have little effect on the standard errors unless the sample size is small, they could serve as a warning that your model is mis-specified, or that the conditional expectation of \\(Y\\) is not good summary.    "]
]
