[
["index.html", "POLS 503: Advanced Quantitative Political Methodology: The Notes Chapter 1 Introduction", " POLS 503: Advanced Quantitative Political Methodology: The Notes Jeffrey B. Arnold 2016-04-03   Chapter 1 Introduction hello, world!  "],
["review-of-statistics.html", "Chapter 2 Review of Statistics 2.1 Terms 2.2 Estimators and Estimates 2.3 Example", " Chapter 2 Review of Statistics Before getting started it is worth recalling a few concepts and terms from statistics.  2.1 Terms  2.1.1 Statistic A statistic is a function of sample data. Let \\(x = (x_1, \\dots, x_N)\\) be our sample data. Examples of statistics include:  mean \\(\\mean(x) = \\frac{1}{N} \\sum_{i = 1}^{N} x_i\\)  variance \\(\\Var(x) = \\frac{1}{n - 1} \\sum_{i = 1}^{N} (x_i - mean(x))^2\\)   Statistics do not need to be single numbers. The sample data \\(x\\) is a statistic of itself,1 albeit one of limited use.   2.1.2 Parameter A parameter is a function of the population. While the value of a statistic depends on a particular sample drawn from the population, the value of a parameter is fixed. Perhaps the most commonly used parameter is the population mean, \\(\\E(X)\\). Another example is the population variance, \\(\\Var(X) = \\E((X - E(X))^)\\). A single population can have many (infinite) parameters that we could consider, although in practice, inference usually is concerned with only a few of these, such as the mean or variance, or parameters by which a distribution is commonly parameterized. A parameter usually represents one particular aspect of the population, not all the features of the population; like a a statistic, it throws away information in doing so. And like a statistic, by throwing away this information, we will often be able to proceed While statistics are random variables, parameters are a fixed feature of the population. In statistical inference we use statistics of the sample to make inferences about parameters of the population.    2.2 Estimators and Estimates An estimator is a statistic (function of the sample data) used to estimate a population parameter. An estimate is the value (number) of an estimator for a specific sample. Many statistics can be used as an estimator for a given parameter, but not all of them will be good estimators. For example, the sample mean, sample median, sample variance, and always guessing 0 are all estimators of the population mean. However, we will prefer using the sample mean as the estimator of the population parameter, for reasons other than it having the sample name To understand why we prefer some estimators over others, we need to consider the sampling distribution of the statistic.  2.2.1 Sampling distribution Since a statistic is a function of the sample, it varies sample to sample. The distribution of the sample statistic over repeated samples from the sample population is called the sampling distribution.   2.2.2 Standard Error The standard deviation of a sampling distribution is called the standard error. The larger the standard error, the variable the statistic is acrros samples. All else equal, it is preferrable to have a statistic with a lower standard error, since it is less liable to vary wildly acrros samples. Since the standard error is a property of the population distribution from which the samples were drawn, this means that the standard error of a statistic is a parameter. But, then, what are the standard errors that are returned by statistical software? The standard errors that are calculated from a sample are an estimate of the population standard error. This means that in addition to the properties of the statistic of interest, we can, and often have to evaluate, different methods of calculating the standard error of the stampling distribution of that statistic.   2.2.3 Methods for evaluating statistics  Bias Variance Mean squared error Consistency Robustness / resilience     2.3 Example TODO   "],
["simple-linear-regression.html", "Chapter 3 Simple Linear Regression 3.1 Estimator 3.2 Properties of the Estimator 3.3 References", " Chapter 3 Simple Linear Regression Simple linear regression, also called bivariate linear regression is linear regression with one outcome variable and one covariate, \\[ Y = \\beta_0 + \\beta_1 X \\]  3.1 Estimator The ordinary least squares estimator estimates \\(\\beta_0\\) and \\(\\beta_1\\) from the data.   3.2 Properties of the Estimator  Unbiased Consistent Asymptotically normal Efficiency. An efficient estimator is the estimator with the lowest standard error. OLS is BLUE—the Best Linear Unbiased Estimator. This is proven by the Gauss-Markov Theorem. For the set of the class of linear, unbiased, estimators (with a few regularity conditions), OLS is the efficient estimator.    OLS Is BLUE (Best Linear Unbiased Estimator)    3.3 References  Tobias Funke meme http://memegenerator.net/instance2/5082792. Generated by Jeffrey Arnold. Hat-tip to Brenton Kenkel for the refence. http://bkenkel.com/psci8357/notes/02-reintroduction.pdf.           The function is the trivial identity function.↩  "]
]
