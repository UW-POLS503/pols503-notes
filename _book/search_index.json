[
["index.html", "POLS 503: Advanced Quantitative Political Methodology: The Notes Chapter 1 Introduction", " POLS 503: Advanced Quantitative Political Methodology: The Notes Jeffrey B. Arnold 2016-05-18   Chapter 1 Introduction Notes for POLS 503.  "],
["linear-regression-and-the-ordinary-least-squares-ols-estimator.html", "Chapter 2 Linear Regression and the Ordinary Least Squares (OLS) Estimator 2.1 Linear Regression Function 2.2 Ordinary Least Squares 2.3 Properties of the OLS Estimator 2.4 Multi-Collinearity 2.5 Weighted Least Squares 2.6 References", " Chapter 2 Linear Regression and the Ordinary Least Squares (OLS) Estimator Since we will largely be concerned with using linear regression for inference, we will start by discussion the population parameter of interest (population linear regression function), then the sample statistic (sample linear regression function) and estimator (ordinary least squares). We will then consider the properties of the OLS estimator.  2.1 Linear Regression Function The population linear regression function is \\[ r(x) = \\E[Y | X = x] = \\beta_0 + \\sum_{k = 1}^{K} \\beta_{k} x_k . \\] The population linear regression function is defined for random variables, and will be the object to be estimated. Names for \\(\\vec{y}\\)  dependent variable explained variable response variable predicted variable regressand outcome variable  Names for \\(\\mat{X}\\),  indpendent variables explanatory varaibles treatment and control variables predictor variables covariates regressors  To estimate the unkonwn population linear regression, we will use the sample linear regression function, \\[ \\hat{r}(x_i) = \\hat{y}_i = \\hat\\beta_0 + \\sum_{k = 1}^{K} \\hat\\beta_{k} x_k . \\] However, we \\(\\hat{Y}_i\\) are the fitted or predicted value The residuals or errors are the prediction errors of the estimates \\[ \\hat{\\epsilon}_i = y_i - \\hat{y}_i \\] \\(\\vec{\\beta}\\) are the parameters; \\(\\beta_0\\) is called the intercept, and \\(\\beta_{1}, \\dots, \\beta_{K}\\) are called the slope parameters, or coefficients. We will then consider the properties of the OLS estimator. The linear regression can be more compactly written in matrix form, \\[ \\begin{aligned}[t]   \\begin{bmatrix}     y_1 \\\\     y_2 \\\\     \\vdots \\\\     y_N   \\end{bmatrix} &amp;=   \\begin{bmatrix}     1 &amp; x_{1,1} &amp; x_{2,1} &amp; \\cdots &amp; x_{K,1} \\\\     1 &amp; x_{1,2} &amp; x_{2,2} &amp; \\cdots &amp; x_{K,2} \\\\     \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\     1 &amp; x_{1,N}&amp; x_{2,n} &amp; \\cdots &amp; x_{K,N}   \\end{bmatrix}   \\begin{bmatrix}     \\beta_0 \\\\     \\beta_1 \\\\     \\beta_2 \\\\     \\vdots \\\\     \\beta_K     \\end{bmatrix}   +   \\begin{bmatrix}     \\varepsilon_1 \\\\     \\varepsilon_2 \\\\     \\vdots \\\\     \\varepsilon_N   \\end{bmatrix} \\end{aligned} . \\] More compactly, the linear regression model can be written as, \\[ \\begin{aligned}[t]   \\underbrace{\\vec{y}}_{N \\times 1} &amp;=   \\underbrace{\\mat{X}}_{N \\times K} \\,\\,   \\underbrace{\\vec{\\beta}}_{K \\times 1} +   \\underbrace{\\vec{\\varepsilon}}_{N \\times 1} . \\end{aligned} \\] The matrix \\(\\mat{X}\\) is called the design matrix. Its rows are each observation in the data. Its columns are the intercept, a column vector of 1’s, and the values of each predictor.   2.2 Ordinary Least Squares Ordinary least squares (OLS) is an estimator of the slope and statistic of the regression line1. OLS finds values of the intercept and slope coefficients by minimizing the squared errors, \\[ \\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_K = \\argmin_{b_0, b_1, \\dots, b_k} \\sum_{i = 1}^{N}  \\underbrace{{\\left(y_i - b_0 - \\sum_{k = 1}^{K} b_k x_{i,k} \\right)}^2}_{\\text{squared error}}, \\] or, in matrix notation, \\[ \\begin{aligned}[t] \\hat{\\vec{\\beta}} &amp;= \\argmin_{\\vec{b}} \\sum_{i = 1}^N (y_i - \\vec{b}\\T \\vec{x}_i)^2 \\\\ &amp;= \\argmin_{\\vec{b}} \\sum_{i = 1}^N u_i^2 \\\\ &amp;= \\argmin_{\\vec{b}} \\vec{u}&#39; \\vec{u} \\end{aligned} \\] where \\(\\vec{u} = \\vec{y} - \\mat{X} \\vec{\\beta}\\). In most statistical models, including even generalized linear models such as logit, the solution to this minimization problem would be solved with optimization methods that require iteration. One nice feature of OLS is that there is a closed form solution for \\(\\hat{\\beta}\\) even in the multiple regression case, so no iterative optimization methods need to be used. In the bivariate regression case, the OLS estimators for \\(\\beta_0\\) and \\(\\beta_1\\) are \\[ \\begin{aligned}[t] \\hat{\\beta}_0 &amp;= \\bar{y} - \\hat\\beta_1 \\bar{x} \\\\ \\hat{\\beta}_1 7= \\frac{\\sum_{i = 1}^N (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum_{i = 1}^N (x_i - \\bar{x})^2} \\\\ &amp;= \\frac{\\Cov(\\vec{x} \\vec{y})}{\\Var{\\vec{x}}} &amp;= \\frac{\\text{Sample covariance betweeen $\\vec{x}$ and $\\vec{y}$}}{\\text{Sample variance of $\\vec{x}$}} . \\end{aligned} \\] In the multiple regression case, the OLS estimator for \\(\\hat{\\vec{\\beta}}\\) is \\[ \\hat{\\vec{\\beta}} = \\left( \\mat{X}&#39; \\mat{X} \\right)^{-1} \\mat{X}&#39; \\vec{y} . \\] The term \\(\\mat{X}&#39; \\mat{X}\\) is similar to the variance of \\(\\vec{x}\\) in the bivariate case. The term \\(\\mat{X}&#39; \\vec{y}\\) is similar to the covariance between \\(\\mat{X}\\) and \\(\\vec{y}\\) in the bivariate case. The sample linear regression function estimated by OLS has the following properties:  Residuals sum to zero, \\[ \\sum_{i = 1}^N \\hat{\\epsilon}_i = 0 . \\] This implies that the mean of residuals is also 0. The regression function passes through the point \\((\\bar{\\vec{y}}, \\bar{\\vec{x}}_1, \\dots, \\bar{\\vec{x}_K})\\). In other words, the following is always true, \\[ \\bar{\\vec{y}} = \\hat\\beta_0 + \\sum_{k = 1}^K \\hat\\beta_k \\bar{\\vec{x}}_k . \\] The residuals are uncorrelated with the predictor \\[ \\sum_{i = 1}^N x_i \\hat{\\epsilon}_i = 0 \\] The residuals are uncorrelated with the fitted values \\[ \\sum_{i = 1}^N \\hat{y}_i \\hat{\\varepsilon}_i = 0 \\]    2.3 Properties of the OLS Estimator  2.3.1 What makes an estimator good? Estimators are evaluated not on how close an estimate in a given sample is to the population, but how their sampling distributions compare to the population. In other words, judge the methodology (estimator), not the result (estimate).[^ols-properties-references] Let \\(\\theta\\) be the population parameter, and \\(\\hat\\theta\\) be an estimator of that population parameter.  Bias The bias of an estimator is the difference between the mean of its sampling distribution and the population parameter, \\[\\Bias(\\hat\\theta) = \\E(\\hat\\theta) - \\theta .\\]  Variance The variance of the estimator is the variance of its sampling distribution, \\(\\Var(\\theta)\\).  Efficiency (Mean squared error) An efficient estimator is one that minimizes a given “loss function”, which is a penalty for missing the population average. The most common loss function is squared loss, which gives the Mean Squared Error (MSE) of an estimator.  \\[\\MSE(\\hat\\theta) = \\E\\left[{(\\hat\\theta - \\theta)}^{2}\\right] =  (\\E(\\hat\\theta) - \\theta)^2 + \\E(\\hat\\theta - \\E(\\hat\\theta))^2 = \\Bias(\\hat\\theta)^2 + \\Var(\\hat\\theta)\\]  The mean squared error is a function of both the bias and variance of an estimator.  This means that some biased estimators can be more efficient : than unbiased estimators if their variance offsets their bias.2   Consistency is an asymptotic property3, that roughly states that an estimator converges to the truth as the number of observations grows, \\(\\E(\\hat\\theta - \\theta) \\to 0\\) as \\(N \\to \\infty\\). Roughly, this means that if you had enough (infinite) data, the estimator will give you the true value of the parameter.   2.3.2 Properties of OLS  When is OLS unbiased? When is OLS consistent? When is OLS efficient?          Assumption Formal statement Consequence of violation     No (perfect) collinearity \\(\\rank(\\mat{X}) = K, K &lt; N\\) Coefficients unidentified   \\(\\mat{X}\\) is exogenous \\(\\E(\\mat{X} \\vec{\\varepsilon}) = 0\\) Biased, even as \\(N \\to \\infty\\)   Disturbances have mean 0 \\(\\E(\\varepsilon) = 0\\) Biased, even as \\(N \\to \\infty\\)   No serial correlation \\(\\E(\\varepsilon_i \\varepsilon_j) = 0\\), \\(i \\neq j\\) Unbiased, wrong se   Homoskedastic errors \\(\\E(\\vec{\\varepsilon}\\T \\vec{\\varepsilon})\\) Unbiased, wrong se   Gaussian errors \\(\\varepsilon \\sim \\dnorm(0, \\sigma^2)\\) Unbiased, se wrong unless \\(N \\to \\infty\\)     Note that these assumptions can be sometimes be written in largely equivalent, but slightly different forms.    2.4 Multi-Collinearity  2.4.1 Perfect Collinearity In order to estimate unique \\(\\hat{\\beta}\\) OLS requires the that the columns of the design matrix \\(\\vec{X}\\) are linearly independent. Common examples of groups of variables that are not linearly independent  Categorical variables in which there is no excluded category. You can also include all categories of a categorical variable if you exclude the intercept. Note that although they are not (often) used in political science, there are other methods of transforming categorical variables to ensure the columns in the design matrix are independent. A constant variable. This can happen in practice with dichotomous variables of rare events; if you drop some observations for whatever reason, you may end up dropping all the 1’s in the data. So although the variable is not constant in the population, in your sample it is constant and cannot be included in the regression. A variable that is a multiple of another variable. E.g. you cannot include \\(\\log(\\text{GDP in millions USD})\\) and \\(\\log({GDP in USD})\\) since \\(\\log(\\text{GDP in millions USD}) = \\log({GDP in USD}) / 1,000,000\\). in A variable that is the sum of two other variables. E.g. you cannot include \\(\\log(population)\\), \\(\\log(GDP)\\), \\(\\log(GDP per capita)\\) in a regresion since \\(\\log(GDP per capita) = \\log(GDP / pop) = \\log(GDP) - \\log(pop)\\).   2.4.1.1 What to do about it? R and most statistical programs will drop variables from the regression until only linearly independent columns in \\(\\mat{X}\\) remain. You should not rely on the softward to fix this for you; once you (or the software) notices the problem check the reasons it occured. The rewrite your regression to remove whatever was creating linearly dependent variables in \\(\\mat{X}\\).    2.4.2 Less-than Perfect Collinearity What happens if variables are not linearly dependent, but nevertheless highly correlated. If \\(\\Cor(\\vec{x}_1, vec{x}_2) = 1\\), then they are linearly dependent and the regression cannot be estimated (see above). But if \\(\\Cor(\\vec{x}_1, vec{x}_2) = 0.99\\), the OLS can estimate unique values of of \\(\\hat\\beta\\). However, it everything was fine with OLS estimates until, suddenly, when there is linearly independence everything breaks. The answer is yes, and no. As \\(|\\Cor(\\vec{x}_1, \\vec{x}_2)| \\to 1\\) the standard errors on the coefficients of these variables increase, but OLS as an estimator works correctly; \\(\\hat\\beta\\) and \\(\\se{\\hat\\beta}\\) are unbiased. With multicollinearly, OLS gives you the “right” answer, but it cannot say much with certainty. Insert plot of highly correlated variables and their coefficients. Insert plot of uncorrelated variables and their coefficients.    2.5 Weighted Least Squares In weighted least squares (WLS), instead of minimizing the sum of squared errors, minimize a weighted sum of squared errors, \\[ \\hat{\\vec{\\beta}}_{WLS} = \\argmin_{\\vec{b}} \\sum_{i = 1}^N w_i {(y_i - \\vec{\\beta}&#39; \\vec{x}_i)}^2 \\] OLS is the special case of WLS in which all observations are weighted equally. What reasons are there to use WLS?  Heteroskedasticity: The observations have different levels of precision. This works well if the form of the heteroskedasticity is known, perhaps measurement error in a meta-analysis. Feasible GLS is when errors from OLS are used as weights in WLS: there are better ways to do this. Aggregation: The observations represent the sizes of different groups, or the probability of being selected into the sample. E.g. weighting countries or states by their population.  How to run WLS in R? Use lm with the weights argument.   2.6 References  Wooldrige, Ch 3. Fox, Ch 6, 9.     "],
["properties-of-the-ols-estimator-1.html", "Chapter 3 Properties of the OLS Estimator 3.1 What makes an estimator good? 3.2 References 3.3 Sampling Distribution of OLS Coefficients 3.4 t-tests for single parameters 3.5 F-tests of Multiple Hypotheses", " Chapter 3 Properties of the OLS Estimator  3.1 What makes an estimator good?  A mathematician, a physicist and a statistician went hunting for deer. When they chanced upon one buck lounging about, the mathematician fired first, missing the buck’s nose by a few inches. The physicist then tried his hand, and missed the tail by a wee bit. The statistician started jumping up and down saying “We got him! We got him!”  Estimators are evaluated not on how close an estimate in a given sample is to the population, but how their sampling distributions compare to the population. In other words, judge the methodology (estimator), not the result (estimate).[^ols-properties-references] Let \\(\\theta\\) be the population parameter, and \\(\\hat\\theta\\) be an estimator of that population parameter.  Bias The bias of an estimator is the difference between the mean of its sampling distribution and the population parameter, \\[\\Bias(\\hat\\theta) = \\E(\\hat\\theta) - \\theta .\\]  Variance The variance of the estimator is the variance of its sampling distribution, \\(\\Var(\\theta)\\).  Efficiency (Mean squared error) An efficient estimator is one that minimizes a given “loss function”, which is a penalty for missing the population average. The most common loss function is squared loss, which gives the Mean Squared Error (MSE) of an estimator.  \\[\\MSE(\\hat\\theta) = \\E\\left[{(\\hat\\theta - \\theta)}^{2}\\right] =  (\\E(\\hat\\theta) - \\theta)^2 + \\E(\\hat\\theta - \\E(\\hat\\theta))^2 = \\Bias(\\hat\\theta)^2 + \\Var(\\hat\\theta)\\]  The mean squared error is a function of both the bias and variance of an estimator.  This means that some biased estimators can be more efficient : than unbiased estimators if their variance offsets their bias.4   Consistency is an asymptotic property5, that roughly states that an estimator converges to the truth as the number of observations grows, \\(\\E(\\hat\\theta - \\theta) \\to 0\\) as \\(N \\to \\infty\\). Roughly, this means that if you had enough (infinite) data, the estimator will give you the true value of the parameter.  3.1.1 Properties of OLS         Assumption Formal statement Consequence of violation     No (perfect) collinearity \\(\\rank(\\mat{X}) = K, K &lt; N\\) Coefficients unidentified   \\(\\mat{X}\\) is exogenous \\(\\E(\\mat{X} \\vec{\\varepsilon}) = 0\\) Biased, even as \\(N \\to \\infty\\)   Disturbances have mean 0 \\(\\E(\\varepsilon) = 0\\) Biased, even as \\(N \\to \\infty\\)   No serial correlation \\(\\E(\\varepsilon_i \\varepsilon_j) = 0\\), \\(i \\neq j\\) Unbiased, wrong se   Homoskedastic errors \\(\\E(\\vec{\\varepsilon}\\T \\vec{\\varepsilon})\\) Unbiased, wrong se   Gaussian errors \\(\\varepsilon \\sim \\dnorm(0, \\sigma^2)\\) Unbiased, se wrong unless \\(N \\to \\infty\\)     Note that these assumptions can be sometimes be written in largely equivalent, but slightly different forms. When is a variable endogenous  Omitted variables Measurement error Simultaneity  Assumptions of CLR models  No perfect collinearity: No exact linear relationships in the predictors. \\(X\\) is full rank. Linearity: Outcome variable is a linear function of a specific set of independent variables and a disturbance: \\[\\vec{y} = \\mat{X} \\vec{\\beta} + \\vec{\\varepsilon}\\]. Observations on independent samples can be considered fixed in repeated samples or \\(X\\) is uncorrelated with the errors. Expected value of the disturbance term is zero. Homoskedasticity: Disturbances have the same variance and are uncorrelated: \\(\\var(\\varepsilon_i) = \\sigma^2\\), \\(\\cov(\\varepsilon_i, \\varepsilon_j) = 0\\) for all \\(i \\neq j\\). Error terms are distributed normal.   OLS solution exists with unique \\(\\beta\\): 1 OLS is unbiased and consistent: 1-4 OLS is best-linear unbiased estimator (BLUE) Gauss-Markov. Large scale inference. 1-5. OLS small scale inference: 1-6. Best unbiased estimator (not just among linear)  Why OLS?  Computational cost: There exists a closed form solution to the OLS estimate and standard errors. Least squares loss: OLS minimizes least squared residuals, and thus is optimal for this criteria. Note, that this is only for within sample. Hightest R^2: Follows from the previous. Unbiased: Best unbiased: Mean squared error: OLS is not the minimum MSE model. Asymptotic criteria: Asymptotically unbiased and consitent. Maximum likelihood: OLS is equivalent to the MLE estimator for \\(\\beta\\).     3.2 References  Wooldrige, Ch 3. Fox, Ch 6, 9.    3.3 Sampling Distribution of OLS Coefficients   3.4 t-tests for single parameters The sampling distribution of the OLS parameters is \\[ \\vec{\\beta} \\sim \\dmvnorm(\\vec{beta}, \\sigma^2 (\\mat{X}&#39; \\mat{X})^{-1}). \\] Thus, the variance of the coefficients is \\[ \\Var(\\hat{\\beta}) = \\sigma^2 (\\mat{X}&#39; \\mat{X})^{-1} . \\] which is a symmetric matrix, \\[ \\Var(\\hat{\\beta}) = \\begin{bmatrix} \\Var(\\hat{\\beta}_0) &amp; \\Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) &amp; \\Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) &amp; \\cdots &amp; \\Cov(\\hat{\\beta}_0, \\hat{\\beta}_K) \\\\ \\Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) &amp; \\Var(\\hat{\\beta}_1) &amp; \\Cov(\\hat{\\beta}_1, \\hat{\\beta}_2) &amp; \\cdots &amp; \\Cov(\\hat{\\beta}_1, \\hat{\\beta}_K) \\\\ \\Cov(\\hat{\\beta}_0, \\hat{\\beta}_2) &amp; \\Cov(\\hat{\\beta}_1, \\hat{\\beta}_2) &amp; \\Cov(\\hat{\\beta}_2) &amp; \\cdots &amp; \\Cov(\\hat{\\beta}_2, \\hat{\\beta}_K) \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\Cov(\\hat{\\beta}_0, \\hat{\\beta}_K) &amp; \\Cov(\\hat{\\beta}_1, \\hat{\\beta}_K) &amp; \\Cov(\\hat{\\beta}_K) &amp; \\cdots &amp; \\Var( \\hat{\\beta}_k) \\end{bmatrix} \\] On the diagonal are the variances of the parameters, and the off-diagonal elements are the covariances of the parameters. The null hypothesis and alternative hypotheses for two-sided tests are, \\[ \\begin{aligned}[t] H_0: &amp;\\beta_k = \\beta_0 \\\\ H_a: &amp;\\beta_k \\neq \\beta_0 \\end{aligned} \\] Then in large samples, \\[ \\frac{\\hat{\\beta}_k - \\beta_k}{\\se(\\widehat{\\beta}_k)} \\sim \\dnorm(0, 1) \\] In small samples, \\[ \\frac{\\hat{\\beta}_k - \\beta_k}{\\se(\\widehat{\\beta}_k)} \\sim \\dt{N - (K + 1)} \\] The estimated standard errors of \\(\\hat{\\beta}\\) come from \\[ \\begin{aligned}[t] \\var(\\hat{\\vec{\\beta}}) &amp;= \\hat{\\sigma}^2 (\\mat{X}&#39; \\mat{X})^{-1} \\\\ \\hat{\\sigma}^2 &amp;= \\frac{\\vec{\\epsilon}&#39;\\vec{epsilon}}{(N - (K + 1))} \\end{aligned} \\] So, under the common null hypothesis test for \\(\\beta_k = 0\\), \\[ \\frac{\\hat{\\beta}_k}{\\se(\\widehat{\\beta}_k)} \\sim \\dt{N - (K + 1)} \\] And the confidence intervals for a \\((1 - \\alpha) \\times 100\\) confidence interval for \\(\\hat{\\beta}_k\\) are, \\[ \\hat{\\beta}_k \\pm t^*_{\\alpha / 2} \\times \\se(\\hat{\\beta}_K) \\] where \\(t^*_{\\alpha / 2}\\) is the quantile of the \\(\\dt_{n - (K + 1)}\\) distribution such that \\(P(T \\leq t^*) &gt; 1 - \\alpha / 2\\).   3.5 F-tests of Multiple Hypotheses   "],
["omitted-variable-bias-and-measurement-error.html", "Chapter 4 Omitted Variable Bias and Measurement Error 4.1 Omitted Variable Bias 4.2 Measurement Error", " Chapter 4 Omitted Variable Bias and Measurement Error  4.1 Omitted Variable Bias          \\(\\Cov(X_1, X_2)\\) \\(\\Cov(X_2, Y) &gt; 0\\) \\(\\Cov(X_2, Y) = 0\\) \\(\\Cov(X_2, Y) &lt; 0\\)     \\(&gt; 0\\) \\(0\\) \\(&lt; 0\\) + 0 - 0 0 0 - 0 +     4.1.1 What’s the problem?   4.1.2 What to do about it? Summary:  OVB is intrinsic to observational methods relying on selection on observables—not just regression. Control for all plausible “pre-treatment” variables Reason about possible biases due to OVB Sensitivity of coefficients to inclusion of control variables is an indication of the plausibility of OVB. Altonji, Elder, and Taber (2005). formalize this.  In practice, this is a primary problem of many papers and papers; and for good reason, it biases the coefficient of interest. Reviewers and discussants will often ask about whether you have considered controlling for foo. Although these may be legitimate concerns, not all commenters understand the purpose of control variables. There two arguments to consider when addressing these arguments.  The omitted variable has to plausibly be correlated with both the variable of interest and the outcome variable, and the burden is on the commenter to provide at a confounding variable and plausible relationships. Simpy stating that there could be an unobservable variable is trivially true, uninteresting, and not a fatal critique. That said, the evidentiary content of your methods would be higher if you used methods less susceptible to potential unobserved confounders. The omitted variable should be a good control and not a “post treatment” variable. If the omitted variable should not be one of the causal pathways by which \\(X\\) affects \\(Y\\), it should not be controlled for. If \\(Z\\) affects the values of \\(X\\) and also affects \\(Y\\), then it needs to be controlled for.  There are two common ways of assessing plausibility.  Informal method. This is what you see in many empirical papers. Estimate the model including different control variables. The less sensitive the coefficient(s) of the variables of interest are to the inclusion of control variables, the more plausible it is that the variable of interest is not sensitive to unobserved variables (Angrist and Pischke 2014). Oster (2013) states  A common heuristic for evaluating the robustness of a result to omitted variable bias concerns is to look at the sensitivity of the treatment effect to inclusion of observed controls. In three top general interest economics journals in 2012, 75% of non-experimental empirical papers included such sensitivity analysis. The intuitive appeal of this approach lies in the idea that the bias arising from the observed controls is informative about the bias that arises from the unobserved ones.  Note that what is important is that the coefficient is stable to the inclusion of controls, not that the coefficient remains statistically significant (which seems to be what many authors focus on). Formal method Several papers, including Altonji, Elder, and Taber (2005), Bellows and Miguel (2009), and Oster (2013), formalize the intuition behind the heuristic of coefficient stability to assess the sensitivity of the treatment to OVB.  Altonji, Elder, and Taber (2005) propose a method for assessing the potential impact of the omitted variable bias as the importance of the omitted variable needed to explain away the entire effect. That work addresses the case for a dichotomous treatment variable, and assumed joint normality. Bellows and Miguel (2009) extend the method to continuous treatment variables. The statistic proposed by Bellows and Miguel (2009) is simple, \\[ \\delta = \\frac{\\hat{\\beta}_F}{\\hat{\\beta}_R - \\hat{\\beta}_C}, \\] where \\(\\delta\\) is \\[ \\Cov{x, \\tilde{w}}{x, w&#39; \\gamma} \\] \\(\\delta\\) is interpreted as the how strong the covariance between the unobserved part of the controls and the treatment variable must be relative to the covariance between the observed part of the controls and the treatment variable to explain away the entire effect of \\(x\\) on \\(y\\). A larger ratio suggests it is implausible that omitted variable bias could explain away the entire observed effect. Suppose we would like to estimate \\[ y = \\beta x + \\gamma z \\] If \\(z\\) is left out of the OLS estimation, then the estimates of \\(\\beta\\) will have omitted variable bias, \\[ \\plim \\hat\\beta_{OLS,NC} = \\beta + \\gamma \\frac{\\Cov(x, z)}{\\Var{x}} \\] Suppose that instead of \\(z\\) we observe a set of controls \\(w^*\\) that are related to the full set of controls, \\[ z = w&#39; \\beta + \\omega \\] See Appendix A of Bellows and Miguel (2009) for the derivation. TODO Insert path diagram. OVB is a intrinsic problem in observational research, and there is nothing you can do to ever ensure that you have controlled for all relevant variables (however, all inference is uncertain, even the designs discussed next, so people should learn to deal with uncertainty). Also, methods such as matching, propensity scores, or inverse weighting still depend on assumptions about selection on observables, even if they may be less sensitive to certain kinds of modeling assumptions. The alternative is to use designs which do not require directly controlling for observable differences. Examples of these designs include: experiments (obviously), natural experiments, instrumental variables, and regression discontinuity.    4.2 Measurement Error  4.2.1 What’s the problem? It biases coefficients:  Variable with measurement error: biases \\(\\beta\\) towards zero (attenuation bias) Other variables: Biases \\(\\beta\\) similarly to omitted variable bias. In other words, when a variable has measurement error it is an imperfect control. You can think of omitted variables as the limit of the effect of measurement error as it increases.    4.2.2 What to do about it? There’s no easy fix within the OLS framework.  If the measurement error is in the variable of interest, then the variable will be biased towards zero, and your estimate is too large. Find better measures with lower measurement errors. If the variable is the variable of interest, then perhaps combine multiple variables into a single index. If the measurement error is in the control variables, then include several measures. That these measure correlate closely increases their standard errors, but the control variables are not the object of the inferential analysis. More complicated methods: errors in variable models, structural equation models, instrumental variable (IV) models, and Bayesian methods.     "],
["multi-collinearity-1.html", "Chapter 5 Multi-Collinearity", " Chapter 5 Multi-Collinearity  5.0.1 Perfect Collinearity In order to estimate unique \\(\\hat{\\beta}\\) OLS requires the that the columns of the design matrix \\(\\vec{X}\\) are linearly independent. Common examples of groups of variables that are not linearly independent:  Categorical variables in which there is no excluded category. You can also include all categories of a categorical variable if you exclude the intercept. Note that although they are not (often) used in political science, there are other methods of transforming categorical variables to ensure the columns in the design matrix are independent. A constant variable. This can happen in practice with dichotomous variables of rare events; if you drop some observations for whatever reason, you may end up dropping all the 1’s in the data. So although the variable is not constant in the population, in your sample it is constant and cannot be included in the regression. A variable that is a multiple of another variable. E.g. you cannot include \\(\\log(\\text{GDP in millions USD})\\) and \\(\\log({GDP in USD})\\) since \\(\\log(\\text{GDP in millions USD}) = \\log({GDP in USD}) / 1,000,000\\). in A variable that is the sum of two other variables. E.g. you cannot include \\(\\log(population)\\), \\(\\log(GDP)\\), \\(\\log(GDP per capita)\\) in a regression since \\[\\log(\\text{GDP per capita}) = \\log(\\text{GDP} / \\text{population}) = \\log(\\text{GDP}) - \\log(\\text{population})\\].   5.0.1.1 What to do about it? R and most statistical programs will run regressions with collinear variables, but will drop variables until only linearly independent columns in \\(\\mat{X}\\) remain. For example, consider the following code. The variable type is a categorical variable with categories “bc”, “wc”, and “prof”. It will data(Duncan, package = &quot;car&quot;) # Create dummy variables for each category Duncan &lt;- mutate(Duncan,                  bc = type == &quot;bc&quot;,                  wc = type == &quot;wc&quot;,                  prof = type == &quot;prof&quot;) lm(prestige ~ bc + wc + prof, data = Duncan) ##  ## Call: ## lm(formula = prestige ~ bc + wc + prof, data = Duncan) ##  ## Coefficients: ## (Intercept)       bcTRUE       wcTRUE     profTRUE   ##       80.44       -57.68       -43.78           NA R runs the regression, but coefficient and standard errors for prof are set to NA. You should not rely on the software to fix this for you; once you (or the software) notices the problem check the reasons it occurred. The rewrite your regression to remove whatever was creating linearly dependent variables in \\(\\mat{X}\\).    5.0.2 Less-than Perfect Collinearity What happens if variables are not linearly dependent, but nevertheless highly correlated? If \\(\\Cor(\\vec{x}_1, vec{x}_2) = 1\\), then they are linearly dependent and the regression cannot be estimated (see above). But if \\(\\Cor(\\vec{x}_1, vec{x}_2) = 0.99\\), the OLS can estimate unique values of of \\(\\hat\\beta\\). However, it everything was fine with OLS estimates until, suddenly, when there is linearly independence everything breaks. The answer is yes, and no. As \\(|\\Cor(\\vec{x}_1, \\vec{x}_2)| \\to 1\\) the standard errors on the coefficients of these variables increase, but OLS as an estimator works correctly; \\(\\hat\\beta\\) and \\(\\se{\\hat\\beta}\\) are unbiased. With multicollinearly, OLS gives you the “right” answer, but it cannot say much with certainty. Insert plot of highly correlated variables and their coefficients. Insert plot of uncorrelated variables and their coefficients.  5.0.2.1 What to do about it? Remember multicollinearity does not violate the assumptions of OLS. If all the other assumptions hold, then OLS is giving you unbiased coefficients and standard errors. What multicollinearity is indicating is that you may not be able to answer the question with the precision you would like.  If the variable(s) of interest are highly correlated with other variables, then it means that there is not enough variation, controlling for other factors. You may check that you are not controlling for “post-treatment” variables. Dropping control variables if they are correctly included will bias your estimates. But otherwise, there is little you can do other than get more data. You could re-consider your research design and question. What does it mean if there is that little variation in the treatment variable after controlling for other factors? If control variables are highly correlated with each other, it does not matter. You should not be interpreting their coefficients, so their standard errors do not matter. In fact, controlling for several similar, but correlated variables, may be useful in order to offset measurement error in any one of them.     "],
["functional-form-and-non-linearity.html", "Chapter 6 Functional Form and Non-linearity 6.1 Non-linearity 6.2 Logarithm 6.3 Miscellaneous 6.4 Polynomials 6.5 Interactions 6.6 Flexible Functional Forms 6.7 References 6.8 Non-constant Variances and Correlated Errors 6.9 Non-Normal Errors", " Chapter 6 Functional Form and Non-linearity  6.1 Non-linearity  6.1.1 What’s the problem? If the relationship between the regression surfacne and \\(E(Y | X)\\) is not captured well, then the results of the regression may be misleading, although this depends on the modeling approach regression is being used for. The extent of the problem varies with which variables are affected, and the purpose of the analysis.  If the analysis is interested in the average marginal effect of the treatment variable, then using the OLS coefficient to estimate the AME is not a bad approximation. The values of the individual marginal effects will be incorrect, but the average should be a reasonable approximation. If you are interested in the AME of sub-populations or other estimands, then you will need to account for the non-linearity. If the non-linearity is in the control variables, then it is another form of omitted variable bias.    6.1.2 What to do about it? And How to Solve it? The general approaches to identifying non-linearity include:  Residual plots with curvature tests: car function residualPlots. Added-variable (AV) plot: car function avPlots. Component+residual (CERES) plot: car functions crPlots and ceresPlots. Ramsay RESET test. lmtest function resettest Compare Robust SE and classical OLS SE. King and Roberts.  In general, I think most of these approaches are time consuming, sub-optimal given new methods and computation, and open up the regression model to too many researcher degrees of freedom that will not be represented in the uncertainty of the model There now exist many models (notably semi-parametric and non-parametric) which allow for more flexible functional forms with less-model dependence. Some of these include:  GAM and spline models K-nearest neighbor models Matching methods LASSO, Ridge and other Shrinkage Regression (especially with basis functions)     6.2 Logarithm  6.2.1 Examples of Relevant Theories  Converts multiplicative theories to additive theories. Theories with diminishing returns to scale. Theories about percentage changes or growth. Most uses of (per capita) GDP, population: Cobb-Douglas growth \\[   y = \\alpha (K^(\\delta) L^(1 - \\delta))^{\\nu}   \\] Linearized, \\[   \\log y = \\log \\alpha + \\log k   \\] Gravity trade equation Lanchester law for casualties \\[   \\Delta x = \\alpha x^\\beta y^\\gamma   \\] where \\(\\Delta x\\) are casualties per period, \\(x\\) is the initial size of forces forces, and \\(y\\) are opposing forces. This can be linearized and estimated with OLS, \\[   \\log \\Delta x = \\log \\alpha + \\beta \\log x + \\gamma \\log y   \\] as long as \\(x, y &gt; 0\\) (and preferrably large).     6.3 Miscellaneous  6.3.1 Square Root and Variance Stabalizing Transformations   6.3.2 Power-Transformation    6.4 Polynomials  6.4.1 Squared  6.4.1.1 What theories?  Kuznets curve: economic development and inequality Environmental Kuznets curve: environmental quality and economic development Democratic Civil Peace: intermediate regimes prone to civil war, democracies and autocracies are less prone to civil war.     6.4.2 Higher-Order Polynomials  Time cubed Seat-Vote curves? Other old examples in Tufte 1975 These are generally     6.5 Interactions Standard errors are more difficult to calculate. See Golder’s page, and Aiken and West (1991).  6.5.1 Theories Golder et. al recommend that for simple interaction model such as: \\[ \\vec{y} = \\beta_0 + \\beta_x \\vec{x} + \\beta_z \\vec{z} + \\beta_{xz} \\vec{x} \\vec{z} + \\vec{varepsilon} \\] the reseearcher make as many of the following predictions as possible  The marginal effect of \\(X\\) is (positive, negative, zero) when \\(Z\\) is at its lowest level. … when \\(Z\\) is at its highest level. The marginal effect of \\(Z\\) is (positive, negative, zero) when \\(X\\) is at its lowest level. … when \\(X\\) is at its highest level. The marginal effect of each \\(X\\) and \\(Z\\) is (positively, negatively) related to the other variable    6.5.2 Recommendations Golder et al recommend  Use multiplicative interaction models for conditional hypotheses. Include all constituent terms of the interaction in the model. Do not interpret coefficients on terms seperately, or as if they are unconditional marginal effects. Calculate substantively meaningful marginal effects and their standard errors.    6.5.3 Plots Golder et al recommend:  Construct marginal effect plots for both X and Z. The range of the horizontal axis should extend from the minimum to the maximum value of variable in the sample. The plot should include a frequency distribution of the variable of interest, as either a rug plot, histogram, or density. Report the product term coefficient and its t-statistic or standard error.     6.6 Flexible Functional Forms  Splines GAM Gaussian Processes Random Forests Neural Networks    6.7 References  Matt Golder Interactions Golder’s papers     6.8 Non-constant Variances and Correlated Errors The OLS coefficient standard errors, \\[ \\Var({\\hat{\\vec{\\beta}}}) = \\sigma^2 (\\mat{X}\\T \\mat{X})^{-1} \\] depends on the assumption of homoskedastic errors. Homoskedasticity has two components,  Disturbances have the same variance, \\(\\Var(\\varepsilon_i) = \\sigma^2\\) for all \\(i\\). No correlation between disturbances, \\(\\Cov(\\varepsilon_i, \\varepsilon_j) = 0\\) for all \\(i \\neq j\\).  Either or both of these components can be violated, and when they are, the standard errors of the OLS estimator are incorrect. The general OLS variance-covariance matrix of the coefficients is, \\[ \\Var(\\hat{\\vec\\beta}) = (\\mat{X}\\T \\mat{X})^{-1} (\\mat{X} \\Sigma \\mat{X}) (\\mat{X}\\T \\mat{X})^{-1} \\] where \\(\\mat{Sigma}\\) is the correlation of the disturbances, \\(\\vec\\varepsilon\\), \\[ \\mat{\\Sigma} = \\vec{\\varepsilon}\\T \\vec{\\varepsilon} = \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_1 \\sigma_2 &amp; \\cdots &amp; \\sigma_1 \\sigma_N \\\\ \\sigma_2 \\sigma_1 &amp; \\sigma_2^2 &amp; \\cdots &amp; \\sigma_2 \\sigma_N \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_N \\sigma_1 &amp; \\sigma_N \\sigma_2 &amp; \\cdots &amp; \\sigma_N^2 \\\\ \\end{bmatrix} \\] When we assume homoskedasticity, the variance-covariance matrix of \\(\\varepsilon\\) is \\[ \\mat{\\Sigma} = \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma^2 \\end{bmatrix} = \\sigma^2 \\begin{bmatrix} 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{bmatrix} =  \\sigma^2 \\mat{I}_{N} \\], Under homoskedasticity, the sampling distribution of \\[ \\begin{aligned}[t] \\Var(\\beta | \\mat{X}) &amp;= (\\mat{X}&#39; \\mat{X})^{-1} \\mat{X}&#39; \\mat{\\Sigma} \\mat{X} (\\mat{X}&#39; \\mat{X})^{-1} \\\\ &amp;= (\\mat{X}&#39; \\mat{X})^{-1} \\mat{X}&#39; \\sigma^2 \\mat{I}_N \\mat{X} (\\mat{X} &#39;\\mat{X})^{-1} \\\\ &amp;= \\sigma^2 (\\mat{X}&#39; \\mat{X})^{-1} \\mat{X}&#39; \\mat{X} (\\mat{X} &#39;\\mat{X})^{-1} \\\\ &amp;= \\sigma^2 (\\mat{X}&#39; \\mat{X})^{-1} \\end{aligned} \\] To estimate \\(\\Var(\\hat{\\vec\\beta}|\\mat{X})\\), replace \\(\\sigma^2\\) with \\(\\hat{\\sigma}^2\\), where \\[ \\hat{\\sigma}^2 = \\frac{1}{N - K - 1} \\sum \\varepsilon_i^2 \\] Q. What if the assumption of homoskedasticity isn’t true? A. The coefficients \\(\\hat{\\vec\\beta}\\) are unbiased, but the standard errors \\(\\hat{\\se}(\\hat{\\vec{\\beta}})\\) are biased. Since we don’t know \\(\\mat{\\Sigma}\\), why not simply estimate the elements of \\(\\mat{\\Sigma}\\) along with \\(\\vec{\\beta}\\)? The problem is that there are \\(N\\) observations, and \\(\\mat\\Sigma\\) is an \\(N \\times N\\) matrix with \\((N * (N + 1)) / 2\\) elements since it is symmetric. So some structure needs to be put on \\(\\mat{\\Sigma}\\), i.e. we need to make additional assumptions, to estimate \\(\\mat{\\Sigma}\\). As we will see, we can make less restrictive assumptions than homoskedasticity, i.e. \\(\\mat{\\Sigma} = \\sigma^2 \\mat{I}\\), but will always have to assume some sort of structure in \\(\\mat{\\Sigma}\\). There’s only one way for homoskedasticity to be correct (\\(\\mat{\\Sigma} = \\sigma^2 \\mat{I}\\)), and many ways for it to be wrong. We’ll consider a few of the most common, and methods to deal with them.  Heteroskedasticity Autocorrelation Clustering   6.8.1 Heteroskedasticity The homoskedastic case assumes that each error term has its own variance. In the heteroskedastic case, each disturbance may have its own variance, but they are still uncorrelated (\\(\\mat{\\Sigma}\\) is diagonal) \\[ \\mat{\\Sigma} = \\begin{bmatrix} \\sigma_1^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma_2^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma_N^2 \\end{bmatrix} = \\sigam^2 \\mat{I}_N \\] With homoskedasticity the estimator of the variance covariance matrix takes a particularly simple form, \\[ \\Var(\\hat{\\beta} | \\mat{X}) &amp;= (\\mat{X}&#39; \\mat{X})^{-1} \\mat{X}&#39; \\mat{\\Sigma} \\mat{X} (\\mat{X} &#39;\\mat{X})^{-1} \\\\ &amp;= \\hat\\sigma^2 (\\mat{X}&#39; \\mat{X})^{-1} \\] where \\[ \\hat\\sigma^2 = \\frac{\\sum \\hat{\\epsilon}^2}{N - K - 1} \\] The consequences of violating homoskedasticity are,  Biased (often downward) standard errors, \\(\\E(\\se{\\hat\\beta}) \\neq \\sd(\\beta)\\). Test statistics do hot have \\(t\\) or \\(F\\) distributions. \\(\\alpha\\)-level tests have the wrong level of Type I error. E.g. a 5% test will not have 5% Type I errors. Confidence intervals do not have the correct coverage. E.g. a 95% confidence does not contain the true mean in 95% of samples. OLS is not BLUE. \\(\\hat{\\vec\\beta}\\) is still and unbiased and consistent estimator for \\(\\vec\\beta\\).  So why don’t we estimate all the elements of \\(\\mat\\Sigma\\) like we estimate \\(\\beta\\)? Since \\(\\mat\\Sigma\\) is an \\(N \\times N\\) symmetric matrix, it has $(N (N + 1)) / 2 $ elements. But we have only \\(N\\) data points to estimate them. In order to estimate \\(\\mat\\Sigma\\) we cannot estimate arbitrary correlations in \\(\\mat\\Sigma\\), but we need to apply some structure to the variance-covariance matrix in order to reduce the number of elements to estimate.  Heteroskedasticity Clustered Standard Errors Serial Correlation  In general there are two types of methods to deal with issues in the error,  New estimators that model the error process and estimate elements of \\(\\mat\\Sigma\\) simultaneously with the coefficients \\(\\beta\\). This includes weighted least squares (heteroskedasticity), Prais-Wiston (AR(1) errors). These methods produce \\(\\hat\\beta \\neq \\hat\\beta_{OLS}\\). Since OLS produces unbiased and consistent estimates of \\(\\hat\\beta\\), we keep the coefficient estimates, but correct the variance-covariance matrix \\(\\hat\\Var{\\beta}\\).    6.8.2 Heteroskedasticity In heteroskedasticity, the errors are still independent, i.e. \\(\\mat\\Sigma\\) is still diagonal, but the variance elements on the diagonal are not equal, \\[ \\Var(\\vec\\varepsilon|\\mat{X}) = \\begin{bmatrix} \\sigma_1^2 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma_2^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; \\sigma_N^2 \\end{bmatrix} . \\] While each observation has a difference variance, they are still independent, \\(\\Cov(\\epsilon_i, \\epsilon_j | \\mat{X}) = 0\\), for all \\(i \\neq j\\).  Example in difference in means Example with a continuous \\(X\\) Simulation of the effects of violations    6.8.3 Diagnostics  Plot residuals vs. fitted values Spread-level plots (car::spreadLevel), Compare Robust SE vs. non-robust SE. If they are different Tests: Breusch-Pagan (lmtest::bptest, car::ncvTest),    6.8.4 Dealing with Heteroskedasticity  Transform the dependent variable. For example, \\(\\log\\) the dependent variable. Model heteroskedasticity using Weighted Least Squares (WLS) Use OLS with an estimator of \\(\\Var(\\hat{\\vec\\beta})\\) that is robust to heteroskedasticity Admit that OLS is insufficient, and use a different model   If the form of heteroskedasticity follows a particularly simple form, transform the dependent variable. For example, log the dependent variable. If the form of the heteroskedasticity is known: weighted least squares. lm() with the weights argument. If the form of the heteroskedasticity is unknown: Huber-White heteroskedasticity consistent standard errors. See sandwich package. You can calculate the heteroskedasticity correct covariance matrix using sandwich::vcovHC and then use lmtest::coeftest to calculate p-values and standard-errors.   6.8.4.1 Advice In practice, often diagnostics are not conducted and robust standard errors are used. This is partially due to the ease with which heteroskedasticity consistent standard errors can be calculated in Stata (see , robust). Robust standard errors, especially when used with MLE estimators, is controversial.  See Freedman See King and Roberts  But this depends on how they are being used, see Angrist.    6.8.5 Clustering  Clusters: \\(g = 1, \\dots, G\\). Units: \\(i = 1, \\dots, N_g\\). \\(N_g\\) is the number of observations in cluster \\(g\\) \\(N = \\sum_g N_g\\) is the total observations Units (usually) belong to a single cluster voters in household individuals in states students in classes This is particularly important when the outcome varies at the unit-level, \\(y_ij\\) and the main independent variable varies at the cluster level. Ignoring clustering overstates the effective number of individuals in the data.  Clustered dependence \\[ \\begin{aligned}[t] y_{ig} &amp;= \\beta_0 + \\beta_1 x_{ig} +\\epsilon_{ig} \\\\ &amp;= \\beta_0 + \\beta_1 x_{ig} + \\nu_g + \\eta_{ig} \\end{aligned} \\] Then the cluster error is \\[ \\nu \\sim N(0, rho \\sigma^2), \\] and the individual error is \\[ \\eta_{ig} \\sim N(0, (1 - \\rho) \\sigma^2) . \\] The cluster and unit errors are assumed to be independent of each other. \\(\\rho \\in (0, 1)\\) is the within-cluster correlation. If we ignore the cluster, and use \\(\\eta_{ig}\\) as the error, the variance is \\(\\sigma^2\\) \\[ \\Var(\\eta_{ig}) &amp;= \\Var(\\nu_g +\\eta_{ig}) \\\\ &amp;= \\Var(\\nu_g) + \\Var(\\varepsilon_{ig}) \\\\ &amp;= \\rho \\sigma^2 + (1 - \\rho) \\sigma^2 = \\sigma^2 \\] The Covariance between units in the same cluster is \\[ \\Cov(\\varepsilon_{ig}, \\varepsilon_{ig}) = \\rho \\sigma^2, \\] meaning that the correlation for units within a group is \\[ \\Cor(\\varepsilon_{ig}, \\varepsilon_{ig}) = rho . \\] But, there is zero covariance and correlation between units in different clusters. For example, the covariance matrix of \\[ \\vec\\varepsilon = \\begin{bmatrix} \\varepsilon_{1,1} &amp;  \\varepsilon_{2,1} &amp; \\varepsilon_{3,1} &amp; \\varepsilon_{4,2} &amp; \\varepsilon_{5,2} \\end{bmatrix}&#39; \\] is \\[ \\Var(\\vec\\varepsilon | \\mat{X}) = \\mat{\\Sigma} = \\begin{bmatrix} \\sigma^2 &amp; \\sigma \\rho &amp; \\sigma \\rho &amp; 0 &amp; 0 \\\\ \\sigma \\rho &amp; \\sigma^2 &amp; \\sigma \\rho &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\] More generally, the variance-covariance matrix of a the errors is block diagonal, \\[ \\Var(\\varepsilon | \\mat{X}) = \\mat{\\Sigma} = \\begin{bmatrix} \\mat{\\Sigma}_1 &amp; \\mat{0}_{N_1 \\times N_2} &amp; \\cdots &amp; \\mat{0}_{N_1 \\times N_G} \\\\ \\mat{0}_{N_2 \\times N_1} &amp; \\mat{\\Sigma}_2 &amp; \\cdots &amp; \\mat{0}_{N_2 \\times N_G} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mat{0}_{N_G \\times N_1} &amp; \\mat{0}_{N_G \\times N_2} &amp; \\cdots  &amp; \\mat{\\Sigma}_G \\end{bmatrix} \\] where \\(\\mat{Sigma}_g\\) are the covariance matrices of each cluster, and \\(\\mat{0}\\) are matrices of zeros of the appropriate sizes. There are several ways to address clustering, including:  Include an indicator variable for each cluster Random effects models Cluster-robust (“clustered”) standard error Aggregate data to the cluster data and use WLS with \\(\\bar{y}_g = \\frac{1}{N}_g \\sum_i y_{ig}\\) where the clusters are weighted by \\(N_g\\).  Cluster-robust standard errors uses the observed residuals, \\(\\hat\\varepsilon_i\\), to estimate a the variance-covariance matrix \\(\\hat\\Var{\\hat{\\vec{beta}}\\) which allows units to be independent across clusters and dependent within clusters. \\[ \\hat{\\Sigma} = \\begin{bmatrix} \\hat{\\varepsilon}_1^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\hat{\\varepsilon}_2^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\hat{\\varepsilon}_N^2 \\end{bmatrix} = \\hat{\\vec{\\varepsilon}} \\mat{I}_N \\hat{\\vec{\\varepsilon}} \\]  CRSE do not change \\(\\hat\\vec{\\beta}\\). Thus they do not fix bias in the coefficients. CRSE is a consistent estimator of \\(\\Var{\\hat\\vec{\\beta}}\\) given the clustered dependence.  This relies on the assumption of independence between clusters Does not rely on the model form CRSE are usually larger than classic standard errors  Consistency of the CRSE are in the number of groups, not the number of individuals  CRSE work well when the number of clusters is large (&gt; 50) Alternative: use a block bootstrap   See the R package plr (Panel linear models in R). See Cameron and Miller, Practioner’s Guide to Cluster-Robust Inference.   6.8.6 Auto-correlation More general case allows for heteroskedasticity, and auto-correlation (\\(\\Cov(\\varepsilon_i, \\varepsilon_j) \\neq 0\\)), \\[ \\mat{\\Sigma} = \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{1,2} &amp; \\cdots &amp; \\sigma_{1,N} \\\\ \\sigma_{2,1} &amp; \\sigma_2^2 &amp; \\cdots &amp; \\sigma_{2,N} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{N,1} &amp; \\sigma_{N,2} &amp; \\cdots &amp; \\sigma_N^2 \\end{bmatrix} \\] As with heteroskedasticity, OLS with be unbiased, but the standard errors will be incorrect. Tests  Breusch-Godfrey Test (lmtest::bgtest)  Solution  If the form is known: Prais-Wiston, include lagged dependent variable. Huber-White Heteroskedasticity and Autocorrelation Robust standard errors. These are an extension of the heteroskedasticity robust standard errors to also include autocorrelation. See sandwich function hcacVCOV.    6.8.7 Clustered and Panel Standard Errors Panel Corrected Standard See the R package plm    6.9 Non-Normal Errors Non-normal errors pose several problems for OLS:  If the sample size is small, normal errors are required for correct confidence interval coverage and p-values in tests. In large samples, CLT properties of OLS kick in and the normality of errors assumption is not needed to justify the sampling distributions of the test statistics. Heavy-tailed errors threaten the efficiency of OLS estimate. Skewed or multi-modal errors suggest that the conditional mean \\(E(Y | \\mat{X})\\), estimated by OLS may not be a good summary of the data.  How to diagnose? Plot the quantiles of the studentized residuals (see the section on outliers) against the expected quantiles of a normal distribution using a QQ-plot. In general, non-normal errors are a minor issue, and towards the bottom in priority of problems in inference. How to fix? There are a couple of ways to fix this:  Transform the dependent variable and use OLS Add different sets of covariates. This is especially likely with multi-modal error distributions, which could suggest an omitted categorical variable. Use a different model other than OLS.  R Calculate Studentized residuals with the function rstudent and a QQ-plot using stat_qq in ggplot2. Diagnostics  QQ-plot of the Studentized residuals  Important things to remember  The assumption is not that \\(Y\\) has a normal distribution, it is that the errors after including covariates are normal. While non-normal errors will not bias \\(\\beta\\) and have little effect on the standard errors unless the sample size is small, they could serve as a warning that your model is mis-specified, or that the conditional expectation of \\(Y\\) is not good summary.    "],
["weighting-in-regression.html", "Chapter 7 Weighting in Regression 7.1 References", " Chapter 7 Weighting in Regression  ``Few things are as confusing to applied researchers as the role of sample weights. Even now, 20 years post-Ph.D., we read the section of the Stata manual on weighting with some dismay.’’  Suppose that the heteroskedasticity is known up to a multiplicative constant, \\[ \\Var(\\varepsilon_i | \\mat{X}) = a_i \\sigma^2 , \\] where \\(a_i = a_i \\vec{x}_i&#39;\\) is a positive and known funtion of \\(\\vec{x}_i\\). Then in weighted least squares multiply \\(y_i\\) by \\(1 / \\sqrt{a_i}\\), \\[ \\begin{aligned}[t] y_i &amp;= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_K x_K + \\varepsilon_i \\\\ y_i / \\sqrt{a_i} &amp;= \\beta_0 / \\sqrt{a_i} + \\beta_1 x_1 / \\sqrt{a_1} + \\beta_2 x_2 / \\sqrt{a_2} + \\dots + \\beta_K x_K / \\sqrt{a_K} + \\varepsilon_i^* \\end{aligned} \\] where \\(\\varepsilon_i^* \\sim N(0, \\sigma^2)\\). This rescales errors to \\(\\varepsilon_i / \\sqrt{a_i}\\) which keeps \\(\\E(\\varepsilon_i) = 0\\), but makes the variance constant, \\[ \\Var\\left( \\frac{1}{\\sqrt{a_i}} \\varepsilon_i | \\mat{X} \\right) = \\frac{1}{a_i} \\Var(\\varepsilon_i | \\mat{X}) = \\frac{1}{a_i} a_i \\sigma^2 = \\sigma^2 \\] If \\(a_i\\) is known, then the model is homoskedastic and the estimator is BLUE. Define the weighting matrix, \\[ \\mat{W} = \\begin{bmatrix} 1 / \\sqrt{a_1} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 / \\sqrt{a_2} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; 0 \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 / \\sqrt{a_N} \\end{bmatrix} . \\] Then run the regression, \\[ \\begin{aligned}[t] \\mat{W} y &amp;= \\mat{W} \\mat{X} \\vec{\\beta} + \\mat{W} \\vec\\varepsilon \\\\ \\vec{y}^* &amp;= \\mat{X}^* \\vec{\\beta} + \\vec{\\varepsilon}^* . \\end{aligned} \\] Run the regression of \\(\\vec{y}^*\\) on \\(\\mat{X}^*\\), and the Gauss-Markov assumptions are statisfied. Then using the usual OLS formula, \\[ \\hat{\\vec\\beta}_{WLS} = ((\\mat{X}^*)&#39; \\mat{X}^*) (\\mat{X}^*)&#39; \\vec{y}^* = (\\mat{X}&#39; \\mat{W}&#39; \\mat{W} \\mat{X})^{-1} \\mat{X}&#39; \\mat{W}&#39; \\mat{W} \\vec{y} . \\] In R Use lm() with the weights argment. TODO: when is WLS useful See MHE on “weighting”,  7.1 References  WLS derivation Fox (2016) 304–306  Textbook discussions: Angrist and Pischke (2009) 91–94, Angrist and Pischke (2014) 202–203, Solon, Haider, and Wooldridge (2015) is a good (and recent) overview with practical advice of when to weight and when not-to weight linear regressions. Gelman (2007b), in the context of post-stratification, proposes controlling for variables related to selection into the sample instead of using survey weights; also see the responses (Bell and Cohen 2007; Breidt and Opsomer 2007; Little 2007; Pfeffermann 2007), and rejoinder (Gelman 2007a) and blog post. Gelman’s approach is similar to that earlier suggested by Winship and Radbill (1994). See also Deaton (1997), Dumouchel and Duncan (1983), and Wissoker (n.d.). For sruvey weighting, see the R package survey and its Examples of Weighting  ESS Afrobarometer Arabbarometer    "],
["interpreting-regression-coefficients.html", "Chapter 8 Interpreting Regression Coefficients 8.1 Standardized Coefficients 8.2 Marginal Effects and First Difference 8.3 References", " Chapter 8 Interpreting Regression Coefficients \\[ Y_i = \\beta_0 + \\beta_1 X_{i} + \\beta_2 Z_{i} + \\varepsilon_i \\] The partial effects , or first differences, of a regression is the change in the expected value of \\(Y\\) with respect to a discrete change of \\(X\\): \\[ \\begin{aligned}[t] \\frac{\\Delta E(Y)}{\\Delta X} &amp;= E(Y | X = x + d) - E(Y | X = x) \\\\ &amp;= \\beta_0 + \\beta_1 (x + d) + \\beta_2 Z_{i} - (\\beta_0 + \\beta_1 x + \\beta_2 Z_{i}) \\\\ &amp;= \\beta_1 d \\end{aligned} \\] When \\(d = 1\\) (a one unit change in \\(X\\)) the first difference is \\(\\beta_1\\), the coefficient of \\(X\\). The marginal effect is the derivative of the regression function with respect to a predictor. The marginal effect of \\(X\\) is \\[ \\begin{aligned}[t] \\frac{\\partial\\,E(Y)}{\\partial\\,X} &amp;= \\frac{1}{\\partial\\,X} (\\beta_0 + \\beta_1 X + \\beta_2 Z) \\\\ &amp;= \\beta_1 \\end{aligned} \\] That the coefficients are the marginal effects of each predictor makes linear regression particularly easy to interpret. However, this interpretation of predictors becomes more complicated once a variable is included in multiple terms through interactions or nonlinear functions, such as polynomials.  8.1 Standardized Coefficients A standardized coefficient is the coefficient on \\(X\\), when \\(X\\) is standardized so that \\(\\mean(X) = 0\\) and \\(\\Var(X) = 1\\). In that case, \\(\\beta_1\\) is the change in \\(\\E(Y)\\) associated with a one standard deviation change in \\(X\\). Additionally, if all predictors are set so that \\(\\mean(X) = 0\\), \\(\\beta_0\\) is the expected value of \\(Y\\) when all \\(X\\) are at their means. However, if any variables appear in multiple terms, then the standardized coefficients are not particularly useful. Standardized coefficients are generally not used in political science. (King How Not to Lie with Statistics, p. 669) More often, the effects of variables are compared by the first difference between the value of the variable at the mean, and a one standard deviation change. While, this is equivalent to the standardized coefficient Note, that standardizing variables can help computationally in some cases. In OLS, there is a closed-form solution, so iterative optimization algorithms are not needed in to find the best parameters. However, in more complicated models which require iterative optimization, standardizing variables can often improve the performance of the optimization. Thus standardizing variables before analysis is common in machine learning. However, the purpose is for ease of computation, not for ease of interpretation.   8.2 Marginal Effects and First Difference The marginal effect of a regressor is the change in the outcome variable associated with a small change in the predictor, The first difference of a regression of a variable with respect to the dependent variable is for two values of a variable \\(x_j\\) and \\(x_j + h\\) as, \\[ \\frac{\\Delta \\vec{y}}{\\Delta x} = y(x_j) - y(x) \\] The marginal effect is the regression coefficient, \\[ \\frac{\\partial \\vec{y}}{\\partial \\vec{x}} = \\beta_1 . \\] For a predictor with only a linear term, \\[ \\vec{y} = \\beta_0 + \\beta_1 \\vec{x}_1 , \\] However, if a predictor appears in multiple terms, the marginal effect will be a more complicated function. For example, if \\(x\\) appears as a squared term and an interaction, \\[ \\vec{y} = \\beta_0 + \\beta_1 \\vec{x} + \\beta_2 \\vec{x}^2 + \\beta_3 \\vec{z} + \\beta_4 \\vec{x} \\vec{z} + \\beta_5 \\vec{x} \\vec{z}^2 \\] then its marginal effect is a function of both its current value, \\(\\vec{x}\\), and the value of \\(\\vec{z}\\), \\[ \\frac{\\partial \\vec{y}}{\\partial \\vec{x}} = \\beta_1 + 2 \\beta_2 \\vec{x} + \\beta_4 \\vec{z} + 2 \\beta_5 \\vec{x} \\vec{z} \\]   8.3 References The R package marfx is still under development, but includes the ability to estimate average marginal and finite difference effects. Some useful R packages  ggplot2 and broom: Once point estimates and the confidence interval are calculated, it is easy to plot them. coefplot: Plots point estimates and confidence intervals for fitted models. dotwhisker: Another point-estimate and confidence interval plot pacakge for fitted models.  Packages for marginal effects or similar  margins port of the margins command Built-in predict() function calculates predictions and confidence intervals mfx marginal effects for beta, logit, negative binomial, poisson and probit models. erer: maBina: marginal effects for binary choice models; ocME: marginal effects for ordered logit or probit. car: mmps: Marginal model plots.    "],
["non-standard-errors.html", "Chapter 9 Non-standard errors 9.1 References", " Chapter 9 Non-standard errors  9.1 References Several packages provide large collections:  lmtest car sandwich  Reading the vignettes or documentation of these packages is a good overview of available regression diagnostics. Also see the Econometrics Task View. The Fox textbook has a particularly extensive overview of regression diagnostics. Also see Faraway. Though for Stata, this tutorial has an overview of many regression diagnostics.   "],
["resampling-methods.html", "Chapter 10 Resampling Methods 10.1 Cross-Validation", " Chapter 10 Resampling Methods  10.1 Cross-Validation  10.1.1 Bias-Variance Tradeoff Error in a model can come from two places  Bias: On average, how close is a model to the estimand. Variance: Between samples, how variable are the estimates of a model.  In a regression model we are trying to pedict a \\(Y\\) with covariates \\(X\\), given a function \\(Y = f(X) + \\varepsilon\\), where \\(f(X)\\) is the conditional expectation function, \\(f(X) = E(Y | X)\\), and \\(\\varepsilon\\) is an error term. Now suppose we estimate \\(f(X)\\) by \\(\\hat{f}(X)\\). In OLS, \\(\\hat{y} = \\hat{f}(X) = \\hat{\\beta}(X)\\). TODO MSE and bias-variance trade-off Diagram and examples of bias variance trade off   10.1.2 Cross-Validation Diagram of cross validation Example of cross validation Difficulties of cross validation  time series categorical variables / fixed effects missing data    10.1.3 Other Quantities  10.1.3.1 Information Criteria There are a class of criteria that we’ll call information criteria. They all take the the log-likelihood of a model but apply a penalty increasing with the complexity of the model, \\[ IC* = -\\mathcal{L} + \\text{penalty} \\] Models with lower IC* are preferred. The Akaike Information Criterion (AIC) is defined as \\[ AIC = -2 \\log \\mathcal{L} + 2K \\] where \\(K\\) is the number of parameters. As \\(N \\to \\infty\\), the AIC is equivalent to minimizing the leave-one-out cross validation value. So the AIC is useful for model selection when the purpose is prediction. The Bayesian Information Criterion (BIC) is defined as, \\[ BIC = -2 \\log \\mathcal{L} + K \\log N \\] where \\(K\\) is the number of parameters, and \\(N\\) is the number of observations. Note that the penalty for BIC is larger than the one for AIC, since \\(2 K &lt; K \\log N\\) for any \\(N &gt; 7\\). The penalty for BIC also increases with the sample size, unlike the AIC. Like AIC, BIC is equivalent to a leave \\(v\\) out cross-validation where \\(v = n (1 - 1 / (\\log N - 1))\\). The nice feature of the BIC is that, unlike AIC, it is consistent; if there is enough data, the BIC will select the true model. However, the probability that your model is the true model is about zero, so I am skeptical about the importance of this property in practice.    10.1.4 Others There are a few other statistics to keep in mind:  Mallows C_p Generalized Cross Validation (GCV)  These statistics generally only apply to linear regression. Since they are approximations to cross-validation, their relative advantage has declined as computational power has increased. However, as our data has also increased, these approximations may be useful for big data or computationally intensive jobs.   10.1.5 References  Hyndsight     "],
["panel-data.html", "Chapter 11 Panel Data 11.1 Longitudinal Data 11.2 Panel Data 11.3 Difference-in-Difference 11.4 TSCS", " Chapter 11 Panel Data  11.1 Longitudinal Data Longitudinal data Repeated observations for  across \\(N\\) units (e.g. countries, states, individuals, parties) over \\(T\\) time-periods (e.g. years, months, days, sample waves)  There is an important distinction between longitudinal data where \\(N\\) is small and \\(T\\) is large, and those in which \\(T\\) is small and \\(N\\) is large.  Time-series cross-section (TSCS): big \\(T\\). many political economy data sets with countries observerved over many years. Panel: small \\(T\\), bib \\(N\\).  This disinction is important for the methods that are used. The distinction is not necessarily whether \\(N &gt; T\\) or \\(N &lt; T\\). The importance is that different methods had asymptotic properties that depend on \\(T \\to infty\\), \\(N \\to \\infty\\), or \\(N \\times T \\to \\infty\\). This means that some methods that work well for panels with many individuals with a few observations will not work well for data with a few individuals and with many repeated observations, and vice-versa. One of the first things to check when encountering a new panel model is which of these cases it works well. Hierarchical (multilevel/nested) data is a similar concept, in which data are nested. For example,  students within classrooms survey respondents within districts within countries  While these are different, some hierarchical models can be useful for modeling panel data.   11.2 Panel Data Causal inference   11.3 Difference-in-Difference   11.4 TSCS  Serial correlation Beck and Katz 2011 http://www.annualreviews.org/doi/pdf/10.1146/annurev-polisci-071510-103222.  LDV or ECM with PCSE ? The longer your time-series, the more you do not need to pool information across units and can model the data within each unit using a time-series model.   "],
["appendix.html", "Chapter 12 Appendix 12.1 Multivariate Normal Distribution", " Chapter 12 Appendix  12.1 Multivariate Normal Distribution The multivariate normal distribution is the generalization of the univariate normal distribution to more than one dimension.6 The random variable, \\(\\vec{x}\\), is a length \\(k\\) vector. The \\(k\\) length vector \\(\\vec{\\mu}\\) are the means of \\(\\vec{x}\\), and the \\(k \\times k\\) matrix, \\(\\mat{\\Sigma}\\), is the variance-covariance matrix, \\[ \\begin{aligned}[t] \\vec{x} &amp;\\sim \\dmvnorm{k}\\left(\\vec{\\mu}, \\mat{\\Sigma} \\right) \\\\ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_k \\end{bmatrix} &amp; \\sim \\dmvnorm{k} \\left(   \\begin{bmatrix}   \\mu_1 \\\\   \\mu_2 \\\\   \\vdots \\\\   \\mu_k   \\end{bmatrix},   \\begin{bmatrix}   \\sigma_1^2 &amp; \\sigma_{1,2} &amp; \\cdots &amp; \\sigma_{1, k} \\\\   \\sigma_{2,1} &amp; \\sigma_2^2 &amp; \\cdots &amp; \\sigma_{2, k} \\\\   \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\   \\sigma_{k,1} &amp; \\sigma_{k,2} &amp; \\cdots &amp; \\sigma_{k, k}   \\end{bmatrix} \\right) \\end{aligned} \\] The density function of the multivariate normal is, \\[ p(\\vec{x}; \\vec{\\mu}, \\mat{\\Sigma}) = (2 k)^{-\\frac{k}{2}} \\left| \\mat{\\Sigma} \\right|^{-\\frac{1}{2}} \\exp \\left( -\\frac{1}{2} (\\vec{x} - \\vec{\\mu})\\T \\mat{\\Sigma}^{-1} (\\vec{x} - \\vec{\\mu}) \\right) . \\] You can sample from and calculate the density for the multivariate normal distribution with the functions dmvnorm and rmvnorm from the package mvtnorm. Density plots of different bivariate normal distributions,     "],
["references-7.html", "Chapter 13 References", " Chapter 13 References    Altonji, Joseph G., Todd E. Elder, and Christopher R. Taber. 2005. “Selection on Observed and Unobserved Variables: Assessing the Effectiveness of Catholic Schools.” Journal of Political Economy 113 (1): 151–84. doi:10.1086/426036.   Angrist, Joshua D., and Jörn-Steffen Pischke. 2009. Mostly Harmless Econometrics: An Empiricist’s Companion. Pr.   ———. 2014. Mastering ’Metrics. Princeton UP.   Bell, Robert M., and Michael L. Cohen. 2007. “Comment: Struggles with Survey Weighting and Regression Modeling.” Statist. Sci. 22 (2). The Institute of Mathematical Statistics: 165–67. doi:10.1214/088342307000000177.   Bellows, John, and Edward Miguel. 2009. “War and Local Collective Action in Sierra Leone.” Journal of Public Economics 93 (11–12): 1144–57. doi:http://dx.doi.org/10.1016/j.jpubeco.2009.07.012.   Breidt, F. Jay, and Jean D. Opsomer. 2007. “Comment: Struggles with Survey Weighting and Regression Modeling.” Statist. Sci. 22 (2). The Institute of Mathematical Statistics: 168–70. doi:10.1214/088342307000000195.   Deaton, Angus. 1997. The Analysis of Household Surveys : A Microeconometric Approach to Development Policy. The World Bank. http://documents.worldbank.org/curated/en/1997/07/694690/analysis-household-surveys-microeconometric-approach-development-policy.   Dumouchel, William H., and Greg J. Duncan. 1983. “Using Sample Survey Weights in Multiple Regression Analyses of Stratified Samples.” Journal of the American Statistical Association 78 (383): 535–43. doi:10.1080/01621459.1983.10478006.   Fox, John. 2016. Applied Regression Analysis &amp; Generalized Linear Models. 3rd ed. Sage.   Gelman, Andrew. 2007a. “Rejoinder: Struggles with Survey Weighting and Regression Modeling.” Statist. Sci. 22 (2). The Institute of Mathematical Statistics: 184–88. doi:10.1214/088342307000000203.   ———. 2007b. “Struggles with Survey Weighting and Regression Modeling.” Statist. Sci. 22 (2). The Institute of Mathematical Statistics: 153–64. doi:10.1214/088342306000000691.   Little, Roderick J. 2007. “Comment: Struggles with Survey Weighting and Regression Modeling.” Statist. Sci. 22 (2). The Institute of Mathematical Statistics: 171–74. doi:10.1214/088342307000000186.   Oster, Emily. 2013. “Unobservable Selection and Coefficient Stability: Theory and Validation.” Working Paper 19054. NBER.   Pfeffermann, Danny. 2007. “Comment: Struggles with Survey Weighting and Regression Modeling.” Statist. Sci. 22 (2). The Institute of Mathematical Statistics: 179–83. doi:10.1214/088342307000000168.   Solon, Gary, Steven J. Haider, and Jeffrey M. Wooldridge. 2015. “What Are We Weighting for?” Journal of Human Resources 50 (2). https://muse.jhu.edu/article/581177.   Winship, Christopher, and Larry Radbill. 1994. “Sampling Weights and Regression Analysis.” Sociological Methods &amp; Research 23 (2): 230–57. doi:10.1177/0049124194023002004.   Wissoker, Douglass. n.d. “Notes on Weighting in Regression.” http://anfdata.urban.org/sdaweb/nsaf_tutorial/reg_weights.pdf.       Ordinary least squares is distinguished from generalized least squares (GLS).↩ It follows from the definition of MSE, that biased estimator, \\(\\hat\\theta_{B}\\), has a lower MSE than an unbiased estimator, \\(\\hat\\theta_{U}\\), if \\(\\Bias(\\theta_B)^2 &lt; \\Var(\\theta_U) - \\Var(\\theta_B)\\).↩ As the number of observations goes to infinity.↩ It follows from the definition of MSE, that biased estimator, \\(\\hat\\theta_{B}\\), has a lower MSE than an unbiased estimator, \\(\\hat\\theta_{U}\\), if \\(\\Bias(\\theta_B)^2 &lt; \\Var(\\theta_U) - \\Var(\\theta_B)\\).↩ As the number of observations goes to infinity.↩ See Multivariate normal distribution and references therein.↩  "]
]
