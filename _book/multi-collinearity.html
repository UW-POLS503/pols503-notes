<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>POLS 503: Advanced Quantitative Political Methodology: The Notes</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>">
  <meta name="generator" content="bookdown 0.0.60 and GitBook 2.6.7">

  <meta property="og:title" content="POLS 503: Advanced Quantitative Political Methodology: The Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>" />
  <meta name="github-repo" content="UW-POLS503/pols503-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="POLS 503: Advanced Quantitative Political Methodology: The Notes" />
  
  <meta name="twitter:description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>" />
  

<meta name="author" content="Jeffrey B. Arnold">

<meta name="date" content="2016-05-18">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="interpreting-regression-coefficients.html">
<link rel="next" href="non-standard-errors.html">

<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>

$$
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Bias}{Bias}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\T}{'}

\newcommand{\distr}[1]{\mathcal{#1}}
\newcommand{\dnorm}{\distr{N}}
\newcommand{\dmvnorm}[1]{\distr{N}_{#1}}
$$

  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>2</b> Appendix</a><ul>
<li class="chapter" data-level="2.1" data-path="appendix.html"><a href="appendix.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>2.1</b> Multivariate Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="functional-form-and-non-linearity.html"><a href="functional-form-and-non-linearity.html"><i class="fa fa-check"></i><b>3</b> Functional Form and Non-linearity</a><ul>
<li class="chapter" data-level="3.1" data-path="functional-form-and-non-linearity.html"><a href="functional-form-and-non-linearity.html#non-linearity"><i class="fa fa-check"></i><b>3.1</b> Non-linearity</a><ul>
<li class="chapter" data-level="3.1.1" data-path="functional-form-and-non-linearity.html"><a href="functional-form-and-non-linearity.html#whats-the-problem"><i class="fa fa-check"></i><b>3.1.1</b> What’s the problem?</a></li>
<li class="chapter" data-level="3.1.2" data-path="functional-form-and-non-linearity.html"><a href="functional-form-and-non-linearity.html#what-to-do-about-it-and-how-to-solve-it"><i class="fa fa-check"></i><b>3.1.2</b> What to do about it? And How to Solve it?</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="functional-form-and-non-linearity.html"><a href="functional-form-and-non-linearity.html#logarithm"><i class="fa fa-check"></i><b>3.2</b> Logarithm</a><ul>
<li class="chapter" data-level="3.2.1" data-path="functional-form-and-non-linearity.html"><a href="functional-form-and-non-linearity.html#examples-of-relevant-theories"><i class="fa fa-check"></i><b>3.2.1</b> Examples of Relevant Theories</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="functional-form-and-non-linearity.html"><a href="functional-form-and-non-linearity.html#miscellaneous"><i class="fa fa-check"></i><b>3.3</b> Miscellaneous</a><ul>
<li class="chapter" data-level="3.3.1" data-path="functional-form-and-non-linearity.html"><a href="functional-form-and-non-linearity.html#square-root-and-variance-stabalizing-transformations"><i class="fa fa-check"></i><b>3.3.1</b> Square Root and Variance Stabalizing Transformations</a></li>
<li class="chapter" data-level="3.3.2" data-path="functional-form-and-non-linearity.html"><a href="functional-form-and-non-linearity.html#power-transformation"><i class="fa fa-check"></i><b>3.3.2</b> Power-Transformation</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="functional-form-and-non-linearity.html"><a href="functional-form-and-non-linearity.html#polynomials"><i class="fa fa-check"></i><b>3.4</b> Polynomials</a><ul>
<li class="chapter" data-level="3.4.1" data-path="functional-form-and-non-linearity.html"><a href="functional-form-and-non-linearity.html#squared"><i class="fa fa-check"></i><b>3.4.1</b> Squared</a></li>
<li class="chapter" data-level="3.4.2" data-path="functional-form-and-non-linearity.html"><a href="functional-form-and-non-linearity.html#higher-order-polynomials"><i class="fa fa-check"></i><b>3.4.2</b> Higher-Order Polynomials</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="functional-form-and-non-linearity.html"><a href="functional-form-and-non-linearity.html#interactions"><i class="fa fa-check"></i><b>3.5</b> Interactions</a><ul>
<li class="chapter" data-level="3.5.1" data-path="functional-form-and-non-linearity.html"><a href="functional-form-and-non-linearity.html#theories"><i class="fa fa-check"></i><b>3.5.1</b> Theories</a></li>
<li class="chapter" data-level="3.5.2" data-path="functional-form-and-non-linearity.html"><a href="functional-form-and-non-linearity.html#recommendations"><i class="fa fa-check"></i><b>3.5.2</b> Recommendations</a></li>
<li class="chapter" data-level="3.5.3" data-path="functional-form-and-non-linearity.html"><a href="functional-form-and-non-linearity.html#plots"><i class="fa fa-check"></i><b>3.5.3</b> Plots</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="functional-form-and-non-linearity.html"><a href="functional-form-and-non-linearity.html#flexible-functional-forms"><i class="fa fa-check"></i><b>3.6</b> Flexible Functional Forms</a></li>
<li class="chapter" data-level="3.7" data-path="functional-form-and-non-linearity.html"><a href="functional-form-and-non-linearity.html#references"><i class="fa fa-check"></i><b>3.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="interpreting-regression-coefficients.html"><a href="interpreting-regression-coefficients.html"><i class="fa fa-check"></i><b>4</b> Interpreting Regression Coefficients</a><ul>
<li class="chapter" data-level="4.1" data-path="interpreting-regression-coefficients.html"><a href="interpreting-regression-coefficients.html#standardized-coefficients"><i class="fa fa-check"></i><b>4.1</b> Standardized Coefficients</a></li>
<li class="chapter" data-level="4.2" data-path="interpreting-regression-coefficients.html"><a href="interpreting-regression-coefficients.html#marginal-effects-and-first-difference"><i class="fa fa-check"></i><b>4.2</b> Marginal Effects and First Difference</a></li>
<li class="chapter" data-level="4.3" data-path="interpreting-regression-coefficients.html"><a href="interpreting-regression-coefficients.html#references-1"><i class="fa fa-check"></i><b>4.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="multi-collinearity.html"><a href="multi-collinearity.html"><i class="fa fa-check"></i><b>5</b> Multi-Collinearity</a><ul>
<li class="chapter" data-level="5.0.1" data-path="multi-collinearity.html"><a href="multi-collinearity.html#perfect-collinearity"><i class="fa fa-check"></i><b>5.0.1</b> Perfect Collinearity</a></li>
<li class="chapter" data-level="5.0.2" data-path="multi-collinearity.html"><a href="multi-collinearity.html#less-than-perfect-collinearity"><i class="fa fa-check"></i><b>5.0.2</b> Less-than Perfect Collinearity</a></li>
<li class="chapter" data-level="5.1" data-path="multi-collinearity.html"><a href="multi-collinearity.html#non-constant-variances-and-correlated-errors"><i class="fa fa-check"></i><b>5.1</b> Non-constant Variances and Correlated Errors</a><ul>
<li class="chapter" data-level="5.1.1" data-path="multi-collinearity.html"><a href="multi-collinearity.html#heteroskedasticity"><i class="fa fa-check"></i><b>5.1.1</b> Heteroskedasticity</a></li>
<li class="chapter" data-level="5.1.2" data-path="multi-collinearity.html"><a href="multi-collinearity.html#heteroskedasticity-1"><i class="fa fa-check"></i><b>5.1.2</b> Heteroskedasticity</a></li>
<li class="chapter" data-level="5.1.3" data-path="multi-collinearity.html"><a href="multi-collinearity.html#diagnostics"><i class="fa fa-check"></i><b>5.1.3</b> Diagnostics</a></li>
<li class="chapter" data-level="5.1.4" data-path="multi-collinearity.html"><a href="multi-collinearity.html#dealing-with-heteroskedasticity"><i class="fa fa-check"></i><b>5.1.4</b> Dealing with Heteroskedasticity</a></li>
<li class="chapter" data-level="5.1.5" data-path="multi-collinearity.html"><a href="multi-collinearity.html#clustering"><i class="fa fa-check"></i><b>5.1.5</b> Clustering</a></li>
<li class="chapter" data-level="5.1.6" data-path="multi-collinearity.html"><a href="multi-collinearity.html#auto-correlation"><i class="fa fa-check"></i><b>5.1.6</b> Auto-correlation</a></li>
<li class="chapter" data-level="5.1.7" data-path="multi-collinearity.html"><a href="multi-collinearity.html#clustered-and-panel-standard-errors"><i class="fa fa-check"></i><b>5.1.7</b> Clustered and Panel Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="multi-collinearity.html"><a href="multi-collinearity.html#non-normal-errors"><i class="fa fa-check"></i><b>5.2</b> Non-Normal Errors</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="non-standard-errors.html"><a href="non-standard-errors.html"><i class="fa fa-check"></i><b>6</b> Non-standard errors</a><ul>
<li class="chapter" data-level="6.1" data-path="non-standard-errors.html"><a href="non-standard-errors.html#references-2"><i class="fa fa-check"></i><b>6.1</b> References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="linear-regression-and-the-ordinary-least-squares-ols-estimator.html"><a href="linear-regression-and-the-ordinary-least-squares-ols-estimator.html"><i class="fa fa-check"></i><b>7</b> Linear Regression and the Ordinary Least Squares (OLS) Estimator</a><ul>
<li class="chapter" data-level="7.1" data-path="linear-regression-and-the-ordinary-least-squares-ols-estimator.html"><a href="linear-regression-and-the-ordinary-least-squares-ols-estimator.html#linear-regression-function"><i class="fa fa-check"></i><b>7.1</b> Linear Regression Function</a></li>
<li class="chapter" data-level="7.2" data-path="linear-regression-and-the-ordinary-least-squares-ols-estimator.html"><a href="linear-regression-and-the-ordinary-least-squares-ols-estimator.html#ordinary-least-squares"><i class="fa fa-check"></i><b>7.2</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="7.3" data-path="linear-regression-and-the-ordinary-least-squares-ols-estimator.html"><a href="linear-regression-and-the-ordinary-least-squares-ols-estimator.html#properties-of-the-ols-estimator"><i class="fa fa-check"></i><b>7.3</b> Properties of the OLS Estimator</a><ul>
<li class="chapter" data-level="7.3.1" data-path="linear-regression-and-the-ordinary-least-squares-ols-estimator.html"><a href="linear-regression-and-the-ordinary-least-squares-ols-estimator.html#what-makes-an-estimator-good"><i class="fa fa-check"></i><b>7.3.1</b> What makes an estimator good?</a></li>
<li class="chapter" data-level="7.3.2" data-path="linear-regression-and-the-ordinary-least-squares-ols-estimator.html"><a href="linear-regression-and-the-ordinary-least-squares-ols-estimator.html#properties-of-ols"><i class="fa fa-check"></i><b>7.3.2</b> Properties of OLS</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="linear-regression-and-the-ordinary-least-squares-ols-estimator.html"><a href="linear-regression-and-the-ordinary-least-squares-ols-estimator.html#multi-collinearity-1"><i class="fa fa-check"></i><b>7.4</b> Multi-Collinearity</a><ul>
<li class="chapter" data-level="7.4.1" data-path="linear-regression-and-the-ordinary-least-squares-ols-estimator.html"><a href="linear-regression-and-the-ordinary-least-squares-ols-estimator.html#perfect-collinearity-1"><i class="fa fa-check"></i><b>7.4.1</b> Perfect Collinearity</a></li>
<li class="chapter" data-level="7.4.2" data-path="linear-regression-and-the-ordinary-least-squares-ols-estimator.html"><a href="linear-regression-and-the-ordinary-least-squares-ols-estimator.html#less-than-perfect-collinearity-1"><i class="fa fa-check"></i><b>7.4.2</b> Less-than Perfect Collinearity</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="linear-regression-and-the-ordinary-least-squares-ols-estimator.html"><a href="linear-regression-and-the-ordinary-least-squares-ols-estimator.html#weighted-least-squares"><i class="fa fa-check"></i><b>7.5</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="7.6" data-path="linear-regression-and-the-ordinary-least-squares-ols-estimator.html"><a href="linear-regression-and-the-ordinary-least-squares-ols-estimator.html#references-3"><i class="fa fa-check"></i><b>7.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="properties-of-the-ols-estimator-1.html"><a href="properties-of-the-ols-estimator-1.html"><i class="fa fa-check"></i><b>8</b> Properties of the OLS Estimator</a><ul>
<li class="chapter" data-level="8.1" data-path="properties-of-the-ols-estimator-1.html"><a href="properties-of-the-ols-estimator-1.html#what-makes-an-estimator-good-1"><i class="fa fa-check"></i><b>8.1</b> What makes an estimator good?</a><ul>
<li class="chapter" data-level="8.1.1" data-path="properties-of-the-ols-estimator-1.html"><a href="properties-of-the-ols-estimator-1.html#properties-of-ols-1"><i class="fa fa-check"></i><b>8.1.1</b> Properties of OLS</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="properties-of-the-ols-estimator-1.html"><a href="properties-of-the-ols-estimator-1.html#references-4"><i class="fa fa-check"></i><b>8.2</b> References</a></li>
<li class="chapter" data-level="8.3" data-path="properties-of-the-ols-estimator-1.html"><a href="properties-of-the-ols-estimator-1.html#sampling-distribution-of-ols-coefficients"><i class="fa fa-check"></i><b>8.3</b> Sampling Distribution of OLS Coefficients</a></li>
<li class="chapter" data-level="8.4" data-path="properties-of-the-ols-estimator-1.html"><a href="properties-of-the-ols-estimator-1.html#t-tests-for-single-parameters"><i class="fa fa-check"></i><b>8.4</b> t-tests for single parameters</a></li>
<li class="chapter" data-level="8.5" data-path="properties-of-the-ols-estimator-1.html"><a href="properties-of-the-ols-estimator-1.html#f-tests-of-multiple-hypotheses"><i class="fa fa-check"></i><b>8.5</b> F-tests of Multiple Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="diagnostics-and-troubleshooting.html"><a href="diagnostics-and-troubleshooting.html"><i class="fa fa-check"></i><b>9</b> Diagnostics and Troubleshooting</a><ul>
<li class="chapter" data-level="9.1" data-path="diagnostics-and-troubleshooting.html"><a href="diagnostics-and-troubleshooting.html#omitted-variables"><i class="fa fa-check"></i><b>9.1</b> Omitted variables</a><ul>
<li class="chapter" data-level="9.1.1" data-path="diagnostics-and-troubleshooting.html"><a href="diagnostics-and-troubleshooting.html#simulations"><i class="fa fa-check"></i><b>9.1.1</b> Simulations</a></li>
<li class="chapter" data-level="9.1.2" data-path="diagnostics-and-troubleshooting.html"><a href="diagnostics-and-troubleshooting.html#what-to-do-about-it-3"><i class="fa fa-check"></i><b>9.1.2</b> What to do about it?</a></li>
<li class="chapter" data-level="9.1.3" data-path="diagnostics-and-troubleshooting.html"><a href="diagnostics-and-troubleshooting.html#examples"><i class="fa fa-check"></i><b>9.1.3</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="diagnostics-and-troubleshooting.html"><a href="diagnostics-and-troubleshooting.html#measurement-errors"><i class="fa fa-check"></i><b>9.2</b> Measurement Errors</a><ul>
<li class="chapter" data-level="9.2.1" data-path="diagnostics-and-troubleshooting.html"><a href="diagnostics-and-troubleshooting.html#what-can-we-do-about-it"><i class="fa fa-check"></i><b>9.2.1</b> What can we do about it?</a></li>
<li class="chapter" data-level="9.2.2" data-path="diagnostics-and-troubleshooting.html"><a href="diagnostics-and-troubleshooting.html#simulations-1"><i class="fa fa-check"></i><b>9.2.2</b> Simulations</a></li>
<li class="chapter" data-level="9.2.3" data-path="diagnostics-and-troubleshooting.html"><a href="diagnostics-and-troubleshooting.html#example"><i class="fa fa-check"></i><b>9.2.3</b> Example</a></li>
<li class="chapter" data-level="9.2.4" data-path="diagnostics-and-troubleshooting.html"><a href="diagnostics-and-troubleshooting.html#references-5"><i class="fa fa-check"></i><b>9.2.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="diagnostics-and-troubleshooting.html"><a href="diagnostics-and-troubleshooting.html#functional-form"><i class="fa fa-check"></i><b>9.3</b> Functional Form</a></li>
<li class="chapter" data-level="9.4" data-path="diagnostics-and-troubleshooting.html"><a href="diagnostics-and-troubleshooting.html#multicollinearity"><i class="fa fa-check"></i><b>9.4</b> Multicollinearity</a></li>
<li class="chapter" data-level="9.5" data-path="diagnostics-and-troubleshooting.html"><a href="diagnostics-and-troubleshooting.html#residuals"><i class="fa fa-check"></i><b>9.5</b> Residuals</a><ul>
<li class="chapter" data-level="9.5.1" data-path="diagnostics-and-troubleshooting.html"><a href="diagnostics-and-troubleshooting.html#non-normal-errors-1"><i class="fa fa-check"></i><b>9.5.1</b> Non-Normal errors</a></li>
<li class="chapter" data-level="9.5.2" data-path="diagnostics-and-troubleshooting.html"><a href="diagnostics-and-troubleshooting.html#non-constant-variance"><i class="fa fa-check"></i><b>9.5.2</b> Non-Constant variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="omitted-variable-bias-and-measurement-error.html"><a href="omitted-variable-bias-and-measurement-error.html"><i class="fa fa-check"></i><b>10</b> Omitted Variable Bias and Measurement Error</a><ul>
<li class="chapter" data-level="10.1" data-path="omitted-variable-bias-and-measurement-error.html"><a href="omitted-variable-bias-and-measurement-error.html#omitted-variable-bias"><i class="fa fa-check"></i><b>10.1</b> Omitted Variable Bias</a><ul>
<li class="chapter" data-level="10.1.1" data-path="omitted-variable-bias-and-measurement-error.html"><a href="omitted-variable-bias-and-measurement-error.html#whats-the-problem-1"><i class="fa fa-check"></i><b>10.1.1</b> What’s the problem?</a></li>
<li class="chapter" data-level="10.1.2" data-path="omitted-variable-bias-and-measurement-error.html"><a href="omitted-variable-bias-and-measurement-error.html#what-to-do-about-it-4"><i class="fa fa-check"></i><b>10.1.2</b> What to do about it?</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="omitted-variable-bias-and-measurement-error.html"><a href="omitted-variable-bias-and-measurement-error.html#measurement-error"><i class="fa fa-check"></i><b>10.2</b> Measurement Error</a><ul>
<li class="chapter" data-level="10.2.1" data-path="omitted-variable-bias-and-measurement-error.html"><a href="omitted-variable-bias-and-measurement-error.html#whats-the-problem-2"><i class="fa fa-check"></i><b>10.2.1</b> What’s the problem?</a></li>
<li class="chapter" data-level="10.2.2" data-path="omitted-variable-bias-and-measurement-error.html"><a href="omitted-variable-bias-and-measurement-error.html#what-to-do-about-it-5"><i class="fa fa-check"></i><b>10.2.2</b> What to do about it?</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="omitted-variable-bias-and-measurement-error.html"><a href="omitted-variable-bias-and-measurement-error.html#longitudinal-data"><i class="fa fa-check"></i><b>10.3</b> Longitudinal Data</a></li>
<li class="chapter" data-level="10.4" data-path="omitted-variable-bias-and-measurement-error.html"><a href="omitted-variable-bias-and-measurement-error.html#panel-data"><i class="fa fa-check"></i><b>10.4</b> Panel Data</a></li>
<li class="chapter" data-level="10.5" data-path="omitted-variable-bias-and-measurement-error.html"><a href="omitted-variable-bias-and-measurement-error.html#difference-in-difference"><i class="fa fa-check"></i><b>10.5</b> Difference-in-Difference</a></li>
<li class="chapter" data-level="10.6" data-path="omitted-variable-bias-and-measurement-error.html"><a href="omitted-variable-bias-and-measurement-error.html#tscs"><i class="fa fa-check"></i><b>10.6</b> TSCS</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="references-6.html"><a href="references-6.html"><i class="fa fa-check"></i><b>11</b> References</a></li>
<li class="chapter" data-level="12" data-path="resampling-methods.html"><a href="resampling-methods.html"><i class="fa fa-check"></i><b>12</b> Resampling Methods</a><ul>
<li class="chapter" data-level="12.1" data-path="resampling-methods.html"><a href="resampling-methods.html#cross-validation"><i class="fa fa-check"></i><b>12.1</b> Cross-Validation</a><ul>
<li class="chapter" data-level="12.1.1" data-path="resampling-methods.html"><a href="resampling-methods.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>12.1.1</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="12.1.2" data-path="resampling-methods.html"><a href="resampling-methods.html#cross-validation-1"><i class="fa fa-check"></i><b>12.1.2</b> Cross-Validation</a></li>
<li class="chapter" data-level="12.1.3" data-path="resampling-methods.html"><a href="resampling-methods.html#other-quantities"><i class="fa fa-check"></i><b>12.1.3</b> Other Quantities</a></li>
<li class="chapter" data-level="12.1.4" data-path="resampling-methods.html"><a href="resampling-methods.html#others"><i class="fa fa-check"></i><b>12.1.4</b> Others</a></li>
<li class="chapter" data-level="12.1.5" data-path="resampling-methods.html"><a href="resampling-methods.html#references-7"><i class="fa fa-check"></i><b>12.1.5</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="weighting-in-regression.html"><a href="weighting-in-regression.html"><i class="fa fa-check"></i><b>13</b> Weighting in Regression</a></li>
<li class="chapter" data-level="14" data-path="examples-of-weighting.html"><a href="examples-of-weighting.html"><i class="fa fa-check"></i><b>14</b> Examples of Weighting</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">POLS 503: Advanced Quantitative Political Methodology: The Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multi-collinearity" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Multi-Collinearity</h1>
<div id="perfect-collinearity" class="section level3">
<h3><span class="header-section-number">5.0.1</span> Perfect Collinearity</h3>
<p>In order to estimate unique <span class="math inline">\(\hat{\beta}\)</span> OLS requires the that the columns of the design matrix <span class="math inline">\(\vec{X}\)</span> are linearly independent.</p>
<p>Common examples of groups of variables that are not linearly independent:</p>
<ul>
<li>Categorical variables in which there is no excluded category. You can also include all categories of a categorical variable if you exclude the intercept. Note that although they are not (often) used in political science, there are other methods of transforming categorical variables to ensure the columns in the design matrix are independent.</li>
<li>A constant variable. This can happen in practice with dichotomous variables of rare events; if you drop some observations for whatever reason, you may end up dropping all the 1’s in the data. So although the variable is not constant in the population, in your sample it is constant and cannot be included in the regression.</li>
<li>A variable that is a multiple of another variable. E.g. you cannot include <span class="math inline">\(\log(\text{GDP in millions USD})\)</span> and <span class="math inline">\(\log({GDP in USD})\)</span> since <span class="math inline">\(\log(\text{GDP in millions USD}) = \log({GDP in USD}) / 1,000,000\)</span>. in</li>
<li>A variable that is the sum of two other variables. E.g. you cannot include <span class="math inline">\(\log(population)\)</span>, <span class="math inline">\(\log(GDP)\)</span>, <span class="math inline">\(\log(GDP per capita)\)</span> in a regression since <span class="math display">\[\log(\text{GDP per capita}) = \log(\text{GDP} / \text{population}) = \log(\text{GDP}) - \log(\text{population})\]</span>.</li>
</ul>
<div id="what-to-do-about-it" class="section level4">
<h4><span class="header-section-number">5.0.1.1</span> What to do about it?</h4>
<p>R and most statistical programs will run regressions with collinear variables, but will drop variables until only linearly independent columns in <span class="math inline">\(\mat{X}\)</span> remain.</p>
<p>For example, consider the following code. The variable <code>type</code> is a categorical variable with categories “bc”, “wc”, and “prof”. It will</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(Duncan, <span class="dt">package =</span> <span class="st">&quot;car&quot;</span>)
<span class="co"># Create dummy variables for each category</span>
Duncan &lt;-<span class="st"> </span><span class="kw">mutate</span>(Duncan,
                 <span class="dt">bc =</span> type ==<span class="st"> &quot;bc&quot;</span>,
                 <span class="dt">wc =</span> type ==<span class="st"> &quot;wc&quot;</span>,
                 <span class="dt">prof =</span> type ==<span class="st"> &quot;prof&quot;</span>)
<span class="kw">lm</span>(prestige ~<span class="st"> </span>bc +<span class="st"> </span>wc +<span class="st"> </span>prof, <span class="dt">data =</span> Duncan)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = prestige ~ bc + wc + prof, data = Duncan)
## 
## Coefficients:
## (Intercept)       bcTRUE       wcTRUE     profTRUE  
##       80.44       -57.68       -43.78           NA</code></pre>
<p>R runs the regression, but coefficient and standard errors for <code>prof</code> are set to <code>NA</code>.</p>
<p>You should not rely on the software to fix this for you; once you (or the software) notices the problem check the reasons it occurred. The rewrite your regression to remove whatever was creating linearly dependent variables in <span class="math inline">\(\mat{X}\)</span>.</p>
</div>
</div>
<div id="less-than-perfect-collinearity" class="section level3">
<h3><span class="header-section-number">5.0.2</span> Less-than Perfect Collinearity</h3>
<p>What happens if variables are not linearly dependent, but nevertheless highly correlated? If <span class="math inline">\(\Cor(\vec{x}_1, vec{x}_2) = 1\)</span>, then they are linearly dependent and the regression cannot be estimated (see above). But if <span class="math inline">\(\Cor(\vec{x}_1, vec{x}_2) = 0.99\)</span>, the OLS can estimate unique values of of <span class="math inline">\(\hat\beta\)</span>. However, it everything was fine with OLS estimates until, suddenly, when there is linearly independence everything breaks. The answer is yes, and no. As <span class="math inline">\(|\Cor(\vec{x}_1, \vec{x}_2)| \to 1\)</span> the standard errors on the coefficients of these variables increase, but OLS as an estimator works correctly; <span class="math inline">\(\hat\beta\)</span> and <span class="math inline">\(\se{\hat\beta}\)</span> are unbiased. With multicollinearly, OLS gives you the “right” answer, but it cannot say much with certainty.</p>
<p><em>Insert plot of highly correlated variables and their coefficients.</em></p>
<p><em>Insert plot of uncorrelated variables and their coefficients.</em></p>
<div id="what-to-do-about-it-1" class="section level4">
<h4><span class="header-section-number">5.0.2.1</span> What to do about it?</h4>
<p>Remember multicollinearity does not violate the assumptions of OLS. If all the other assumptions hold, then OLS is giving you unbiased coefficients and standard errors. What multicollinearity is indicating is that you may not be able to answer the question with the precision you would like.</p>
<ol style="list-style-type: decimal">
<li>If the variable(s) of interest are highly correlated with other variables, then it means that there is not enough variation, controlling for other factors. You may check that you are not controlling for “post-treatment” variables. Dropping control variables if they are correctly included will bias your estimates. But otherwise, there is little you can do other than get more data. You could re-consider your research design and question. What does it mean if there is that little variation in the treatment variable after controlling for other factors?</li>
<li>If control variables are highly correlated with each other, it does not matter. You should not be interpreting their coefficients, so their standard errors do not matter. In fact, controlling for several similar, but correlated variables, may be useful in order to offset measurement error in any one of them.</li>
</ol>

</div>
</div>
<div id="non-constant-variances-and-correlated-errors" class="section level2">
<h2><span class="header-section-number">5.1</span> Non-constant Variances and Correlated Errors</h2>
<p>The OLS coefficient standard errors, <span class="math display">\[
\Var({\hat{\vec{\beta}}}) = \sigma^2 (\mat{X}\T \mat{X})^{-1}
\]</span> depends on the assumption of homoskedastic errors. Homoskedasticity has two components,</p>
<ol style="list-style-type: decimal">
<li>Disturbances have the same variance, <span class="math inline">\(\Var(\varepsilon_i) = \sigma^2\)</span> for all <span class="math inline">\(i\)</span>.</li>
<li>No correlation between disturbances, <span class="math inline">\(\Cov(\varepsilon_i, \varepsilon_j) = 0\)</span> for all <span class="math inline">\(i \neq j\)</span>.</li>
</ol>
<p>Either or both of these components can be violated, and when they are, the standard errors of the OLS estimator are incorrect.</p>
<p>The general OLS variance-covariance matrix of the coefficients is, <span class="math display">\[
\Var(\hat{\vec\beta}) = (\mat{X}\T \mat{X})^{-1} (\mat{X} \Sigma \mat{X}) (\mat{X}\T \mat{X})^{-1}
\]</span> where <span class="math inline">\(\mat{Sigma}\)</span> is the correlation of the disturbances, <span class="math inline">\(\vec\varepsilon\)</span>, <span class="math display">\[
\mat{\Sigma} = \vec{\varepsilon}\T \vec{\varepsilon} =
\begin{bmatrix}
\sigma_1^2 &amp; \sigma_1 \sigma_2 &amp; \cdots &amp; \sigma_1 \sigma_N \\
\sigma_2 \sigma_1 &amp; \sigma_2^2 &amp; \cdots &amp; \sigma_2 \sigma_N \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\sigma_N \sigma_1 &amp; \sigma_N \sigma_2 &amp; \cdots &amp; \sigma_N^2 \\
\end{bmatrix}
\]</span></p>
<p>When we assume homoskedasticity, the variance-covariance matrix of <span class="math inline">\(\varepsilon\)</span> is <span class="math display">\[
\mat{\Sigma} =
\begin{bmatrix}
\sigma^2 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma^2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \sigma^2
\end{bmatrix}
= \sigma^2 \begin{bmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 1 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 1
\end{bmatrix}
=  \sigma^2 \mat{I}_{N}
\]</span>,</p>
<p>Under homoskedasticity, the sampling distribution of <span class="math display">\[
\begin{aligned}[t]
\Var(\beta | \mat{X})
&amp;= (\mat{X}&#39; \mat{X})^{-1} \mat{X}&#39; \mat{\Sigma} \mat{X} (\mat{X}&#39; \mat{X})^{-1} \\
&amp;= (\mat{X}&#39; \mat{X})^{-1} \mat{X}&#39; \sigma^2 \mat{I}_N \mat{X} (\mat{X} &#39;\mat{X})^{-1} \\
&amp;= \sigma^2 (\mat{X}&#39; \mat{X})^{-1} \mat{X}&#39; \mat{X} (\mat{X} &#39;\mat{X})^{-1} \\
&amp;= \sigma^2 (\mat{X}&#39; \mat{X})^{-1}
\end{aligned}
\]</span> To estimate <span class="math inline">\(\Var(\hat{\vec\beta}|\mat{X})\)</span>, replace <span class="math inline">\(\sigma^2\)</span> with <span class="math inline">\(\hat{\sigma}^2\)</span>, where <span class="math display">\[
\hat{\sigma}^2 = \frac{1}{N - K - 1} \sum \varepsilon_i^2
\]</span></p>
<p><strong>Q.</strong> What if the assumption of homoskedasticity isn’t true?</p>
<p><strong>A.</strong> The coefficients <span class="math inline">\(\hat{\vec\beta}\)</span> are unbiased, but the standard errors <span class="math inline">\(\hat{\se}(\hat{\vec{\beta}})\)</span> are biased.</p>
<p>Since we don’t know <span class="math inline">\(\mat{\Sigma}\)</span>, why not simply estimate the elements of <span class="math inline">\(\mat{\Sigma}\)</span> along with <span class="math inline">\(\vec{\beta}\)</span>? The problem is that there are <span class="math inline">\(N\)</span> observations, and <span class="math inline">\(\mat\Sigma\)</span> is an <span class="math inline">\(N \times N\)</span> matrix with <span class="math inline">\((N * (N + 1)) / 2\)</span> elements since it is symmetric. So some structure needs to be put on <span class="math inline">\(\mat{\Sigma}\)</span>, i.e. we need to make additional assumptions, to estimate <span class="math inline">\(\mat{\Sigma}\)</span>. As we will see, we can make less restrictive assumptions than homoskedasticity, i.e. <span class="math inline">\(\mat{\Sigma} = \sigma^2 \mat{I}\)</span>, but will always have to assume some sort of structure in <span class="math inline">\(\mat{\Sigma}\)</span>.</p>
<p>There’s only one way for homoskedasticity to be correct (<span class="math inline">\(\mat{\Sigma} = \sigma^2 \mat{I}\)</span>), and many ways for it to be wrong. We’ll consider a few of the most common, and methods to deal with them.</p>
<ol style="list-style-type: decimal">
<li>Heteroskedasticity</li>
<li>Autocorrelation</li>
<li>Clustering</li>
</ol>
<div id="heteroskedasticity" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Heteroskedasticity</h3>
<p>The homoskedastic case assumes that each error term has its own variance. In the heteroskedastic case, each disturbance may have its own variance, but they are still uncorrelated (<span class="math inline">\(\mat{\Sigma}\)</span> is diagonal) <span class="math display">\[
\mat{\Sigma} =
\begin{bmatrix}
\sigma_1^2 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma_2^2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \sigma_N^2
\end{bmatrix} = \sigam^2 \mat{I}_N
\]</span> With homoskedasticity the estimator of the variance covariance matrix takes a particularly simple form, <span class="math display">\[
\Var(\hat{\beta} | \mat{X}) &amp;= (\mat{X}&#39; \mat{X})^{-1} \mat{X}&#39; \mat{\Sigma} \mat{X} (\mat{X}
&#39;\mat{X})^{-1} \\
&amp;= \hat\sigma^2 (\mat{X}&#39; \mat{X})^{-1}
\]</span> where <span class="math display">\[
\hat\sigma^2 = \frac{\sum \hat{\epsilon}^2}{N - K - 1}
\]</span></p>
<p>The consequences of violating homoskedasticity are,</p>
<ul>
<li>Biased (often downward) standard errors, <span class="math inline">\(\E(\se{\hat\beta}) \neq \sd(\beta)\)</span>.</li>
<li>Test statistics do hot have <span class="math inline">\(t\)</span> or <span class="math inline">\(F\)</span> distributions.</li>
<li><span class="math inline">\(\alpha\)</span>-level tests have the wrong level of Type I error. E.g. a 5% test will not have 5% Type I errors.</li>
<li>Confidence intervals do not have the correct coverage. E.g. a 95% confidence does not contain the true mean in 95% of samples.</li>
<li>OLS is not BLUE.</li>
<li><span class="math inline">\(\hat{\vec\beta}\)</span> is still and unbiased and consistent estimator for <span class="math inline">\(\vec\beta\)</span>.</li>
</ul>
<p>So why don’t we estimate all the elements of <span class="math inline">\(\mat\Sigma\)</span> like we estimate <span class="math inline">\(\beta\)</span>? Since <span class="math inline">\(\mat\Sigma\)</span> is an <span class="math inline">\(N \times N\)</span> symmetric matrix, it has $(N (N + 1)) / 2 $ elements. But we have only <span class="math inline">\(N\)</span> data points to estimate them. In order to estimate <span class="math inline">\(\mat\Sigma\)</span> we cannot estimate arbitrary correlations in <span class="math inline">\(\mat\Sigma\)</span>, but we need to apply some structure to the variance-covariance matrix in order to reduce the number of elements to estimate.</p>
<ol style="list-style-type: decimal">
<li>Heteroskedasticity</li>
<li>Clustered Standard Errors</li>
<li>Serial Correlation</li>
</ol>
<p>In general there are two types of methods to deal with issues in the error,</p>
<ol style="list-style-type: decimal">
<li>New estimators that model the error process and estimate elements of <span class="math inline">\(\mat\Sigma\)</span> simultaneously with the coefficients <span class="math inline">\(\beta\)</span>. This includes weighted least squares (heteroskedasticity), Prais-Wiston (AR(1) errors). These methods produce <span class="math inline">\(\hat\beta \neq \hat\beta_{OLS}\)</span>.</li>
<li>Since OLS produces unbiased and consistent estimates of <span class="math inline">\(\hat\beta\)</span>, we keep the coefficient estimates, but correct the variance-covariance matrix <span class="math inline">\(\hat\Var{\beta}\)</span>.</li>
</ol>
</div>
<div id="heteroskedasticity-1" class="section level3">
<h3><span class="header-section-number">5.1.2</span> Heteroskedasticity</h3>
<p>In heteroskedasticity, the errors are still independent, i.e. <span class="math inline">\(\mat\Sigma\)</span> is still diagonal, but the variance elements on the diagonal are not equal, <span class="math display">\[
\Var(\vec\varepsilon|\mat{X}) =
\begin{bmatrix}
\sigma_1^2 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma_2^2 &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; \sigma_N^2
\end{bmatrix} .
\]</span> While each observation has a difference variance, they are still independent, <span class="math inline">\(\Cov(\epsilon_i, \epsilon_j | \mat{X}) = 0\)</span>, for all <span class="math inline">\(i \neq j\)</span>.</p>
<ul>
<li>Example in difference in means</li>
<li>Example with a continuous <span class="math inline">\(X\)</span></li>
<li>Simulation of the effects of violations</li>
</ul>
</div>
<div id="diagnostics" class="section level3">
<h3><span class="header-section-number">5.1.3</span> Diagnostics</h3>
<ul>
<li>Plot residuals vs. fitted values</li>
<li>Spread-level plots (<code>car::spreadLevel</code>),</li>
<li>Compare Robust SE vs. non-robust SE. If they are different</li>
<li>Tests: Breusch-Pagan (<code>lmtest::bptest</code>, <code>car::ncvTest</code>),</li>
</ul>
</div>
<div id="dealing-with-heteroskedasticity" class="section level3">
<h3><span class="header-section-number">5.1.4</span> Dealing with Heteroskedasticity</h3>
<ol style="list-style-type: decimal">
<li>Transform the dependent variable. For example, <span class="math inline">\(\log\)</span> the dependent variable.</li>
<li>Model heteroskedasticity using Weighted Least Squares (WLS)</li>
<li>Use OLS with an estimator of <span class="math inline">\(\Var(\hat{\vec\beta})\)</span> that is <strong>robust</strong> to heteroskedasticity</li>
<li>Admit that OLS is insufficient, and use a different model</li>
</ol>
<ul>
<li>If the form of heteroskedasticity follows a particularly simple form, transform the dependent variable. For example, log the dependent variable.</li>
<li>If the form of the heteroskedasticity is known: weighted least squares. <code>lm()</code> with the <code>weights</code> argument.</li>
<li>If the form of the heteroskedasticity is unknown: Huber-White heteroskedasticity consistent standard errors. See <strong>sandwich</strong> package. You can calculate the heteroskedasticity correct covariance matrix using <code>sandwich::vcovHC</code> and then use <code>lmtest::coeftest</code> to calculate p-values and standard-errors.</li>
</ul>
<div id="advice" class="section level4">
<h4><span class="header-section-number">5.1.4.1</span> Advice</h4>
<p>In practice, often diagnostics are not conducted and robust standard errors are used. This is partially due to the ease with which heteroskedasticity consistent standard errors can be calculated in Stata (see <code>, robust</code>).</p>
<p>Robust standard errors, especially when used with MLE estimators, is controversial.</p>
<ul>
<li>See Freedman</li>
<li>See King and Roberts</li>
</ul>
<p>But this depends on how they are being used, see Angrist.</p>
</div>
</div>
<div id="clustering" class="section level3">
<h3><span class="header-section-number">5.1.5</span> Clustering</h3>
<ul>
<li>Clusters: <span class="math inline">\(g = 1, \dots, G\)</span>.</li>
<li>Units: <span class="math inline">\(i = 1, \dots, N_g\)</span>.</li>
<li><span class="math inline">\(N_g\)</span> is the number of observations in cluster <span class="math inline">\(g\)</span></li>
<li><span class="math inline">\(N = \sum_g N_g\)</span> is the total observations</li>
<li><p>Units (usually) belong to a single cluster</p></li>
<li>voters in household</li>
<li>individuals in states</li>
<li><p>students in classes</p></li>
<li>This is particularly important when the outcome varies at the unit-level, <span class="math inline">\(y_ij\)</span> and the main independent variable varies at the cluster level.</li>
<li><p>Ignoring clustering overstates the effective number of individuals in the data.</p></li>
</ul>
<p>Clustered dependence <span class="math display">\[
\begin{aligned}[t]
y_{ig} &amp;= \beta_0 + \beta_1 x_{ig} +\epsilon_{ig} \\
&amp;= \beta_0 + \beta_1 x_{ig} + \nu_g + \eta_{ig}
\end{aligned}
\]</span> Then the cluster error is <span class="math display">\[
\nu \sim N(0, rho \sigma^2),
\]</span> and the individual error is <span class="math display">\[
\eta_{ig} \sim N(0, (1 - \rho) \sigma^2) .
\]</span> The cluster and unit errors are assumed to be independent of each other. <span class="math inline">\(\rho \in (0, 1)\)</span> is the <em>within-cluster correlation</em>. If we ignore the cluster, and use <span class="math inline">\(\eta_{ig}\)</span> as the error, the variance is <span class="math inline">\(\sigma^2\)</span> <span class="math display">\[
\Var(\eta_{ig}) &amp;= \Var(\nu_g +\eta_{ig}) \\
&amp;= \Var(\nu_g) + \Var(\varepsilon_{ig}) \\
&amp;= \rho \sigma^2 + (1 - \rho) \sigma^2 = \sigma^2
\]</span> The Covariance between units in the same cluster is <span class="math display">\[
\Cov(\varepsilon_{ig}, \varepsilon_{ig}) = \rho \sigma^2,
\]</span> meaning that the correlation for units within a group is <span class="math display">\[
\Cor(\varepsilon_{ig}, \varepsilon_{ig}) = rho .
\]</span> But, there is zero covariance and correlation between units in different clusters. For example, the covariance matrix of <span class="math display">\[
\vec\varepsilon = \begin{bmatrix} \varepsilon_{1,1} &amp;  \varepsilon_{2,1} &amp; \varepsilon_{3,1} &amp; \varepsilon_{4,2} &amp; \varepsilon_{5,2} \end{bmatrix}&#39;
\]</span> is <span class="math display">\[
\Var(\vec\varepsilon | \mat{X}) = \mat{\Sigma} =
\begin{bmatrix}
\sigma^2 &amp; \sigma \rho &amp; \sigma \rho &amp; 0 &amp; 0 \\
\sigma \rho &amp; \sigma^2 &amp; \sigma \rho &amp; 0 &amp; 0 \\
\end{bmatrix}
\]</span></p>
<p>More generally, the variance-covariance matrix of a the errors is <strong>block diagonal</strong>, <span class="math display">\[
\Var(\varepsilon | \mat{X}) = \mat{\Sigma} =
\begin{bmatrix}
\mat{\Sigma}_1 &amp; \mat{0}_{N_1 \times N_2} &amp; \cdots &amp; \mat{0}_{N_1 \times N_G} \\
\mat{0}_{N_2 \times N_1} &amp; \mat{\Sigma}_2 &amp; \cdots &amp; \mat{0}_{N_2 \times N_G} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\mat{0}_{N_G \times N_1} &amp; \mat{0}_{N_G \times N_2} &amp; \cdots  &amp; \mat{\Sigma}_G
\end{bmatrix}
\]</span> where <span class="math inline">\(\mat{Sigma}_g\)</span> are the covariance matrices of each cluster, and <span class="math inline">\(\mat{0}\)</span> are matrices of zeros of the appropriate sizes.</p>
<p>There are several ways to address clustering, including:</p>
<ol style="list-style-type: decimal">
<li>Include an indicator variable for each cluster</li>
<li>Random effects models</li>
<li>Cluster-robust (“clustered”) standard error</li>
<li>Aggregate data to the cluster data and use WLS with <span class="math inline">\(\bar{y}_g = \frac{1}{N}_g \sum_i y_{ig}\)</span> where the clusters are weighted by <span class="math inline">\(N_g\)</span>.</li>
</ol>
<p>Cluster-robust standard errors uses the observed residuals, <span class="math inline">\(\hat\varepsilon_i\)</span>, to estimate a the variance-covariance matrix <span class="math inline">\(\hat\Var{\hat{\vec{beta}}\)</span> which allows units to be independent across clusters and dependent within clusters.</p>
<p><span class="math display">\[
\hat{\Sigma} =
\begin{bmatrix}
\hat{\varepsilon}_1^2 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \hat{\varepsilon}_2^2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \hat{\varepsilon}_N^2
\end{bmatrix}
= \hat{\vec{\varepsilon}} \mat{I}_N \hat{\vec{\varepsilon}}
\]</span></p>
<ul>
<li>CRSE do not change <span class="math inline">\(\hat\vec{\beta}\)</span>. Thus they do not fix bias in the coefficients.</li>
<li>CRSE is a consistent estimator of <span class="math inline">\(\Var{\hat\vec{\beta}}\)</span> given the clustered dependence.
<ul>
<li>This relies on the assumption of independence between clusters</li>
<li>Does not rely on the model form</li>
<li>CRSE are usually larger than classic standard errors</li>
</ul></li>
<li>Consistency of the CRSE are in the number of groups, not the number of individuals
<ul>
<li>CRSE work well when the number of <strong>clusters</strong> is large (&gt; 50)</li>
<li>Alternative: use a block bootstrap</li>
</ul></li>
</ul>
<p>See the R package <strong>plr</strong> (Panel linear models in R).</p>
<p>See Cameron and Miller, <a href="http://cameron.econ.ucdavis.edu/research/Cameron_Miller_Cluster_Robust_October152013.pdf">Practioner’s Guide to Cluster-Robust Inference</a>.</p>
</div>
<div id="auto-correlation" class="section level3">
<h3><span class="header-section-number">5.1.6</span> Auto-correlation</h3>
<p>More general case allows for heteroskedasticity, and auto-correlation (<span class="math inline">\(\Cov(\varepsilon_i, \varepsilon_j) \neq 0\)</span>), <span class="math display">\[
\mat{\Sigma} =
\begin{bmatrix}
\sigma_1^2 &amp; \sigma_{1,2} &amp; \cdots &amp; \sigma_{1,N} \\
\sigma_{2,1} &amp; \sigma_2^2 &amp; \cdots &amp; \sigma_{2,N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\sigma_{N,1} &amp; \sigma_{N,2} &amp; \cdots &amp; \sigma_N^2
\end{bmatrix}
\]</span> As with heteroskedasticity, OLS with be unbiased, but the standard errors will be incorrect.</p>
<p>Tests</p>
<ul>
<li>Breusch-Godfrey Test (<code>lmtest::bgtest</code>)</li>
</ul>
<p>Solution</p>
<ul>
<li>If the form is known: Prais-Wiston, include lagged dependent variable.</li>
<li>Huber-White Heteroskedasticity and Autocorrelation Robust standard errors. These are an extension of the heteroskedasticity robust standard errors to also include autocorrelation. See <strong>sandwich</strong> function <code>hcacVCOV</code>.</li>
</ul>
</div>
<div id="clustered-and-panel-standard-errors" class="section level3">
<h3><span class="header-section-number">5.1.7</span> Clustered and Panel Standard Errors</h3>
<p>Panel Corrected Standard</p>
<p>See the R package <strong>plm</strong></p>
</div>
</div>
<div id="non-normal-errors" class="section level2">
<h2><span class="header-section-number">5.2</span> Non-Normal Errors</h2>
<p>Non-normal errors pose several problems for OLS:</p>
<ol style="list-style-type: decimal">
<li>If the sample size is small, normal errors are required for correct confidence interval coverage and p-values in tests. In large samples, CLT properties of OLS kick in and the normality of errors assumption is not needed to justify the sampling distributions of the test statistics.</li>
<li>Heavy-tailed errors threaten the efficiency of OLS estimate.</li>
<li>Skewed or multi-modal errors suggest that the conditional mean <span class="math inline">\(E(Y | \mat{X})\)</span>, estimated by OLS may not be a good summary of the data.</li>
</ol>
<p>How to diagnose? Plot the quantiles of the <em>studentized</em> residuals (see the section on outliers) against the expected quantiles of a normal distribution using a QQ-plot.</p>
<p>In general, non-normal errors are a minor issue, and towards the bottom in priority of problems in inference.</p>
<p>How to fix?</p>
<p>There are a couple of ways to fix this:</p>
<ol style="list-style-type: decimal">
<li>Transform the dependent variable and use OLS</li>
<li>Add different sets of covariates. This is especially likely with multi-modal error distributions, which could suggest an omitted categorical variable.</li>
<li>Use a different model other than OLS.</li>
</ol>
<p><strong>R</strong> Calculate Studentized residuals with the function <code>rstudent</code> and a QQ-plot using <a href="http://docs.ggplot2.org/current/stat_qq.html">stat_qq</a> in <strong>ggplot2</strong>.</p>
<p>Diagnostics</p>
<ul>
<li>QQ-plot of the Studentized residuals</li>
</ul>
<p>Important things to remember</p>
<ul>
<li>The assumption is not that <span class="math inline">\(Y\)</span> has a normal distribution, it is that the errors <em>after</em> including covariates are normal.</li>
<li>While non-normal errors will not bias <span class="math inline">\(\beta\)</span> and have little effect on the standard errors unless the sample size is small, they could serve as a warning that your model is mis-specified, or that the conditional expectation of <span class="math inline">\(Y\)</span> is not good summary.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="interpreting-regression-coefficients.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="non-standard-errors.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/UW-POLS503/pols503-notes/edit/gh-pages/multicollinearity.Rmd",
"text": "Edit"
},
"download": ["pols503-notes.pdf", "pols503-notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>


</body>

</html>
