<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>POLS 503: Advanced Quantitative Political Methodology: The Notes</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>">
  <meta name="generator" content="bookdown 0.0.60 and GitBook 2.6.7">

  <meta property="og:title" content="POLS 503: Advanced Quantitative Political Methodology: The Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>" />
  <meta name="github-repo" content="UW-POLS503/pols503-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="POLS 503: Advanced Quantitative Political Methodology: The Notes" />
  
  <meta name="twitter:description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>" />
  

<meta name="author" content="Jeffrey B. Arnold">

<meta name="date" content="2016-04-06">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="multiple-regression.html">
<link rel="next" href="matrix-algebra-review.html">

<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







$$
\usepackage{booktabs}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\cor}{Cor}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator*{\argmin}{arg min}
\DeclareMathOperator*{\argmax}{arg max}

\newcommand{\distr}[1]{\ensuremath{\mathcal{#1}}}
\newcommand{\dnorm}{\distr{N}}
$$


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="review-of-statistics.html"><a href="review-of-statistics.html"><i class="fa fa-check"></i><b>2</b> Review of Statistics</a></li>
<li class="chapter" data-level="3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>3</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="4" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>4</b> Multiple Regression</a></li>
<li class="chapter" data-level="5" data-path="regression-in-matrix-form.html"><a href="regression-in-matrix-form.html"><i class="fa fa-check"></i><b>5</b> Regression in matrix form</a><ul>
<li class="chapter" data-level="5.1" data-path="regression-in-matrix-form.html"><a href="regression-in-matrix-form.html#multiple-linear-regression-in-matrix-form"><i class="fa fa-check"></i><b>5.1</b> Multiple linear regression in matrix form</a></li>
<li class="chapter" data-level="5.2" data-path="regression-in-matrix-form.html"><a href="regression-in-matrix-form.html#residuals"><i class="fa fa-check"></i><b>5.2</b> Residuals</a></li>
<li class="chapter" data-level="5.3" data-path="regression-in-matrix-form.html"><a href="regression-in-matrix-form.html#ols-estimator-in-matrix-form"><i class="fa fa-check"></i><b>5.3</b> OLS estimator in matrix form</a></li>
<li class="chapter" data-level="5.4" data-path="regression-in-matrix-form.html"><a href="regression-in-matrix-form.html#scalar-inverses"><i class="fa fa-check"></i><b>5.4</b> Scalar inverses</a></li>
<li class="chapter" data-level="5.5" data-path="regression-in-matrix-form.html"><a href="regression-in-matrix-form.html#matrix-inverses"><i class="fa fa-check"></i><b>5.5</b> Matrix inverses</a></li>
<li class="chapter" data-level="5.6" data-path="regression-in-matrix-form.html"><a href="regression-in-matrix-form.html#inference"><i class="fa fa-check"></i><b>5.6</b> Inference</a></li>
<li class="chapter" data-level="5.7" data-path="regression-in-matrix-form.html"><a href="regression-in-matrix-form.html#back-to-ols"><i class="fa fa-check"></i><b>5.7</b> Back to OLS</a></li>
<li class="chapter" data-level="5.8" data-path="regression-in-matrix-form.html"><a href="regression-in-matrix-form.html#intuition-for-the-ols-in-matrix-form"><i class="fa fa-check"></i><b>5.8</b> Intuition for the OLS in matrix form</a></li>
<li class="chapter" data-level="5.9" data-path="regression-in-matrix-form.html"><a href="regression-in-matrix-form.html#most-general-ols-assumptions"><i class="fa fa-check"></i><b>5.9</b> Most general OLS assumptions</a></li>
<li class="chapter" data-level="5.10" data-path="regression-in-matrix-form.html"><a href="regression-in-matrix-form.html#no-perfect-collinearity"><i class="fa fa-check"></i><b>5.10</b> No perfect collinearity</a></li>
<li class="chapter" data-level="5.11" data-path="regression-in-matrix-form.html"><a href="regression-in-matrix-form.html#expected-values-of-vectors"><i class="fa fa-check"></i><b>5.11</b> Expected values of vectors</a></li>
<li class="chapter" data-level="5.12" data-path="regression-in-matrix-form.html"><a href="regression-in-matrix-form.html#ols-is-unbiased"><i class="fa fa-check"></i><b>5.12</b> OLS is unbiased</a></li>
<li class="chapter" data-level="5.13" data-path="regression-in-matrix-form.html"><a href="regression-in-matrix-form.html#variance-covariance-matrix-of-random-vectors"><i class="fa fa-check"></i><b>5.13</b> Variance-covariance matrix of random vectors</a></li>
<li class="chapter" data-level="5.14" data-path="regression-in-matrix-form.html"><a href="regression-in-matrix-form.html#matrix-version-of-homoskedasticity"><i class="fa fa-check"></i><b>5.14</b> Matrix version of homoskedasticity</a></li>
<li class="chapter" data-level="5.15" data-path="regression-in-matrix-form.html"><a href="regression-in-matrix-form.html#sampling-variance-for-ols-estimates"><i class="fa fa-check"></i><b>5.15</b> Sampling variance for OLS estimates</a></li>
<li class="chapter" data-level="5.16" data-path="regression-in-matrix-form.html"><a href="regression-in-matrix-form.html#inference-in-the-general-setting"><i class="fa fa-check"></i><b>5.16</b> Inference in the general setting</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="matrix-algebra-review.html"><a href="matrix-algebra-review.html"><i class="fa fa-check"></i><b>6</b> Matrix algebra review</a><ul>
<li class="chapter" data-level="6.1" data-path="matrix-algebra-review.html"><a href="matrix-algebra-review.html#matrices-and-vectors"><i class="fa fa-check"></i><b>6.1</b> Matrices and vectors</a></li>
<li class="chapter" data-level="6.2" data-path="matrix-algebra-review.html"><a href="matrix-algebra-review.html#examples-of-matrices"><i class="fa fa-check"></i><b>6.2</b> Examples of matrices</a></li>
<li class="chapter" data-level="6.3" data-path="matrix-algebra-review.html"><a href="matrix-algebra-review.html#vectors"><i class="fa fa-check"></i><b>6.3</b> Vectors</a></li>
<li class="chapter" data-level="6.4" data-path="matrix-algebra-review.html"><a href="matrix-algebra-review.html#vector-examples"><i class="fa fa-check"></i><b>6.4</b> Vector examples</a></li>
<li class="chapter" data-level="6.5" data-path="matrix-algebra-review.html"><a href="matrix-algebra-review.html#transpose"><i class="fa fa-check"></i><b>6.5</b> Transpose</a></li>
<li class="chapter" data-level="6.6" data-path="matrix-algebra-review.html"><a href="matrix-algebra-review.html#transposing-vectors"><i class="fa fa-check"></i><b>6.6</b> Transposing vectors</a></li>
<li class="chapter" data-level="6.7" data-path="matrix-algebra-review.html"><a href="matrix-algebra-review.html#transposing-in-r"><i class="fa fa-check"></i><b>6.7</b> Transposing in R</a></li>
<li class="chapter" data-level="6.8" data-path="matrix-algebra-review.html"><a href="matrix-algebra-review.html#write-matrices-as-vectors"><i class="fa fa-check"></i><b>6.8</b> Write matrices as vectors</a></li>
<li class="chapter" data-level="6.9" data-path="matrix-algebra-review.html"><a href="matrix-algebra-review.html#addition-and-subtraction"><i class="fa fa-check"></i><b>6.9</b> Addition and subtraction</a></li>
<li class="chapter" data-level="6.10" data-path="matrix-algebra-review.html"><a href="matrix-algebra-review.html#scalar-multiplication"><i class="fa fa-check"></i><b>6.10</b> Scalar multiplication</a></li>
<li class="chapter" data-level="6.11" data-path="matrix-algebra-review.html"><a href="matrix-algebra-review.html#the-linear-model-with-new-notation"><i class="fa fa-check"></i><b>6.11</b> The linear model with new notation</a></li>
<li class="chapter" data-level="6.12" data-path="matrix-algebra-review.html"><a href="matrix-algebra-review.html#matrix-multiplication-by-a-vector"><i class="fa fa-check"></i><b>6.12</b> Matrix multiplication by a vector</a></li>
<li class="chapter" data-level="6.13" data-path="matrix-algebra-review.html"><a href="matrix-algebra-review.html#back-to-regression"><i class="fa fa-check"></i><b>6.13</b> Back to regression</a></li>
<li class="chapter" data-level="6.14" data-path="matrix-algebra-review.html"><a href="matrix-algebra-review.html#matrix-multiplication"><i class="fa fa-check"></i><b>6.14</b> Matrix multiplication</a></li>
<li class="chapter" data-level="6.15" data-path="matrix-algebra-review.html"><a href="matrix-algebra-review.html#special-multiplications"><i class="fa fa-check"></i><b>6.15</b> Special multiplications</a></li>
<li class="chapter" data-level="6.16" data-path="matrix-algebra-review.html"><a href="matrix-algebra-review.html#special-matrices-and-jargon"><i class="fa fa-check"></i><b>6.16</b> Special matrices and jargon</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">POLS 503: Advanced Quantitative Political Methodology: The Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-in-matrix-form" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Regression in matrix form</h1>
<div id="multiple-linear-regression-in-matrix-form" class="section level2">
<h2><span class="header-section-number">5.1</span> Multiple linear regression in matrix form</h2>
<ul>
<li>Let <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> be the matrix of estimated regression coefficients:</li>
</ul>
<p><span class="math display">\[\widehat{\boldsymbol{\beta}} = \left[
\begin{array}{c}
    \widehat{\beta}_0 \\
    \widehat{\beta}_1 \\
    \vdots \\
    \widehat{\beta}_k
    \end{array}
\right]\]</span></p>
<ul>
<li>Now, then our estimated regression fits will be:</li>
</ul>
<p><span class="math display">\[\widehat{\mathbf{y}} = \mathbf{X}\widehat{\boldsymbol{\beta}}\]</span></p>
<ul>
<li>It might be helpful to see this again more written out:</li>
</ul>
\begin{small}
\[\widehat{\mathbf{y}} = \left[
\begin{array}{c}
    \widehat{y}_1 \\
    \widehat{y}_2 \\
    \vdots \\
    \widehat{y}_n
    \end{array}
\right]  = \mathbf{X}\widehat{\boldsymbol{\beta}} =
\left[
\begin{array}{c}
    1\widehat{\beta}_0  + x_{11}\widehat{\beta}_1 +  x_{12}\widehat{\beta}_2 + \dots + x_{1K} \widehat{\beta}_K \\
    1\widehat{\beta}_0 + x_{21}\widehat{\beta}_1 + x_{22}\widehat{\beta}_2  + \dots + x_{2K}\widehat{\beta}_K \\
   \vdots  \\
   1\widehat{\beta}_0 + x_{n1}\widehat{\beta}_1 + x_{n2}\widehat{\beta}_2  + \dots + x_{nK}\widehat{\beta}_K
\end{array}
\right]
\]
\end{small}
<ul>
<li>Just a tad bit more tidy, I’d say!</li>
</ul>
</div>
<div id="residuals" class="section level2">
<h2><span class="header-section-number">5.2</span> Residuals</h2>
<p>We can easily write the <strong>residuals</strong> in matrix form: <span class="math display">\[\widehat{\mathbf{u}} = \mathbf{y} - \mathbf{X}\widehat{\boldsymbol{\beta}}\]</span></p>
<p>Our goal as usual is to minimize the sum of the squared residuals, which we saw earlier we can write:</p>
<p><span class="math display">\[\widehat{\mathbf{u}}&#39;\widehat{\mathbf{u}} = (\mathbf{y} - \mathbf{X}\widehat{\boldsymbol{\beta}})&#39;(\mathbf{y} - \mathbf{X}\widehat{\boldsymbol{\beta}})\]</span></p>
</div>
<div id="ols-estimator-in-matrix-form" class="section level2">
<h2><span class="header-section-number">5.3</span> OLS estimator in matrix form</h2>
<ul>
<li>By finding the values of <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> that minimizes the sum of the squared residuals, we arrive at the following formula for the OLS estimator:</li>
</ul>
<p><span class="math display">\[\mathbf{X}&#39;\mathbf{X}\widehat{\boldsymbol{\beta}} = \mathbf{X}&#39;\mathbf{y}\]</span></p>
<ul>
<li>In order to isolate <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>, we need to move the <span class="math inline">\(\mathbf{X}&#39;\mathbf{X}\)</span> term to the other side of the equals sign. -We’ve learned about matrix multiplication, but what about matrix “division”?</li>
</ul>
</div>
<div id="scalar-inverses" class="section level2">
<h2><span class="header-section-number">5.4</span> Scalar inverses</h2>
<ul>
<li>What is division in its simplest form? <span class="math inline">\(\frac{1}{a}\)</span> is the value such that <span class="math inline">\(a\frac{1}{a} = 1\)</span>:</li>
<li><p>For some algebraic expression: <span class="math inline">\(au = b\)</span>, let’s solve for <span class="math inline">\(u\)</span>: <span class="math display">\[\begin{aligned}
\frac{1}{a}au &amp;= \frac{1}{a}b\\
u &amp;= \frac{b}{a}\\
\end{aligned}\]</span></p></li>
<li><p>Need a matrix version of this: <span class="math inline">\(\frac{1}{a}\)</span>.</p></li>
</ul>
</div>
<div id="matrix-inverses" class="section level2">
<h2><span class="header-section-number">5.5</span> Matrix inverses</h2>
<p><strong>Definition:</strong> If it exists, the <strong>inverse</strong> of square matrix <span class="math inline">\(\mathbf{A}\)</span>, denoted <span class="math inline">\(\mathbf{A}^{-1}\)</span>, is the matrix such that <span class="math inline">\(\mathbf{A}^{-1}\mathbf{A} = \mathbf{I}\)</span>.</p>
<p>We can use the inverse to solve (systems of) equations: <span class="math display">\[\begin{aligned}
\mathbf{Au} &amp;= \mathbf{b} \\
\mathbf{A^{-1}Au} &amp;= \mathbf{A^{-1}b} \\
\mathbf{Iu} &amp;= \mathbf{A^{-1}b} \\
\mathbf{u} &amp;= \mathbf{A^{-1}b} \\
\end{aligned}\]</span></p>
<p>If the inverse exists, we say that <span class="math inline">\(\mathbf{A}\)</span> is <strong>invertible</strong>, <strong>nonsingular</strong>, or <strong>nondegenerate</strong>. Not his implies that not all matrices are invertible. If a matrix is not invertible it is called <strong>singular</strong> or <strong>degenerate</strong>. All non-square matrices are not invertible. Square are invertible if all of its columns are linearly independent.</p>
</div>
<div id="inference" class="section level2">
<h2><span class="header-section-number">5.6</span> Inference</h2>
</div>
<div id="back-to-ols" class="section level2">
<h2><span class="header-section-number">5.7</span> Back to OLS</h2>
<p>Let’s assume, for now, that the inverse of <span class="math inline">\(\mathbf{X}&#39;\mathbf{X}\)</span> exists (we’ll come back to this)</p>
<p>Then we can write the OLS estimator as the following:</p>
<p><span class="math display">\[\widehat{\boldsymbol{\beta}} = (\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{y}\]</span></p>
<p>Memorize this: “x prime x inverse x prime y” sear it into your soul.</p>
</div>
<div id="intuition-for-the-ols-in-matrix-form" class="section level2">
<h2><span class="header-section-number">5.8</span> Intuition for the OLS in matrix form</h2>
<ul>
<li>What’s the intuition here?</li>
<li>First, note that the “numerator” <span class="math inline">\(\mathbf{X}&#39;\mathbf{y}\)</span> is roughly composed of the covariances between the columns of <span class="math inline">\(X\)</span> and <span class="math inline">\(y\)</span></li>
<li>Next, the “denominator” <span class="math inline">\(\mathbf{X}&#39;\mathbf{X}\)</span> is roughly composed of the sample variances and covariances of variables within <span class="math inline">\(\mathbf{X}\)</span></li>
<li>Thus, we have something like:</li>
</ul>
<p><span class="math display">\[\widehat{\boldsymbol{\beta}} \approx (\text{variance of } \mathbf{X})^{-1} (\text{covariance of } \mathbf{X} \text{ \&amp; } \mathbf{y})\]</span></p>
<ul>
<li>This is a rough sketch and isn’t strictly true, but it can provide intuition.</li>
<li>We’re also sidestepping the issues of what the variance of a matrix is for now.</li>
</ul>
</div>
<div id="most-general-ols-assumptions" class="section level2">
<h2><span class="header-section-number">5.9</span> Most general OLS assumptions</h2>
<ol style="list-style-type: decimal">
<li>Linearity: <span class="math inline">\(\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{u}\)</span></li>
<li>Random/iid sample: <span class="math inline">\((y_i, \mathbf{x}&#39;_i)\)</span> are a iid sample from the population.</li>
<li>No perfect collinearity: <span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\(n \times (K+1)\)</span> matrix with rank <span class="math inline">\(K+1\)</span></li>
<li>Zero conditional mean: <span class="math inline">\(\E[\mathbf{u}|\mathbf{X}] = \mathbf{0}\)</span></li>
<li>Homoskedasticity: <span class="math inline">\(\text{var}(\mathbf{u}|\mathbf{X}) = \sigma_u^2 \mathbf{I}_n\)</span></li>
<li>Normality: <span class="math inline">\(\mathbf{u}|\mathbf{X} \sim N(\mathbf{0}, \sigma^2_u\mathbf{I}_n)\)</span></li>
</ol>
</div>
<div id="no-perfect-collinearity" class="section level2">
<h2><span class="header-section-number">5.10</span> No perfect collinearity</h2>
<ul>
<li>In matrix form: <span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\(n \times (K+1)\)</span> matrix with rank <span class="math inline">\(K+1\)</span></li>
<li><strong>Definition</strong> The <strong>rank</strong> of a matrix is the maximum number of linearly independent columns.</li>
<li>If <span class="math inline">\(\mathbf{X}\)</span> has rank <span class="math inline">\(K+1\)</span>, then all of its columns are linearly independent</li>
<li>…and none of its columns are linearly dependent <span class="math inline">\(\implies\)</span> no perfect collinearity</li>
<li><span class="math inline">\(\mathbf{X}\)</span> has rank <span class="math inline">\(K+1 \implies (\mathbf{X}&#39;\mathbf{X})\)</span> is invertible</li>
<li>Just like variation in <span class="math inline">\(X\)</span> led us to be able to divide by the variance in simple OLS</li>
</ul>
</div>
<div id="expected-values-of-vectors" class="section level2">
<h2><span class="header-section-number">5.11</span> Expected values of vectors</h2>
<ul>
<li>The expected value of the vector is just the expected value of its entries.</li>
<li>Using the zero mean conditional error assumptions: <span class="math display">\[\E[\mathbf{u}|\mathbf{X}] = \left[\begin{array}{c} \E[u_1 | \mathbf{X}] \\ \E[u_2|\mathbf{X}] \\ \vdots \\ \E[u_n|\mathbf{X}] \end{array} \right] = \left[\begin{array}{c} 0 \\ 0 \\ \vdots \\ 0 \end{array} \right] = \mathbf{0}\]</span></li>
</ul>
</div>
<div id="ols-is-unbiased" class="section level2">
<h2><span class="header-section-number">5.12</span> OLS is unbiased</h2>
<ul>
<li>Under matrix assumptions 1-4, OLS is unbiased for <span class="math inline">\(\boldsymbol{\beta}\)</span>: <span class="math display">\[\E[\widehat{\boldsymbol{\beta}}] = \boldsymbol{\beta}\]</span></li>
</ul>
</div>
<div id="variance-covariance-matrix-of-random-vectors" class="section level2">
<h2><span class="header-section-number">5.13</span> Variance-covariance matrix of random vectors</h2>
<ul>
<li>The homoskedasticity assumption is different: <span class="math inline">\(\text{var}(\mathbf{u}|\mathbf{X}) = \sigma_u^2 \mathbf{I}_n\)</span></li>
<li>In order to investigate this, we need to know what the variance of a vector is.</li>
<li>The variance of a vector is actually a matrix:</li>
</ul>
<p><span class="math display">\[\text{var}[\mathbf{u}] = \Sigma_u = \left[ \begin{array}{cccc}
\text{var}(u_1) &amp; \text{cov}(u_1,u_2) &amp; \dots &amp; \text{cov}(u_1, u_n) \\
\text{cov}(u_2,u_1) &amp; \text{var}(u_2) &amp; \dots &amp; \text{cov}(u_2,u_n) \\
 \vdots &amp; &amp; \ddots &amp; \\
\text{cov}(u_n,u_1) &amp; \text{cov}(u_n,u_2) &amp; \dots &amp; \text{var}(u_{n})
 \end{array} \right]\]</span></p>
<ul>
<li>This matrix is symmetric since <span class="math inline">\(\text{cov}(u_i,u_j) = \text{cov}(u_i,u_j)\)</span></li>
</ul>
</div>
<div id="matrix-version-of-homoskedasticity" class="section level2">
<h2><span class="header-section-number">5.14</span> Matrix version of homoskedasticity</h2>
<ul>
<li>Once again: <span class="math inline">\(\text{var}(\mathbf{u}|\mathbf{X}) = \sigma_u^2 \mathbf{I}_n\)</span></li>
<li>Visually:</li>
</ul>
<p><span class="math display">\[\text{var}[\mathbf{u}] =  \sigma^2_u \mathbf{I}_n=
\left[
\begin{array}{ccccc}
\sigma_u^2 &amp; 0 &amp; 0 &amp; \dots &amp; 0 \\
 0 &amp; \sigma_u^2 &amp; 0 &amp; \dots &amp; 0 \\
 &amp; &amp; &amp; \vdots &amp; \\
 0 &amp; 0 &amp; 0 &amp; \dots &amp; \sigma_u^2
 \end{array}
\right]\]</span></p>
<ul>
<li>In less matrix notation:
<ul>
<li><span class="math inline">\(\text{var}(u_i) = \sigma^2_u\)</span> for all <span class="math inline">\(i\)</span> (constant variance)</li>
<li><span class="math inline">\(\text{cov}(u_i,u_j) = 0\)</span> for all <span class="math inline">\(i \neq j\)</span> (implied by iid)</li>
</ul></li>
</ul>
</div>
<div id="sampling-variance-for-ols-estimates" class="section level2">
<h2><span class="header-section-number">5.15</span> Sampling variance for OLS estimates</h2>
<ul>
<li><p>Under assumptions 1-5, the sampling variance of the OLS estimator can be written in matrix form as the following: <span class="math display">\[\text{var}[\widehat{\boldsymbol{\beta}}] = \sigma^2_u(\mathbf{X}&#39;\mathbf{X})^{-1}\]</span></p></li>
<li><p>This matrix looks like this:</p></li>
</ul>
\begin{center}
\begin{tabular}{c|ccccc}
           &amp;    $\widehat{\beta}_0$ &amp;    $\widehat{\beta}_1$ &amp;    $\widehat{\beta}_2$ &amp;  $\cdots$   &amp;    $\widehat{\beta}_K$ \\
\hline
   $\widehat{\beta}_0$ &amp; $\text{var}[\widehat{\beta}_0]$ &amp; $\text{cov}[\widehat{\beta}_0,\widehat{\beta}_1]$ &amp;
   $\text{cov}[\widehat{\beta}_0,\widehat{\beta}_2]$ &amp; $\cdots$ &amp;  $\text{cov}[\widehat{\beta}_0,\widehat{\beta}_K]$   \\

   $\widehat{\beta}_1$ &amp; $\text{cov}[\widehat{\beta}_0,\widehat{\beta}_1]$ &amp; $\text{var}[\widehat{\beta}_1]$ &amp;
    $\text{cov}[\widehat{\beta}_1,\widehat{\beta}_2]$ &amp;  $\cdots$    &amp;  $\text{cov}[\widehat{\beta}_1,\widehat{\beta}_K]$ \\

   $\widehat{\beta}_2$ &amp; $\text{cov}[\widehat{\beta}_0,\widehat{\beta}_2]$ &amp; $\text{cov}[\widehat{\beta}_1,\widehat{\beta}_2]$ &amp; $\text{var}[\widehat{\beta}_2]$ &amp;   $\cdots$  &amp;   $\text{cov}[\widehat{\beta}_2,\widehat{\beta}_K]$  \\
    $\vdots$    &amp;  $\vdots$  &amp; $\vdots$    &amp;  $\vdots$    &amp;    $\ddots$        &amp;    $\vdots$        \\
   $\widehat{\beta}_K$ &amp; $\text{cov}[\widehat{\beta}_0,\widehat{\beta}_K]$ &amp; $\text{cov}[\widehat{\beta}_K,\widehat{\beta}_1]$ &amp; $\text{cov}[\widehat{\beta}_K,\widehat{\beta}_2]$ &amp;     $\cdots$       &amp; $\text{var}[\widehat{\beta}_K]$ \\
\end{tabular}
\end{center}
</div>
<div id="inference-in-the-general-setting" class="section level2">
<h2><span class="header-section-number">5.16</span> Inference in the general setting</h2>
<ul>
<li><p>Under assumption 1-5 in large samples: <span class="math display">\[\frac{\widehat{\beta}_k - \beta_k}{\widehat{SE}[\widehat{\beta}_k]} \sim N(0,1)\]</span></p></li>
<li><p>In small samples, under assumptions 1-6,</p></li>
</ul>
<p><span class="math display">\[\frac{\widehat{\beta}_k - \beta_k}{\widehat{SE}[\widehat{\beta}_k]} \sim t_{n - (K+1)}\]</span></p>
<ul>
<li><p>Thus, under the null of <span class="math inline">\(H_0: \beta_k = 0\)</span>, we know that <span class="math display">\[\frac{\widehat{\beta}_k}{\widehat{SE}[\widehat{\beta}_k]} \sim t_{n - (K+1)}\]</span></p></li>
<li><p>Here, the estimated SEs come from: <span class="math display">\[\begin{aligned}
\widehat{\text{var}}[\widehat{\boldsymbol{\beta}}] &amp;= \widehat{\sigma}^2_u(\mathbf{X}&#39;\mathbf{X})^{-1} \\
 \widehat{\sigma}^2_u &amp;= \frac{\widehat{\mathbf{u}}&#39;\widehat{\mathbf{u}}}{n-(k+1)}
\end{aligned}\]</span></p></li>
</ul>
<p>We can access this estimated covariance matrix in R:</p>
<p>Note that the diagonal are the variances. So the square root of the diagonal is are the standard errors:</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="multiple-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="matrix-algebra-review.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/UW-POLS503/pols503-notes/edit/master/03-multiple-regression.Rmd",
"text": "Edit"
},
"download": ["pols503-notes.pdf", "pols503-notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>


</body>

</html>
