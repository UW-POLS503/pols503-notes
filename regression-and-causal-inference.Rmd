---
title: "Regression in Causal Inference"
---

## Parameteric vs. Agnostic Views on Regression

The **parametric modeling** approach to structural or causal inference is that you need to model the data generating process (DGP) to get estimates for the parameters of interest. In the Classical Linear Regression (CLR) assumptions of linearity, iid sample, zero conditional mean error, homoskedasticity, and nromal Errors ensure that OLS is BLUE and that the SE's are correct even in small samples.

But what are the basic assumptions for making "causal" or structural inferences?
OLS is treated as a model for the conditional distribution of $Y$ given $X$,
$$
Y_i | X_i \sim N(X_i' \beta, \sigma^2) .
$$
And effectively, a coefficient of some variable of interest, e.g. $\beta_k$, is "correct" if the entire model is "correct".
Even if we slightly weaken this, the coefficient of interest is good enough, if the entire model of the data-generating process is good enough, where good enough satisfies some tests of the residuals, functional form, etc.
This is difficult to achieve and/or imposes some strong distributional assumptions.

The **agnostic regression** approach uses OLS without these distributional assumptions.
It focuses on OLS as a model (or approximation) of the conditional expectation function (CEF).
$$
\mu(x) = \E(Y_i | X_i = x) = \int_y y \cdot p(Y_i = y | X_i = x) d\,y
$$
Both the CEF *and* the linear regression function are defined in the population,
$$
\beta = \argmin_{b} \E\left\[ (Y_i - X_i' b)^2 \right]
$$
with the solution,
$$
\beta = \E(X_i X_i)^{-1} \E(X_i Y_i) .
$$
The *population linear regression function* is an approximation of the *population CEF*,
and the *sample linear regression function* is an approximation of the *population linear regression function*.

So what are our justifications for using OLS or linear regression, more generally,

1. If the CEF is linear, then the population linear regression function is the CEF, $\E(Y_i | X_i) = X_i' b$, where $b = \beta$. The CEF is linear in two cases,
    1. $Y$ and $X$ are distributed multivariate normal
    2. The linear regression model is *saturated*.
    
2. $X_i'$ is the best linear predictor (minimum mean squared error) of $Y_i$. Because, $\beta = \argmin_b \E((Y_i - X_i' \b)^2)$
3. $X_i \beta$ is the minimum mean squared error linear approximation of the CEF. So even if the CEF if not linear, the linear regression function is the best linear approximation to it. However, if the CEF is very nonlinear, then this approximation may not be good.

What does **saturated** mean? 


In summary, we do not need to believe that the CEF is linear in order to use linear regression to estimate it.


So what do we do when estimating the CEF with linear regression? 

- Use OLS as the estimator
- Use heteroskedasticity robust standard errors

This is a different justification, though the same result, for using heteroskasticity consistent standard errors with OLS. 
There is no assumption of homoskedasticity, and heteroskedasticity will occur when,

- CEF is linear, but the $\sigma^2(X) = \Var(Y_i | X_i = x)$ is not constant in $x$.
- $\E(Y_i | X_i)$ is not linear, and the linear regression function is used to approximate it. An example is using linear regression to for the CEF of a binary dependent variable.

## Regression and Casuality

Regression can be used without any notion of causality, and thus far, it has been introduced without any formal notion on causality. 
However, without a causal interpretation what does is mean for $\hat\beta$ to be biased? 

When does the linear regression produce causal estimates? When the CEF (it approximates) is causal. 
This means identification is the most important criteria for causal inference with regression.
Under certain conditions, a regression can recover a casual parameter, although it is not always the one we are interested in.


## References

- Matt Blackwell, [Regression and Causality](http://www.mattblackwell.org/files/teaching/s07-regression-handout.pdf)
