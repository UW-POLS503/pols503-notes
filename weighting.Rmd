---
title: "Weighted Regression"
---

# Weighting in Regression

## Weighted Least Squares (WLS)

Ordinary least squares estimates coefficients by finding the coefficients that minimize the sum of squared errors,
$$
\hat{\beta}_{OLS} = \argmin_{\vec{b}} \sum_{i = 1}^N (y_i - \vec{x}\T \vec{b})^2 .
$$
In the objective function, it treats the errors of all observations equally. 
However, there may be situations where we are more concerned about minimizing some errors more than others.
For example, suppose we know that some $y_i$ have more measurement error than others, then we may care more about minimizing errors for those $y_i$ which we are more certain about.

In weighted least squares (WLS) we estimate the coefficients by finding the values that minimize the *weighted* sum of squared errors,
$$
\hat{\beta}_{WLS} = \argmin_{\vec{b}} \sum_{i = 1}^N w_i (y_i - \vec{x}\T \vec{b})^2 ,
$$
where $w_i$ are the weights for each observation.
Note that OLS is a special case of WLS where $w_i = 1$ for all the observations.

## When should you use WLS?

The previous section showed what WLS is, but when should you use weighted regression?

Suppose we have weights for observations, when should we use them?
Well, it depends on the purpose of your analysis:

1. If you are estimating population descriptive statistics, then weighting is needed to ensure that the sample is representative of the population.
2. If you are concerned with causal inference, then weighting is more nuanced. You may or may not need to weight, and it will often be unclear which is better.

There are three reasons for weighting in causal inference [@SolonHaiderWooldridge2015a]:

1. Correct standard errors for heteroskedasticity
2. Get consistent estimates by correcting for endogenous sampling
3. Identify average partial effects when there is unmodeled heterogeneity in the effects.

*Heteroskedasticity:* Estimate OLS and WLS. If the model is misspecified or there is endogenous selection, then  OLS and WLS have different probability limits. The constrast between OLS and WLS estimatates is a diagnostic for model misspecification or endogenous sampling.  Always use robust standard errors.

*Endogenous sampling:* If the sample weights vary exogenously instead of endogenously, then weighting may be harmful for precision. The OLS still specifies the conditional mean. Sampling is exogenous if the sampling probabilities are independent of the error - e.g. if they are only functions of the explanatory variables. If the probabilities are a function of the dependent variable, then they are endogenous. (1) if sampling rate is endogenous, weight by inverse selection. (2) use robust standard errors. (3) if sampling rate is exogenous, then OLS and WLS are consistent. Use OLS and WLS as test of model mispecification.

*Heterogeneous effects:* Identifying average partial effects. WLS estimates the linear regression of the population, but this is not the same as the average partial effects. But that is because OLS does not estimate the average partial effect, but weights according to the variance in X.


### Correcting for Known Heteroskedasticity

When are there cases with known heteroskedasticity?
This is probably rare, but it arises in a few circumstances:

1. The outcome variable consists of measurements with a given measurement error. For example, the $y$ come from instruments or are estimated themselves.
2. The error of output depends on input variables in known ways. For example, the sampling error of polls.

Suppose that the heteroskedasticity is known up to a multiplicative constant,
$$
\Var(\varepsilon_i | \mat{X}) = a_i \sigma^2 ,
$$
where $a_i = a_i \vec{x}_i'$ is a positive and known function of $\vec{x}_i$.

Then in weighted least squares multiply $y_i$ by $1 / \sqrt{a_i}$,
$$
\begin{aligned}[t]
y_i &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_K x_K + \varepsilon_i \\
y_i / \sqrt{a_i} &= \beta_0 / \sqrt{a_i} + \beta_1 x_1 / \sqrt{a_1} + \beta_2 x_2 / \sqrt{a_2} + \dots + \beta_K x_K / \sqrt{a_K} + \varepsilon_i^*
\end{aligned}
$$
where $\varepsilon_i^* \sim N(0, \sigma^2)$.
This rescales errors to $\varepsilon_i / \sqrt{a_i}$ which keeps $\E(\varepsilon_i) = 0$, but makes the variance constant,
$$
\Var\left( \frac{1}{\sqrt{a_i}} \varepsilon_i | \mat{X} \right) = \frac{1}{a_i} \Var(\varepsilon_i | \mat{X}) = \frac{1}{a_i} a_i \sigma^2 = \sigma^2
$$
If $a_i$ is known, then the model is homoskedastic and the estimator is BLUE.

Define the weighting matrix,
$$
\mat{W} =
\begin{bmatrix}
1 / \sqrt{a_1} & 0 & \cdots & 0 \\
0 & 1 / \sqrt{a_2} & \cdots & 0 \\
\vdots & \vdots & \ddots & 0 \\
0 & 0 & \cdots & 1 / \sqrt{a_N}
\end{bmatrix} .
$$
Then run the regression,
$$
\begin{aligned}[t]
\mat{W} y &= \mat{W} \mat{X} \vec{\beta} + \mat{W} \vec\varepsilon \\
\vec{y}^* &= \mat{X}^* \vec{\beta} + \vec{\varepsilon}^* .
\end{aligned}
$$
Run the regression of $\vec{y}^*$ on $\mat{X}^*$, and the Gauss-Markov assumptions are satisfied.
Then using the usual OLS formula,
$$
\hat{\vec\beta}_{WLS} = ((\mat{X}^*)' \mat{X}^*) (\mat{X}^*)' \vec{y}^* = (\mat{X}' \mat{W}' \mat{W} \mat{X})^{-1} \mat{X}' \mat{W}' \mat{W} \vec{y} .
$$

**In R** Use `lm()` with the `weights` argument.


## References

- WLS derivation @Fox2016a 304--306

Textbook discussions: @AngristPischke2009 91--94, @AngristPischke2014 202--203, 

@SolonHaiderWooldridge2015a is a good (and recent) overview with practical advice of when to weight and when not-to weight linear regressions.

@Gelman2007a, in the context of post-stratification, proposes controlling for variables related to selection into the sample instead of using survey weights; also see the responses [@BellCohen2007a; @BreidtOpsomer2007a; @Little2007a; @Pfeffermann2007a], and rejoinder [@Gelman2007b] and [blog post](http://andrewgelman.com/2015/07/14/survey-weighting-and-regression-modeling/).
Gelman's approach is similar to that earlier suggested by @WinshipRadbill1994a.

See also @Deaton1997a, @DumouchelDuncan1983a, and @WissokerYYYYa.

For survey weighting, see the R package [survey](https://cran.r-project.org/web/packages/survey/survey.pdf) and its 
