# Causal Questions

## Forward and Reverse Causal Questions

- Forward Causal Questions: Does $X$ cause $Y$? What is the effect of $X$ on $Y$?
- Reverse Causal Questions: Which $X$ causes $Y$? Which $X$ has the largest effect on $Y$? 

Statistical causal inference focuses on the first question, does $X$ cause $Y$?
Only the forward causal questions are well defined. 
Much empirical research is structured as reverse causal questions with "competing theories", asking what is "the" or "the most important" cause  $H1$, $H2$, or $H3$. 
And philosophically, this is the "infinite regress of causation".


Generally, regression cannot be simulataneously used to answer how multiple causes are related to $Y$; the "all cause model".
The reason is that it is hard to specify a model that does not rely on parametric assumptions for identification, and also can account for the causal ordering of treatment effects (see "bad controls" or "post-treatment bias").
See @KeeleStevenson2014a for a discussion of this.

### Causal Questions v. Prediction Questions

Consider this model:
$$
Y = \alpha + \beta X + \epsilon
$$
The difference between a prediction problem and a causal inference problem is our estimand of interest.

- prediction: $\E(Y | X) = \hat{y}$
- casual inference: $\E(\Delta Y / \Delta X) = \hat{\beta}$

See @KleinbergLudwigMullainathanEtAl2015b for discussion of how prediction problems may still be useful for policy.

Different models and estimators may be appropriate for each task.
@Shmueli2010b discusses the difference between "explanation" (causal inference) and "prediction".

At least in the "potential outcomes" framework, there is an idea that there is "no causation without manipulation" @Holland1986a.  
This may be more a statement about the specific definition and estimators used, but it nevertheless an important distinction.
Causal inference are methods to answer what would happen to $Y$ if $X$ were *exogenously* altered---e.g. a policy intervention.
For some questions that makes sense. 
However, if $X$ is not being exogenously altered by an intervention, it is worth asking whether the question is really causal inference or prediction.



# Potential Outcomes

- Sample $i \in 1, \dots, n$
- Observed values: 

    - binary treatment variable: $D_i = \{0, 1}$
    - response variable: $Y_i$
  
- Unobserved values:

    Potential outcomes: $Y_{0i}$, $Y_{1i}$. These are the outcomes that would be observed if $i$ does not receive, or does receive the treatment.
    
    $$
    \text{Potential outcome} = 
    \begin{cases}
    y_{1i} & \text{if $D_i = 1$} \\
    y_{0i} & \text{if $D_i = 0$}    
    \end{cases}
    $$

T 
Y_i = Y_{1i} & \text{if $D_i = 1$} 
    
The observed outcome $Y_i$ can be written in terms of the potential outcome.
$$
Y_i = Y_{0i} + \underbrace{(Y_{1i} - Y_{0i})}_{\text{treatment (causal) effect}} \times D_i
$$

*SUTVA* Assumption that allows us to assume that the observed value of $Y_i$ is equal to the potential outcome,
$$
Y_i =
\begin{cases}
Y_{1i} &= \text{if $D_i = 1$} \\
Y_{0i} &= \text{if $D_i = 1$} 
\end{cases}
$$
This is an assumption and need not be the cases.
Violations of SUTVA would include mismeasured treatments, and spillovers (treatments received by other units affects the observed $Y_i$)

*Fundamental Problem of Casual Inference* Only one of the poten

What does a difference of means with observed data give us? 

$$
\begin{aligned}[t]
\E(Y_i | D_i = 1) - \E(Y_i | D_i = 0)
&= \E(Y_{1i} | D_i = 1) - \E(Y_{0i} | D_i = 0) \\
&= \underbrace{\E(Y_{1i} | D_i = 1) - \E(Y_{0i} | D_i = 1)}_{\text{Avg. treatment effect on treated (ATT)}} \\
&\quad + \underbrace{\E(Y_{0i} | D_i = 1) - \E(Y_{0i} | D_i = 0)}_{\text{Selection bias}} \\
\end{aligned}
$$


The *average treatment effect on the treated (ATT)* is the average of the treatment affects from those that are treated,
$$
ATT = \frac{1}{n} \sum_{\{i : D_i = 1\}} (Y_{1i} - Y_{0i})
$$

How random assignment solves the selection problem?

If $D_i$ is randomly assigned, then $D_i$ is independent of $Y_{0i}$  and $Y_{1i}$.
$$
\begin{aligned}
\E(Y_i |D_i = 1) - \E(Y_i | D_i = 0) &= \E(Y_{1i} | D_i = 1) - \E(Y_{0i} | D_i = 0) \\
&= \E(Y_{1i} | D_i = 1) - \E(Y_{0i} | D_i = 1) & \text{Indep of $D_i$ and potential outcome} \\
&= \E(Y_{1i} - Y_{0i} | D_i = 1) & \text{ATT} \\
&= \E(Y_{1i} - Y_{0i}) & \text{ATE}
\end{aligned}
$$

Also, with random assignment, the ATE = ATT
$$
\begin{aligned}
\E(Y_{1i} |D_i = 1) - \E(Y_{0i} | D_i = 1) &= 
&= \E(Y_{1i} | D_i = 1) - \E(Y_{0i} | D_i = 1) & \text{Indep of $D_i$ and potential outcome}
\end{aligned}
$$


The key results from random assignment of $D_i$ (as in an experiment):

- no selection bias
- difference in means = ATT = ATE

### Regression Analysis of Experiments

How does the regression formula relate to potential outcomes? 

$$
Y_i = \underbrace{\alpha}_{E(Y_{0i})} + \underbrace{\rho}_{Y_{1i} - Y_{0i}} D_i + \underbrace{\eta_i }_{Y_{0i} - \E(Y_{0i})}

$$

Take the expected values when $D_i = 1$ and $D_i = 0$,
$$
\begin{aligned}[t]
\E(Y_i | D_i = 1) &= \alpha + \rho + \E(\eta_i | D_i = 1) \\
\E(Y_i | D_i = 0) &= \alpha + \E(\eta_i | D_i = 0)
\end{aligned}
$$
But then the difference,
$$
\E(Y_i | D_i = 1) - \E(Y_i | D_i = 0) = \underbrace{\rho}_{\text{Treatment effect}} + \underbrace{\E(\eta_i | D_i = 1) - \E(\eta_i | D_i = 0)}_{\text{Selection bias}}
$$

The selection bias is the correlation between the error $\eta_i$ and the regressor $D_i$,
$$
\E(\eta_i | D_i = 1) - \E(\eta_i | D_i = 0) = \E(Y_{0i} | D_i = 1) - \E(Y_{0i} | D_i = 0) .
$$
This is the same requirement for unbiased regression coefficients that the error and covariates are uncorrelated.



## DAGs and Casual Graphs

**TODO** 

See @MorganWinship2015a

