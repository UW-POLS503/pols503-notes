# Prediction

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE}
library("tidyverse")
library("broom")
library("modelr")
library("stringr")
filter <- dplyr::filter
```

## Example: Predicting the Price of Wine

The `bordeaux` dataset contains the prices of vintages of Bordeaux wines sold at auction in New York, as well as the the weather and rainfall for the years of the wine.
This data was used by economist, Orley Ashenfelter, to show that the quality of a wine vintage can be
as measured by its price, can largely be predicted by its age, and the weather (temperature and rainfall) of its vintage year.
At the time, these prediction were not taken kindly by the wine conoisseurs.[^wine]

[wine]: Peter Passell. ``[Wine Equation Puts Some Noses Out of Joint](http://www.nytimes.com/1990/03/04/us/wine-equation-puts-some-noses-out-of-joint.html)'', *New York Times*, March 4, 1990.

```{r}
# devtools::install_github("jrnold/datums")
bordeaux <- datums::bordeaux %>%
  mutate(lprice = log(price / 100)) %>%
  dplyr::filter(!is.na(price))
```

The data contains `r nrow(bordeaux)` prices of Bordeaux wines sold at auction in New York in 1993 for vintages from 1952 to 1980; the years 1952 and 1954 were excluded because they were rarely sold.
Prices are measured as an index where 100 is the price of the 1961.
Since prices are 7

The dataset also includes three predictors of the price of each vintage:

- `time_sv`: Time since the vintage, where 0 is 1983.
- `wrain`: Winter (October--March) rain
- `hrain`: Harvest (August--September) rain
- `degrees`: Average temperature in degrees centigrade from April to September in the vintage year.

The first variable to consider is the age of the vintage and the price: 
```{r}
ggplot(filter(bordeaux, !is.na(price), !is.na(vint)),
       aes(y = log(price), x = vint)) +
  geom_point() +
  geom_rug() +
  geom_smooth(method = "lm")
```

Ashenfleter, Ashmore, and Lalonde (1995) run two models.
All models were estimated using OLS with log-price as the outcome variable. The predictors in the models were:

1. vintage age
2. vintage age, winter rain, harvest rain

We'll start by considering these models.
Since we are running several models, we'll define the model formulae
in a list
```{r}
mods_f <- list(lprice ~ time_sv, 
               lprice ~ time_sv + wrain + hrain + degrees)
```
and, run each model and store the results in a data frame as list column of `lm` objects:
```{r}
mods_res <- tibble(
  model = seq_along(mods_f),
  formula = map_chr(mods_f, deparse),
  mod = map(mods_f, ~ lm(.x, data = bordeaux))
)
mods_res
```

Now that we have these models, extract the coefficients into a data frame with the **broom** function `tidy`:
```{r}
mods_coefs <- mods_res %>%
  # Add column with the results of tidy for each model  
  # conf.int = TRUE adds confidence intervals to the data
  mutate(tidy = map(mod, tidy, conf.int = TRUE)) %>%
  # use unnest() to expand the data frame to one row for each row in the tidy
  # elements
  unnest(tidy, .drop = TRUE)
glimpse(mods_coefs)
```

```{r}
walk(mods_res$mod, ~ print(summary(.x)))
```

Likewise, extract model statistics such as, $R^2$, adjusted $R^2$, and $\hat{\sigma}$:
```{r}
mods_glance <-
  mutate(mods_res, .x = map(mod, glance)) %>%
  unnest(.x, .drop = TRUE)
mods_glance %>% 
  select(formula, r.squared, adj.r.squared, sigma)
```

## Out of Sample Error

To compare predictive models, we want to compare how well it predicts (duh), which means estimating how well it will work on new data.
The problem with this is that new data is just that, ..., new.

The trick is to resuse the sample data to get an estimate of how well the model will work on new data.
This is done by fitting the model on a subset of the data, and predicting another subset of the data which was not used to fit the model; often this is done repeatedly.
There are a variety of ways to do this, depending on the nature of the data and the predictive task.
However, they all implictly assume that the sample of data that was used to fit the model is representative of future data.

```{r}
k <- 20
f <- map(seq_len(k),
         ~ as.formula(str_c("lprice ~ poly(time_sv, ", .x, ")")))
names(f) <- seq_len(k)
```

```{r}
mods_overfit <- map(f, ~ lm(.x, data = bordeaux))
fits <- map_df(mods_overfit, glance, .id = ".id")
```

```{r}
fits %>% 
  select(.id, r.squared, adj.r.squared, df.residual) %>%
  gather(stat, value, -.id) %>%
  mutate(.id = as.integer(.id)) %>%
  ggplot(aes(x = .id, y = value)) + 
  geom_point() + 
  geom_line() +
  facet_wrap(~ stat, ncol = 2, scales = "free")
```

The larger the polynomial, the more wiggly the line. 
```{r}
library("modelr")
invoke(gather_predictions, .x = c(list(data = bordeaux), mods_overfit)) %>%
  ggplot(aes(x = vint)) +
  geom_point(aes(y = lprice)) +
  geom_line(aes(y = pred, group = model, colour = as.numeric(model)))
```
Intuitively it seems that as we increase the flexibility of the model by increasing the number of variables the model is overfitting the data, but what does it actually mean to overfit?
If we use $R^2$ as the "measure of fit", more variables always leads to better fit.
Adjusted $R^2$ does not increase, because the decrease in errors is offset by the increase in the degrees of freedom. 
However, there is little justification for the specific formula of $R^2$.

The problem with over-fitting is that the model starts to fit pecularities of the sample (errors) rather than the underlying model. We'll never know the underlying model, but what we can see is if the model predicts new data.




```{r}
wine_mods_f <- list(
  lprice ~ time_sv,
  lprice ~ poly(time_sv, 2),
  lprice ~ wrain,
  lprice ~ hrain,
  lprice ~ degrees,
  lprice ~ wrain + hrain + degrees,  
  lprice ~ time_sv + wrain + hrain + degrees,
  lprice ~ time_sv + wrain * hrain * degrees,
  lprice ~ time_sv * (wrain + hrain + degrees),
  lprice ~ time_sv * wrain * hrain * degrees,
  lprice ~ time_sv * wrain * hrain * degrees + I(time_sv ^ 2)
)
```


### Held-out data

A common rule of thumb is to use 70% of the data for training, 
and 30% of the data for testing.

In this case, let's partition the data to use the first 70% of the observations as training data, and the remaining 30% of the data as testing.
```{r}
n_test <- round(0.3 * nrow(bordeaux))
n_train <- nrow(bordeaux) - n_test

mod_train <- lm(lprice ~ time_sv + wrain + hrain + degrees, 
                data = head(bordeaux, n_train))
mod_train
# in-sample RMSE
sqrt(mean(mod_train$residuals ^ 2))
```

The out-of-sample RMSE is higher than the in-sample RMSE.
```{r}
outsample <- augment(mod_train, newdata = tail(bordeaux, n_test))
sqrt(mean((outsample$lprice - outsample$.fitted) ^ 2))
```
This is common, but not necessarily the case. 
But note that this value is highly dependent on the subset of data used for testing.
In some sense, we may choose as model that "overfits" the testing data.


### k-fold Cross-validation

A more repeat this training / testing split multiple times. 

The most common approach is to to partition the data into k-folds,
and use each fold once as the testing subset, where the model is fit on the other $k - 1$ folds.

A common rule of thumb is to use 5 to 10 folds.
We will use the `crossv_kfold` function in the **modelr** package to generate a set of cross-validations
```{r}
cv <- modelr::crossv_kfold(bordeaux, k = 10)
glimpse(cv)
```

```{r}
cv$train[[1]]
cv$test[[1]]
```

```{r}
as.data.frame(cv$train[[1]])
as.data.frame(cv$test[[1]])
```

In k-fold cross-validation, each observation appears in the test-set in one fold, and is in the the training set in the remaining $k - 1$ folds. 
The following plot shows this,

```{r}
cv_folds <-
  cv %>%
  rowwise() %>%
  do(tibble(vint = c(as.data.frame(.$train)$vint,
                     as.data.frame(.$test)$vint),
            fold = .$.id,
            set  = c(rep("train", dim(.$train)[1]),
                     rep("test", dim(.$test)[1]))))

ggplot(cv_folds, aes(y = factor(vint), x = fold, fill = set)) +
  geom_raster() +
  scale_fill_manual(values = c("train" = "black", "test" = "gray")) +
  theme_minimal() +
  theme(axis.ticks = element_blank(), legend.position = "bottom") +
  labs(x = "vint", y = "CV fold", fill = "")
            
```

Example with one fold
```{r}
fit_train <- lm(f[[1]], data = as.data.frame(cv$train[[1]]))
fit_train
fit_test <- augment(fit_train, newdata = as.data.frame(cv$test[[1]])) %>%
  select(vint, lprice, .fitted) %>%
  mutate(.resid = .fitted - lprice)
fit_test
# could also use modelr::rmse()
mod_rmse <- sqrt(mean(sum(fit_test$.resid ^ 2)))
mod_rmse
```

Let's apply that to each row using `map`.
That way we keep the results together in the same data frame.
```{r}
fit_train <- lm(f[[1]], data = as.data.frame(cv$train[[1]]))
fit_train
fit_test <- augment(fit_train, newdata = as.data.frame(cv$test[[1]])) %>%
  select(vint, lprice, .fitted) %>%
  mutate(.resid = .fitted - lprice)
fit_test
# could also use modelr::rmse()
mod_rmse <- sqrt(mean(sum(fit_test$.resid ^ 2)))
mod_rmse
```

```{r}
cv_results <-
  cv %>% 
  mutate(
    # fit each row using train as the data
    fit_train = map(train, 
                    ~ lm(f[[1]], data = as.data.frame(.x))),
    # predicted values
    predict_test = map(train, 
                       ~ predict(fit_train, newdata = as.data.frame(.x))),
    # calculate out-of-sample RMSE
    rmse = map2_dbl(train, predict_test,
                ~ sqrt(mean((as.data.frame(.x)$lprice - .y) ^ 2))))

```

Let's apply this to all the models.
In the previous steps we kept a lot of extra information in order to understand how cross-validation worked.
But really, all we care about is the average RMSE.

```{r}
mod_rmse_fold <- function(f, train, test) {
  fit <- lm(f, data = as.data.frame(train))
  # sqrt(mean(residuals(fit, newdata = as.data.frame(test)) ^ 2))
  test_data <- as.data.frame(test)
  err <- test_data$lprice - predict(fit, newdata = test_data)
  sqrt(mean(err ^ 2))
}
mod_rmse_fold(f[[1]], cv$train[[1]], cv$test[[1]])
```

Now calculate this for a single model formula, averaging over all folds:
```{r}
mod_rmse <- function(f, data) {
  map2_dbl(data$train, data$test, 
           function(train, test) {
             mod_rmse_fold(f, train, test)
           }) %>%
    mean()
}
mod_rmse(f[[1]], cv)
```

Now we can apply this to all the models:
```{r}
mod_comparison <- 
  tibble(formula = wine_mods_f,
         .name = map_chr(formula, deparse),
         .id = seq_along(wine_mods_f),
         rmse = map_dbl(wine_mods_f, mod_rmse, data = cv))
mod_comparison %>% select(.name, rmse)  
```



### Leave-one-Out Cross-Validation

```{r}
cv_5fold <- modelr::crossv_kfold(bordeaux, k = 5)
cv_loo <- modelr::crossv_kfold(bordeaux, k = nrow(bordeaux))

mod_comparison <- 
  tibble(formula = wine_mods_f,
         .name = map_chr(formula, deparse),
         .id = seq_along(wine_mods_f),
         rmse_5fold = map_dbl(wine_mods_f, mod_rmse, data = cv_5fold),
         rmse_loo = map_dbl(wine_mods_f, mod_rmse, data = cv_loo))
mod_comparison %>% select(.name, rmse_loo, rmse_5fold)
```


## Approximations

For some models, notably linear regression, analytical approximations to the expected out of sample error can be made.
Each of these approximations will make some slightly different assumptions to plug in some unknown values.

In linear regression, the LOO-CV MSE can be calculated analytically, and without simulation.  It is (ISLR, p. 180):
$$
\text{LOO-CV} = \frac{1}{n} \sum_{i = 1}^n {\left(\frac{y_i - \hat{y}_i}{1 - h_i} \right)}^2 = \frac{1}{n} \sum_{i = 1}^n {\left(\frac{\hat{\epsilon}_i}{1 - h_i} \right)}^2 = \frac{1}{n} \times \text{PRESS}
$$
where PRESS is the predictive residual sum of squares, and $h_i$ is the *hat-value* of observation $i$ (Fox, p. 270, 289)
$$
h_i = \mat{X}(\mat{X}' \mat{X})^{-1} \mat{X}'
$$

```{r}
loocv <- function(x) {
  mean((residuals(x) / (1 - hatvalues(x))) ^ 2)
}
```

An alternative approximation of the expected out-of-sample error is the generalized cross-validation criterion (GCV) is (Fox, 673)
$$
GCV = \frac{n \times RSS}{df_{res}^2} = \frac{n \sum \hat{\epsilon}^2}{(n - k - 1)^2}
$$
```{r}
gcv <- function(x) {
  rss <- sum(residuals(x) ^ 2)
  n * rss / (x[["df.residual"]] ^ 2)
}
```

Other measures that are also equivalent to some form of an estimate of the out-of-sample error are the AIC and BIC.


## References


