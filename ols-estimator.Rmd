---
title: "Ordinary Least Squares Estimator"
---

# Linear Regression 

## Matrix Representation

The linear regression function can be written as a scalar function for each observation, $i = 1, \dots, N$,
$$
\begin{aligned}[t]
y_i &= \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \cdots + \beta_{K,i} + \varepsilon_i \\
 &= \beta_0 + \sum_{k = 1}^{K} \beta_k x_{k,i} + \varepsilon_i \\
&= \sum_{k = 0}^{K} \beta_k x_{k,i} + \varepsilon_i
\end{aligned}
$$
where $x_{0,i} = 1$ for all $i \in 1:N$.

The linear regression can be more compactly written in matrix form,
$$
\begin{aligned}[t]
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_N
\end{bmatrix} &=
\begin{bmatrix} 
1 & x_{1,1} & x_{2,1} & \cdots & x_{K,1} \\
1 & x_{1,2} & x_{2,2} & \cdots & x_{K,2} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{1,N}& x_{2,n} & \cdots & x_{K,N}
\end{bmatrix}
\begin{bmatrix} 
\beta_0 \\
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_K
\end{bmatrix}
+ 
\begin{bmatrix}
\varepsilon_1 \\
\varepsilon_2 \\
\vdots \\
\varepsilon_N
\end{bmatrix}
\\
\underbrace{\vec{y}}_{N \times 1} &= \underbrace{\mat{X}}_{N \times K} \,\, \underbrace{\vec{\beta}}_{K \times 1} + \underbrace{\vec{\varepsilon}}_{N \times 1}
\end{aligned}
$$
The matrix $\mat{X}$ is called the *design* matrix.
Its rows are each observation in the data.
Its columns are the intercept, a column vector of 1's, and the values of each predictor.

The mean of the disturbance vector is 0,
$$
\E(\epsilon) = 0
$$

## Estimating OLS

The OLS estimator finds estimates of a paramters


## Assumptions 

With these assumptions

- When is OLS unbiased?
- When is OLS efficient?
- When are the OLS standard errors correct?

| Assumption | Formal statement | Consequence of violation |
|:-------------------|:-------------------|:-----------------------|
| No (perfect) collinearity | $\rank(\mat{X}) = K, K < N$ | Coefficients unidentified |
| $\mat{X}$ is exogenous | $\E(\mat{X} \vec{\varepsilon}) = 0$ | Biased, even as $N \to \infty$ |
| Disturbances have mean 0 | $\E(\varepsilon) = 0$ | Biased, even as $N \to \infty$ |
| No serial correlation | $\E(\varepsilon_i \varepsilon_j) = 0$, $i \neq j$ | Unbiased, wrong se |
| Homoskedastic errors | $\E(\vec{\varepsilon}\T \vec{\varepsilon})$ | Unbiased, wrong se |
| Gaussian errors | $\varepsilon \sim \dnorm(0, \sigma^2)$ | Unbiased, se wrong unless $N \to \infty$ |

If assumptions 1--5, then OLS is the best linear unbiased estimator (BLUE) estimator.


![OLS is BLUE](tobias-funke-blue.jpeg)


However, 1--5 does not impy that OLS has the lowest MSE.

But if assumptions 1--6 hold, then OLS is the the minimum-variance unbiased (MVU) estimator.
This means that for all estimators that are unbiased, OLS has the least variance in its sampling distribution.

## Errors in Linear Regression

Note, that OLS assumes that the variance of the the disturbances is constant $\hat{Y} - Y = \varepsilon = \sigma^2$.
What happens if it isn't? 

$$
\mat{\Sigma} =
\begin{bmatrix}
\Var(\varepsilon_1) & \Cov(\varepsilon_1, \varepsilon_2) & \cdots & \Cov(\varepsilon_1, \varepsilon_N) \\
\Var(\varepsilon_2, \varepsilon_1) & \Var(\varepsilon_2) & \cdots & \Cov(\varepsilon_2, \varepsilon_N) \\
\vdots & \vdots & \ddots & \vdots \\
\Cov(\varepsilon_N, \varepsilon_1) & \Cov(\varepsilon_N, \varepsilon_2) & \cdots & \Cov(\varepsilon_N) \\
\end{bmatrix} \\
\Sigma =
\begin{bmatrix}
\E(\varepsilon_1^2) & \E(\varepsilon_1 \varepsilon_2) & \cdots & \E(\varepsilon_1 \varepsilon_N) \\
\E(\varepsilon_2 \varepsilon_1) & \E(\varepsilon_2^2) & \cdots & \E(\varepsilon_2 \varepsilon_N) \\
\vdots & \vdots & \ddots & \vdots \\
\E(\varepsilon_N \varepsilon_1) & \E(\varepsilon_N \varepsilon_2) & \cdots & \E(\varepsilon_N^2) \\
\end{bmatrix} \\
$$
The matrix can be written more compactly as,
$$
\mat{\Sigma} = \E(\vec{\varepsilon} \vec{\varepsilon}\T)
$$

An assumption is that errors are independent, $\E(\epsilon_i \epsilon_j)$ for all $i \neq j$.
This means that all off-diagonal elements of $\mat{\Sigma}$ are 0$.
Additionally, all $\epsilon_i$ are assumed to have the same variance, $\sigma^2$.
Thus, the variance-covariance matrix of the errors is a assumed to have a diagonal matrix with the form,
$$
\mat{\Sigma} = 
\begin{bmatrix}
\sigma^2 & 0 & \cdots & 0 \\
0 & \sigma^2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma^2
\end{bmatrix} 
= \sigma^2 \mat{I}_N
$$
If these assumptions of the errors do not hold, then $\Sigma$ does not take this form, and more complicated models than OLS need to be used to get correct standard errors.

## Non-constant variance (Heteroskedasticity)

The homoskedastic case assumes that each eror term has its own variance. 
In the heteroskedastic case, each distrurbance may have its own variance, but they are still uncorrelated ($\mat{\Sigma}$ is diagonal)
$$
\mat{\Sigma} = 
\begin{bmatrix}
\sigma_1^2 & 0 & \cdots & 0 \\
0 & \sigma_2^2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma_N^2
\end{bmatrix}
$$
The problem is that now there are $N$ variance parameters to estimate, in addition to the $K$ slope coefficients.
Now, there are more parameters than we can estimate.
With heteroskedasticity, OLS with be unbiased, but the standard errors will be incorrect.

More general case allows for heteroskedasticity, and autocorrelation ($\Cov(\varepsilon_i, \varepsilon_j) \neq 0$),
$$
\mat{\Sigma} = 
\begin{bmatrix}
\sigma_1^2 & \sigma_{1,2} & \cdots & \sigma_{1,N} \\
\sigma_{2,1} & \sigma_2^2 & \cdots & \sigma_{2,N} \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{N,1} & \sigma_{N,2} & \cdots & \sigma_N^2
\end{bmatrix} 
$$
As with heteroskedasticity, OLS with be unbiased, but the standard errors will be incorrect.

**TODO** Different views on heteroskedasticity.

## Why Control for Variables

The reason for controlling for variables depends on the purpose of regresion

Description

:   Get a sense of the relationships in the data

Prediction

:   More variables may improve prediction


Causal

:   Blocks **confounding**, which is when $X$ does not cause $Y$, but is correlated
:   with $Y$ because another variable, $Z$, causes $X$ and $Y$.


Multiple regression vs. bivariate regression

1. Slopes are predicted differences *conditional* on the other predictors.
2. Estimates are still found by minimizing the squared residuals
3. Regression with multiple covariates is equivalent to sequentially running
   multiple OLS regressions, each time removing the part explained by that predictor.
4. Adding or omitting variables in a regression can affect the bias and variance of
   the OLS estimator.


## Multicollinearity

## Omitted Variable Bias


$$
$$



