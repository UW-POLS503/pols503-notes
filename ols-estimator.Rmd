---
title: "Ordinary Least Squares Estimator"
---

# Linear Regression and the Ordinary Least Squares (OLS) Estimator

Since we will largely be concerned with using linear regression for inference, we will start by discussion the population parameter of interest (population linear regression function), then the sample statistic (sample linear regression function) and estimator (ordinary least squares).

We will then consider the properties of the OLS estimator.

## Linear Regression Function

The **population linear regression function** is
$$
r(x) = \E[Y | X = x] = \beta_0 + \sum_{k = 1}^{K} \beta_{k} x_k .
$$
The population linear regression function is defined for random variables, and will be the object to be estimated.

Names for $\vec{y}$

- dependent variable
- explained variable
- response variable
- predicted variable
- regressand
- outcome variable

Names for $\mat{X}$,

- indpendent variables
- explanatory varaibles
- treatment and control variables
- predictor variables
- covariates
- regressors

To estimate the unkonwn population linear regression, we will use the **sample linear regression function**,
$$
\hat{r}(x_i) = \hat{y}_i = \hat\beta_0 + \sum_{k = 1}^{K} \hat\beta_{k} x_k .
$$
However, we

$\hat{Y}_i$ are the fitted or predicted value
The **residuals** or **errors** are the prediction errors of the estimates
$$
\hat{\epsilon}_i = y_i - \hat{y}_i
$$


$\vec{\beta}$ are the parameters; $\beta_0$ is called the *intercept*, and
$\beta_{1}, \dots, \beta_{K}$ are called the *slope parameters*, or *coefficients*.    

We will then consider the properties of the OLS estimator.

The linear regression can be more compactly written in matrix form,
$$
\begin{aligned}[t]
  \begin{bmatrix}
    y_1 \\
    y_2 \\
    \vdots \\
    y_N
  \end{bmatrix} &=
  \begin{bmatrix}
    1 & x_{1,1} & x_{2,1} & \cdots & x_{K,1} \\
    1 & x_{1,2} & x_{2,2} & \cdots & x_{K,2} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & x_{1,N}& x_{2,n} & \cdots & x_{K,N}
  \end{bmatrix}
  \begin{bmatrix}
    \beta_0 \\
    \beta_1 \\
    \beta_2 \\
    \vdots \\
    \beta_K
    \end{bmatrix}
  +
  \begin{bmatrix}
    \varepsilon_1 \\
    \varepsilon_2 \\
    \vdots \\
    \varepsilon_N
  \end{bmatrix}
\end{aligned} .
$$
More compactly, the linear regression model can be written as,
$$
\begin{aligned}[t]
  \underbrace{\vec{y}}_{N \times 1} &=
  \underbrace{\mat{X}}_{N \times K} \,\,
  \underbrace{\vec{\beta}}_{K \times 1} +
  \underbrace{\vec{\varepsilon}}_{N \times 1} .
\end{aligned}
$$
The matrix $\mat{X}$ is called the *design* matrix.
Its rows are each observation in the data.
Its columns are the intercept, a column vector of 1's, and the values of each predictor.

## Ordinary Least Squares

Ordinary least squares (OLS) is an estimator of the slope and statistic of the regression line[^ols-gls].
OLS finds values of the intercept and slope coefficients by minimizing the squared errors,
$$
\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_K
=
\argmin_{b_0, b_1, \dots, b_k} \sum_{i = 1}^{N}  \underbrace{{\left(y_i - b_0 - \sum_{k = 1}^{K} b_k x_{i,k} \right)}^2}_{\text{squared error}},
$$
or, in matrix notation,
$$
\begin{aligned}[t]
\hat{\vec{\beta}} &= \argmin_{\vec{b}} \sum_{i = 1}^N (y_i - \vec{b}\T \vec{x}_i)^2 \\
&= \argmin_{\vec{b}} \sum_{i = 1}^N u_i^2 \\
&= \argmin_{\vec{b}} \vec{u}' \vec{u}
\end{aligned}
$$
where $\vec{u} = \vec{y} - \mat{X} \vec{\beta}$.

In most statistical models, including even generalized linear models such as logit, the solution to this minimization problem would be solved with optimization methods that require iteration.
One nice feature of OLS is that there is a closed form solution for $\hat{\beta}$ even in the multiple regression case, so no iterative optimization methods need to be used.

In the bivariate regression case, the OLS estimators for $\beta_0$ and $\beta_1$ are
$$
\begin{aligned}[t]
\hat{\beta}_0 &= \bar{y} - \hat\beta_1 \bar{x} \\
\hat{\beta}_1 7= \frac{\sum_{i = 1}^N (x_i - \bar{x}) (y_i - \bar{y})}{\sum_{i = 1}^N (x_i - \bar{x})^2} \\
&= \frac{\Cov(\vec{x} \vec{y})}{\Var{\vec{x}}}
&= \frac{\text{Sample covariance betweeen $\vec{x}$ and $\vec{y}$}}{\text{Sample variance of $\vec{x}$}} .
\end{aligned}
$$
In the multiple regression case, the OLS estimator for $\hat{\vec{\beta}}$ is
$$
\hat{\vec{\beta}} = \left( \mat{X}' \mat{X} \right)^{-1} \mat{X}' \vec{y} .
$$
The term $\mat{X}' \mat{X}$ is similar to the variance of $\vec{x}$ in the bivariate case.
The term $\mat{X}' \vec{y}$ is similar to the covariance between $\mat{X}$ and $\vec{y}$ in the bivariate case.

The sample linear regression function estimated by OLS has the following properties:

1. Residuals sum to zero,
    $$
    \sum_{i = 1}^N \hat{\epsilon}_i = 0 .
    $$
    This implies that the mean of residuals is also 0.
2. The regression function passes through the point $(\bar{\vec{y}}, \bar{\vec{x}}_1, \dots, \bar{\vec{x}_K})$.
    In other words, the following is always true,
    $$
    \bar{\vec{y}} = \hat\beta_0 + \sum_{k = 1}^K \hat\beta_k \bar{\vec{x}}_k .
    $$
3. The residuals are uncorrelated with the predictor
    $$
    \sum_{i = 1}^N x_i \hat{\epsilon}_i = 0
    $$
4. The residuals are uncorrelated with the fitted values
    $$
    \sum_{i = 1}^N \hat{y}_i \hat{\varepsilon}_i = 0
    $$


## Properties of the OLS Estimator

### What makes an estimator good?

Estimators are evaluated not on how close an estimate in a given sample is to the population, but how their sampling distributions compare to the population.
In other words, judge the *methodology* (estimator), not the *result* (estimate).[^ols-properties-references]

Let $\theta$ be the population parameter, and $\hat\theta$ be an estimator of that population parameter.

Bias

:   The bias of an estimator is the difference between the mean of its sampling distribution and the population parameter, $$\Bias(\hat\theta) = \E(\hat\theta) - \theta .$$

Variance

:   The variance of the estimator is the variance of its sampling distribution, $\Var(\theta)$.

Efficiency (Mean squared error)

:   An efficient estimator is one that minimizes a given "loss function", which is a penalty for missing the population average. The most common loss function is squared loss, which gives the *Mean Squared Error (MSE)* of an estimator.
:   $$\MSE(\hat\theta) = \E\left[{(\hat\theta - \theta)}^{2}\right] =  (\E(\hat\theta) - \theta)^2 + \E(\hat\theta - \E(\hat\theta))^2 = \Bias(\hat\theta)^2 + \Var(\hat\theta)$$
:   The mean squared error is a function of both the bias and variance of an estimator.
:   This means that some biased estimators can be more efficient :   than unbiased estimators if their variance offsets their bias.[^mse]

```{r results='asis',echo=FALSE}
clocks <-
  data.frame(Biased = c("Yes", "No", "Yes", "Yes", "No"),
             Variance = c("High", "High", "Low", "Low", "Low"),
             check.names = FALSE,
             row.names =
               c("Stopped clock",
               "Random clock",
               "Clock that is \"a lot \" fast",
               "Clock that is \"a little\" fast",
               "Atomic clock")
             )
```


Consistency is an asymptotic property[^asymptotic], that roughly states that an estimator converges to the truth as the number of observations grows, $\E(\hat\theta - \theta) \to 0$ as $N \to \infty$.
Roughly, this means that if you had enough (infinite) data, the estimator will give you the true value of the parameter.

### Properties of OLS

- When is OLS unbiased?
- When is OLS consistent?
- When is OLS efficient?


| Assumption | Formal statement | Consequence of violation |
|:-------------------|:-------------------|:-----------------------|
| No (perfect) collinearity | $\rank(\mat{X}) = K, K < N$ | Coefficients unidentified |
| $\mat{X}$ is exogenous | $\E(\mat{X} \vec{\varepsilon}) = 0$ | Biased, even as $N \to \infty$ |
| Disturbances have mean 0 | $\E(\varepsilon) = 0$ | Biased, even as $N \to \infty$ |
| No serial correlation | $\E(\varepsilon_i \varepsilon_j) = 0$, $i \neq j$ | Unbiased, wrong se |
| Homoskedastic errors | $\E(\vec{\varepsilon}\T \vec{\varepsilon})$ | Unbiased, wrong se |
| Gaussian errors | $\varepsilon \sim \dnorm(0, \sigma^2)$ | Unbiased, se wrong unless $N \to \infty$ |

<!--
1. Nonlinearity
    - Result: biased/inconsistent estimates
    - Diagnose: scatterplots, added variable plots, component-plus-residual plots
    - Correct: transformations, polynomials, different model
2. iid/random sample
    - Result: no bias with appropriate alternative assumptions (structured dependence)
    - Result (ii): violations imply heteroskedasticity
    - Result (iii): outliers from different distributions can cause inefficiency/bias
    - Diagnose/Correct: next week!
3. Perfect collinearity
    - Result: can't run OLS
    - Diagnose/correct: drop one collinear term
4. Zero conditional mean error
    - Result: biased/inconsistent estimates
    - Diagnose: very difficult
    - Correct: instrumental variables (Gov 2002)
5. Heteroskedasticity
    - Result: SEs are biased (usually downward)
    - Diagnose/correct: next week!
6. Non-Normality
    - Result: critical values for $t$ and $F$ tests wrong
    - Diagnose: checking the (studentized) residuals, QQ-plots, etc
    - Correct: transformations, add variables to $\X$, different model
-->   

Note that these assumptions can be sometimes be written in largely equivalent, but slightly different forms.

## Multi-Collinearity

### Perfect Collinearity

In order to estimate unique $\hat{\beta}$ OLS requires the that the columns of the design matrix $\vec{X}$ are linearly independent.


Common examples of groups of variables that are not linearly independent

- Categorical variables in which there is no excluded category.
  You can also include all categories of a categorical variable if you exclude the intercept.
  Note that although they are not (often) used in political science, there are other methods of transforming categorical variables to ensure the columns in the design matrix are independent.
- A constant variable. This can happen in practice with dichotomous variables of rare events; if you drop some observations for whatever reason, you may end up dropping all the 1's in the data. So although the variable is not constant in the population, in your sample it is constant and cannot be included in the regression.
- A variable that is a multiple of another variable. E.g. you cannot include $\log(\text{GDP in millions USD})$ and $\log({GDP in USD})$ since $\log(\text{GDP in millions USD}) = \log({GDP in USD}) / 1,000,000$. in
- A variable that is the sum of two other variables. E.g. you cannot include $\log(population)$, $\log(GDP)$, $\log(GDP per capita)$ in a regresion since $\log(GDP per capita) = \log(GDP / pop) = \log(GDP) - \log(pop)$.

#### What to do about it?

R and most statistical programs will drop variables from the regression until only linearly independent columns in $\mat{X}$ remain.
You should not rely on the softward to fix this for you; once you (or the software) notices the problem check the reasons it occured. The rewrite your regression to remove whatever was creating linearly dependent variables in $\mat{X}$.

### Less-than Perfect Collinearity

What happens if variables are not linearly dependent, but nevertheless highly correlated.
If $\Cor(\vec{x}_1, vec{x}_2) = 1$, then they are linearly dependent and the regression cannot be estimated (see above).
But if $\Cor(\vec{x}_1, vec{x}_2) = 0.99$, the OLS can estimate unique values of of $\hat\beta$. However, it everything was fine with OLS estimates until, suddenly, when there is linearly independence everything breaks. The answer is yes, and no.
As $|\Cor(\vec{x}_1, \vec{x}_2)| \to 1$ the standard errors on the coefficients of these variables increase, but OLS as an estimator works correctly; $\hat\beta$ and $\se{\hat\beta}$ are unbiased.
With multicollinearly, OLS gives you the "right" answer, but it cannot say much with certainty.


Insert plot of highly correlated variables and their coefficients.

Insert plot of uncorrelated variables and their coefficients.



## Weighted Least Squares

In weighted least squares (WLS), instead of minimizing the sum of squared errors, minimize a weighted sum of squared errors,
$$
\hat{\vec{\beta}}_{WLS} = \argmin_{\vec{b}} \sum_{i = 1}^N w_i {(y_i - \vec{\beta}' \vec{x}_i)}^2
$$
OLS is the special case of WLS in which all observations are weighted equally.

What reasons are there to use WLS?

- Heteroskedasticity: The observations have different levels of precision. This works well if the form of the heteroskedasticity is known, perhaps measurement error in a meta-analysis. Feasible GLS is when errors from OLS are used as weights in WLS: there are better ways to do this.
- Aggregation: The observations represent the sizes of different groups, or the probability of being selected into the sample. E.g. weighting countries or states by their population.

How to run WLS in R? Use `lm` with the `weights` argument.


## References

- Wooldrige, Ch 3.
- Fox, Ch 6, 9.

<!-- Footnotes -->

[^ols-properties-refences]: This section draws materials from  Chris Adolph's 503 slides [Linear Regression in Matrix Form / Properties & Assumption of Linear Regression](http://faculty.washington.edu/cadolph/503/topic4.pw.pdf).

[^clocks]: Example from [Chris Adolph](http://faculty.washington.edu/cadolph/503/topic3.pw.pdf)

[^mse]: It follows from the definition of MSE, that biased estimator, $\hat\theta_{B}$, has a lower MSE than an unbiased estimator, $\hat\theta_{U}$, if $\Bias(\theta_B)^2 < \Var(\theta_U) - \Var(\theta_B)$.

[^asymptotic]: As the number of observations goes to infinity.

[^ols-gls]: Ordinary least squares is distinguished from *generalized least squares* (GLS).
