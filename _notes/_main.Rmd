---
title: "Yule Replication"
output: html_document
---

```{r include=FALSE}
library("dplyr")
library("readr")
library("tidyr")
library("haven")
library("plm")
library("magrittr")
library("purrr")
library("ggplot2")
library("broom")
```

```{r}
ratiodiff <- function(x) {
  z <- x / lag(x)
  z[is.infinite(z)] <- NA_real_
  z
}
```

```{r}
pauperism <-
  left_join(yule, yule_plu, by = "plu")
```

Table 2: Metropolitan Group, 1871-1881
```{r results = 'asis'}
filter(yule_long, Type == "Metropolitan") %>%
  filter(year == 1881) %>%
  select(ID, Union, pauper_diff, outratio, Prop65_diff,
         Popn_diff) %>%
  arrange(ID) %>%
  select(-ID) %>%
  knitr::kable()
```

$$
\begin{aligned}[t]
\Delta\mathtt{Paup} &= \beta_0  \\
          &+ \beta_1 \Delta\mathtt{Out} \\
          &+ \beta_2 \Delta\mathtt{Old} \\
          &+ \beta_3 \Delta\mathtt{Pop} + \varepsilon
\end{aligned}
$$

# Summary Statistics

```{r}
filter(yule_long, year > 1871) %>%
  group_by(year, Type) %>%
  select(pauper_diff, outratiodiff, Prop65_diff, Popn_diff) %>%
  gather(variable, value, -Type, -year) %>%
  group_by(variable, year, Type) %>%
  summarize(mean = mean(value, na.rm = TRUE),
            sd = sd(value, na.rm = TRUE)) %>%
  knitr::kable()

```


# Regression

```{r}
lm(pauper ~ outratio, data = yule_long)
lm(pauper ~ year + Type + outratio, data = yule_long)
lm(pauper ~ year + Type + outratio + Prop65 + Popn65, data = yule_long)
lm(pauper ~ Type * (year + outratio + Prop65 + Popn65), data = yule_long)
```


```{r}
yule_diff <- yule_long %>%
  filter(year > 1871) %>%
  mutate(year = as.factor(year)) %>%
  select(ID, Union, Type, year, pauper_diff, outratiodiff, Popn_diff,
         Prop65_diff)

lm(pauper_diff ~ outratiodiff, data = yule_diff)
lm(pauper_diff ~ Type * year + outratiodiff, data = yule_diff)
lm(pauper_diff ~ Type * year + outratiodiff + Popn_diff + Prop65_diff, data = yule_diff)
lm(pauper_diff ~ (Type * year) * (outratiodiff + Prop65_diff + Popn_diff),
   data = yule_diff)



```

Individual regression for each Type and Region
```{r}
diff_mod_3 <-
  yule_long %>%
  filter(year %in% c(1881, 1891)) %>%
  group_by(year, Type) %>%
  do(tidy(lm(pauper_diff ~ outratiodiff + Popn_diff + Prop65_diff, data = .)))

diff_mod_3 %>%
  select(year, Type, term, estimate) %>%
  spread(term, estimate) %>%
  knitr::kable()
```

## Summary Statistics

### Outratio

```{r}
ggplot(select(filter(yule_long, !is.na(outratio)),
              outratio, ID, year, Type),
       aes(x = outratio, y = ..density..)) +
  geom_histogram(binwidth = 2) +
  facet_grid(year ~ Type)
```

```{r}
ggplot(select(filter(yule_long, !is.na(outratiodiff)),
              outratiodiff, ID, year, Type),
       aes(x = outratiodiff, y = ..density..)) +
  geom_histogram(binwidth = 20) +
  facet_grid(year ~ Type)
```

## Pauperism

```{r}
ggplot(select(filter(yule_long, !is.na(pauper)),
              pauper, ID, year, Type),
       aes(x = pauper, y = ..density..)) +
  geom_histogram(binwidth = .01) +
  facet_grid(year ~ Type)
```

There appear to be some big outliers in the ratio difference
in pauperism,
```{r}
ggplot(select(filter(yule_long, !is.na(pauper_diff)),
              pauper_diff, ID, year, Type),
       aes(x = pauper_diff, y = ..density..)) +
  geom_histogram(binwidth = 15) +
  facet_grid(year ~ Type)
```

<!--chapter:end:pauperism.Rmd-->

# Questions

```{r setup,include=FALSE}
library("DiagrammR")
```

# Tukey (1980)

> Tukey, John W. 1980. "We Need Both Exploratory and Confirmatory" *The American Statistician.* https://dx.doi.org/10.2307/268299

John Tukey discussed exploratory and confirmatory analysis and the need for both:

The stylized view of science is the "straight-line paradigm"
```{r}
mermaid("diagrams/science.mmd")
```

But where does the question or idea come from? Tukey notes four issues with this straight-line paradigm:

- Questions come from theory and insights derived from previous explorations of similar data
- Designs come are also driven by insights from previous studies of similar data
- Data collection is monitored by exploring the data and looking for unexpected patterns
- The analysis proceeds often by exploring the data to avoid bad or pursue good avenues of discovery?

All science has peeked at the data before answering the question. 
In fact, if science as a whole persued the straight-line paradigm only the first question ever posed could be analyzed without some corruption from knowing something about domain of study.

Instead, a more realistic formulation of the scientific process is
```{r}
mermaid("diagrams/scienc2.mmd")
```

> The formulation of the question itself involves what can in fact be asked, what designs are feasible, as well as how likely a given design is to give a useful answer.
> Both inchoate insight and extensive exploration (of past data) can---and should---play a role in this process of formulating and question.
> 
> Science ... DOES NOT BEGIN WITH A TIDY QUESTION. Nor does it end with a tidy answer.
> 
> The picture of a scientist struck---as by lightning---with a question is very far from the truth.

But if you do do confirmatory analysis:

1. randomize
2. pre-plan

After choosing a question, limit your analysis to one main question---specified by the entire design, collection, monitoring, and analysis.

# Peng and Leek

The epicycles of analysis (CH 2).
There are 5 core activities of data analysis: 

1. Stating the question
2. Exploratory data analysis
3. Model building
4. Interpreting
5. Communicating

Each of those activities consists of three epicycles:

1. setting expectations
2. collecting data, comparing data to expectations
3. if the data don't match expectations, then revise data or expectations and repeat

Types of questions. There are six types of questions (p. 18--19)
Leek and Peng. What is the question? 2015. *Science* http://science.sciencemag.org/content/347/6228/1314

1. Descriptive: Summarizes a characteristic of data.
2. Exploratory: Find patterns in data. Hypothesis generating analysis.
3. Inferential: Given a hypothesis, extrapolate from the sample to the population or different sample.
4. Predictive: Predict new data. In this you don't necessarily care about the predictors, only that the model predicts well.
5. Causal: Does X cause Y? How does changing one factor change another (on average) in the population?
6. Mechanistic: How does X cause Y?

What is a good question (p. 21)?

1. interest to the audience
2. it is not already answered
3. it stems from a plausible framework
4. it should be answerable
5. it is also useful to be specific - because that helps answerability.

# Exploratory Data Analysis

Goals of EDA (Art of Data Science, Ch 4.):

1. Find problems in the data
2. Detemine whether the question can be answered with the data at hand (proof of concept)
3. Develop a "sketch of the answer"

Their EDA checklist

1. Formulate your question
2. Read in your data
3. Check the packaging: How many observations and variables? What are the observations and variables in the data?
4. Look at the top and the bottom of your data: Look at the beginning and end of the data---is it in order, is it properly formatted, in a time series does it have the right times?
5. Check your "n"s: Always check the number of observations. This is quick way to check that there aren't mistakes in the sample, especially when merging.
6. Validate with at least one external data source: This doesn't need to be formal. But compare values of variables to other known values to ensure they are in the right ballpark. This catches unit-of-measurement issues, variables not measuring what you thought they were measuring, data entry errors.
7. Make a plot. Comparing the data to what you expect it to look like is a good way to catch both data errors and also to find new patterns.
8. Try the easy solution first. This is a proof of concept that your answer will work.
9. Follow up. Challenge the solution. Why might it be wrong.

   - do you have the right data?
   - do you need more data?
   - do you have the right question?

<!--chapter:end:questions.Rmd-->

3.1 Bivariate Regression Model

    - estimation

3.2 Random variation in Coefficient estimates

    - distribution of $\hat{\beta}$ estimates
    - $\hat{\beta}$ are normally distributed

3.3 Exogeneity and Ubiasedness

    - conditions for unbiased estimator
    - characterizing biaas

3.4 Precision of Estimate
3.5 Probability limits and consistency

    - probability limit
    - consistency

3.6 Homoskedasticity

    - heteroskedasticity
    - correlated errors - autocorrelation

<!--chapter:end:realstats.Rmd-->

# Cross-Validation

## Prerequisites {-}

```{r message=FALSE}
library("tidyverse")
library("broom")
library("modelr")
library("purrr")
library("stringr")
```

## Example: Predicting Bordeaux Wine

The `bordeaux` dataset contains the prices of vintages of Bordeaux wines sold at auction in
New York, as well as the the weather and rainfall for the years of the wine.
AshenfelterAshemoreLalonde1995a uses this data was used  to show that the quality of a wine vintage, as measured by its price, can largely be predicted by a small number of variables.
At the time, these prediction were not taken kindly by wine connoisseurs.[^wine]

```{r}
bordeaux <- datums::bordeaux %>%
  mutate(lprice = log(price / 100)) %>%
  filter(!is.na(price))
```

The data contains `r nrow(bordeaux)` prices of Bordeaux wines sold at auction in New York in 1993 for vintages from 1952 to 1980; the years 1952 and 1954 were excluded because they were rarely sold.
Prices are measured as an index where 100 is the price of the 1961.

The dataset also includes three predictors of the price of each vintage:

-   `time_sv`: Time since the vintage, where 0 is 1983.
-   `wrain`: Winter (October--March) rain
-   `hrain`: Harvest (August--September) rain
-   `degrees`: Average temperature in degrees centigrade from April to September in the vintage year.

The first variable to consider is the age of the vintage and the price:
```{r}
ggplot(filter(bordeaux, !is.na(price), !is.na(vint)),
       aes(y = log(price), x = vint)) +
  geom_point() +
  geom_rug() +
  geom_smooth(method = "lm")
```

Ashenfelter, Ashmore, and Lalonde (1995) run two models.
All models were estimated using OLS with log-price as the outcome variable. The predictors in the models were:

1.  vintage age
1.  vintage age, winter rain, harvest rain

We'll start by considering these models.
Since we are running several models, we'll define the model formulas in a list

```{r}
mods_f <- list(lprice ~ time_sv,
               lprice ~ time_sv + wrain + hrain + degrees)
```

Run each model and store the results in a data frame as list column of `lm` objects:

```{r}
mods_res <- tibble(
  model = seq_along(mods_f),
  formula = map_chr(mods_f, deparse),
  mod = map(mods_f, ~ lm(.x, data = bordeaux))
)
mods_res
```

Now that we have these models, extract the coefficients into a data frame with the **broom** function `tidy`:

```{r}
mods_coefs <- mods_res %>%
  # Add column with the results of tidy for each model
  # conf.int = TRUE adds confidence intervals to the data
  mutate(tidy = map(mod, tidy, conf.int = TRUE)) %>%
  # use unnest() to expand the data frame to one row for each row in the tidy
  # elements
  unnest(tidy, .drop = TRUE)
glimpse(mods_coefs)
```

```{r}
walk(mods_res$mod, ~ print(summary(.x)))
```

Likewise, extract model statistics such as, $R^2$, adjusted $R^2$, and $\hat{\sigma}$:
```{r}
mods_glance <-
  mutate(mods_res, .x = map(mod, glance)) %>%
  unnest(.x, .drop = TRUE)
mods_glance %>%
  select(formula, r.squared, adj.r.squared, sigma)
```

## Cross Validation

```{r}
k <- 20
f <- map(seq_len(k),
         ~ as.formula(str_c("lprice ~ poly(time_sv, ", .x, ")")))
names(f) <- seq_len(k)
```

```{r}
mods_overfit <- map(f, ~ lm(.x, data = bordeaux))
fits <- map_df(mods_overfit, glance, .id = ".id")
```

```{r}
fits %>%
  select(.id, r.squared, adj.r.squared, df.residual) %>%
  gather(stat, value, -.id) %>%
  mutate(.id = as.integer(.id)) %>%
  ggplot(aes(x = .id, y = value)) +
  geom_point() +
  geom_line() +
  facet_wrap(~ stat, ncol = 2, scales = "free")
```

The larger the polynomial, the more wiggly the line.
```{r}
library("modelr")
invoke(gather_predictions, .x = c(list(data = bordeaux), mods_overfit)) %>%
  ggplot(aes(x = vint)) +
  geom_point(aes(y = lprice)) +
  geom_line(aes(y = pred, group = model, colour = as.numeric(model)))
```
Intuitively it seems that as we increase the flexibility of the model by increasing the number of variables the model is overfitting the data, but what does it actually mean to overfit?
If we use $R^2$ as the "measure of fit", more variables always leads to better fit.
Adjusted $R^2$ does not increase, because the decrease in errors is offset by the increase in the degrees of freedom.
However, there is little justification for the specific formula of $R^2$.

The problem with over-fitting is that the model starts to fit peculiarities of the sample (errors) rather than the true model.
Since we never know the true model, all we can check is if the model predicts new data.

## Out-of-Sample Error

To compare predictive models, we want to compare how well they predicts (duh).
This means estimating how well it will work on new data.
The problem with this is that new data is just that, ..., new.

The trick is to reuse the sample data to get an estimate of how well the model will work on new data.
This is done by fitting the model on a subset of the data, and predicting another subset of the data which was not used to fit the model; often this is done repeatedly.
There are a variety of ways to do this, depending on the nature of the data and the predictive task.
However, they all implicitly assume that the sample of data that was used to fit the model is representative of future data.

```{r}
many_mods <- list(
  lprice ~ time_sv,
  lprice ~ wrain,
  lprice ~ hrain,
  lprice ~ degrees,
  lprice ~ wrain + hrain + degrees,
  lprice ~ time_sv + wrain + hrain + degrees,
  lprice ~ time_sv + wrain * hrain * degrees,
  lprice ~ time_sv * (wrain + hrain + degrees),
  lprice ~ time_sv * wrain * hrain * degrees
)
```

### Held-out data

A common rule of thumb is to use 70% of the data for training,
and 30% of the data for testing.

In this case, let's partition the data to use the first 70% of the observations as training data, and the remaining 30% of the data as testing.
```{r}
n_test <- round(0.3 * nrow(bordeaux))
n_train <- nrow(bordeaux) - n_test

mod_train <- lm(lprice ~ time_sv + wrain + hrain + degrees,
                data = head(bordeaux, n_train))
mod_train
# in-sample RMSE
sqrt(mean(mod_train$residuals ^ 2))
```

The out-of-sample RMSE is higher than the in-sample RMSE.
```{r}
outsample <- augment(mod_train, newdata = tail(bordeaux, n_test))
sqrt(mean( (outsample$lprice - outsample$.fitted) ^ 2))
```
This is common, but not necessarily the case.
But note that this value is highly dependent on the subset of data used for testing.
In some sense, we may choose as model that "overfits" the testing data.

### k-fold Cross-validation

A more robust approach is to repeat this training/testing split multiple times.

The most common approach is to to partition the data into k-folds,
and use each fold once as the testing subset, where the model is fit on the other $k - 1$ folds.

A common rule of thumb is to use 5 to 10 folds.
```{r}
cv <- modelr::crossv_kfold(bordeaux, k = 10)

cv_rmse <- function(f, cv) {
  fits <- map(cv$train, ~ lm(f, data = .x, model = FALSE))
}
```

TODO: plot of observations included in testing and training folds

TODO: plot cross validation results

TODO: Do model comparison

### Leave-one-Out Cross-Validation

Leave-one-out cross validation estimates is a $k$-fold cross-validation with folds equal to the number of observations.
The model is fit $n$ times, leaving training the model on $n - 1$ observations and predicted the remaining observation.

```{r}
cv <- modelr::crossv_kfold(bordeaux, k = nrow(bordeaux))
```

## Approximations

For some models, notably linear regression, analytical approximations to the expected out of sample error can be made.
Each of these approximations will make some slightly different assumptions to plug in some unknown values.

In linear regression, the LOO-CV MSE can be calculated analytically, and without simulation.  It is (ISLR, p. 180):
$$
\text{LOO-CV} = \frac{1}{n} \sum_{i = 1}^n {\left(\frac{y_i - \hat{y}_i}{1 - h_i} \right)}^2 = \frac{1}{n} \sum_{i = 1}^n {\left(\frac{\hat{\epsilon}_i}{1 - h_i} \right)}^2 = \frac{1}{n} \times \text{PRESS}
$$
where PRESS is the predictive residual sum of squares, and $h_i$ is the *hat-value* of observation $i$ [@Fox2016a, p. 270, 289],
$$
h_i = \Mat{X}(\Mat{X}' \Mat{X})^{-1} \Mat{X}'
$$

```{r}
loocv <- function(x) {
  mean( (residuals(x) / (1 - hatvalues(x))) ^ 2)
}
```

An alternative approximation of the expected out-of-sample error is the generalized cross-validation criterion (GCV) is [@Fox2016a],
$$
\text{GCV} = \frac{n \times RSS}{df_{res}^2} = \frac{n \sum \hat{\epsilon}^2}{(n - k - 1)^2}
$$
```{r}
gcv <- function(x) {
  err <- residuals(x)
  rss <- sum(err ^ 2)
  length(err) * rss / (x[["df.residual"]] ^ 2)
}
```

Other measures that are also equivalent to some form of an estimate of the out-of-sample error are the [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion) and [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion).

## References

-   <https://tomhopper.me/2014/05/16/can-we-do-better-than-r-squared/>
-   <http://andrewgelman.com/2007/08/29/rsquared_useful/>


[wine]: Peter Passell. ``[Wine Equation Puts Some Noses Out of Joint](http://www.nytimes.com/1990/03/04/us/wine-equation-puts-some-noses-out-of-joint.html)'', *New York Times*, March 4, 1990.

<!--chapter:end:wine.Rmd-->

